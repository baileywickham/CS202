Hehe. Looks like SBCL has the edge in numerics [on the shootout](http://shootout.alioth.debian.org/gp4/benchmark.php?test=all&amp;lang=ghc&amp;lang2=sbcl) but otherwise its a bit of a wash ('cept the memory overhead of SBCL programs...). 

Hmm, we really should submit some ByteString-based entries.For having so many people that love xkcd and having such an interesting comment thread for todays cartoon, I actually want an up arrow for reddit as a whole.&gt; There's ABSOLUTELY NO WTF whatsoever with that code.

You mean, besides really not doing anything useful?[deleted]Weird... I would have imagined they'd compile to the same thing, but they don't. const char* indeed involves an extra variable (pointer) and a relocation.

Noted.I think it is fair to say that the key problem with concurrent programming is managing state. And "prominent" libraries might not have big thread-safety issues but that's because they are debugged on the way to becoming prominent. I would not feel at all comfortable picking up a random module that someone on my team created for use in a threaded situation. I'd need to do an audit. The posters point is that the Haskell compiler can greatly simplify those audits for you.You're misreading the spec.  It's saying that the *host* portion of a URL can either be a FQDN *or* an IP address in dotted quad notation.  It is *not* saying that FQDNs and IP addresses are the same thing or that they match each other, but merely that you can use either in the *host* portion of URLs.
So, basically... `$index = $selected`

Edit: My bad. An input of 5 wouldn't produce the required value of 0.Most "PHB"s don't even know what bug-tracking software is. I've never used Fogbugz, but if it saves just a few developer hours per installation, then it pays for itself, and is worth it. If it manages to solve even one customer problem per installation, then it is worth it. "PHB"s and even some developers are aware of such things as "ROI".

"making the Right Thing happen"

I suppose the right thing involves installing bugzilla, which is a monstrous piece of garbage if there ever was one.Well Google obviously had a tough time after launching a free search engine.No. I mean this: in Python, you can do `some_var = some_obj.methodname`, and `some_var` will contain a reference to the method. In Ruby `some_var` will contain the _return value_ of the method, because referring to it by name calls it. You have to do a little bit more work to actually get the method itself.It says that network hosts have FQDN and they have IP addresses. You can use either to map to that particular host. If it meant what you say, it would read:

&gt;Host: The fully qualified domain name of a network host, or **an** IP address as a set of four decimal digit groups separated by ".".[deleted][deleted]Common sense advises against commenting this deep in this silly thread, but...

"exp pi - pi" *is* the original algorithm.  e^pi is spelled "exp pi", not "let e = exp 1 in e ** pi".&gt; It says that network hosts have FQDN and they have IP addresses.

But it doesn't say that FQDN and IP addresses are equivalent.  It is saying that you can use either in the *host* part of URLs.

&gt; You can use either to map to that particular host.

But that doesn't mean that they refer to the same resources.

If example.com and example.org both point to 10.0.0.1, then you can write these three URLs down:

* http://example.com/
* http://example.org/
* http://10.0.0.1/

All of these are different URLs.  The fact that you may be talking to the same server is irrelevant.  And if a library function says that these are equivalent URLs, then it is **broken**.

Edit:

For a more apt example, reddit.com and programming.reddit.com both resolve to 72.5.28.218.  Now try these three URLs and tell me that they are the same:

* http://reddit.com/
* http://programming.reddit.com/
* http://72.5.28.218/
&gt; it's not idempotent because it's not the same as just calling it once

Sounds exactly like what we're talking about to me.

The broken comparison function that resolves the hostname returns different results if you call it repeatedly (or at least it can do).

A proper comparison function would return the same thing each time no matter how many times you call it.  Calling it more than once doesn't have any effect beyond calling it once &amp;mdash; unless you count the wasted cycles, in which case nothing a computer does could ever be idempotent.
I compiled 4 different flavors of this function with optimizations:

The article's questionable code produces:

00401000  push        ebp  

00401001  mov         ebp,esp 

00401003  sub         esp,8 

00401006  mov         dword ptr [ebp-4],0 

0040100D  mov         eax,dword ptr [ebp+8] 

00401010  mov         dword ptr [ebp-8],eax 

00401013  cmp         dword ptr [ebp-8],3 

00401017  ja          00401045 

00401019  mov         ecx,dword ptr [ebp-8] 

0040101C  jmp         dword ptr [ecx*4+40104Eh] 

00401023  mov         dword ptr [ebp-4],0 

0040102A  jmp         00401045 

0040102C  mov         dword ptr [ebp-4],1 

00401033  jmp         00401045 

00401035  mov         dword ptr [ebp-4],2 

0040103C  jmp         00401045 

0040103E  mov         dword ptr [ebp-4],3 

00401045  mov         eax,dword ptr [ebp-4] 

00401048  mov         esp,ebp 

0040104A  pop         ebp  

0040104B  ret         4 



I wrote what I thought was the most natural code:

if((unsigned)selected&lt;4)
\t\treturn selected;

       else
\t\treturn 0;


which compiled to:

01000000  push        ebp  

00401001  mov         ebp,esp 

00401003  cmp         dword ptr [ebp+8],4 

00401007  jae         0040100E 

00401009  mov         eax,dword ptr [ebp+8] 

0040100C  jmp         00401010 

0040100E  xor         eax,eax 

00401010  pop         ebp  

00401011  ret         4   



Then i tried endian675's 'crap' 
return ((selected &lt; 0 || selected &gt; 3) ? 0 : selected);
which generated:


00401000  push        ebp  

00401001  mov         ebp,esp 

00401003  push        ecx  

00401004  cmp         dword ptr [ebp+8],0 

00401008  jl          00401018 

0040100A  cmp         dword ptr [ebp+8],3 

0040100E  jg          00401018 

00401010  mov         eax,dword ptr [ebp+8] 

00401013  mov         dword ptr [ebp-4],eax 

00401016  jmp         0040101F 

00401018  mov         dword ptr [ebp-4],0 

0040101F  mov         eax,dword ptr [ebp-4] 

00401022  mov         esp,ebp 

00401024  pop         ebp  

00401025  ret         4


and Finally, I tried petevalle's

return (!(selected&gt;&gt;2))*selected;


which generated:


00401000  push        ebp  

00401001  mov         ebp,esp 

00401003  mov         eax,dword ptr [ebp+8] 

00401006  sar         eax,2 

00401009  neg         eax  

0040100B  sbb         eax,eax 

0040100D  inc         eax  

0040100E  imul        eax,dword ptr [ebp+8] 

00401012  pop         ebp  

00401013  ret         4  



It seems clear that there actually is a substantial tax on bad code, namely number of instructions and binary size increases (in this case by a factor of 4). [you can see the size by looking at the last 2 hex digits of the 'ret 4' instruction's address. (4B,11,25,13)

You can also see that the most obvious 'KISS' version of the code is the smallest!  The overly clever code written by petevalle is still longer than the obvious case!  Endian's 'crap' hard to read code is even a bit larger.  



--I apologize for the formatting, but I'm done fighting with reddit's formatter :(actually e is euler's number, exp is short for exponential, so it's not read the same, but I get what you're saying anyway.
*keeps fighting*&gt; That would be a nice tool to start with when trying to make a modern engine for an old game... (Master of Magic, anyone? :)

In booming wizard-like voice: "If that is the truth, then your work must stop!".Time to get Zen:

Explicit is better than implicit. 
Simple is better than complex. 
Readability counts. 
Special cases aren't special enough to break the rules. 
In the face of ambiguity, refuse the temptation to guess. 
There should be one-- and preferably only one --obvious way to do it. 
If the implementation is hard to explain, it's a bad idea.&gt; As far as I'm concerned, if it's not documented, it doesn't exist.

That's a useful maxim when managing a software project.  Not so useful when making claims outside of that context.And a lot of people that use Maya work in a high-tech environment and there's probably a co-worker that knows a little bit of programming, and can extend the app.Good read.  I would like to see iBatis vs ActiveRecord though.  iBatis seems a lot less complicated, can automatically generate most of the work, and doesn't have all the overhead.Excellent example! Well done. *clap clap*The java source has always been available.No, Slava is right.

&gt; You call equals twice and get different results.

That's referential transparency, not idempotence.

Idempotence is when performing an operation repeatedly has the same effect as performing it once;  it doesn't "stack" or accumulate.

In HTTP terms (as the article you linked to explains), GET, PUT, and DELETE are all idempotent, even though they're allowed/expected to generate different results for each request.  POST, on the other hand, is non-idempotent because it specifically allows accumulative effects, like deducting money from your bank account.Java on Reddit?  WTF?
:P&gt; A proper comparison function would return the same thing each time no matter how many times you call it.

Right.  The point is that what you are talking about here is not idempotence, but referential transparency/determinism.

Even the broken, non-deterministic `equals` implementation discussed in the article is actually idempotent.[removed][removed]Those interested in this may also enjoy reading about [table-oriented programming](http://www.geocities.com/tablizer/top.htm).Some do. They are the 'good' group: motivated, willing to learn something new, and are on the right track. I enjoy lending them help. The rest just... want to get their work done and couldn't care less.His approach is basically a reimplementation of a large chunk of CPython.  Instead of bytecode, you get ocaml, but everything is still looked up in hashes, etc.

Also, the hash by ID thing... wow.  His optimization of constants him from noticing this:

    foo = {"someword":0}
    bar = "someword"
    foo[bar]

works just fine.  He might cry himself to sleep when he tries:

    foo = {"someword":0}
    bar = "some" #I would have just used "some"+"word", but
                 #he might have optimized that as well
    bar += "word"
    foo[bar] #key error

Likewise, the 100 integer optimization makes this work:

    foo = {2:0}
    bar = 1
    bar += 1
    foo[bar]

But don't try:

    foo = {101:0}
    bar = 100
    bar += 1
    foo[bar] #key errori think this is the search engine ive been waiting for
screw lycosI do not contest that you do not. However of course the site was not designed with your approval in mind; you are, after all, only a sample of one. If the litany of newspaper, magazine, and book designs does not convince you that the *population* reacts, perhaps [this oft-cited study](http://psychology.wichita.edu/surl/usabilitynews/72/columns.htm) might.

But back to the original point, the design of the website is not necessarily faulty. It is hastily rash to say that there exists no reason whatsoever to justify narrow columns.[deleted]I think someone should point out that Gosling likely had nothing to do with this.or endian675.We must go to war.Wow! This could be better than Google Source Code SearchDude it doesn't compete with lycos. Its just for java code.&gt;You can also see that the most obvious 'KISS' version of the code is the smallest! The overly clever code written by petevalle is still longer than the obvious case!

Let me doubt of that. Your simple version is compiled with two jump instructions, jmp and jae. Conversely, the compiled petevalle's version only uses arithmetic instructions. Therefore, I would believe his version is faster.Like you couldn't do that in Java?

    class MyURL extends URL {
        bool equals (MyURL url2) {
            ...
        }
    }
This sounds like a standard programming argument. How deep below the abstraction should you go? Sure, there's going to be a host of people who learn just the framework and never delve in to ruby, but they'll be significantly weaker developers for it, so it's really their loss. No need for hand-wringing over it.Great stuff... sexy time!That is, assuming 5 is a possible input.  If I actually found this code in a real system, the first thing I'd do is look at how it's being used, because chances are the whole thing is unnecessary.

Bad code is usually a harbinger of more bad code.Huh? You mean another article with the same URL??It won't come out *ever*.  That's the problem with sucky things that become widely used.  They're impossible to fix.

In the meantime, I'm glad I know about it so I can avoid putting URLs in TreeSets or Hashtables or anything else that calls equals().  Gah.  I wonder how much of our code has performance problems resulting from this.Well, according to the blog author, Gosling is listed as the author via the @author tag.  So he has reason to believe that it's Gosling other than Gosling being popularly known as the inventor of Java. 

Edit: here's the [old Javadoc](http://tns-www.lcs.mit.edu/manuals/java-api-old/java.net.URL.html).  The author is listed clearly as James Gosling there, so yeah, it looks like Gosling had everything to do with this.It's deeper than that, I believe. I never did much algebraic number theory, but I have some vague memories related to exp(pi*sqrt(163)), and that the explanation involves looking at the q-expansion for a modular function (the j-function probably) associated with the imaginary quadratic field Q[sqrt(-163)]. The key is that this field has class number 1.Since its only for Java, its kind of useless for those of us who work in .Net, C++, Delphi, VB....[removed]When I said 'longer' I meant bytewise: the KISS version is 0x11 bytes while the petavalle code is 0x13 bytes.

As far as speed goes, we'd have to benchmark it to be sure.  On  a modern intel chip I suspect that a near jump into the cache is quite fast; and since clock cycles for individual instructions are probably overrun by memory fetch times, shorter code would  on average be faster since there is less fetch.   The context would certaily matter.
[removed]Are you arguing that your code is equivalent? Care to finish it off if not?To Whom This May Concern:
The following review I am providing is regarding my organization's usage and outcome of the allthecode product :
Recently,a sales angent representing themselves as AllTheCode approached my organization to evaluate and comment on the product on the basis of a 30-day risk-free trial of their pre-release edition of allthecode.  After a relatively quick (could have been a little bit faster), I received a allthecode pre-release appliance from allthecode and the initial setup of the machine was relatively simple, simply connect the power, ethernet cable to my switch and connect to the appliance to configure a few settings (such as integration with Visual Studio Team Foundation Server).  The initial setup ran overnight, "parsing my project files," as the status panel mentioned before I left my office.  After a relatively quick walkthrough with my team of developers on how to use the product inside of Visual Studio 2005, my developers went back to work with very little interruptions (a minor hiccup occured part way through the evaluation, however this was caused by an improper binding on the source code control system on our side).  At the end of the day, I asked a few developers what they thought of the software and they commented that it was quite useful.  One mentioned that within Visual Studio they could use a search bar that allthecode places in Visual Studio to quickly look for references within the source code.  Yet another developer told me that on numerous occasions that allthecode was able to guess (many of times correctly) that a function that implements similar capabilities already existed inside of the code and was able to place a reference to the code, rather than having a developer writing needless code.  These features coupled with many minor, yet handy features at the end resulted in a overall developer producitivity increase, which resulted in our developers catching up to schedules (they were about 2 days behind when they first started using the product).  

All in all, the product allthecode while it certainly is not as polished as other appliances I have used, has demonstrated a significant deal of technical sophistication.  Finally, for what it's worth the overall developer productivity in my software company was quite noticable (approximately a 15 percent developer productivity boost I reckon).  

I would also like to mention that I have not experienced the allthecode website, so I cannot directly comment on the website, however if the technical sophistication of their appliance is present on the web edition of allthecode, then it should be quite clear that allthecode is right for your organization, as the appliance was clearly applicable to my development team.

Sincerely,

James Simpson
President
Straightway Technologies Inc.Yes, I couldn't agree more. It does seem slightly redundant, although it'd certainly help to put this code in context.Hasn't Python been in Maya for years?good article. I have known Amazon more from hereThat's a big problem, and I'm surprised how he didn't realize how important it is.

It's not hard to fix. He needs to add a hash method to his types, which can either use object identity for mutable structures (lists, etc.,) or a cached value for immutable.[deleted][deleted][deleted]um? excuse me? it doesn't matter which database won. un-tuned databases aren't a very useful basis on measure performance. Least not in anything resembling  the real world. 

--vathttp://www.krugle.com/ is better.Lisp macros without dynamic typing are completely pointless:
 
1. Lisp macros are more than just parse-tree transformations

2. Dynamic typing is inherently required to achive even remotely the elegance, simplicity and power of Lisp macros
Isn't the general solution just to deref HTTP 301s before storing the URL?I'm not going to mention the common L word here, but really, high level languages have had compilers for a _long_ time.I thought this was going to have some code that would make doing this somehow easier than it otherwise would be.

You could use any other fast, expressive GC language here instead and still get the benefits the article is talking about.I was just showing this link to my friend, and he came up w/ this solution, which is smaller, faster and easier to read -&gt; 

p1() -&gt; 
   lists:sum(
      [ X || X &lt;- 
                  lists:seq(1,10),
                    ((X rem 3) == 0) 
             orelse ((X rem 5) == 0) ]
        ).


I agree wholeheartedly. If they had implemented macros with regular haskell syntax then _that_ would be something special...[removed]Obligatorily: [OI comonad](http://www.haskell.org/hawiki/CoMonad)!&gt;You mean, besides really not doing anything useful?

Have you any idea what context the code is taken in?  It could be in a nuclear reactor's codebase for all you know, deciding the number of rods to be pulled.&gt;It seems clear that there actually is a substantial tax on bad code

There's no tax until it's 

(a) been identified as as bottleneck, which unless the above is being called 100m times I suspect it never will be
(b) the alternative code has been benchmarked and the decrease in readability justified by the increase in speed&gt;Using a screenful of code when one line will do is a waste.

No it's not!  If it means that somebody picking up the code base in 5 years can understand it totally within 1 second.  If you're really worried about it, use your editor to insert a fold/#region/whatever-your-editor-offers.  I've been using those since the mid 90s, and I'm sure they were around before that.

&gt;Next time you spell it out for us, spell NOT correctly.

Ah, spelling nazi!  I wish you worked for me, because then I could fire you.linux is just not a serious OS for doing real work, sure it's good if you want to use it as a server that sits in the corner and serve out some files or run some apps but you can't do any real productive work on it, it's still too much of a toy OS compared with Windows which is excellent as a server, buisness and consumer OSNo you can't:  `public final class URL [...]`I think you're missing what the author is trying to say:

&gt; code authors worth their salt generally state very early and loudly in their documentation whether the library is thread-safe or not.

Thread safety is not an on-off condition, though.

&gt; you would put customer 123's data in a database [...]

Which doesn't help when you need to concurrently access other resources too, which is what the author was talking about.  (Besides, even if you're just using the database, you can still get deadlocks.)What a silly article. It's pretty obvious that swapping with XOR is slower than with a temporary variable (on my system,     almost twice as slow. Code at bottom of this post). XOR swaps are used on resource-constrained systems, where using sizeof(int) memory is actually a big deal.

    #define real_swap(a, b) { int c = a; a = b; b = c; }
    
    #define xor_swap(a, b) a ^= b; b ^= a; a ^= b;
    
    int main() {
      unsigned int ii, jj; 

      /* WARNING: Less-than signs are changed for Reddit! Fix them in your editor! */
      for (ii = 0; ii ≺ 100000u; ii++) {
        for (jj = 0; jj ≺ 10000u; jj++) {
          /*real_swap(ii, jj);*/
          /*real_swap(ii, jj);*/
          xor_swap(ii, jj);
          xor_swap(ii, jj); 
        }   
      }
      return 0;
    }Actual link: [here](http://groups.google.com/group/comp.lang.scheme/msg/18d1d6d4f84e70e3).  Great read.[removed]Unless they special-cased it, it also breaks the first
and most fundamental rule: `x.equals(x)` might or
[might not](http://en.wikipedia.org/wiki/Round_robin_DNS) be true.
&gt; Sounds like the poster wants to compare Strings.

No. URLs are not Strings, and should not be treated as such.[removed][removed]Apache still serves out 60% of the web - thanks [netcraft](http://news.netcraft.com/archives/2007/01/05/january_2007_web_server_survey.html) - and in the HPC / scientific computing sphere it seems that Windows is not a serious OS for doing real work.  So it depends on your definition of real work.[Mark Pilgrim](http://diveintomark.org/archives/2006/06/26/essentials-2006) disagrees with you.Well, you could (if it's true) prove it. According to wikipedia, e^pi is transcendental by the Gelfond-Schneider theorem, but it's not known if e^pi - pi is transcendental.&gt; Windows which is excellent as a server, buisness (sic) and consumer OS

If that was a joke, it was both funny and well executed.[removed]All of his problems essentially stem from trying to interface Linux with closed, proprietary software (aka Microsoft products). The exchange server connection in Evolution is at best, a hack... basically a screen scraper using the Web Outlook interface. But what else can you do when your server is designed to only be compatible with Outlook.

Try using Windows in a real unix network environment, and you'll quickly realize which is the toy OS.Trying to open Visio documents, or communicating over Windows Messenger, you quickly realize that MS designs most things that way. ;)[removed]it looks like Jim Sampson's neck has been twisted 180 degrees in [that photo.](http://www.networkperformancedaily.com/images/jimsampson.jpg)Looks like a good approach to demonstrating the language. Waiting impatiently for the next part!&gt;Try to stay away from the ad hominem

Well, your post didn't--you called him a fanboy just because his technical views don't match yours. I apologize if I came across as offensive, though.

&gt;in replying to my accusation about Joel getting rich on people's ignorance, you say that I think I am the shit and am therefore annoying, completely missing my point about Joel. (Aside: I don't think I'm the shit, I didn't say I was the shit, and I didn't imply it, either.) Do you have a comment about "reading about people who provide inferior stuff getting rich on people's ignorance"?

I missed the point indeed, because I'm not really acquianted with his software, so I didn't get your implied point--that it's inferior, and that he's become rich off of it based on people's ignorance.

What I did get, however, was the explicit point that Microsoft products are generally inferior and that people buy them out of ignorance, making Microsoft rich. I completely disagree with this statement, and I happen to find many Microsoft products unparalleled by rivals *for my needs and priorities*.

&gt;However, he still is, as far as I know, a big proponent of using Microsoft products, like VBscript and IIS. Which are famous among the competent programming community for being inferior to their open-source counterparts. Furthermore, he repeats (elsewhere) Microsoft's FUD about how open-source projects can't innovate, they can only copycat, etc, etc.

First, he doesn't make mention of this in this article, and thus the article has no mention of the so-called fanboyism. Yet you said it was nauseating to read such fanboysim in the article.

Second, don't you think calling him a fanboy for preferring IIS/VBScript instead of criticizing his technical argument is ad hominem?i don't know. as "clean" as the language might be, their webpage is [a dirty shitbagel](http://clean.cs.ru.nl/About_Clean/Clean_Language_Features/clean_language_features.html).

it's good to see a purely functional language with strong numerical performance (something GHC simply isn't optimized for).And yet he asks not to be reddit'd :)To be honest I found this navigation confusing. I much prefer hyperbolic trees.I think the article misses a trick. In an ideal world I'd specify my data in a relational way and all the extra guff would be generated for me. Hints about relationships (a la Rails belongs\_to, has\_many etc.) and the basic structure should be enough.

Trees and DAGs are the bane of the Relational model. Makes me wonder if something based on algebraic datatypes, maps, and folds would work better...

IMO most of SQLs crap (especially from a language design perspective) was sorted out by XQuery.There's always going to be these cargo-culting "programmers" taking bits and pieces of broken code off "script" sites and knows-enough-to-be-dangerous tutorials. Sure they're very loud and annoying, but it's nothing to worry about.

Us Perl fans owe the PHP/Rails guys a huge debt of gratitude for taking them.

There should be an Open Source fund for whatever language community has to deal with these people, the way public schools get additional funding for special education programs.
yes, but haskell is so NOW!Back in the late 80s all the lazy functional programming researchers set aside their toy languages to work on Haskell, the new, free and open source language standard, all except the Clean guys.

In the end, Haskell is popular because it was free, and had a large core of researchers kicking it along. Clean had a small base and was non-free.

Languages live and die on the strength of social issues: communities, licenses, and random disruptive events.

Also, uniquness types are harder than monads ;-)

Also 2, Clean's Dynamic type is really cool (it supports polymorphic dynamics).How slow is the ruby code and why, surely there's something less heavy-weight than bloated ActiveRecord for Ruby.

Can you do this with a little shell script and some fast C-based XML parsing programs? How about Perl/LibXML2?

Sounds like someone is just trying to be haskewll. I think that's fine, but it seems lame to be in denial about it. Come out of the closet you Haskell-fanboy, don't worry, it's 2007!It's not fair.  One reason many try to charge/sue them for being a monopoly.

But not being fair doesn't make it any less true.  For a very large segment of the business desktop market Linux is not usable.  This is, as you point out, due to MS products and not Linux.

Hardly anybody bitches about how MS Word won't open Open Office documents properly.What little I have studied the language, it seems to be extremely cool functional language. Even when I'm not great fan of functional programming, it still has many new ideas and features you can learn from. I almost feel compelled to try it out.

The consept of *uniqueness typing* is really neat. If object, an array for example, has only one reference to it, function that has that reference can modify it without breaking functional abstraction. 

Another cool feature is it's take on *dynamic typing* kind of thing Dynamics. You can load code and data runtime.[deleted]You are right, but it's not Zen.via
http://blog.laubach.at/smalltalk/blogView?showComments=true&amp;entry=3347782296[deleted]Trees and DAGs are especially simple in the relational model: 

Simply use a relation like "graph(parent-node, child-node)". If you want to use this as tree, add a constraint that every child-node has to have exactly one parent-node in this relation.
symbian ftwYHBT YHL HANDIndeed it is. Still, it would've been a much better idea to post the actual link on reddit, rather than the blog post.He's comparing RAID5 on XServe RAID device to S3. That's hardly the cheapest route you can go. Just because he knows how to build a great photo site doesn't mean he's a storage expert.

He could probably beat S3 considerably by using a [distributed file system](http://danga.com/mogilefs) and true commodity disks.

Besides the technical aspects, I would never want to out-source such a critical component of my site to some huge and unmovable company's beta product. Who knows if Amazon will decide to cancel the project when some new random VP takes over that department.
&gt; Free software and open source software programmers are not allowed to benefit from commercial/closed-source/slave software because they can't look at the code.

So to spite them, the GPL institutes a "no touch" policy of its own... fair enough.  The problem is that this hits more liberal, non-GPL open-source software just as hard:  while the GPL is benefits from the latter, it doesn't allow the reverse to happen.

&gt; So why would we allow commercial/closed-source/slave software to benefit from GPL code?

Because what's good for your users is also good for you.  Consider some prominent examples of open-source software with proprietary off-shoots (and the companies behind them): 

* FreeBSD:  Mac OS X (Apple), Interjet (IBM/Whistle), many other/embedded systems (Juniper, Wind River, BSDi)
* PostgreSQL:  Mammoth PostgreSQL (Command Prompt Inc.), EnterpriseDB, Bizgrez
* Python, Lua:  too many major products and games to list

What these examples all have in common is:

1. Far from threatening the open-source projects in question, these companies have been (and are) a major boon to them, providing source code, employing developers, funding events, sponsoring infrastructure and publicity, and so on.  They do this not out of some altruistic whim, but because it's in their own best interest to tend to and improve the technology they depend on.
2. With the GPL's intervention, none of these would be possible;  the open-source projects would be poorer as a result.I believe Google's Sketchup has an embedded Ruby interpreter.&gt; How slow is the ruby code and why, surely there's something less heavy-weight than bloated ActiveRecord for Ruby.

Python?  (SQLAlchemy?)I use Linux as a desktop at work (on a standard windows domain) - All the problems that are mentioned in the article about Exchange / Evolution are true - Not that I'm bothered because I use Thunderbird which seems to work fine. As regards the rest of "Windows" functionallity - I can log onto my networked "my docs" folder automatically and use 99.9% of the office docs I need to - The 0.1% being our expenses form (which doesn't work properly in genuine MS Office either) - The main problem I have is the speed at which Openoffice opens... easiest measured in weeks...CONDENSED VERSION: I am such a geek that for ten years I have used non-Windows (presumably just Linux) only intermittently -- each time long enough to determine that I can't function in my MSOffice-bound office.  My story has a title connected to the story only by the words "ten", "years", "linux", and "workplace".  I easily confuse my workplace with 'enterprise', and 'the workplace'.[deleted]Indeed.  A few years ago when I looked at Haskell, someone suggested Clean -- and I disgarded it as quickly as I saw the license.Mercury also has this, at least apparently -- and it also threads them through code like a monad.Good pointWell, it's certainly not like Euler's identity, which is really just a special case of Euler's formula e^ix=cosx+isinx. The proof of that is straightforward using the taylor expansions of e cos and sin.

More interesting is if you use different values of x, you still get nice looking expressions 

eg  e^i(pi/2) - i =0 

or e^2ipi -1 = 0

and so on

Euler's identity is just the value that gives you the most satisfying looking expression.

As a quick aside, the locus of points eix in the complex plane is a circle where x is the angle between the real axis and the radius, if you let x=pi, ie 180 deg, then you find yourself at -1, no big suprise.

I'm afraid that Euler's Identity gets less exciting when you know a small amount of Complex Analysis&gt; (It's interesting that the Pragmatic Programmers put forth the idea of language-of-the-year in 2002, dubbed Haskell the language of the year, and haven't updated it in 5 years. ;-)

That's *sad*, not ":-)"-interesting.  LOTY failed completely after one year, because -- rather than kick its happy little community to the curb, choose a new language for a new year per its explicit designs, and then begin repeating what it had learned with evangelization and wikimentation -- it just disintegrated into democracy.  Everything everyone said about LOTY, all of their values and rationals and goals, disappeared when it turned out that a significant part of the community only had one leap-into-the-new in them, and thereafter mainly wanted to conservatively build on and try to get paid for what they knew.[removed]I love the rich cut and thrust of these discussions.

A: I can't use OSS products in my workplace for [these specific reasons].

B: Yes. This is because MS sucks. You should be an OSS only shop.

A: Sure, but these problems exist now, and my employer won't stop using Exchange suddenly next week.

B: These problems are MS' fault. Where do you get off complaining about OSS?

A: I'm not complaining about OSS, I'm describing the problems that exist. Help?

B: You people *so* suck Gates cock.Disagree.  In this case, the blog post provided useful context.  This is not always the case, but here was an exception.

I agree that it's a good post, though.  Are the denizens of comp.lang.scheme always that much more polite than the [morons like "Cobol" Bourguignon](http://groups.google.com/group/comp.lang.lisp/browse_frm/thread/88a36fb2c239a44e/89405e202723a377?lnk=raot#89405e202723a377) on comp.lang.lisp?The codespace tax is there no matter what (and often hard to notice in a profile, if in general less code just fits in the cache(s)).

Readability potentially suffers too -- I find needing to examine four cases, and wondering *why* they didn't do it more simply ("There must be something I'm missing") add up to more time than *most* of the simpler versions.

I'm for multiple simple lines vs. one complex line. But that's not the decision here.OT: there's an interesting bug in Reddit.  This was listed on the main page as having five comments; I count three others, this being the fourth.  *Edit:* now it says six.  I wonder why.  I blame the move to Python.  Everyone knows snakes can't count.[deleted]Here's whachya doo:

Push VISTA into the workplace!

Say how awesome it will be that the company is going to be ahead of the curve of everybody else.

Wave away any security complaints.
Wave away any financing complaints.

When you can, install a "windows" box, well actually a Linux box with a complete Windows GUI on top of KDE. Tell the boss its a hard edges Windows box. Good ol' crypto Linux box. Nothing beats crypto Linux box. Oooh yeah.

If boss gets suspicious, it's a developer .NET Windows box and he shouldn't mess with it.Lists are your loops, they just haven't happened yet.

    fibs = 0 : 1 : zipWith (+) fibs (tail fibs)
    fibonacci n = fibs !! n

(I'd have posted this directly to the blog, but nothing I wrote would get it to accept it as non-spam.)&gt; Anyway, this post is not about the font selection dialog - it’s about GNOME’s font configuration dialog.

Downmodded for unedited but out-of-context title.And then the usual
fibonacci n = fibs !! n where fibs = 0:1:zipWith (+) fibs (tail fibs)

Or the somewhat more unusual
-- 2x2 matrix
data M = M Integer Integer 
           Integer Integer
    deriving (Show, Eq)
instance Num M where
    M x11 x12 x21 x22 * M y11 y12 y21 y22 = M (x11*y11 + x12*y21) (x11*y12+x12*y22) (x21*y11 + x22*y21) (x21*y12 + x22*y22)

fibonacci n = let M _ fn _ _ = (M 1 1 1 0) ^ n in fn
I know very well how you feel.  :)
I too work in an environment with tons of C++ code.  Even it were possible to rewrite all this code in Haskell and keep performance it would not be done.  It's just a matter of cost.
The day Haskell can actually demonstrate that your code runs faster of written in Haskell it might be done.

But even so, I've found that trying to figure out a clean interface to the C++ code in terms of Haskell types helps me.  Strong typing helps you clear up muddy thoughts. :)
I saw the site after they switched to LGPL, but was put off by the facts that

1. It has a single, in-house, unstandardized compiler - in fact, it appears to be basically a one site language.

2. They appear to be deprecating Linux support.I don't understand. Why wouldn't

    foo = {"someword":0}
    bar = "some"
    bar += "word"
    foo[bar]

work? `bar` is not a constant, after all, and the interpreter might very well hash by value and not by objid.

*Edit*: code block, not block quote.I've encountered that too (with different posts)We even had Clean guys on the original Haskell committee, but they went their own way.  And I have no problem with that, but I think they could have reached an even wider audience if they used Haskell syntax where possible.  Then we could actually use their very good compiler with Haskell programs with little trouble.
[deleted]I'd say Pascal is much smarter than you.Ad 1: Lisp macros are less powerful than parse tree transformers. The implementation of defmacro as parse tree transformer clearly demonstrates that. On the other hand, please show me aggregation of parse tree elements (as done by the Prolog parse tree transformer) implemented with defmacro. That does not work.

Ad 2: Is there any argument why dynamic typing leads to elegance?I hope Haskell takes off, but if it doesn't, I shudder to think of what a maintenance programmer is going to do when he discovers that critical little nugget of Haskell code in the middle of a Rails app in ten years.
not to be a dick or anything, but the whole config file is valid ruby, comments and everything, and to get those variables locally available all you do is

eval(IO.read('config_file'))&gt; Idempotence is whether f(f(x)) == f(x) always.

That’s incorrectly stated. Since idempotence is about side effects, as you correctly point out, you can’t define it in terms of return values as you tried to.

(When _f(f(x)) = f(x)_ applies, this implies _f(x) = x_, and we say that _x is a fixed point of f_ – that’s used in various formalisms including lambda calculus. It has nothing to do with idempotence though.)But any non-trivial operation on a tree requires recursion. Supported and efficient (in SQL), not so much.No. Only Mel and C++. Long live Python!*Everyone knows snakes can't count.*

the lord said go forth an multiply

but how can i ? i am an adder
It is [Python Zen](http://www.python.org/dev/peps/pep-0020/)On the other hand, maybe that's a good thing.  There's probably no way they could fuck it up and get it to compile.Guh. What part of

&gt; If you imagine x = "state of the universe" a la Haskell in f(f(x)) == f(x), you get the side-effects interpretation of idempotence.

and

&gt; Sorting is idempotent because sort(sort(list)) = sort(list). So is the remove-duplicates operation. Edit: and absolute value, and transitive closure.

didn't you understand?Ruby, despite its many other virtues, is substantially slower than Perl or Python on many benchmarks. Python typically runs at 1/15th to 1/25th the speed of C; I've seen Ruby range from 1/50th to 1/500th.

Ruby is a lovely language, but it would benefit from either a bytecode interpreter or an actual JIT. The theoretical maximum performance of Ruby should match that of a top-flight SELF or SmallTalk implementation: 1/2th to 1/4th the speed of C.

Haskell runs at 1/2th to 1/4th the speed of highly optimized C, unless you make the mistake of using String (instead of ByteString) for heavy text processing. Then it performs more like Python...from the link:

    lists:sum([X || X &lt;- lists:seq(1,N-1), (X rem 3 == 0) or (X rem 5 == 0)]).

vs from your comment:

    lists:sum([X || X &lt;- lists:seq(1,N-1), ((X rem 3) == 0) orelse ((X rem 5) == 0)]).

I'm not sure how that qualifies as "smaller", "faster", or "easier to read"? They are almost precisely the same. (Except I can't figure out how to get markdown to actually display a less than character.)&gt; Ruby is a complex language

Give him a lollipop.i think you missed a C(inf) manifold over a binary icosahedral group having an order 120. what gives ?&gt; Who knows if Amazon will decide to cancel the project when some new random VP takes over that department.

True, but GB per $ will be twice as good by then.  ;-)You've picked a lisp, but haven't picked a scheme. Some schemes are better or worse than others, including Common Lisp.this list is not exhaustive, Smalltalk/X is missing.It's because linux alternatives do not even come close to what microsoft offers.  It's bigger than linux vs windows, this is active directory vs NIS/LDAP/flat files/kerberos, exchange vs pop/imap server+calendaring software+file sharing+everything else exchange does, paid microsoft support vs paid redhat/suse/etc support.

Everything Windows does now linux has the potential to do, but a company would have to hire a staff of programmers and administrators to implement their proprietary "enterprise" solution, and then pay them enough money that they don't leave.  And that's just to get started, they can't hit the ground running.  They have to develop and test the solution.Maybe it counts edits, not comments.  (Or deleted comments?)This is standard FUD.  

Sure, there is a *very* small percentage who expect Rails to build them the next Flickr in two nights of coding and a dreamhost account.  These folks either get disappointed when they see reality, or get educated by the community.

The rest of the people are somewhere along the continuum between "just playing with Rails, not sure about this Ruby thing, but learning more" to "expert in Ruby, writing my own DSL's/libraries/ruby implementation - oh ya, can do Rails too".  

Pretty much like any other powerful high level language available.There is nothing that hinders you to use the LGPL licenced version for commercial work.No, I was right.

(1 month) * ((US$ 0.01) per (megabyte * month)) * (200 gigabytes) = 2 048 U.S. dollars

&gt; Please, nobody post this to Reddit until I’ve completed at least the first part (probably tomorrow).  :) Nobody likes vaporware.Would it be possible to use [MVar](http://www.haskell.org/ghc/docs/latest/html/libraries/base/Control-Concurrent-MVar.html)s in Haskell to achieve the same effect as Clean's uniqueness types?

That is, where a Clean function has a signature "*a -&gt; *a", you could write an equivalent Haskell function with the signature "IO (MVar a) -&gt; IO (Mvar a)"; in both cases, the type system is guaranteeing that the input can only be read once. Can a Haskell implementation optimize functions with such a signature in the same way that Clean optimizes functions using uniqueness types?Why not show us an example of how it's used, how you would have Ruby interface with Haskell, and all that?I wish I had made that up! Alas, it's pretty close to the truth. Google's top hit for "q-expansion 163" is http://www.math.cornell.edu/~alozano/Integrality.ps - this appears to explain the connection, if you're interested.Please, please don't give me any money! I'm not ready yet![deleted]&gt; Sounds like someone is just trying to be haskewll. I think that's fine, but it seems lame to be in denial about it. Come out of the closet you Haskell-fanboy, don't worry, it's 2007!

I don't quite understand that phrase.

There are two parties here:
1. dons, which is a very active Haskell developer and community member and therefore has a tendency to submit interesting articles on Haskell he finds to reddit. Fairly normal if you ask me.
2. The author of the article, who is NOT dons and talks about his experience, stating first that Haskell (any high-level compiled language really, but he knows and used haskell, could've been OCaml, could've been Objective C, could've been Lisp, it was Haskell, and dons thus posted it), and then that he talked about this experience at Rails Edge and people were interested.This debate is just so old now. I can understand why in many cases (and only too often out of thin air) you should use multi processes but if an application runs and has been runing fine as a multi thread application for what it was supposed to be doing. Why should people change or decide threads are damn evil?
Except that the issue wasn't only with ActiveRecord, it was with Ruby in general and ActiveRecord only made things worse, since the problem was processing XML to load it into a DB in the backend.All caps global variables? A `for ... in` loop instead of `Enumerable.each`? *bleh*Pascal Bourguignon is smarter than most anyone here.  Or anywhere else, for that matter.

That doesn't change the fact that he was acting like a total jerk on the day he posted the "rm -rf /" code.IS IT AS ANNOYING WHEN I DO IT?

&gt; We'll be needing a few modules.

Will we?  Excellent!  I dasn't ask why!

&gt; The natural result of parsing a config file, at least in my mind, is a finite map from keys to values, where both are represented as strings.

Shocker!  Of course, this isn't the 'natural result of parsing a config file', and what passes for your in-mind conception of this is itself merely a blindingly obvious subsequent thought to 'I have a config file consisting of lines of string1=string2'.  When you come across a config-file with structure interesting enough that its 'natural result of parsing in mind' is actually interesting and worth comment (I've made a few of these -- mainly in Lisp and Erlang, where the 'parser' is READ), please then comment at length.

&gt; type Config = Map.Map String String

Hey, is this a Map of String to String?  And is it a Config?  A Config type, eh?  I wonder what any config-parsing module would need Config type of data for?  Why didn't you comment on this?

&gt; Comments are easily dealt with.

In other universes, sitting alien creatures regularly focus on a lit screen controlled by an uncomfortable -- standing -- alien creature.  That standing alien will do things we humans can only guess at the purpose of it: it will make the lit screen show the next 'slide' of some informative material, and then -- with that slide visible on the screen, and wholly comprehensible in of itself -- the alien will either A) look at the slide and read it aloud, or else B) lamely introduce the slide: e.g., when the slide has its own bold introduction of INFEST PLANET BETA-KAPPA: THE PLAN , the standing alien will say "and here is our plan to infest planet BK".

&gt; We'd like to be agnostic about line endings.

Well, apprently we'd like to identify either a `\r` or a `\n` as an EOL -- our crude agnosticism trampling local customs, and our boring config file ignorant of such evils as a CSV on windows that uses `\n` to break lines within a field.

&gt; Now to parse an actual config item.

Such active language!  But you don't go on to *parse*, really: you instead define a Parser (String, String) named `item`.  With the immediate history of such similar Parsers named `eol` and `comment`, I don't think I could've connected that `item` here refered to such that parses items.

By the way, 'config item'?  Did you come up with this inane term yourself, or did it seem to make sense after you'd already written enough Parsec to consume the key/value pair at once?

&gt;  Note that skipMany space above will happily consume newlines, so we don't need to explicitly check for empty lines. It also consumes leading whitespace.

Oh, for shame.  You may as well have written a three-line Perl program with this kind of sloppy attitude.

&gt; The readConfig action parses a config file, so it must run in the IO monad. If the parse fails, it returns a parse error; on success, it returns a Config map.

    readConfig :: SourceName -&gt; IO (Either ParseError Config)

Heh!  Here: "readConfig takes a SourceName and -- running in the IO monad, to access a file -- returns either a ParseError parse error or a Config map."

&gt; This is a dauntingly dense definition. Rather than jumping in to explain it piecewise, let's first turn the code into something more like "beginner Haskell".

Oh, thank you so much for failing to explain

it

piecewise.

I might've died.

Although somehow this decision to not simply explain it (or else not have written it atall, and instead have 'a clear example of how clear Haskell code can be') allows you to continue on into your unadvertised second tutorial: WRITING A SIMPLE FUNCTION IN HASKELL.  Although "So now we can rewrite listToMap in terms of foldr." looks enticing, I'll leave my literate comments for when this second tutorial gets its own reddit post.

&gt; But it was quick to write (about an hour,

You have 91 words of disclaimer at the end of of a 40-line module that took you an hour to write -- to parse a config file that you yourself made up, to be somewhat useful and yet still easily processed, for your personal use.

    vocabulary config-parser also config-parser definitions
    : set create 0 parse save-mem swap , , ;
    ' \ alias #
    previous definitions

    vocabulary config

    : parse-config ( c-addr u -- )
      2&gt;r get-order get-current 2r&gt;
      only config definitions also config-parser
      ( filename ) included
      set-current set-order ;

There you go, config files like

    set nick ayrnieu  # &lt;-- my nick!
    set mood cheery overall, but annoyed with this literary trend.
I'm no Ruby, let alone RoR, fan but this has to be the least meaningful article ever on the subject. Listen closely... pschttttt does the ballon.recursively?The part where it came after your assertions so my brain was already in “composing reply” mode even as I read your clarification. Sorry. :)

Still, though, if you consider x as _state of the universe_, you get the opposite problem that defining referential transparency cannot be defined in these terms. As you noticed yourself…

I suppose you’d have to say something of the following form: _u_ is a _state of the universe_ monad; _s_ is a function which returns its second argument: _y = s(x,y)_; _t_ is a function which returns its first argument: _x = t(x,y)_; and we will  consider functions defined to take and return a universe monad in addition to their other arguments and return values, as in _x',u' = f(x,u)_.

Then we can say that _f is idempotent_ when _u' = s(f(x,u)) = f(x,s(f(x,u)))_ applies; ie. _f_ returns a different universe monad from the one it was applied to, but when given the universe monad it just returned and the same input value from the previous invocation, returns an identical universe monad.

Conversely, we can say that _g is referentially transparent_ when _x',u = g(x,u)_ and _t(g(x,u)) = t(g(x,u'))_, ie. regardless of what universe monad you pass the function, it does not affect the computation and the monad returned is the same as the one passed.&gt; Matz, the author of Ruby, is working hard on a new virtual machine that should make Ruby just as fast, if not faster than Python.

Since I'm a credit Nazi, the new VM (YARV) is primarily the work (primarily in the sense that Sasada was the only developer on the project for more than a year and wrote almost all of it) of Koichi Sasada not Matz.Ruby has quotes around its strings, dear.Maybe I'm missing something, but I would do that example as

    data Archive = ZipArchive ZipArchive
                 | TarArchive TarArchive
                 | TarGzArchive TarGzArchive
                 | TarBz2Archive TarBz2Archive

    makeArchive :: String -&gt; Archive
    makeArchive fn
      | ".zip" `isSuffixOf` fn = ZipArchive (makeZipArchive fn)
      | ".tar" `isSuffixOf` fn = TarArchive (makeTarArchive fn)
      | ...[Ouroboros](http://en.wikipedia.org/wiki/Ouroboros) is always a big fat zero.&gt; On the other hand, checking that the HTTP referrer header of the request is within one’s domain is too restrictive. That’s because many browsers often do not send this header.

Does anyone know of any browsers in common use which don't send the referer header?I can't resist quoting Geddis:

"Lisp is better.

[Imagine a few blank lines here for effect]

 Unless you prefer Scheme".



Yes, that was the exercise ;)

I feel it's clearer in a two-dimensional encoding.

Idempotence:

           .-------.            .-------.
    x ----&gt;|       |---- y ----&gt;|       |----&gt; y
           |   f   |            |   f   |
    u ----&gt;|       |---- u'----&gt;|       |----&gt; u'
           '-------'            '-------'

Referential transparency:

           .-------.            
    x ----&gt;|       |----&gt; y
           |   f   |            
    u ----&gt;|       |----&gt; u
           '-------'           

*Edit:* I hadn't noticed you used *u* for "state of the *u*niverse" rather than *s* for "*s*tate". Updated for consistency.

*Edit 2:* No scratch that, *this* is referential transparency:

           .-------.            
    x ----&gt;|       |----&gt; y
           |   f   |            
    u --.  |       |
        |  '-------'           
        V

and this is what URL.equals violates.It looks like Jim Sampson needs a little more fiber in his diet in that photo.http://programming.reddit.com/info/y98u/comments/cyemp&gt; Here, that dastardly attacker placed code on his site, that puts the monthly statement from the bank’s site, inside a DIV. Then, once that data arrives, it is transmitted right into the waiting attacker’s hands.

What on earth is he talking about? No browser would allow that sort of cross site access. He obviously doesn't know what he's talking about.[removed]That notation is indeed clearer. But your first diagram is wrong and the second is incomplete _(edit: or was, anyway)_.

Idempotency:

           .-------.                 .-------.
    x ----&gt;|       |---- x'  x -----&gt;|       |----&gt; x'
           |   f   |                 |   f   |
    u ----&gt;|       |------ u' ------&gt;|       |----&gt; u'
           '-------'                 '-------'

Referential transparency:

            .-------.            
    x -----&gt;|       |----&gt; x'
            |   f   |            
    u -----&gt;|       |----&gt; u
            '-------'
            .-------.            
    x -----&gt;|       |----&gt; x'
            |   f   |            
    u' ----&gt;|       |----&gt; u'
            '-------'

where _x_ is the same _x_ in every place it’s mentioned, and likewise for _x'_, _u_ and _u'_.

And indeed, as `URL.equals` does not have side effects (as a first approximation – in reality, it does, since it leads to network I/O), it is referential transparency that it violates, not idempotency. Or to put it in terms of the mathematical notation in my previous reply: _t(URL.equals(x,u)) =/= t(URL.equals(x,u'))_, ie the state of the program universe may affect the computation.And on top of all that, god but it was dry and lifeless writing. He must be an engineer. The whole thing was summarized in the first 2 paragraphs.Hmmmm, lots of good suff there and yet he somehow missed the primary reason Common Lisp is preferred for "real work": declarations enable it to be much, much, much faster.All browsers send referrer headers. But apparently sometimes they just don't. Just look on any web server logs.I wonder if anybody in Ruby-land is keeping up with PyPy. Ruby and Python are so similar that backing to PyPy ought to be easy, and from what I understand, PyPy might already be a Ruby performance enhancement.

I just did a quick google on "ruby pypy" and nothing really lept out at me.

As I've said before, I wish one of these projects would "win". If everybody can back to PyPy or Parrot or something, we can get cross-language libraries and really challenge the .Net platform, whereas right now, Ruby and Perl 5/6 and Python are unnecessary adversaries. I can see the virtue of multiple VMs based in multiple domains (JVM, C, Smalltalk, etc), but I don't see the need for ((scripting language) x (domain)) number of virtual machines. And we can concentrate on making them _all_ faster at the same time.You're right about referential transparency. I updated my post to reflect that.

You're probably more correct about idempotence as well. But actually, I've only ever seen examples of idempotence where only one of return values and side effects was relevant but not both. So I probably shouldn't have conflated the two. But I'm not sure what to say about it when both are involved. Would you say the following is not idempotent?

           .-------.                 .-------.
    x ----&gt;|       |--- y     x ----&gt;|       |----&gt; y' =/= y
           |   f   |                 |   f   |
    s ----&gt;|       |------ s' ------&gt;|       |----&gt; s'
           '-------'                 '-------'

The one below is idempotent, but isn't compatible with your diagram.

           .-------.            .-------.
    x ----&gt;|       |---- y ----&gt;|       |----&gt; y
           |   f   |            |   f   |
    u --.  |       |            |       |
        |  '-------'            '-------'
        V

Bah. The only useful definition of idempotence is

         .---.        .---.
    x --&gt;| f |-- y --&gt;| f |--&gt; y
         '---'        '---'

where x and y can be values or states, or whatever else. Let's leave it at that.
&gt; No other mail client will connect to exchange except outlook.

Are you sure? That sounds a bit overstated to me, but I have no experience otherwise.

&gt; Vendor lock is a bitch and you are now owned by MS

I use Linux every day for a range of web applications, and small scale mail. For real business-use email, Exchange rocks and has no equivalent.
Easy to say “D’uh”, but it does not seem like you read the article. Quoth SCO:

&gt; We believe that the inclusion of our UNIX code and derivative works in Linux has been a contributor to the decline in our UNIX business revenue because users of Linux generally do not pay for the operating system itself, but for services and maintenance.

If such theft had happened, wouldn’t *all* UNIX vendors be affected? That’s the question being asked.I completely agree with you and I think that's the point we were trying to get across in the article.  I apologize if anyone thinks we're digging at Linux - we're not.  Hell, I'm working on a major Linux sideproject myself.  

-- Brian Boyko
-- Editor, Network Performance DailyIn the Java world working programmers are second-class citizens, not trusted by the language implementors to be able to do the right thing and so not allowed to overload operators themsleves. 

They have to wait for the implementors to figure out a "safe" implementation of whatever it is that the programmers really want and grant that marginal boon to the working programmer. You want templates? Have generics. You want metaprogramming? Have annotations. You want operator overloading...they've yet to find a way to blunt it to the extent that you silly wokring programmers won't hurt yourselves without just turning it off, so no can do for now.I don't think his interpreter hashes by value, doing `bar = "some"; bar += "word"` would create a new object with a new ID in the OCaml system, causing the key lookup to fail.I agree here - if there was an Exchange equivalent, I'd love to know about it.  Feel free to give me an e-mail:

brian dot boyko at netqos dot com
-- Brian Boyko
-- Editor, Network Performance Daily&gt; Bah. The only useful definition of idempotence is

Argh! No! :) That applies *only* when _x_ is a universe monad.

&gt; The one below is idempotent, but isn't compatible with your diagram.

No, it is referentially transparent, where _y_ is a fixed point of _f_.

&gt; Would you say the following is not idempotent?

Hmm, I’m not sure about that one, actually. I would say no, but I’m just going by intuition on this point…&gt;     listToMap ((k,v):xs) = Map.insert k v (listToMap xs)
&gt;     listToMap []         = Map.empty
&gt;
&gt; A Haskell programmer with a little bit of experience will
&gt; notice that the above function looks almost like a fold from 
&gt; the right of a list.

A Haskell programmer who doesn't care to reinvent the wheel will realize that this function already exists in `Data.Map`

    fromList :: Ord k =&gt; [(k, a)] -&gt; Data.Map.Map k a

&gt; It’s possible to pare readConfig back even further, so that it’s entirely point-free, but this renders the code more confusing, at least to my eyes.

I'm not sure that I understand the point of even demonstrating this at all in a blog posting about using Parsec.Actually - that was me, not Jim.  I try to summarize everything in the first two paragraphs.  I come from a print journalism background, and I also know that on our main page, www.networkperformancedaily.com, only the first two paragraphs would show up, so I wanted to pack as much information in there as possible. 

-- Brian Boyko
-- Editor, Network Performance Daily.Actually?  It has.  That was the only photo we had available of him, and he was, indeed, looking backwards at the camera.  Maybe not 180 degrees, but a good 90.  

I asked him if he wanted to retake another picture - even had the camera with me and he said: "What's wrong with the one we've got?" 

I said "Okay, Jim" and tried my best to make the picture look okay for publication.  

-- Brian Boyko
-- Editor, Network Performance Daily
You'd think that would work, but I think they might get suspicious at all the uptime...I agree here - OpenOffice is a real bear, enough to the point that running MSOffice in Wine is an actual alternative...

Personally, I go with AbiWord for 99.9% of my actual Linux word processing - it's only when I have to pop open a spreadsheet that I bother with OOO.Technically, his method allows you to later extend the class of Archives without modifying anything here.  However, his "makeArchive" wouldn't work for that.
[removed]I don't know Python, I do OCaml; so I'm sorry if I'm being stupid here. If strings are immutable in Python, `bar += "word"` would also create a new object with a different address. If strings are mutable, on the other hand, the OCaml code would *modify* the value (it's obvious from the code that the record fields are `mutable`), but the address would be the same. Either way, I don't see a problem with it.

This is a completely standard optimization called "hash-consing"; it seems as if it wouldn't work but the key to it is to have "smart constructors" that mediate between the syntax and the actual implementation of the interpreter to ensure uniqueness of values at each step of the interpretation process.

I don't see how his tests would pass if this were to fail, and I can't believe he's so rookie so as to have this kind of bug in his code. Besides, his advisor (Walid Taha) *really* knows his stuff (he has actually a couple of very interesting papers about normalizing translations in two-level interpreters), that's why I don't think there's really a flaw with his approach.Agreed. I posted something about this [here](http://reddit.com/info/012918/comments/c1293f). In my opinion, businesses should follow suit with what the ISPs do. All these ISPs can't be wrong. I would prefer open standards like POP, SMTP, NNTP (private folders), LDAP, and then throw in a virus scanner, spam filter, and webmail interface like SquirrelMail. When you combine Thunderbird with the Lightning calendar plugin, it makes a great Outlook knockoff.

The Exchange Server in my office has too many outages and lockups. And how come typing in a message window in Outlook should lock up when the Exchange Server is having issues on the backend??? Shouldn't they be detached??? That's just plain stupid.that's all very interesting but that white text on the black background was burning holes in my eyesSqueak may have a pastel-like user interface, but that interface hides a tremendous amount of power not so far underneath that few other environments can match. Direct manipulation of Morphs allows you to quickly assemble a new interface, or investigate how an existing one is put together simply by right-clicking and inspecting. The development tools themselves are highly extensible and in general more powerful than those in other environments. (For example, key bindings can be trivially modified, and users have added Eclipse-like completion and dynamic type inference without a giant overhaul.) Even the pastel colors themselves actually serve a purpose: each major development component (change sorter/Monticello, Workspace, Browser, Transcript, Inspector/Explorer, unit test runner) has a differently colored window, allowing you to tell at a glance where you're likely to find any given component. Once you get over the initial weirdness of the environment, I  think that it's hands-down the most productive Smalltalk environment out there.&gt;&gt; The one below is idempotent, but isn't compatible with your diagram.

&gt; No, it is referentially transparent, where y is a fixed point of f.

The term *idempotence* actually originated in algebra:"A unary operation (i.e., a function), is idempotent if, whenever it is applied twice to any element, it gives the same result as if it were applied once."[(Wikipedia)](http://en.wikipedia.org/wiki/Idempotence) See also [Mathworld](http://mathworld.wolfram.com/Idempotent.html) for a more authoritative source (where if *x* is an operator, *x*^2 = *x* o *x*. It's a common notation in mathematics, e.g. when matrices are treated as linear operators).

What you're thinking of is the special case which commonly occurs in the imperative paradigm, when the argument to the operator in question is, as you say, a universe monad. But the notion of idempotence is much more general than that, which was my point.

You're right that *f* does take every *x* to a fixed point of itself *y*. That's one way to define idempotence. :)The bad thing (or good thing) is that the OSS philosophy implies there is no "lock in" to open source products. 

For example: you won't be forced to use openoffice for the rest of your life (because it uses open standards), you can access ext3 partitions from windows, you can easily port games programmed in opengl-sdl. The nature of Open source makes a Linux-&gt;Windows migration much easier than a Windows-&gt;Linux migration.

This is good, because it is good to have choices. But it is bad, because it puts Microsoft in a very strong position and Linux in a weak position.Admittedly the guy who wrote the article made the same mistake.  A better title would be "Let's redesign the GNOME font chooser dialog box".  It's not a tabloid, it's a blog.  Stick to non-sensational, descriptive titles if you want people to read your article.Just disable the stylesheet (if you're using Firefox, under View -&gt; Page Style).I use Gnumeric for spreadsheets.Thanks for pointing this error out. I've updated the article.http://en.wikipedia.org/wiki/Stalin_(Scheme_implementation)

The small nature of scheme makes such things possible, and there is no reason you couldn't add type declarations to Scheme as an extension.

I think Common Lisp is on a decline because they tried to package up everything into a huge standard (which worked well for unification at the time) that is quite obsolete. (Hello, no standardized socket support?) whereas Scheme in more recent years has been using SRFIs that are a good way of enumerating what an implementation does or doesn't offer and allowing standardized growth without adding baggage to the language.

Also, speed seems less important to most than it used to be -- consider the success of Ruby, PHP, and Python. PLT Scheme is also progressing well, you should check it out -- the community is quite active.That is what I do, but I "saw" white text on white background--invisible.  Oh, well, just highlight it all or copy it to a text editor.

Switching between white pages and black pages is too much for my eyes.  I must not be alone.  Black background is for "arsty" and "music" websites--not tech reading.Those of you interested in the constraint+search part of this solver may also enjoy learning about [the Mozart/Oz language](http://www.mozart-oz.org/features.html) -- look for the Inferencing part of that page.In this case, "all the backend is doing is reading the filesystem, parsing XML, and stuffing it into a database.". That's really not very difficult code to write correctly, and the code, once written, should be fairly easy to modify. The heavy lifting is all handled by libraries, so the Haskell you write should be surprisingly readable.off topic mostly:

i WANT a GOOD text to speech synthesizer so bad! (multiple voices of both genders...all ethnicities...ages...maybe even an ability to 'seed' (basic phonetic sounds recorded and input) your own voices.

i don't see anything like that available though im certain research in such things like modeling the vocal tract on computers is/has been/being done.To be honest, I find that font rendering in Linux (or BSD, don't matter) is awful compared to Windows. Even OSX doesn't seem to be able to render fonts as nicely as Windows does.

Even when I add a Windows Font package to Linux, the rendering is still off, so it's not just the absence of good fonts.

Ordinary anti-aliasing is useless for low DPI displays like PC monitors (in contrast to printers: anti-aliasing looks good when printing, because printers are 600 DPI instead of rougly 72). You just get blurred letters. If I wanted my text blurred, I'd just take off my glasses.

Why hasn't anybody implemented decent sub-pixel-hinting for linux?&gt; The Exchange Server in my office has too many outages and lockups. And how come typing in a message window in Outlook should lock up when the Exchange Server is having issues on the backend??? Shouldn't they be detached??? That's just plain stupid.

That is because Outlook is one of the most retarded pieces of software out there. Try doing *anything* while it downloads an attachment. FFS, it's called threading people, learn to use it.**Users can choose under which license they wish to operate**

[December 12, 2002](http://clean.cs.ru.nl/Download/License_Conditions/license_conditions.html)OSX has more blurry fonts than Windows ClearType. Which is better is personal opinion.

When printing you don't antialias at all.

Linux (well Cairo and probably QT too) can do hinting and sub-pixel-hinting. The problem is the fonts. Windows and Apples fonts are higher quality, which means for example better hinting information. Use Bitstream Vera/Deja Vu or the Microsoft Core Fonts.

If you get bad font rendering you probably haven't configured your font system correctly or use the wrong fonts. File a bug, if you use your distribution defaults.

In general font rendering on Linux is not as good as Windows or OSX, but good enough that most people shouldn't see a difference.Ocaml may be better than Haskell in this case.  Unless you're generous with your strictness annotations, Haskell effectively compiles an interpreter into your executable.  Naive Haskell code isn't all that fast; it's faster than Ruby, but not really up to par with C, Ocaml, or Common Lisp.

Ocaml gives you most of the typesafety, garbage collection, and productivity benefits of Haskell, and it's pretty damn fast.  Plus, people have already done some initial work on [Ocaml/Ruby](http://sciruby.codeforpeople.com/sr.cgi/ProjectIdeas/RubyOCaml) bridges.&gt; They appear to be deprecating Linux support.

Is that anything more than FUD?


"Clean 2.2 is now available for Windows (IA32 and AMD64), Mac OS X (PowerPC), **Linux (IA32 and AMD64)** and Solaris (Sparc)"
[19 Dec 2006](http://mailman.science.ru.nl/pipermail/clean-announce/2006-December/000027.html)yes that's what I did, I just wanted to point it out anywayThe question is not wether to refactor thread-based programs into multi-process apps. The question is how to design it, when you start a new app.&gt; all the lazy functional programming researchers set aside
&gt; their toy languages

I don't suppose there's video of those meetings? :-)The author wrote
&gt; Here is a **non-exhaustive** list in no particular orderEr is it me or would that also work in other voice-recognizing environments such as Opera 9 or OSX?what is that you find confusing about the navigation?I'm getting tired of these. Is there a language that we _haven't_ used to solve Sudoku?I think osx has a password you have to say to initialise the process.And opera does not allow you to delete files asaik.Unless it's a utility function that's used everywhere.  Then which would you prefer to do - fix a bug in 100 locations, or one location?Exactly.  If people went "pure Linux" everything would work equally well as "pure Windows".  OO.org handles word documents (including the "collaboration" features) fine now; so you can still interact with outside firms.  Internally you can use OO.org which I find to be generally nicer than Office.First off, declarations != "type declarations".  Second, there are very good reasons you can't easily add declarations to scheme: it has a formally defined type system which is at odds with how computers actually do numerical computations, that's the big one.  Also, without them being a formal part of the specification, declarations are worse than useless.  They create nonstandard extension crap which is nonportable in that case.&gt; The  Object I/O Library 1.2 is currently only available for the Wintel   platform. We will make a port of this library to the Mac as well. An earlier version 1.1 is already available for the Macintosh (both PPC and 68k). There are no plans to make a port of version 1.2  for the other platfoms.

(from the page about the GUI library)

&gt; The new version 2.0 of the IDE is currently only available on the Wintel platform, a port to the Mac will be made. On the other platforms a much more restricted and older version of the IDE is available which is not discussed here.

(from the page about the IDE)Every time I've tried AbiWord, I've gotten it to crash within five minutes, without really trying. I most recently tried it about a month ago. I've tried Windows and Mac OS X versions. I don't run Linux, so I've never tried it there.There is a VW version of Seaside see: http://www.seaside.st/Download/..to us.  To the typical maintenance programmer it will probably look like Martian.  That's not a dig, I love Haskell, but I do think it's an accurate observation.I think that can be said about a lot of things, not just URLs.HP used to have a solution, OpenMail, that Samsung purchased, but it looks like they have discontinued it.  A quick web search reveals something called Scalix competing in that space now.  

I seriously doubt that if you have the mindset that you must have something exactly like Exchange that you'll be all that happy with it, though.  Nothing is going to look exactly like outlook/exchange, you'll have some sort of learning curve with new features, etc.  When I had to use Exchange from Linux, I either opened up a window to the Citrix server and ran outlook natively, or I used the web-interface, but I didn't have to deal with calendar stuff at the time, so I don't know how that works.  Perhaps you should complain about Msft not having their web-based Exchange interface advanced enough for use in the Enterprise.  They could easily enhance it and anyone with a web-browser would essentially be in good shape for that piece.

If you use MSFT's proprietary document formats for Visio, Word, etc.  I don't see how you can expect it to work that well under Linux unless Msft starts making those pieces of software for Linux.  I still make people use Plain Text as often as possible, or some non-proprietary format when I can, or suffer Windows when I can't.I would program in Common Lisp, but I simply cannot stomach "Lisp-2." By having separate namespaces for functions and other things, requiring abominations like "funcall", it does its best to discourage a functional style. Could a supporter of the Lisp-2 style explain just why it fails to turn their stomach?&gt; Could a supporter of the Lisp-2 style explain just why it fails to turn their stomach?

Sure, just as soon as you tell us if you still kick puppies for fun.Brainfuck?'Haskell is &lt;fill-in-the-superlative-gap-here&gt;'

'Haskell coders are &lt;fill-in-the-above-mere-mortals-gap-here&gt;'

Zero content.

Why did 29 people consider this up-mode worthy?
the first part of my career i was ALWAYS managed by ageing men with absolutely NO formal CS/IT/SE training. they were all fundmentally happy to steal credit for your work as at their direction, while simultaneously blaming you for ANY and ALL things that don't go right. had no talent for timeline creation either...but will sure sue your pants off if they want to patent YOUR work, even drop your name from the declaration page and replace it with their own..

no i don't have much faith in 'the land of the free' anymore...since it doesn't exist, in fact.why don't you email the haymaster and ask why. my guess? they graduated and got paying gigs.&gt; The firm has pointed out that in order for the flaw to be exploited the speech recognition feature would need to be activated and configured and both microphone and speakers would have to be switched on.

AHAHAHAHAHAHAHAHAWhy.... he'll call the guy who wrote it... who will make lots of money on a one-off contracting gig... ;-)Summary:  Managers who start managing in their 30s suck because their career is only in the adolescent phase, and their style of managing reflects this "career immaturity".

My question:  What do we, the worker, do about it?  

I'm guessing they get into manager work because they want to keep moving on to newer &amp; more interesting challenges that their programming careers aren't providing for them anymore, either due to age discrimination from other 30-40 yr old managers ("you're too old to be doing this sort of work"), or the author's premise of the immature attitudes that people around this age develop that they can manage and already know it all.

I really don't care for the author's assessment that it will take a disaster for anything to change.  Small changes, over time, can make a difference in an environment (see: global warming).  What small changes can a person make (other than deciding to wait before applying for a managerial job) to help?[deleted]Now if only the software managers could be convinced of this phenomenon. Sadly, most corp IT projects are function over form, even when the project is sold to/used by non-techies.This post is poorly thought out. In many web applications you are identified by your username, making keeping it secret impossible. And even if not, guessing an existing username is hardly difficult; the sign-up process reveals whether a username has been taken or not. So the entire section on how you should not tell them if they got the username or password wrong is totally wrong headed.

And then he says that your web application will never be secure against brute force attacks, when in fact web applications are the one type of applications that *can* be secure against brute force. Simply rate limit the number of times per second a password can be tried for an account, or lock out anyone who gets it wrong more than 1000 times in a row.

Finally, as long as the user knows that the password will be sent in clear text over email, it's perfectly acceptable to do that. Extremely low security situations, like mailing lists, often use this solution.staunch:  we use single individual RAW IDE disks now.  Your information is out-of-date.  :)

And we do, and always have, used a distributed file system.  But when you're dealing with &gt;300TB in your filesystem, it can be useful to have some of the chunks be large ones with internal redundancy.

I'm not sure what your definition of "storage expert" is, but I likely qualify.  :)

Since I know Jeff Bezos, I'm fairly certain that some "random VP" isn't going to shut down the project, too.[removed]You could define a declare macro and use cond-expand to expand it into (begin) on platforms that didn't support type annotation.Sure, it has value.

It just happens to be a negative value since the article is horribly misleading and only deals with extremely superficial numbers without analyzing how these numbers would actually affect performance of an application once it's gone into production.

One example among many I could choose...

Not to get all Rails-fanboi on you, but he's using it in development mode, which reloads every class on each request so that you don't have to constantly restart your web server when you make a change do your application. The speed of Rails in development mode is simply not a relevant factor in evaluating it as a framework, yet it's the only number which he's chosen to use to represent Rails. He notes that the speed (in development mode) has decreased in version 1.2 without even trying to note why: the reloading of changed files now works for your support and configuration files rather than just your application code.Indeed. I've yet to find a time when gnumeric doesn't work better than OO.o for spreadsheets. It seems to have better Excel compatibility, and it's much faster to boot.Using trees always needs recursion. If you use algebraic datatypes and have no recursion you run into the same problems.

So it's not the problem of the relational model, it a problem of the query language. 
Try to say with a straight face: "This speech recognition exploit is not dangerous, because it requires speech recognition to be enabled in order to work". Being a PR person is no easy job...I don't do a ton of word processing these days, but when I was using it on Linux last it was rock-solid and fast.My understanding is that Lisp-2 enables optimization of function call speed. In a Lisp-2 all places where a function is being invoked are explicitly marked so you can do a few speedups using this approach. Static function calls no longer require type checking (merely checking that the symbol is bound in the function environment ensures that it is one) which can yield some speedup along with a few other small things. Note that Lisp-1 systems can internally be Lisp-2 and gain benefit from these with simple code rewriting.D perhaps?I switched a carpet shop to OpenOffice when I upgraded their server, database, and workstations. They were none worse for wear and have never called me for tech support with it. What if system integrators around the country started offering this for free with small mom and pop businesses? That would be a huge chunk of change that M$ wouldn't get. Kewl.The 100 integer optimization is actually part of CPython, and that's where I got the idea.

We are already aware of the hash-by-objid issue, which was brought up during the talk.

That makes plenty of sense.
It just seems to me that if you're going to sacrifice flexibility and elegance at the altar of optimization, you might as well have static typing.Grobner bases and elimination theory is a natural generalization of solving systems of linear equations via row reduction. Just like every programmer should know the latter, I think the former can be a useful part of your arsenal too, especially if you end up writing code for geometric or simulation applications.There's [Citadel](http://www.citadel.org/doku.php?id=start) for people who want it easy, and also people can install [Ubuntu Server](http://ubuntu.com/) and then add the POP, SMTP, NNTP, LDAP, spam blocker, virus scanner, and webmail client. To administer the LDAP, use one of many free web interfaces to manage it. There are several docs that people have written on the web on how to build a mail server on Ubuntu Server and manage it with relative ease. You can also build a Linux cluster and syndicate the LDAP and email with a cluster at various regional locations across your company's geographical area. This is what the ISPs do, although they may be using RedHat and Solaris. (But I prefer Ubuntu or any Debian server.)
Managers should follow this simple mantra: accept the blame, pass the credit. The key to making this work is creating trust that the other managers who take the credit and pass the blame will be correctly identified as such and thrown out on their lying asses. The reason it doesn't work is that most managers and the middle management who manage them follow the reverse mantra and are continually rewarded for it instead of the opposite. Why? Because nobody really cares if the employees are happy, because 90% of us are fully replaceable.1.) Amazon's S3 service is not in beta.  The term "beta" is so overused these days anyway.  How many of us trust our entire online correspondence to Gmail, also in "beta"?

2.) Jeff Bezos has given SmugMug's Don MacAskill his personal assurances that S3 will not be cancelled on a whim.  You can't get much better than that.  And again, most web businesses are built upon platforms that aren't entirely controlled internally.  Your hosting provider could just as easily decide to cancel on a whim.  Given the shaky financials and poor customer service reputations of most budget hosting companies, I'd be far more worried about that than about Amazon going under.[deleted]the article does not even contain the word database. dont't waste your time because from a real world point of view this discussion doesn't even happen.There's no hole. There's just speech recognition. This is how speech recognition is supposed to work.

A more plausible attack involves really loud audio that makes the victims head asplode.I have two speech recognition holes in my head.[deleted]&gt; Is that anything more than FUD?

Do you count as FUD the fact that when I try to "register" just to download the stuff (register for a language? that's a stupid idea BTW), I'm redirected to a page that says: "thank you very much for your registration. It has been posted to the Clean team" Where is the link? What can I do? Where is the source? Where are the specifications? The answer to all these questions are: nowhere. This language is dead.Ah, extensibility via inheritance. That would be nice to have. Is there no way you can do that in straight Haskell?I generally pull my mail down with POP from the work Exchange server.  When I can't, for whatever reason, I use FF/Opera/Konq to access OWA and it's...icky.  The other day, I was already using IE on a Windows desktop, and wanted to check my mail...  The user experience is *significantly* different with IE6 than any other browser when accessing OWA.  It's a completely different beast.  Not that I'm surprised, but it was enlightening.Opening .doc files that were editted in OO.o, I've noticed they look like crap in Word still.  It's really apparent, at least using the layout I have to for reports, that I didn't use Word.  I'm also still seeing the ballooning in file sizes that was mentioned in the article.  One week's report, done entirely in Word, is 59904 bytes.  The next week, which was almost an exact copy, done in OO.o, is 112640 bytes.  Wish I knew what I was doing "wrong", as opening that in Word and doing a "save as" shrinks it back to almost the same size as the Word-only version.It's not dead. It's quit popular in Japan. It's just being maintained by a very small community.

You can download Clean [here](http://clean.cs.ru.nl/Download/main/main.htm). It is not necessary to complete any registration before downloading. You probably couldn't find the link because the website is set up pretty bad. (It appears in the menu on the left when you click "Download Pages" -&gt; "Download Clean")

Please note that you can download the entire source code (compiler + libraries). If you want specifications you should download the [Clean Language Report](http://clean.cs.ru.nl/contents/contents.html).(defun frob (list)
       [frob list while still using the function list])

Can a supporter of Lisp-1 explain why variable names such as `lst` fails to turn their stomach?

Seriously, most people have no trouble with using associations with symbols when adding new features (e.g. exceptions -- a real lisp-1 would shadow the variable `tag` when using `tag` as a catch tag). Lisp-2s simply extend that idiom to functions.

EDIT: markdown. BTW, dear downmodder, there is such a thing as sarcasm. *goes back to working in gsi*Is there any standardised language that defines how sockets work?I would program in Scheme, but I simply cannot stomach "Lisp-1."  By having a single namespace for functions and other things, requiring abominations like "lst", it does its best to discourage a clean style.  Could the supporter of the Lisp-1 style explain why he/she begins with the assumption that Lisp-1 is absolutely the Right Thing, "funcall" being an abomination, the conflation between functional style and higher-order style, why the separation "does its best" to discourage this style, and his/her method of asking complex questions, apparently only to argue about the answers?

On second thought, nevermind.  That's not really interesting because this horse has been dead for years.&gt; For example: you won't be forced to use openoffice for the rest of your life (because it uses open standards),

Let me know when that actually becomes a reality. From what I've seen and heard, ODF is still a long way from being a complete standard and most implementations have to reverse engineer OpenOffice.
Either way but it seems that there is little pratical evidence that you do get more chance of problems if you careful design around thread at first. For some reason people who run into issues when they deal with threads always curse threads themselves but rarely their own design.Hmm...
This is all well and good, but before Java spends too much time implementing this they really should look into handling some of the basics like delegates and real event handlers. You know, the stuff that actually makes working with Java hard.Part of it is that, if you want to be in the C suite by a reasonable age (i.e., before you retire or die) you have to pass through certain stages and that dictates a schedule where you are a manager in your 30s.They need to rip their website out by the roots and install a [Trac](http://trac.edgewall.org/) wiki or something similarly modern.Ummm POSIX compliance?  Not a language per se.  It wouldn't really be the job of a given language though anyway as networking and drivers are OS level concerns to my way of thinking.So I missed the part of the article that explains how experienced programmers become competent managers after 20 years of writing code. Do they wrap us up in a silk cocoon and we emerge as managers? 

Now, I must say I have had managers that resemble butterflies, flitting from cubicle to cubicle. But mostly they seemed to be sucking the will to work out of the people they visit. 
oh stupid me. i was looking at the final code sample, thinking that it was his assertion that each solution was "better" tahn the one before it. so i took this solution as his 'final' one -&gt; 

-module(euler_1_foldl).

-export([start/0,solve_euler_1/1]).

start() -&gt; io:format("~w~n",[solve_euler_1(1000)]).

solve_euler_1(N) -&gt; lists:foldl(fun accum_fun/2,0,lists:seq(1,N-1)).

accum_fun(X,Sum) when X rem 3 == 0; X rem 5 == 0 -&gt; X + Sum;
accum_fun(_X,Sum) -&gt; Sum.

btw, how did you change the font to be fixedsize for the code? i must have missed that in the markdown docs.

--vatThe *real* difference between CL and Scheme (and why I stopped being a Schemer and became a Lisper): CL has *heavy duty optimizing compilers* with declarations. A good few of them (SBCL, CMUCL, OpenMCL, etc.) are free.

Most Schemes are byte-code interpreted. Everyone and his goldfish has written a Scheme intrepreter. Type declarations, when they exist, are non-standard. Garbage collection is not as finely honed. Scheme is not heavy duty. When Scheme is heavy duty, it is not portable.The argument seems to be that IPC is more robust and less complex than shared memory schemes.  I don't have a well-formed opinion on this, but the debate should focus on the differences between IPC and shared memory.The absolute worst managers I've dealt with are like this.  They take perverse pleasure in "playing house" with their departments and/or teams.  They play the "parent" and their workers are supposed to play the "children."  I believe this is where you get your petty office politics/backstabbing from.  Adults don't like being treated like children, and tend to resent the person treating them that way.  The result is bad morale, little work being accomplished, and  fiefdoms being created all throughout the office.

I think the managers act this way because they are used to dealing with children, and assume that they can motivate people with the same techniques.  Perhaps money could be made finding a way to attract managers with these habits to some sort of reward system that would encourage them to treat people as peers?  Is that a little too optimistic?No, this is a legitimate hole (though it is a rather unlikely one to be seriously exploited). Anyhow, it can be fixed: if the OS is playing a sound, _don't try to recognize voice commands until that stops!_ Well, except maybe, "Stop playing sound." But certainly allow nothing else.Just to state that this exchance one of the nicest on reddit for a while. Thanks folks.Not without working around the type-system.  That's what existential types (related, rank 2 polymorphism; meaning quantification within the lhs/antecedent of the function type/implication) gives you: modularity.  If you have a copy of TaPL around, Pierce covers it.

However, adding existential types to H-M makes type inference undecidable in the general case.  That's why SML's module system *requires* type annotations in module signatures, but nowhere else.  Haskell doesn't, but you're probably going to have to stick them somewhere, so best practice is to annotate top-level "exported" functions.

I think most people are willing to accept the trade-off, so the next Haskell standard will have existential types (I believe).
Which takes us from 
&gt; They appear to be deprecating Linux support.

to - some years ago they deprecated Object I/O Library support apart from Wintel (and someday Mac).Because emacs already does all of that in a way that lets you customize it to fit your needs perfectly.I don't see how that type guarantees anything like that - the IO action can be invoked several times, and you can get and put stuff into the resulting MVar as much as you like. Maybe there is some other encoding - should ask this sort of thing on the Haskell mailing list.Does yelling FORMAT C! in a room full of computers count as a hole?I'd probably do it something like this:

    makeSearch msg = do
        args &lt;- mapFstM getInput [("from", from), ("subject", subject), ("body", body)]
        return $ and $ catMaybes $ map (\ (ms, sel) -&gt; fmap (flip search (sel msg)) ms) args


I really dislike splitting the names and the selector functions.  That way lies mismatches lengths.
Oh, and mapFstM is one of those missing functions I use a lot:

    mapFstM :: (Monad m) =&gt; (a -&gt; m b) -&gt; [(a, c)] -&gt; m [(b, c)]
    mapFstM f xys = mapM (\ (x, y) -&gt; do x' &lt;- f x; return (x', y)) xys
These types of manager-slams are getting really, really tiresome.  They are so limited in their understanding of what makes an office or project go that it's almost sickening.

Code-monkey is so into his own little universe that he doesn't see the whole picture.  If he did, he would be manager, not code-monkey.  I am a 12 year coder who's taken the natural transition to project manager - so I know what I speak.

I would love the bitching code-monkey to have do all his coding tasks, and on top of all that, take on all the tasks that a manager does on a daily basis.

I'm talking about documenting customer requirements, coordinating all arms of a development team, liasoning clients and 2nd, 3rd and 4th parties, scheduling, planning and allocating resources, politely listening to your clients while they scream and call you and your whole team a bunch of idiots, motivating your team, ensuring the budget doesn't get blown, fire-walling the team from asshat VP's who want to get 'involved' in the design of the project, the list goes on and on and on and on.You don't need explicit type annotations to be statically typed. All of the good CL compilers do as much static type inference as they can, but, unlike a statically typed language cases where the inference engine fails and the program is rejected, inference engine failures are handled at runtime.

Keep in mind that CL is Lisp-2 (really Lisp-n when you count the tagbody, type, etc. namespaces) primarily because Lisp has traditionally been Lisp-2, and CL needed to easily run most existing code with minimal modification (hence *Common* Lisp).[removed][removed]Actually, the best managers I have had only knew the fundamentals of programming.  For example: they may be able to write an Excel macro or make a "Hello World" in VB.  The worst managers were always those with a large programming background.

I will explain.  We will call the less programmer as Manager A and the other as Manager B.

Manager A has no idea how long a project will take.  Because of this he comes out of his six hour meeting, bleary eyed, and asks the developers.  The developers huddle and produce a number.  Manager A smiles and goes back to meeting.  Manager A will later happily produce a constant stream of Excel documents and update the MS Project Plan.

Manager B comes out of meeting.  He's already grumpy because he would rather sit and write code, and all these demands on time and cost bother him.  He looks at the developers and wonders "why should I even ask the twerps?  I have 20 years programming experience and they might suggest using Ruby or something.  I know that C++ is the best option PERIOD".  Manager B turns back around and goes back with HIS figure.

Now, Manager A may get cocky and not talk to his developers.  That only works so many times for these types.  Manager B types will ALWAYS think they know what is best.  ALWAYS.I filed a bug on this 6/7/2006. They never accepted, acknowledged, or commented on the bug ...

http://derekslager.com/speech_recognition_feedback.jpgOk, i hate to feed a troll, but tell me how you think a lisp-1 is more 'flexible' than a lisp-2.. seriously.

Also, performance is not the only reason to prefer a lisp-2. All the effort in scheme-land on hygienic macro systems, for example, are mostly irrelevant in CL due to its lisp-2 nature.



Good old-fashioned Holy War.

More seriously, on second thought it seems like this is exactly the sort of problem smart IDEs might be able to solve. Why should names be such a hang-up?

Why is it that almost everyone is turned off by the notion of representing programs as something other than plain text? One might, for instance, have a keyboard command that toggles a symbol between the function and non-function namespaces in a Lisp-2, with a color change to indicate the difference. 

Now that I think about it, could this be done as an Emacs extension, with ordinary Common Lisp code becoming somewhat Scheme-like in appearance (colorize the namespace differences, hide funcalls, etc.) ?Yeah, see, YOU'RE not what the article is about.  This article is not a personal affront to you.  This article is about what makes a *bad* manager, who *doesn't* do the stuff you mentioned in the last paragraph of your post.  I'm hoping the discussion around this topic could help in a communal thinking on how a code-monkey can steer (or at least cope with) a bad manager towards doing this stuff, instead of making the code-monkeys do it and blaming them for not doing his job very well.[removed]As someone this author is describing, I'll tell you why: as a programmer, even a senior one, I had little actual control over my projects. Deciding how to implement a functional spec is not control, nor is making the decision between ArrayList and Vector for a particular collection. I made the move because I wanted to have some say in the problems I got to tackle, who I got to work with in tackling them, and when to expand or contract the project scope for when we're trying to build a better mousetrap or just patch a leaky dam.

Good software developers are people who scratch their own itch. The ones who spend their career as a leaf node on an org chart focus on the action of scratching,; the ones who move on to management are interested in where the hell these itches are coming from and what might need scratching when we're done with this one. There's no one right or wrong way, just whichever focus the developer prefers.

Sure, people who've been "in the trenches" tend to make better managers - but why 20 years? What do you learn about an environment in the latter 15-20 years that you don't learn in the first 5? Fuck it, why not 30 years? 40? Let's make it so all we have are awesome managers who've been coding for 40 years. Of course, there'll be 5 of them. Why? Well, some guy said so, who "talks to his code". (Seriously, did anyone else snicker a little when they read that?)

Good managers often have very little to do with their experience in a given industry, and more about how to manage client expectations. Depending on the situation, those clients can be another group, the CEO, end-users. I've had tremendous managers who barely knew how to code; they knew that their people understood better, and they provided for their people's needs. I've also had managers who had their requisite 15 years experience (though by this guy's point of view that's obviously too little), and I never got micromanaged worse - in fact I often had code written for me by this guy when I'd simply say "function x is taking a little longer to implement than I thought, but it's going". It has nothing to do with experience, and everything to do with knowing when to steer the ship and when to the let the people you hired do it for you.Premature optimization is the root of all evil?[removed]That's a pretty blatant troll. Have you no shame?If you can do that, you have physical access to the machines and could probably take a baseball bat to the HD as well. So it's not a huge threat as far as it goes. That said, "Format C" shouldn't work without a password of some sort. That's a basic don't-always-run-as-superuser issue.[removed]Ah, we're arguing about two different realities. [Please read the author's comment: Development mode was a misprint.](http://programming.reddit.com/info/11otm/comments/c11rtz)The author added, "I can't believe I have to write this."If you want to grok why Lisp-2, you first must understand that Common Lisp is not Lisp-2. It is Lisp-n where *n* is 9 for symbols in Common Lisp package (go tags, block names, loop keywords...) and can be arbitrarily more when users put more namespaces into action. Common Lisp just uses the same convention as human languages overloading the meanings of words according to type. Btw. C has different namespace for struct names and Java has different namespace for methods and variables and no-one notices.

If you really want to program in Common Lisp you must stop programming in Scheme style. Common Lisp is multi paradigm language and it's design choices reflect the fact. Just because both use parentheses does not mean that the languages are similar at all.
Here's the thing that makes me mad: we were promised that this time Microsoft was really, really, _really_ serious about securities… for reals! And yet here's this blatant bug. 

Yes, it's a bit subtle in that I wouldn't have thought of it on my own since I don't use voice recognition, but if Microsoft was serious when they said that they've now got religion about security, then the people who programmed the voice recognition should have thought to themselves, "Hmm, are there any security implications to this? How can we minimize them?" Which clearly they didn't, otherwise they'd have speech recognition turn itself off when you hear a Flash video or whatever. 

On top of all that, we have your bug report, so MS can't even claim ignorance. They had clear warning that this bug exists and they didn't do anything about it. This makes me think that the whole song and dance about "Most Secure Version of Windows Evar!" is just smoke and they still Don't Get It when it comes to security.Seems like apple made this same mistake about 10 years ago. http://programming.reddit.com/info/12cru/commentsOld news to me, but it's good spread the word, I guess.Waxing gibbous comes early this year, Ma!People try things and sometimes they turn out to be good, and other times it turns out something else was better.

dynamic =&gt; lexical scoping

lisp-2 =&gt; lisp-1

message passing oo =&gt; separate generics

...&gt; rate limit the number of times per second a password can be tried for an account

If I recall correctly, when you are using HTTP authentication, each request is authenticated separately - so you could be talking about dozens of requests in the space of a few seconds for *legitimate* traffic.

&gt; lock out anyone who gets it wrong more than 1000 times in a row.

Sounds like an incredibly easy way to piss somebody off if you know their username.

&gt; Finally, as long as the user knows that the password will be sent in clear text over email, it's perfectly acceptable to do that. Extremely low security situations, like mailing lists, often use this solution.

The problem is that people can and do re-use passwords, meaning that the low security for things like mailing lists translates into low security for every web application they use.
my head feels stretched!I've read a lot of "CSS Hints You NEVER Heard Bout!!11!" articles, but this is the first one to feature stuff I've actually never heard about.&gt; (+) Code = data (outweighs everything else)

Agreed!

&gt; (-) Spec missing various bits, w/ no hope of ever changing.

I don't know about you, but i find it really hard to program with a specification. The implementation i use, OTOH, is very complete and is changing and growing at an incredible speed. Perhaps your problem is that you've been trying to program using the hyperspec! I'd find that a trainwreck as well.. try going to http://www.sbcl.org and downloading a compiler for your platform, i think you'll find it quite complete, and a lot easier than a specification for creating executable programs ;).

&gt; (-) Balkanization of implementations

 (+) Diversity of implementations. I think you are drawing lines in the sand where there are none. Different implementations serve different needs, and i am not aware of a widely used language that does not have multiple implementations with different purposes.

&gt; (-) Hit-or-miss libraries

And this differs from any other language how? The CL libraries i use are all high quality. Regardless, i don't see how the quality of existing open source libraries (commercial implementations usually provide very complete libraries) makes the language itself a trainwreck.

&gt; (-) Clumsy package system

There are some issues with the packages system, agreed. Do you have any specific gripes? I've found that there are various libraries out there that have solved any issue i've ever had with packages (a rare occurrence i admit).

&gt;  (-) Prefix-only notation

(+) Programmable syntax. Prefix-only notation? Gimme a break, you must know this to be false. There have been infix packages for CL as long as there has been a CL. In fact, one of the defining features of the language is that it makes such things possible. Example :

CL-USER&gt; '#I(if x&lt;y&lt;=z then f(x)=x^^2+y^^2 else f(x)=x^^2-y^^2)

=&gt; (IF (AND (&lt; X Y) (&lt;= Y Z))
         (SETF (F X) (+ (EXPT X 2) (EXPT Y 2)))
         (SETF (F X) (- (EXPT X 2) (EXPT Y 2))))

None of these point really say 'train wreck' to me. If anything, CL is one the the better and more cohesive general -purpose languages out there. The designers worked very hard to support things like infix syntax and almost infinite expressiveness while still maintaining some backwards compatibility with older lisp systems. I think they did a wonderful job! 




What's this 'we' and 'you' bullshit? Or for that matter this 'purposfully allowed' [sic] bullshit?Here's the exact quote:

&gt; However, the backend is all about throughput, and the added overhead of running Ruby is quite noticeable. Doubly so because it uses Rails' ActiveRecord models, which spend a lot of time doing reflection on the fly.

Summary:
Ruby = bad
Ruby + ActiveRecord = worseI've always understood those log entries to be the user typing in the url manually.

I am interested to here of reasons why a browser might not send the header though, I've heard it said that they can't be relied on quite a few times.[removed]Do you happen to know of any good unicode Linux fonts with [good coverage](http://www.cl.cam.ac.uk/~mgk25/ucs/examples/UTF-8-demo.txt ) of things like the mathematical operators, brackets, arrows, combining chars, etc. (in monospace/fixed-width for use in xterm)?  

 
            ⎧0               if n = 1;
     F(n) ≡ ⎨1               if n = 1;
            ⎩F(n-1) + F(n-2) if n &gt; 1.

    ⎛ ∇∙D⃑ = ρ         ⎞
    ⎜ ∇∙B⃑ = 0         ⎟
    ⎜ ∇×E⃑ = -∂B⃑/∂t    ⎟
    ⎝ ∇×H⃑ = J⃑ + ∂D⃑/∂t ⎠

         ⌠¹
    π = 2⎮ √1̅̅-̅̅x̅̅²̅̅ dx
         ⌡₋₁

     ⎡1 0 1⎤ ⎡î⎤
     ⎢0 1 0⎥ ⎢ĵ⎥
     ⎣1 0 1⎦ ⎣k̂⎦

    Γ ⊢ t:S    S&lt;:T
    ―――――――――――――――  (T-Sub)
        Γ ⊢ t:T
So he's been thinking about it for ten years and the best he can
come up with is, "People in a group I don't belong to are ignorant
and arrogant."

Maybe he should adapt his own advice and think about it for another
ten years before he announces he's figured it all out.
[removed]kind of an unintelligible linkjack of a good IBM artice and a chapter from a book.

http://www-128.ibm.com/developerworks/java/library/j-jtp01274.html

http://www.oreilly.com/catalog/hardcorejv/chapter/index.htmlIt does do funny things, like incrementing the address pointed to by shizzle by 2. That right there is going to be one hellish tracking job to find and fix, as there's no warning for that, especially if the first member of Foo is a short.I'm not sure of that either, but it's an evident. I get to see it a lot in my logs.

Maybe an evil browser add-on or some privacy tool does that.Then somebody will yell "Delete My Documents" instead. The my documents directory is the only valuable directory on the HD, and yet.

We're not dealing with a security hole here, but simply with the essence of speech recognition.

A simple command should never do irreversable damage. Neither by Mouse, Keyboard... or, you guessed it, voice.[deleted]But how would you fix this? This is indeed a "security hole", but it is by no means a bug or a defect. 

I mean, think about it: "Oh no, malicious users can log in as an administrator if they have the right password." Your house has a security hole. "Oh no, someone could smash in the window glass and crawl through the window and steal stuff." There comes a time when you just have to use a little common sense and accept that the world is not 100% safe from every minute exploit.They did know about it. They also did consider the implications. They just don't consider it a bug.

What do you suggest? That every command is followed by 'Are you sure?'? That won't help. So what exactly do you propose? Leaving speech recognition out altogether?ask him if he'll make me a burgerSmarter?  Certainly.  A better person?  Not remotely.But someone could not remove all your furniture by standing 5000 miles away and saying "delete furniture". You would fix this by using a password.[removed]You have forgotten Chez Scheme. A *heavy duty optimizing* Scheme compiler.
[removed]&gt; they'd have speech recognition turn itself off when you hear a Flash video or whatever.

Or just outsample anything that the computer is sending to the speakers. That is, the computer knows (or damn well should know) what's being played. Make sure to used those waveforms as a mask against the speech input.Of course. If you look closely, you'll notice it was a documentation bug, and one that is specific to the tutorial (a one time occurrence). Since most people start with speech recognition by taking the tutorial, I thought that was an excellent place to warn people of the security implications.

A big bonus would be to give people the ability to trade some usability for security (password, etc.), but again -- the bug was simply to add a warning to the tutorial.I don't agree with some of his design decisions, but his CSS techniques are solid.

But hey, I'm the kind of guy who likes my screens to be light-gray-on-black.Good stuff - we've used the Monte Carlo method at work for backgammon, and it's nice to finally see good computer Go players...indent 4 spaces for code. (how did you get a less than sign?)

&gt; thinking that it was his assertion that each solution was "better" tahn the one before it.

hm... maybe i should make it more clear that the subsequent solutions are simply different and not better?First, few people use Stalin. It is a batch mode compiler. The beauty of Lisp (any Lisp) is its friendly, interactive, modifiable nature. With Stalin, you get fast *instead* of friendly. With (say) a Lisp like SBCL you get fast *and* friendly.

Next, scheme with extensions isn't scheme any more. 

Sure, "there is no reason you couldn't add type declarations to Scheme as an extension" but who is this *you* of whom you speak? I, for one, don't want to write (yet another) new, improved, best ever scheme implementation. I just want to pick something up and use it. And takes many man-years to produce a good Lisp/Scheme *compiler*. One of the reasons I switched to Lisp is that I got tired of waiting for that miraculous scheme compiler that this mysterious *you* person was going to write *real soon now*. 

Sure, speed doesn't matter all the time. If you need a computer 10% as fast as its potential, fine. Many applications are like that, or are bound by other bottlenecks. If you do need more than 10% of your machine's speed, better get a real compiler.[removed][deleted]I doubt 99% of C programmers care, or even need to care. 32 bytes is almost meaningless these days, and without benchmarks I have to assume the speed difference is trivial in all but the simplest programs.*After all, how hard can it be to program some lists of names that drop down when someone types a dot?*

Yeah, what could be so hard about writing about 1/3 of a compiler, that is connected to real-time lexical input, and then wire that up to a constantly updated database of language and library contructs?

*Ruby is a so-called "dynamic" language, which is a polite way of saying that it's hugely unpredictable.*

Uh no.  It means its costructs are not all resolvable at compile time.  That may seem "unpredictable" to a static point of view, but it's not.  And btw, that isn't a bad thing...

*If we'd been developing an IDE for a more traditional language – C# or Java, say – life would have been so much easier.*

Traditional?!?  You mean like Smalltalk or Lisp?  At least if C was the example there'd be enough longevity to make a comment like that without having to defend it up with: "Well they have an Algol style syntax and that's old..."
I like that he cites some of the sources for his practices/suggestions.It took me a little bit to figure out what part of the navigation was the "coolest navigation ever".

It's good. But I have seen better.I stand corrected! I had the bandwidth and storage charges reversed."Search for porn!"
"email porn to me!"



Is that a tutorial of Haskell, monad, or incoherent bubbling ? I don't know any Haskell and that "tutorial" sounds just like incoherent babbling to me.

I think this is what's wrong with all the functional language tutorials/introductions I have read (or maybe the languages themselves): either they're incredibly simple (like a factorial function) that doesn't show the language at all, or they're just incoherent babbling.OMG, this could have some hilarious consequences. Just visiting a website has never been more dangerous for Windows users. :Dbody { display:none; } and be done with it!

[removed]LAN parties are going to be much more fun in the future!Posted 7 days ago, the discussion is [here](http://reddit.com/info/10q1x/comments)

Edit: Wrong URL, ooops!!!Effective technical management: Hires experts, _listens_ to them, understands what they are saying at a high level, and rationally combines that information with tactical business information to form a strategy. Does not begin/end projects without a (flexible) definition of said strategy.It wouldn't. Haskell would be used to populate the database.

Ruby and Rails would then be used to read from the database. The connection between Ruby and Haskell here is shallow at best.

Basically the author of the article is saying: "Haskell exists".You are aware that

- some people use both MS and OSS tools
- your average computer user or small business hasn't 'purposfully' chosen MS over OSS
- one of the side-effects of snorting ESR is a messiah complex?

did someone replace Reddit with Slashdot while I wasn't looking?Wrong URL?Seems a reasonable introduction to typical Haskell development: write something naive, refactor till its one line. Repeat. And all with humour. Good contribution.Incredibly unsafe.This article is about _applications_, not operating systems. MS Office != Windows, this StarOffice/OpenOffice != Linux.

Won't fix everything...my wife and I keep our desktop computers within a few feet of each other. There's no way to prevent them from "talking to each other".[deleted]&gt; Next, scheme with extensions isn't scheme any more.

Rubish. It may  not be R5RS Scheme, but it is still Scheme.
Yeah, but it's a rough introduction, he doesn't try to be gentle about it.

I think I prefer that though, after one or two brain explosions like that you nearly don't notice them anymore, and it tells you whether you really want to study haskell.

If you don't (can't bear it, don't have the time, can't afford to have your brain explode or melt) just bookmark the page and come back to it later, when you've put your helmet on and are ready for some wholesale brain rearranging.

i'm looking forward to his next installments.Yes but you can make value judgements about what goes into those holes. If a computer tells you to "erase childhood memories", you'd probably decide not to. If computer A told computer B to do so, then you've got a good recipe for  acute amnesia.This was before the video camera was invented. ;-)You should be thankful, it's supposed to explode, as remarked by the population of #haskell:

&gt; "I like how you conveniently gloss over the part where your head explodes." -- cjerisThis could be quite dangerous for the end user...  "Let’s set so double the killer delete select all ..."[deleted]I'd like to see other navigation that is as novel as this - can you show me examples?[Technical Issues of Separation in Function Cells and Value Cells](http://www.nhplace.com/kent/Papers/Technical-Issues.html)OISC machine with "Reverse-subtract and skip if borrow".

Have fun!It looks very nice. It is not free (libre or gratis), alas. The interpreted version is free. 

And it isn't really portable - I can't write fast code and move from Chez Scheme to Foo Scheme. 

I can move from SBCL to Allegro or LispWorks or MCL, however.

In the time that the scheme community has produced dozens of lightweight implementations plus one or two heavy duty commercial implementations, the Lisp community has a handful of heavy free compilers plus a handful of heavy duty commercial compilers.Parity: All programmers should have to do some hard time dealing with people and politics.

Strings in python are immutable.&gt; And we can concentrate on making them all faster at the same time.

[Ask and thou shall receive](http://www.parrotcode.org/)Yeah, the hashing thing was an oversight that can be readily fixed.  My main point is about what he is creating.  It is basically a reimplementation of CPython; it isn't doing function specializing, etc., like psyco.  I don't see him ever gaining any performance over CPython.[removed]You've clearly never tried it. Seriously, once you go puppy-kicking you never go back. :-PMy comment was about the linked article, why would you get confused about *that*?

I made that statement in good fun. All I meant was that the author seemed intent on justifying Haskell as the clear and obvious solution to a problem that was probably just as easily solved a number of different ways. He's enjoying Haskell -- and that's great. He can just say that, no need to beat around the bush.In Stalin's case, the more pressing issue is that Stalin doesn't support all of R5RS, but just an optimization-friendly subset. Most of the time that's just fine, but what happens when you want to use some code that depends on language features Stalin doens't have?Don't skip the comments on the page.Java has this. When I took networking in college we weren't allowed to use Java for the course projects because it made everything too opaque.best comment evar!&gt; My comment was about the linked article, why would you get confused about that?

Because at the time I saw this post of yours dons had just gone on a posting spree and had around half a dozen articles posted by his name on programming/new.

I therefore wasn't sure whether you were talking about the article itself or the fact that a single individual had posted many articles at the same time on the same (haskell) language.

Oh well.[deleted]&gt; I am a 12 year coder who's taken the natural transition to project manager - so I know what I speak.

You're only 12 years old and already in management?Its about time they updated Swing to use the "new" collections API which was introduced in 1998 as well.Good point.  The lack of contractual obligations from Amazon is a pretty significant worry.

That said, contracts don't stop the other party from going out of business.  If they do go bankrupt or otherwise shut down, there's still not that much you can do about it.

Let's say your critical business data is hosted by a third party, like SalesForce.com or Gmail or a hosted CVS service.  You can sue them to get your data back, but by the time a lawsuit works its way through the courts, irreparable harm will already be done.  In theory, contracts and the legal system provide you protection, but in reality it may not do you much good.

Outsourcing critical business functions to a partner is a risk.  But so is operating your own infrastructure.  I've been doing a lot of this risk analysis myself.  So far, outsourcing to Amazon still seems less risky than many of the alternatives.&gt; staunch: we use single individual RAW IDE disks now. Your information is out-of-date. :)

I was referring to the calculation about savings -- which I believe was based on XServe RAID costs. But you changed, so I guess you agree with what I said about that hardware then.

&gt; And we do, and always have, used a distributed file system.

Cool. Did you post about this?

&gt; But when you're dealing with &gt;300TB in your filesystem, it can be useful to have some of the chunks be large ones with internal redundancy.

Why's that? I can see the need for caching the hottest files on faster storage (or in memory ideally).

&gt; I'm not sure what your definition of "storage expert" is, but I likely qualify. :)

You probably know plenty more than most so-called "storage experts". There was no need for me to add that, my apologies.

&gt; Since I know Jeff Bezos, I'm fairly certain that some "random VP" isn't going to shut down the project, too.

That's great for you, but it doesn't help anyone else in evaluating the decision to use S3 or not. It also makes your opinion of S3 subject to a huge bias.Interesting. Lately I've been using [yahoo's reset](http://developer.yahoo.com/yui/reset/).well, how do teams using Erlang manage to achieve high uptimes with such large codebases? it's not like fault-tolerant real-time systems are especially easy to do.Yeah, because anonymously trashing accomplished programmers on reddit makes you a better person.&gt; How many of us trust our entire online correspondence to Gmail, also in "beta"?

How many of us think our personal email storage is comparable to our company's critical storage infrastructure?

&gt; Jeff Bezos has given SmugMug's Don MacAskill his personal assurances that S3 will not be cancelled on a whim.

Great, so it will be a long-hard-thought-out cancellation. That assurance still doesn't help anyone else but Don.

&gt; And again, most web businesses are built upon platforms that aren't entirely controlled internally.

Most web businesses are using totally open source platforms on commodity hardware -- zero risk of vendor lock-in or getting screwed by whims.

&gt; Your hosting provider could just as easily decide to cancel on a whim.

"Budget hosting companies" is hardly what we're talking about when we talk of storing hundreds of terabytes of data. If you do it yourself you can diversify your hosting datacenters across multiple reliable companies. If you go with Amazon they hold all the cards.
I must be really stupid, then.  GNU Go regularly kicks my ass.  :-/Chances of that working (i.e. computer is in proximity to another computer, *two* users simultaneously missing to monitor the situation) are already lower than the current system.

I think that by default an application should open up muted (since Vista has per-app volume controls) - especially Internet Explorer.

Apparently Apple realised this (problem) 15 years ago and allowed you to specify a personal "starting" codeword.My browser can't display 7 letters in the linked test (5 of them in the APL part) and all chars in your comment. I don't know exactly what fonts to install. I simply installed most of the fonts i could find in Debian - excluding some languages/countries i never heared of.I think this is lame. So I have my Vista configured for speech recognition and it picks up "something in the background" whether it be background audio on a webpage, my wife, or the TV and since it's able to pick up "delete" commands, this is a bug? I think this is a little over the top.
IPC is more work, but it's harder to make those hard-to-debug, subtle errors, so it seems to be the better default to me. I have no real experience, so i'll better be quiet now. ;)Do what Apple does and have a per-user spoken keyword that turns on the command recognition; turn off command recognition while audio out is in use; warn users about the potential problems associated with it… There are a lot of possible ways to mitigate the problems. The key is to notice how many the "new, security focused" Microsoft chose to implement: zero.Right, so the list of complaints is narrowed to [programmers need to learn statistics or I will kill them all](http://zedshaw.com/rants/programmer_stats.html), the use of--good hell--modal dialogs, and the fact that it still doesn't test factors that are relevant to production.I morn for those who still have to use Java.I've said this before about the Unix permissions issue: 

&gt; It's often said that Unix style permissions make systems secure. I find that argument totally unconvincing. Yes, it's nice to have the confidence that with OS X I won't end up like Windows where you have to reformat a disk to try to clear the deeply dug in roots of some spyware crap from the system, but there's still the pretty damn big issue of all my personal data being vulnerable. Namely, while having to reinstall OS X would be a pain, and I'm glad I don't have to waste an hour doing it, losing all my data (documents, photos, music, and to a lesser extent application preferences) would be devastating. The data on my PowerBook is my life, and the reassurance that at least I don't have to reinstall OS X would be cold comfort at best. True, I do make a monthly backup onto an external drive that is normally unplugged (and thus out of range of `rm *`ing attacks), but probably most users don't follow this practice.
&gt; 
&gt; There is a solution to the problem, but it requires a deep rooted change in how things are done. What I propose is that we shift from permissions by user to permissions by application. Right now, any app that my user launches can erase any of my files. That's ridiculous! Much more logical would be allowing me to decide which subset of my files each app can use and how. So, for example, I would let FireFox write downloads to my desktop and its preferences and caches to subfolders of the Library, but I wouldn't allow it to erase any of my other files or launch other programs under any circumstances. In fact, most of the time I don't even want FireFox to be able to read my local files, but I'd be willing to put in a password to let it do on a time limited basis so during uploads and the like.
&gt; 
&gt; Basically, what I'm proposing amounts to sandboxing every app. This may seem harsh, but why not do it? What's the advantage of letting any app destroy any of my files? Make them at least beg me for permission first, I say!
&gt; 
&gt; So, that's what's on my wishlist for OS X.6.
Promise the moon. Deliver a rock from the parking lot.I'm pretty sure nobody Python is talking about Parrot.

I'm pretty sure Ruby is flying off in about three directions, none of the Parrot.

Python's got IronPython, PyPy, CPython (which will still be going for a while yet), and it'll be a minor miracle if Jython doesn't stagger back to life here at some point. None of them parrot.

Perl 6 may or may not be using Parrot, but nobody else is planning on it.

So no, that's not it. It hasn't "won". Nobody's "won", and the amount of redundant work is pretty amazing. (Fortunately, that can be sort of a good thing if the best implementation wins.)Exactly. I've often argued that exactly that, that permissions in unix do not protect the files that matter, and are therefore almost pointless.

It would be a pain in the ass to work with the system you propose though, because in essence you have to give permissions to every app for every file. 

It would be a lot easier, and more correct, to have a completely transactional file system. Where every transaction can be rolled back at any time for any reason. If files get trashed, just rewind time.

(Vista has a primitive support for this, shadow volumes or something. It's said to be nifty - but I haven't tried it myself)[deleted]Per user keyword? I think a generic 'start' and 'stop' is a lot easier. People are going to record 'start' and 'stop' anyway if you let them record their own keywords.

Can't play music when using voice recognition? Uhm, no thanks.

I agree a warning would be a good thing. But we're talking about something that's pretty hard to exploit remotely; I doubt we're going to see online voice-exploits anytime soon.The best manager I've ever had was one who had only a basic understanding of programming.

The worst manager I've ever had was one who had been a programmer for 15 years beforehand.

Maybe it was just a coincidence.  But I think that you'd be much more likely to get a manager who trusts your opinion about the software when they *know* that you have a better understanding of it than them.I like dynamic scoping. I use it all the time alongside lexical scoping. If dynamic scope was a mistake, it was a particularly fortunate one for me.*When Scheme is heavy duty, it is not portable*

Um, no.

[Chicken](http://www.call-with-current-continuation.org/index.html) compiles Scheme to ANSI C -- that's a lot more portable than anything I'm aware of ending in -CL.  Chicken plays better on Windows/*n.x, and can also generate a "Hello, World!" binary that's less than 40 MB in size.Anyone interested in point-free programming should check out http://en.wikipedia.org/wiki/Tacit_programmingWell, I specifically have been thinking about the risks from data centers.  There are a lot of data center horror stories out there, especially on WebHostingTalk. "My data center didn't pay their bills.  Now the building is padlocked, the power is shut off, and my servers are locked inside!"  This is particularly a problem when you work with resellers.

My hunch is that the people interested in S3 are those who *are* on a very limited budget.  This is the crowd that has maybe $100 or $200/mo. to spend on hosting.  So they do turn to virtual private servers, budget colos, resellers, etc.  In that case, the options aren't as clear-cut as you make it sound.

SmugMug's hundreds-of-terabytes is the extreme.I read your post too quickly (mentally inserting type), but the general point still stands:

 * Stalin is just an example of how a fairly minimal language enables interesting works. Consider Pyrex, which is incredibly useful but a limited subset of Python designed for interacting with C/C++ function calls -- does it deserve to be consigned to oblivion because it's not full CPython? (not quite apples to apples, I know, but I'm trying to underscore a slightly more relaxed point)
 * Many implementations has been a mixed good/bad thing.
 * You could add declarations. Yes, it would be an extension, or if widely needed would warrant a SRFI. I think this is an acceptable tradeoff considering that there's only a de-facto standard for bloody sockets in CL -- for which I have to install a compatibility package in SBCL to make it look like ACL (or use the BSD socket layer it defines) and there is no obvious codification in sight.
 * Speed is good. I use common lisp when I need speed. But I don't need speed all the time is all...[deleted]As someone who has written numerical codes (for which stalin is designed), it's something like this in terms of code volume:

|--|------------------------------------|

(math stuff)       (everything else)

You put the stuff that's expensive in the stalinized thing and then use your favorite IPC to ask it to do things, whether it be pipes or sockets or whatever. Common Lisp is better for something that needs to be "kinda fast all around."

This model works with Python/Perl/whatever and FORTRAN/C/C++...Stalin was designed for this one use case, and it does it quite well. Yes, it's a bit more painful, and in an ideal world Stalin would be R5RS complete, but hey...the alternatives in that space are FORTRAN and C.Another example of a 100% point-free language is [Joy](http://www.latrobe.edu.au/philosophy/phimvt/joy.html), probably the most concise language I've ever seen.Most people become managers to get more pay and more power, and so naturally they want to get into management as fast as possible. 

As far as coding experience, 20 years is too long, I think 10 years is more than enough.

Of course it also depends on the induhvidual.I generally just don't name my args things like "list". It doesn't bother me most of the time. Consider that the same problem exists in say, Python -- but there's no huge movement to make a "Python_2" as it were. 

why would I do that anyway? I don't write functions that generally work on a "list", I write something that accepts "deck-of-cards" or "spline-segments" or similar. Abstraction and all that. lst, l, etc. are only for throwaways.

Side note: 
I think that the lisp community is moving towards the idea of using namespacing/modules, ala PLT Scheme's. One could fathom that, using such a system, one could refer to a basic datatype in a "builtin" module ala Python.Yeah, right, so I should just love Lisp, or else I'm a blithering idiot. Replace "Lisp" with "C#", and then people in Reddit will mod me up.Really? I noticed that after I posted *this* link, it was [motzer](http://programming.reddit.com/user/motzer/submitted) who posted some [3 more](http://programming.reddit.com/search?q=notes-on-haskell.blogspot.com) from that site, a couple of minutes later (not the first time he's done this, big on submitting, not so big on contributing comments) I try not to post [more than 2 a day](http://programming.reddit.com/user/dons/submitted), though of course, some days are busy news days.Stack languages are not exactly equivalent to point-free style. A superficial example is that Haskell's 'f . g' is written 'g f' in Joy. A more fundamental difference; if g returns 2 values and f consumes only 1, then 'g f' will produce the first output of g as well as any outputs of f. Haskell's . does not support this form of composition. Haskell's arrows are closer to stack programming than point-free style, I think: see http://kpreid.livejournal.com/7351.htmlNo one uses HTTP authentication for web services, so that's not actually a problem.

You're right about locking a user out, I didn't think of that. It would be easy to combine it with rate limiting though, where you only rate limit the account after the password has been guessed incorrectly 1000 times.

People can and do re-use passwords, but people also use high and low security passwords. Supporting that usage is fine, as long as it's made clear to the user when they sign up.[deleted]&gt; Also, uniquness types are harder than monads ;-)

Perhaps as Haskell becomes more slightly known by more programmers we'll hear them saying that monads make hard what C makes easy?
(Note: that is not an opinion I hold - let's be clear that I don't know anything about it.)

The only thing I've seen that discussed I/O in Haskell, C and Clean, was really talking about [the difficulty of doing proofs](http://citeseer.ist.psu.edu/butterfield01comparing.html)

Do you know of a direct comparison of using monads, imperative programming and uniquness typing?&gt; Here's the thing that makes me mad: we were promised that this time Microsoft was really, really, really serious about securities.

Here's the thing that makes me mad: Microsoft managed to turn what should be a basic attribute of their software into a feature by ignoring it practically forever, and now people are *happy* that it is in place.

That is like Ford selling cars without locks on the doors then finally introducing a mostly-working lock system as a feature on the latest model. People shouldn't be praising this, they should be saying "well it's about fucking time"If you're on a limited budget I can't imagine why S3 would be good -- variable costs suck. You can quite easily switch regular-hosting providers in a very short amount of time, switching off using S3 API is considerably more difficult. 

Anyone who's big (like SmugMug) can go with reputable datacenters like Equinix, and have more than one.Interesting, like this?


    import Char
    import Control.Arrow

    -- a function that returns two outputs
    g n = (replicate n 'x', chr n)

    -- a function to consume one result of 'g'
    f s = reverse s

    -- funny composition with arrows
    g .&gt;. f = (f *** id) . g

    main = print $ (g .&gt;. f) 10

Result:

    ("xxxxxxxxxx",'\n')Um. They are? You can have variable capture in CL. You can't in Scheme. I am led to understand that that is the point of hygiene in macros. Whether this is good is up for debate, but it certainly doesn't seem to related to Lisp-n-ness.You ask a tricky question. My answer is that Common Lisp -- by virtue (vice?) of having such an encompassing standard purports to have pretty much have everything (in 1300 pages or so), but doesn't and, to make the wound mortal, doesn't have a optional module-style standardization process.

Implementations have to figure out how they are going to do sockets. They can use POSIX and/or copy ACL, but there is no de-facto implementation, nor any process like PEP or SRFI to make a standard way to add optional modules. This is why it's possible to have a perfectly complaint Common Lisp implementation that doesn't include sockets -- something about as fundamental as the file in the modern age.

Edit: 

Hold the horses, perhaps light at the end of the tunnel:
http://lambda-the-ultimate.org/node/300

There may be hope yet. Still..I don't like how the common lisp spec is so entangled with so many non-intrinsic things, yet not some of the more non-intrinsic things that you really /need/Bad headline, but a great joke-turned-exploit if you bother to read the page.
[deleted]Might you not also sometimes work with `car`s, `position`s, `search`es, ...? Anyway, re namespacing, if you're ready to disambiguate your usage of built-in functions like `list` or `car`, it seems to be that qualifying domain/package specific symbols works just as well and is less of an eyesore. In CL, I tend to create dummy packages that don't use any package and export conflicting symbols I want to use.[deleted][deleted][removed]For Matt: See Colon. Return. Why, return? Echo. I am sofa king stupid.Yes. I'm not sure if this would come up in idiomatic Haskell, though.reddit thinks that was an awesome post.  reddit also thinks that when your most problematic choice is whether to use scheme or common lisp, you're in a very good place.  Oh hell, reddit doesn't think those things, I do, I'm just trying to keep my schtick going.reddit loves lisp.[Gambit Scheme](http://www.iro.umontreal.ca/~gambit/) to me is the perfect amount of "lisp"-ness when I need to be in that world. CL is just so damned huge. SBCL is the sweet spot for "real" Lisp IMHO. But I'll stick with gambit and enjoy an optimizing compiler, incredibly efficient lightweight threads with mailboxes, a good and clean and simple set of socket functions...Maybe a root-managed automatic backup of user homes?

It would be nice to have that option. To have an automatic updater, that saved your home (or whatever you chose) as root in a different directory. 

That way, information is stored in a way that makes it impossible for viruses to erase it. I know that there are no such viruses yet, but better be safe than sorry.

This can easily be done using cron and such... but a gui would be nice... I'll see if somebody has already done that...I recall the notes in CLtL and other places made more of a fuss over it than that.  In general, you're right, but the devil is in the (historical) details.I'm sick of this.  What's so bad about threads?  I use them all the time!  You just have to be careful, avoid doing really crazy things, and think about threading implications as you build things.  It's not *that* bad... a little challenging, but damn... you'd think that threads were a sign of the apocalypse reading some of these articles.
Alphabetizing properties is certainly not for me. Somehow I find it much harder to look for a property I want; I almost always have to scan through the whole element. Most of the times priority sorting works much better.With a Lisp-2, you can do things like `(defun glue (list item) (list list item))` which has the effect of `(glue '(1 2 3) 'rest)` =&gt; `((1 2 3) rest)`.  (OK, it's a contrived example; so sue me.)  The equivalent in a Lisp-1 would be `(define (glue lst item) (list lst item))` because the parameter named `list` would shadow the standard function named `list`.

That's the major reason I changed my mind about Scheme and decided I like CL more.  But they're still streets ahead of the competition.  This really is one of those Sunni/Shi'ite, Vegan/Vegetarian, black American/white American debates, where only those who are immersed in the culture can even tell the difference.Wow, I've heard that joke many ways before, but not in reference to a div tag. You made my day :)That's a great one, thanks for sharing it.I guess it has to do with the fact that I'm a word geek... alphabetizing works really well for me.Well, I'm working on a more attractive design, so maybe you'll like what I come up with... whenever I manage to finish it!How do you prevent a friend standing nearby, or even the computer itself, from reusing your password, then?

You: "password1 compose e-mail"  
Coworker: "password1 format c:"I call bull shit. Anything you thought you knew 5 years ago is already irrelevant today because technology changes so quickly. Good manager skills don't ferment like wine, you either got it or you don't. If you do, you will be a great leader with far less than 20 years experience. If you don't, 100 years of experience wouldn't help you.A regular Larsa Solidor, he is. :)Tell me how Bourguignon can be called anything but a complete futtock after how he behaved.  He was malicious.  He should be ashamed, and you should be ashamed for supporting him.You should be ashamed for being just another anonymous reddit retard.I think it really depends on what kind of program you'll be writing.  Shared memory works well in cunjunction with an event-based design (since events have to access shared state at non-deterministic times by nature), while IPC works better for sequential programs that absolutely must synchronize.Professor Bradley J. Lucier prefers Gambit Scheme for his Numerical Methods for Partial Differential Equations class.

http://www.math.purdue.edu/~lucier/

Gambit is a Scheme-to-C scheme.



Oh, yeah, and according to him, it's as fast as Fortran or C.
And uses generic functions and classes. Huh. 
Take that, Fortran zealots!&gt;trying to build a better mousetrap or just patch a leaky dam

I think this is the problem that the original author is gunning for. Anybody with any modicum of engineer knowledge knows it has to be:

&gt;trying to patch a leaky dam or just build a better mousetrap.

This priority inversion for what seems cool against what is actually a potential killer (both of people in this case and the project in general) is exactly the problem the author was highlighting.&gt;Um. They are?

Yes.

&gt; You can have variable capture in CL. You can't in Scheme.

That statement doesn't make any sense. The hygienic macro systems that schemers use exist precisely because it's a lot easier to get bit by variable capture in scheme. Due to it being a lisp-1, accidental _function_ capture becomes a major issue, and hygienic macros exist in order to prevent this.

In Common Lisp GENSYM is enough to avoid variable capture, and CLHS 11.1.2.1.2 and the package system come together to help avoid accidental function capture.

This issue has been hashed out elsewhere ad nauseum, a simple google search for "lisp-2 hygiene" is more then enough to get started. So, next time you are 'led to believe', rather than follow blindly why not try a little independent research and thought?!How about: "Entropy Increases"?IMO the CLRFI process is dead, if it was ever really alive in the first place. OTOH, The Common Lisp Document Repository (http://cdr.eurolisp.org/) fills a similar niche and is an active project supported by the lisp 'community'.

And while there is no de-facto standard socket implementation, there are portable libraries (usocket and bordeaux-threads come to mind) that do present the same API across implementations.

And as i've mentioned elsewhere, you don't program with specs, you program with implementations. I use SBCL, which has a perfectly serviceable socket implementation. So, in practice the lack of standardised sockets is not a problem, just like it's not a problem in C/C++ or any other language which does not have standardised sockets (which is pretty much all of them actually).

[removed][removed]Uh, Novell Groupwise.&gt;&gt;When Scheme is heavy duty, it is not portable

&gt;Um, no.
&gt;Chicken compiles Scheme to ANSI C -- that's a lot more portable than anything I'm aware of ending in -CL.

Here is the [manual](http://chicken.wiki.br/The%20User's%20Manual)

I see nothing about declarations besides the ability to declare all math fixnum. Indeed, they say such fixnum math is inline, implying that floating point math isn't. In my experience (in the absence of a global optimizer like Stalin) declarations are what makes a Lisp fast. Simply compiling to C is nice, but I doubt it will give you CMUCL-like performance. I've spent lots of time tuning numerical lisp code, and declarations are critical.

Do you have benchmark results for Chicken? Like Chez, it looks really nice - nicer than the Scheme interpreters I used years and years ago, but it doesn't look like a heavy duty Lisp compiler like SBCL or CMUCL or Allegro. These took many man years to write.

edit: ECLS common lisp (http://ecls.sourceforge.net/) also compiles to C. I can't comment on speed, except that it probably benefits from declarations.Each command that involves the loss of data could involve a random word, it could put a word on the screen and say "read this word to confirm" or, for the illiterate it say "identify this object" and have a picture of a simple object. There's no excuse for this bug.Years of experience have only small correlation with managing talent. Primarily, the requirements of a good manager are a keen sense of what the important and unimportant issues are, the knowledge, courage, and tact to challenge and remedy incorrect ideas, and the vision to inspire, motivate, and influence your end users and employees.It is an interesting way to do a swap, though I'd probably never use it.

Maybe it could be useful if you wanted to obfuscate your code...&gt; |--|------------------------------------|
&gt; (math stuff) (everything else)
&gt; ... Common Lisp is better for something that needs to be "kinda fast all around."

Agreed. What I like about Lisp is that you can write a huge program, and worry about speed in only a small optimized subset.  There's a continuum from being a lazy slapped together language, and a finely tuned fully declared one. You choose each model function by function, as appropriate.Thanks! I'll do that. I didn't intend to be confrontational, and apparently I was led astray; I shall do research when I get done with this math assignment. 

But in the meantime, why can't you gensym in Scheme?It fails to turn my stomach because I value generic list operations that don't customarily name their argument `lst`.  I'm much more annoyed by scheme's internal DEFINEs, only justifiably a hack against a packageless environment, when indenting operations like LABELS and FLET serve well enough.  Making a distinction between DEFINE at the toplevel and DEFINE elsewhere has foiled me in a traditional exercise in lexical scoping, in which I LET a variable and then wanted to DEFINE functions that operate on that variable.  Probably it also annoys in conditional compilation.

It also doesn't turn my stomach for this reason: I don't emit hyperbole like 'FUNCALL is an abomination!'.  Common Lisp discourages some manners functional style by not guaranteeing Tail Call Optimization and instead offering numerous methods of iteration, and possibly also by offering CLOS.  FUNCALL has nothing to do with anything -- it certainly doesn't have anything to do with the *use* of a higher-order function, which -- except for `#'foo` -- is exactly the same as in Scheme.That's a good idea in general, but it seems like a band-aid for this particular vulnerability. In particular, I wonder if adding a root service will add new security vulnerabilities. Also, the backup software doesn't solve the problem of spyware and browser fishing through my files for credit card numbers of whatever.&gt; Consider that the same problem exists in say, Python

The farthest thing from a 'functional programming language' in its family, and possibly in its entire generation.

&gt; why would I do that anyway?

Because you have functions that operate on lists of anything and not on *decks of cards*.  Find The Little Schemer and read it.

&gt; I think that the lisp community is moving towards the idea of using namespacing/modules,

The Scheme community already uses this at length, in any interesting implementation.  Common Lisp, of course, calls them packages and has them in its standard.There are all kinds of 'heavy duty': Gauche Scheme has excellent support for unicode strings, even having "these common algorithms sucks on strings for which NTH is not O(1) -- do these other things instead" in its documentation.  A screenshot of a hacked CMUCL exists, showing Chinese function names and such -- but otherwise, your best heavy duty multilingual CL is, uh, Emacs Lisp.  You can load cl.el , at least.Morever, CL *does* standardize the utilities (#+cmucl #-gcl) that allow you to easily write code that works on multiple implementations -- nevermind that so much of the code works *automatically* on multiple implementations.eh..  slashdot's influence doesn't seem to have the same sort of clout.  Good for bringing down websites, yes..  but I think the anti-microsoft rants on slashdot weren't ever taken that seriously by those in the mainstream.You can, in almost any real implementation, and you can use CL-style unhygienic macros in, AFAIK, every serious implementation (if you count explicit renaming in Scheme48, which does the same thing in a slightly different).  In at least two major implementations, Gambit and Chicken, CL-style macros are actually preferred.  

In practice, it doesn't seem to be a problem, despite what fans of Lisp-2 or hygienic macros may tell you."Higher-order functions", not "closures"!Dynamic scoping for contextual values is perfect;  dynamic scoping *by default* was a mistake.[removed]It *is* portable. It runs on all sorts of hardware.

Saying that Chez Scheme isn't portable, doesn't make sense. A given program written by may or may not be portable. If you absolutely (I can't see why) want the same program to run on more than one implementation, you just stick the intersection of the languages - which of course includes R5RS.

You have the same situation in C btw. If you use the extra features GCC offers, your program isn't portable to other compilers -- but that is hardly the fault of GCC.


&gt; It is not free (libre or gratis), alas. 
&gt; ...
&gt; the Lisp community has a handful of ... plus a handful of heavy duty commercial compilers.

What are you trying to say?
http://justfuckinggoogleit.com/?q=lisp-2+hygiene

Look at the _very first hit_. Again, independent research and thought is your friend, and the Internet is an amazing resource. Use it.I appreciate your criticism. I think Bogtha (hrm, why does he only use his name once here instead of thrice) made compelling counterpoints, so I see no need to start a debate. Thanks for reading, and for thinking it was worth commenting on. :)You know what else is your friend?  Righteousness.[removed]&gt; [...] just like it's not a problem in C/C++ or any other language which does not have standardised sockets (which is pretty much all of them actually).

Err, C's socket API is standardized in POSIX and SUS.Certainly; I'm fairly aware of Lisp's history in this regard, but I'm perfectly glad this profound "mistake" happened. I didn't have to suffer the negative consequences, but would've been poorer without it. (I hear Emacs lisp has this problem, but I rarely use it.)

There's far more recent mistakes that have bitten me directly, like those within Java.[removed]&gt; The firm has pointed out that in order for the flaw to be exploited the speech recognition feature would need to be activated and configured and both microphone and speakers would have to be switched on.

Or, just pipe a sound file to the audio device.

Heh, you know what would be funny would be a popular YouTube video that gets altered so someone's shouting various commands, which anyone using speech recognition gets bitten by.

I don't know why this was modded down.  I modded it up.

You're 100% correct -- the system files are the LEAST valuable files becuase they are easily replaced.  If my root partition gets wiped, I can just install Ubuntu again with a few clicks.

If /home gets wiped, that is a whole different story.
Is this in the same basket as Thinking Forth?omigosh!  He's acting righteous!  Look at that TONE!  SO VERY HOSTILE IN HERE WHY AREN'T WE GETTING ALONG LIKE BUTTERFLIES IN A SWEETLY-SCENTED FIELD??  I know, I know: I'll leap in and 'break up' the 'fight'.  Better: I'll call one party 'pompous' or 'arrogant' or -- this is a new one!  I can't seem to use the root, though! -- 'full of righteousness'.  I'm such a humble person that I *just can't stand* the *attitude* of some people.  Nevermind what they say, or what I say, or whether it's raining blood: everything stops when I get the tiniest bit affronted.  My friends: affronteousness and hyperconcern for what other people are full of.That he likes lightweight implementations.  This point has little relation to his other point of 'it disappoints me that Chez Scheme is not some kind of free'.It wounds the mortal soul to not have an optional module-style standarization process, eh?  Look: in Common Lisp you can write standard CL to do something interesting and thereafter expect a smart CL implementation to take this code and do it efficiently.  You don't have Scheme-code-for-reference-purposes, you just have the code.  In an environment where you do something *particularly* interesting, you can continue to write standard CL with easily-identifiable implementation-specific bootstrapping.  As for SRFIs, look at them through a CLers eyes: how many are plainly unnecessary to CL?  How many are incompatible with CL?  How many of them are trivially implementable in standard CL?  A glance identified five as interesting and potentially worth a community-wide test of `*features*`.Tending to an itch is more about focusing on the symptoms versus what the article speaks of which is looking for a cure. To me that means focusing on the strengths of a code base so that time is spent refactoring for the business payoffs to come later, and greater. I would like to know if the tasks involved in management are really NP-complete or whether they can be broken down into areas that can be automated. Managers should get involved as little as possible. If you look at some of the best NASA projects they were often ones that took out the human element; where the engineering encompasses the entire project out to the point of eliminating points where human errors, management or otherwise will come into play. Usually it involves smaller probes that by necessity had to tend to themselves because of vast distances, like NASA DS1 and its related code.I've tested them, and MAN, they are REALLY efficient!!
Beats Anthem AND Microsoft Ajax Asp.Net.
Recommended!I'm trying to say that Scheme has, as far as I can tell, one heavy duty non-free Scheme implementation. Other than this, there are lots of interchangeable lightweight scheme implementations that were someone's personal project. *"I'm going to write a(nother) scheme interpreter"* is a phrase guaranteed to generate both laughter and tears in the more weary members of the audience. 

Lisp, however, has a number of both free and non-free powerful compilers. Lisp has fewer implementations, but they tend to be durned good. You can't find scheme version of a multi-platform optimized beast (in the good sense) like SBCL.Dear Aunt, let’s set so double the killer delete select all.Retirement would drive me insane.Hehe, they don't reset the max-width my personal stylesheet sets.Q. On Linux, guess what you get when you combine this small piece of Perl source code...

http://labs.cybozu.co.jp/blog/kazuhoatwork/my_projects/c/C

...with this information...

http://en.wikipedia.org/wiki/C_preprocessor

...about C pre-processor macros?

A. You get the ability to create your own scripting language by redefining C or C++ in your own terms. Plus, it will run 100 times faster than Perl. (The author ran tests with a Fibonacci function.)

By default it already includes stdio and stdlib, but you could include others.

Here's a very simplistic example of re-interpreting C or C++ in your own language:

    #define println(s) printf(s "\n");
    println("hello world");

The binary is [here](http://labs.cybozu.co.jp/blog/kazuho/archives/c/C-0.05-1.i386.rpm) and tarball of binary to compile on your own is [here](http://labs.cybozu.co.jp/blog/kazuho/archives/c/C-0.05.tar.gz).

(- Invented by Kazuho Oku, Japan.)
Drivel, indeed.to 1: You can do *any* sort of source transformation with defmacro

to 2: you do away with the data and code distinction - it is infinitely simpler
[deleted]This is okay document but I still think that the "official" getting started documentation on erlang website is a bit better.

http://www.erlang.org/doc/doc-5.5.3/doc/getting_started/part_frame.html&gt;MS doesn't even publish the specs of how to connect to exchange.

This is a lie. MAPI is a published API, although the protocol is proprietary.

&gt;No other mail client will connect to exchange except outlook.

This is a lie of especially ludicrous size, since the god damn article talks about Evolution.

Why have two lies got 32 upmods? Who's spreading the FUD now?&gt;You are much better off leaving those to MS, MS has locked them in 

You've got your engineering confused with your theology. Using MS products is not sin. We are not the damned.

&gt;they can never switch to another product no matter how nice it is,

(switched from VSS to SVN)
(switched from TFS to NUnit and Cruise Control)
(switched from IE to FF)
(...)

do you read this shit when you type it?see also the related
http://www.knowing.net/PermaLink,guid,568b949c-b09c-4b45-a85d-0b1ac284a144.aspx
Knowing.NET - Use Eiffel for OSS, Get The IDE for freeYou could not. In the same way that if someone sees you type in your password they will then know your password.&gt;Are you sure? That sounds a bit overstated to me, 

It's not overstated, it's what is technically known as 'a lie'. You'll notice malcontent answers the descendants but not your point, because otherwise he'd have to admit it's a lie.I think the guy tries too hard to be funny.all these match Outlook feature for feature, and are all as usable for non-technicals, are they?

(Domino?? fuck that shit.)[removed][removed][removed]No.'rely solely on' is a nice fuzzy term, but in the meantime, glad to see you're admitting that being able to connect to Exchange according to a published API is possible.

&gt;The article is about how evolution is not able to connect to public folders

...while connected to god damn Exchange. So your assertion that you can't connect with any other mail client is, I'm afraid, a lie.

&gt;Vendor lock is a bitch for the customer but it's a license to print money for the vendor.

Ah, now you've repeated the exact same thing for the *eighth* time I'm finally convinced. Halleluliah!Outlook is why people buy Exchange.

Let me rephrase that, since I see I'm arguing with a 17-year-old trekkie slashdot moron who prefers ad hominems.

Outlook is why people buy Exchange, you 17-year-old trekkie slashdot moron. If clients which don't match Outlook aren't available, then you're not answering his question.

&gt;What the fuck are you talking about fuckwad. 

Go get a cup of coffee, take your pills, do the deep breathing exercises, put three seconds thought into replying, and try again.and? non-immediately-recent informatiom has educational/entertainment value.This is why I mentioned maps and folds, the folds would perform the required recursion. I was only posting an idea I haven't really thought too much about. I still think it would be interesting to see a database built from such a perspective.

You have a point re. the query language is the problem rather than the Relational model per se.&gt; Can the CLR "go dynamic"? Absolutely... and arguably, already is
 
Which argument could counter [IronPython](http://www.codeplex.com/Wiki/View.aspx?ProjectName=IronPython)[removed][removed][removed]&gt;Huh? Go back and read what I wrote.
"No other mail client will connect to exchange except outlook."
"evolution is not able to connect to public folders [while connected to Exchange]"

g'wan, tell me you can't get your mail from Exchange with Evolution. I'm enjoying this.

&gt;so I have keep repeating myself.

So I notice.
This was actually yesterday :-) The ARM port is almost done now. Today was a debugging day, nothing new or notable. Porting the C library interface to ARM is the only major task which remains to be done.SBCL has Unicode support, including the use of unicode characters in symbols. Since we are already comparing implementation features, tell me if there is any Scheme implementation that supports all of:

* An optimizing compiler.

* Unicode support.

* An interactive development environment, like slime. A half assed repl without even readline support and an interpreter that executes my code at half the speed of ruby doesn't count.

* An object system that comes close in power to CLOS. (And isn't an order of magnitude slower than Ruby.)

* An error handling mechanism that comes close in power to the CL condition system.

* Fast binary IO.

* Native threads.

Note that SBCL supports all of these. Additionally, it's feasible to support a single version of a program/library that runs on different CL implementations.Funny?  Lame shilling for MS Vista isn't funny.&gt;Apparently he likes evolution and wants to keep using it so he needs a server that he can connect to with evolution.

This is where the 17-year-old trekkie problem kicks in.

Business sometimes consist of more than one person. Using Exchange or something else is probably not a personal choice he's made off the top of his head. Maybe, just maybe, there are what we call 'users' who want to use Outlook. This means there is pressure to use Exchange.

Hope this helps in your future career.

&gt;post stupid shit confusing exchange and outlook.

Sweetie, I know and behind the blustering you obviously know that I'm not that ignorant, and no other fucker is still reading this, so you can stop kicking up shit now.&gt;Yes sure as soon as your meds kick in.

Hi, just a hint, but you often sound cleverer if you

(a) don't keep repeating yourself
(b) don't repeat other people's insults back at them

HTHhttp://www.informatik.uni-kiel.de/~fhu/PUBLICATIONS/2000/ifl.ps.gz
Distributed Programming in Haskell with Ports (2000) [Postscript]

http://www-i2.informatik.rwth-aachen.de/old/Research/distributedHaskell/ifl2001.ps.gz.
Implementation of Port-based Distributed Haskell (2001) [Postscript]

http://homepages.inf.ed.ac.uk/stg/workshops/TFP/book/DuBois/duboismhaskell/cameraready.pdf.
Implementing Mobile Haskell (2001) [Postscript]\t

www.macs.hw.ac.uk/~trinder/papers/SBLP05.pdf
Haskell: Mobile Computation in a Purely Functional Language (2004/5) [PDF]

http://www.cee.hw.ac.uk/~dsg/gph/papers/ps/jfp01.ps.gz.
Parallel and Distributed Haskells (2001) [Postscript]

http://137.193.200.177/ediss/braun-oliver/inhalt.pdf.
Constructing Mobile Agents using Transformations (2004)

http://www.mathematik.uni-marburg.de/~eden/paper/PAPP04.pdf.
Towards a Generalised Runtime Environment for Parallel Haskell (2003) [Postscript]
&gt;Until I used PyDEV I never truly debugged a Python application...

I wanted to stop reading right there. He is writing an article about an IDE for a language he's **never truly** debugged!? And we are supposed to get him seriously!?

But, wait, it gets better:

&gt;would just add print statements to the code and try to follow the logic to catch the bugs.

Does he even realize what a waste of time this is?! If I worked like that nowadays, I'd at least kept my mouth shut!1:
Given two top-level declarations:
 
    (prolog-assert (edge "a" "b"))
    (prolog-assert (edge "a" "c"))

With defmacro, how do you combine this into a single statement that looks like

    (define (edge x y) (or (and (== x "a") (== y "b"))
                           (and (== x "a") (== y "c"))))

I would prefer code as answer. No, you are not allowed to use side-effects or assignment.

2: I do not see the connection between dynamic typing and code/data distinction. Please elaborate that point.See also [the bibliography](http://www.haskell.org/haskellwiki/Research_papers/Parallelism_and_concurrency#Parallel_Haskell)As noted in the article Komodo 4 can be used with other languages, and in the past week I've been using it for Perl, PHP &amp; Javascript.  In particular regarding the latter it's actually incorporated *intellisense*.  If it's as good for Ruby as for these other languages, it's worth a look.&gt; Then you trudged down to the keypunch room to have the code typed on those precious cards. The room was a gridwork of machines five across and seven deep, staffed by young women...

Guess it wasn't all so bad, eh?&gt; Due to it being a lisp-1, accidental function capture becomes a major issue, and hygienic macros exist in order to prevent this.

Unhygienic macros are also avaliable in all the Schemes I've tried.If you're implying that I'm some kind of shaved ape, you'd be right.[deleted]Obligatorily: [colorForth](http://www.colorforth.com/cf.html)![removed][removed]_"The IT industry has come to a point where its engineering techniques have no way to cope with the coming technological change. This catastrophic failure will not by fixed by rearranging deck chairs - or even running around with a rivet gun and a bucket. We are advised to learn to swim."_[removed]&gt; SBCL has Unicode support, including the use of unicode characters in symbols.

Good!

&gt; of ruby than Ruby

Fascinating.  Years ago, I loathed 'the shootout' for focusing on trivialties and only properly testing the desire of implementations to produce happy little executables.  But I suppose it has actually had this great benefit of making Ruby 'the canonically slow language'.  Someone supportive enough of CL to attach this gigantic preaching-to-the-choir list to a 12-word answer should at least be somewhat mindful of the presence of non-CPU/non-memory optimizations -- e.g., optimizing for programmer speed, for maintainer speed, for ease in extending this code later, &amp;c.  Programming Perl has a number of these uncommonly-referenced optimization types and lists of ways to optimize your code for them.  Someone supportive of CL might also be expected to distinguish between a *language* and an *implementation*, although of course Ruby implementation development has been static for a while.  I blame the pickaxe book.[removed][removed]In principle, I agree with the philosophy of not patenting software but it seems pretty hard to find a software company that

a) will take you on, and 

b) do not use patentsEmacs Lisp doesn't suffer much for it, though.  Like other horrific deficiencies like 'not having packages', dynamic scope forces state to exist in world-readable variables that can then be configured in ~/.emacs, given temporary values for a call, made buffer-local, attached to excellent online documentation or M-x customize or so on.  Emacs doesn't need a better language as much as people like to think -- it *could* use some cleverer optimizations from the likes of SBCL.I found David Black's "Ruby for Rails" instrumental in helping me over this hump. But then again, I care enough to buy a book and learn.Indeed!  And POSIX is... drumroll... *not C*.Yeah, I shouldn't denigrate Ruby. I actually have a sweet spot in my heart for it.

&gt;  e.g., optimizing for programmer speed, for maintainer speed, for ease in extending this code later

A few years ago I accepted that a dynamic language like Ruby was inherently slow (you know the solution: just drop down to C when algorithm improvements don't suffice). What made me search for a "better" language were Ruby's limits in embedding DSLs, not speed concerns. After learning a bunch of other languages (including Scheme) I picked up Common Lisp. I was (and I am still) fascinated: Lisp is more expressive than Ruby and the practical problems the Ruby community was (and partly still is) struggling with were solved: IDEs, speed, native threads, Unicode.

&gt; Someone supportive of CL might also be expected to distinguish between a language and an implementation

I'm a programmer, not an academic. I can only use implementations that exist *now*, not an abstract language.&gt; Gambit is a Scheme-to-C scheme.

I almost exclusively use it without explicit compilation to C (gsi-script).[deleted]This is still truely the ugliest phone out there, but I'll be able to get over that easily if it's hackable and of course works well.So sad, so true. Regarding replaceability, I think it's more that 90% are *seen as* fully replaceable.

I find often 'management' is too lazy to determine who is actually doing good work, and instead listens to the one who shouts the loudest. I've learned to shout, but it's not a good approach for a team/company/project.The GUI is ugly, but the phone is ok for me.I mostly agree. With a pair of addidions:

* a manager that has programming experience will be of help to a team of programmers who lack it
* a manager with no experience will be of help to a team of experienced programmers

The other two combinations pretty much suck.
I guess people don't like cynics. And in general if you don't approve of a link that's highly ranked, you will be downmodded.[removed]Not really. The Chandler/Haystack super-PIM concept is based on the "no silos" idea that your emails, contacts, appointments and so on are just items in a single database/filesystem, and that you can freely apply predicates/labels to items of different types. So for example you could apply a "deal with this on Tuesday evening" label to two emails, a to-do note and a scanned document, then find them all again by doing a single search for the items that carry that label. There's nothing like that in Emacs (at least as standard). Your typical Emacs email/calendar/appointment setup is more like a more-scriptable version of Outlook.

Even if it were true, that wouldn't explain why there are no GUI products that do the same thing. Emacs does basic plain-text editing very well, but the world isn't lacking for Notepads and TextMates.Another thing to consider is the company's history of how it uses patents.  Do they really file "defensive" patents, or are they a pack of lawyers out for blood?

I think specific software patents are easy to code around once you know they exist, so are a hollow threat.  The really general ones like 'one-click', or that new Microsoft office ribbon, which cover a general business idea, are the ones that cause the real trouble.

It's also probably a safe bet that a company that seeks to make a large share of its revenue with patents and a pack of lawyers is more concerned about winning court cases than actually developing good products for its customers.  Take Qualcomm or SCO for example :PThe proper way to sort legos is by complexity, then number of dots. The basic "squarish" pieces vs odd shaped pieces. My brother and I even came up with a good descriptive system to tell each other quickly what we were looking for. The hardest to find enough of was the "Bendy-bit". It was the one with two dots, and could turn from 180 to 90 degrees of its base.FSF "takes ownership" in the sense of copyright assignment. You can't sue to defend a copyright unless it is assigned to you.

A commons, when defended by no one, will be enclosed.The page has disappeared?Have you seen this comparison?

http://mywheel.net/blog/index.php/2007/01/08/visual-comparison-of-major-oss-font-rendering/

&gt; Ordinary anti-aliasing is useless for low DPI displays like PC monitors (in contrast to printers...).

It's just the other way around.

&gt; You just get blurred letters.

In Gnome, Kde and Xfce you can control the level of antialiasing, from none to strong.

&gt; Why hasn't anybody implemented decent sub-pixel-hinting for linux?

Hmm... just what version of X, freetype and fontconfig are you using? May I welcome you to the 21st century? :)

Like somebody else said: make sure your fonts are high quality too. If the font hinting is low-quality, no system will render them properly.What a gross name.Any practical definition of C includes its core libraries:  ANSI, and POSIX.

Besides, unless your language interfaces to STREAMS/TLI or implements its own TCP/IP stack from scratch, the very *socket interface itself* ([Berkeley sockets](http://en.wikipedia.org/wiki/Berkeley_sockets)) is also defined in POSIX.  You can't presuppose POSIX the one moment when defining "sockets", and then deny it the next when talking about C's interface to them.AI can beat beginners at Go but after a matter of weeks of study a human can beat every computer without really trying too hard or even knowing the idiosyncracies of the particular robot. This can't be said for chess.One thing not addressed was IO. I was surprised to see fwrite as a normal function and no explanation as to how this can be done in a stateless language like Erlang.the part about demoing in the mac store is particularly awkward.Erlang has plenty of state.  Why, even Haskell has a tremendous amount of state -- any actual Haskell program will refer to the entire state of the universe, during execution.  Erlang keeps this state-of-the-universe implicit, or at least managed by the process that io:fwrite/N, not given a port as its initial argument, defaultly tries to communicate with.Yes, but I thought overall it was pretty funny.  He seems to have a good sense of humor.  I can't imagine Bill Gates producing something like this.Exactly my point too. "shoolz" type of manager is not what developers hate. I had a manager ( who thankfully got promoted and left) who expected me to design, code, lead the team and maintain the project plan. On top of that he made sure to tell me how he was helping me by letting me do the management stuff. Those are the types I hate. They expect you to not only earn their living for you but also put the blame on you when the failure happens while they merrily go around sucking their bosses d***. Ofcourse when you succeed it is because if their deeply inspirational quality of leading a team.
if you are following ruby design, there is idea to unify strings and :symbols, and the dubious distinction will go away - as result there will be no need for such "indifferent access" utility.How can Jim Allchin even remotely be considered a shill? On the official Windows Vista Team Blog no less?&gt; How many [SRFIs] are trivially implementable in standard CL?

SRFIs are (intentionally) not just about substantial chunks of functionality, but about small, common idioms too.  Take SRFI 8, for example:

    (define-syntax receive
      (syntax-rules ()
        ((receive formals expression body ...)
         (call-with-values (lambda () expression)
                           (lambda formals body ...)))))

(Yep, that's the whole implementation.)The advancement in mobile phone surveillance has taken a massive leap in technology and we are very proud to announce a whole new range of fantastic features. TSH Technology utilises the most powerful software applications for remotely monitoring an individuals mobile phone activities from ANYWHERE in the world. Our powerful and highly advanced software is a world first and takes GSM communications to a whole new level.

The [spyphones](http://www.thespyphone.com) and features available here are the end result of an extensive beta testing program eliminating the need for any hardware applications whatsoever. Using 100% software based technology we are genuine suppliers of THE most powerful and trace resistant spyphones on the planet.More like Thinking in Java.Erlang, much like OCaml and various Lisps, is an _impure_ functional language, which means that side-effects are not forbidden, and neither are they sandboxed.

So usage of side-effectful functions and operators (the (!) operator, for example, which has the side effect of sending a message to another erlang process) don't require anything special.

Very few language actually try for the "pure" functional route (haskell is one of those few), because even though purity has advantages, a truly pure functional programming language is purely useless (since IO or state can't be handled, among other things), and the "does purity beat practicality" issue is a debate in and of itself (in Python, practicality beats purity -- ZoP:11 -- and most languages pick that root)Are phones listed in your FAQ list the target phones or your own phone where your software is to be installed?Thanks for the link.

The comparison does seem a bit of a rush job though, especially because

(a) it doesn't seem like the author looked at the different ways to tune typesetting in OSX/Windows and Linux

(b) you have to tune your cleartype settings for _your_ eyes and _your_ monitor. So if text looks gorgeous on my screen, you can make a screenshot of it and display it somewhere else and it may look crappy. Therefore, I think that comparing with screenshots isn't such a good idea.

I've used the gnome and KDE tuning tools, but the effect was never really satisfactory. Window's cleartype tuner isn't perfect either, but I'm pretty happy with it. I'm really not fond of the way OSX renders fonts.

I mentioned I've tried to use windows fonts under linux, so I think it's really a rendering problem.

Most people I know don't even notice when fonts are rendered poorly (when two letters are too close together, for instance). But if you notice this kind of thing, it's very distracting.&gt; 2. A Ruby symbol is a label in a free-form enumeration

Except for the fact that you can't enumerate them...Did anyone else go into that blob post not knowing what *functional pointers* are, and then come away from it not knowing what *functional pointers* are?I've never understood just who this document was aimed at.  If you're writing shell scripts because you need a portable code that can run (uncompiled) on many different target machines (think of the 'configure' script), then you can't use bash and all of the advanced bash features described in this guide.  If you're writing a script that's just intended to run on Linux systems, where you can count on having bash installed, then why aren't you using a more capable scripting language like Perl or Python, which you can also count on having installed? 

My own feeling is that learning advanced bash (or zsh) scripting techniques is generally detrimental to your ability to write portable shell scripts, which are the only sort you generally need to write.  So, I generally try to avoid using bash features that are unique to bash.this is the list of target phones where you need to install the software in order to spy the owner of the phoneI actually wrote something remarkably similar to that for a Connect-4 AI when I was 14.

It sucked.

Still, I feel clevar :-)I didn't like. I don't want to look for arrows while reading threads. Plus there would be ambiguity with the modding arrows.

One thing I love about Reddit is its simplicity. I would even remove the time that the comment was posted here.&gt; A GUIDE TO FUNCTIONAL PROGRAMMING IN ERLANG FOR THE
EXPERIENCED PROCEDURAL DEVELOPER

Any recommendations for a guide to Erlang for the experienced *functional* developer?&gt; I highly recommend staying away from odd, tortuous code, unless your purpose is to create efficiency where it isn't needed, to create a maintenance nightmare, or to prove how clever you are – in which case you probably shouldn't be programming at all.

Baruch's Law for programmers?&gt; the fools become more ingenious

So, the effect is like micro-organisms becoming resistant to each new antibacterial compound?

The answer to that is to use them only sparingly.  Perhaps that applies to programming languages too.
At least two errors so far:

typo in Figure 14:

     7 factorial(0, Acc) -&gt;
     8 Acc,
     9 factorial(N, Acc) -&gt;
    10 factorial(N-1, N*Acc).

The comma after Acc in line 8 should be a semi-colon.

factual:

&gt; A process can block to receive a message by using the receive construct. It works just like case: it takes any number of patterns which are matched against the first received message. The first pattern that matches the message will have its body executed. However, unlike case, if a message does not match, the message is thrown away.

The message is actually not "thrown away", it remains in the process's "mailbox" (to use the author's terminology) and could be subsequently received if a proper match is used, or *explicitly* cleared. It is not thrown away, it remains in the message queue.Well, in this case, I mean "enumeration" in the C++ sense, not the mathematical sense.I don't know how it can work with just spacing. I also like simplicity but there is some evidence that when comments are like A-&gt;B-&gt;C-&gt;D-&gt;E and so on to continue to move on the right for something that is just linear is a problem.

Btw I guess that to be able to take a complex reddit thread and render it in the proposed system is the only way to really check if it is a real improvement or not.

Thanks for the comment.I think he means the use of some indirection.  In this case, it was via integer indices into a map.
[removed]If you should use bytestring rather than regular string does this have any implications for using different formats such as unicode?

Most people don't use Word for that.  They use it as Notepad with fonts and bold.  If being able to have perfectly-formatted reports is your goal, though, you're using the wrong tool.  Save as PDF or use... Crystal Reports.  Or continue to pay Microsoft $500 every 3 years to get the formatting and slightly-smaller files.  It's up to you.  I'd prefer to keep the cash around.in another article, MS says
&gt;Successful attackers would need to ... figure out a way to trick the computer's owner to download and play an audio recording of the malicious commands.

you mean like visiting a web page?I'm guessing it means that you either need to use UTF-8, or build a version of ByteString which operates over Unicode characters.

Haskell's Unicode support is pretty minimal at this point. It might be worth writing an ICU wrapper for use with ByteString, if that's what you need.What does installation of the software involve?  e.g. a data cable?  A phone call to your server?  Keying in numbers?  How do you uninstall?I think it's that "weeks of study" part I haven't quite gotten down yet.  ;)"...humanity may ultimately have to accept defeat on yet another front"

Isn't it strange that to the (presumably) non-technical writer, figuring out how to best play a game is considered a defeat?  I would consider it a victory for humanity.Try valgrind (google it), it might help.  I don't know if there's a Windows version though, and I suspect from your description that it's on Windoze.
(Q) On Linux, guess what you get when you combine this small piece of Perl source code...

http://labs.cybozu.co.jp/blog/kazuhoatwork/my_projects/c/C

...with this information...

http://en.wikipedia.org/wiki/C_preprocessor

...about C pre-processor macros?

(A) You get the ability to create your own scripting language by redefining C or C++ in your own terms. Plus, it will run 100 times faster than Perl. (The author ran tests with a Fibonacci function.)

By default it already includes stdio and stdlib, but you could include others.

Here's a very simplistic example of re-interpreting C or C++ in your own language:

    #define println(s) printf(s "\n");
    println("hello world");

The binary is here and tarball of binary to compile on your own is here.

(- Invented by Kazuho Oku, Japan.)
So did he.

     enum { one, two, three };
     int i;
     for(i = one; i &lt;= three; i++ ) {
         printf("Enumerating an enumeration %d\n", i );
     }

edit: How do you format code here?
edit: Figured it outYou can transfer the file by either Infa-red or by using the cable supplied with the handset. We recommend using Bluetooth as this is the quickest and easiest method which takes seconds. Simply open the received file on your phone and follow the instructions. Installation takes less than 1 min

To uninstall:
1. Do a hard reset of the phone
2. Send a sms command to the target phone asking to uninstall the software from the phone (not all phone models supports this functions).Thanks, this is great info.  Now I know a little about how to protect myself.  I won't be buying a nokia phone in my lifetime.&gt; I've used the gnome and KDE tuning tools, but the effect was never really satisfactory. Window's cleartype tuner isn't perfect either, but I'm pretty happy with it. I'm really not fond of the way OSX renders fonts.

We're probably at the point where it's a matter of taste. I like the way I've got my fonts to look on Linux (even though I still tweak settings often). My windows-using colleague's fonts look ugly to me.

I guess my point was that Linux is no longer behind OSX and Windows on rendering quality -- all three systems render fonts in their own way. I guess I'm lucky that my system of choice also renders fonts in (to me) the most pleasant way.Its is not just the Nokia phone it could potentially be any phone based on Symbian operation system.. just to let you know..;)Does he sleep 10 hours straight?Frankly, were it up to me, plain ASCII would be more than sufficient for what I'm supposed to be doing.  However, it's NOT up to me, and I'm the only non-MS person in the group, and one of maybe a dozen in a company of over 700 people.  Our entire infrastructure is Microsoft-centric, from Exchange to ISA Server to IIS to "web" pages that only work if you claim to be IE to SharePoint.  Even development work internally is all Microsoft-centric...Visual SourceSafe, VisualStudio, .Net, etc. (Luckily, clients can and do specify other technologies, though we have people on-site at clients who are locked out due to the clients using/preferring Linux or OSX)

And by "report" I think you and I are thinking different things.  This is more "What I did in the past week" than "This information was exracted out of some other tool".  Hell, a wiki or internal blog would be more useful from my PoV, but despite trying to make some in-roads, the perception is that "we're more productive doing it this way", and it's not a fight I feel like fighting anymore.Damn, [that](http://en.wikipedia.org/wiki/Symbian_OS)'s like all the major mobile phone companies!

Thanks again.What do you mean with reverse engineer? the code is there for you to solve whatever doubt you have...

And the specifications of the open document format are open: http://en.wikipedia.org/wiki/OpenDocumentGood point. If you want to write portable code /bin/sh is the portable way.

Otoh. unlike Python or Perl, you can count (knock, knock) that bash script runs after 10 years from now in Linux. Python3000 and Perl6 scripts may also run, but your current Python or Perl code may not.So, in other words, Java creates 256 instances of the Integer object in the hopes that I will want Integer objects with values between -127 and 128?  Hey, memories cheap so why not cache 4,294,967,294 Integer objects so you have the whole range covered?  While we are at it, we shouldn't forget Long or Double either.  Yes I'm being sarcastic.  This seems to me to be a serious hack.QurlyQ.com will shrink your long URLs into a more compact and user-friendly form that is easy to remember. Give it a try!Whether or not Scala or Haskell need macros has nothing to do with my point (or Entroy's). The point was that this was "programming", not "metaprogramming". Our comments had nothing to do with with macros vs. no macros or whether not they were needed. (Note that I was being slightly facetious when I said that it _must_ be metaprogramming. It was neither macros nor metaprogramming.)
That is wrong in so many ways.

1. It only works on a very, very small subset of integers. Most of the time it is a needless check followed by the same allocation you would be anyways.
2. Allocations are cheap in a GC system and Integers are small, so it is solving the wrong problem.
3. Since the Interger objects tend to allocated randomly and live forever, you lose locality. Looping through numbers 1 to 10 could literally mean ten page faults.
4. It can kill performance on a multi-threaded application. If you have two or more threads that need to create Integers, they will be constantly blocking one-another. (I wonder if multi-core would make it even worse.)

The bench marks are not going to show this because, unlike most real applications, it doesn't expose the memory locality or the multi-threading issues.

I really think that Java's developers have no idea what they are doing. No wonder it is dirt slow.You know what else is your friend?  Irony.Now that Java is open source, I would like to propose the "The Horrors of Java" site. It will be like TheDailyWTF, but only code from Sun's version of Java will be allowed.I think, with the exception of native threads (because Scheme implementors might have a different opinion wrt the role of native threads than the SBCL implementors), I would say there are at least 3 open source scheme implementations that very much overlap in functionality with your list: PLT Scheme, Bigloo and Chicken Scheme. The proprietary Chez Scheme might make it too (they propose Emacs as a development environment).
There are pros and cons. For instance, PLT and Bigloo have better development environments than what Slime is. Bigloo, I believe, compiles to faster code than SBCL.It's just a clone of http://tinyurl.com. Nothing to see here. Move along.Long is also covered (as well as Short and Byte).  But if you don't ever call valueOf(), no instances are created.  Float and Double are obviously not covered.  I'm assuming the decision to cache this range of values was not a hack but a design choice made after contemplation of performance characteristics of real systems (particularly in light of the addition of autoboxing in JDK 5, which will frequently need to convert between primitive and object form).  

The caching is actually required due to the Java Language Spec (section 5.1.7) which requires that (most) boxing conversions return identical (not equal) instances.  I say "most" as they state this is not practical in all cases and this is a compromise.  

The discussion in the spec actually suggests that some memory-rich implementations may wish to cache integers from -32K to +32K.  

Now if you want to argue whether autoboxing should exist at all, I think that's a great question."All that"? The extra coding effort falls into the "negligible" category, and it's pretty much just as readable. I'd say that's worth a 15% performance boost. YMMV.Ok, my lisp implementation includes SB-BSD-SOCKETS as well. Any practical lisp implementation will likely have BSD sockets. So, by your logic, Lisp also has standardised sockets, especially when run in a POSIX environment. 

But, neither ANSI Common Lisp nor C99 specify sockets. That's right, sockets are not in the standard (which is the point i'm trying to make). Wishing it were so, telling people it's so, and expanding the definition of 'standard' to include anything you want is not going to magically change the ISO or ANSI definition!

Here is a copy of the C standard : http://www.open-std.org/JTC1/SC22/WG14/www/docs/n1124.pdf

Strangely, the word 'socket' _never_ appears, and POSIX gets a brief mention in the bibliography.

[deleted]1. I think "most" of the time, you're probably using integers in this range.  Hard to say whether "most" is 25%, 50%, or 95% of the time though.
2. I think speed is immaterial here.  It's more an issue of space allocation.
3. The Sun impl will allocate all of the numbers at the same time, probably actually increasing locality.
4. If you read the blog, you'll see that this is discussed - the Sun implementation has 0 synchronization overhead due to  the semantics of the JVM.  (The GCJ and Harmony code does incur synchronization.)

Regarding speed, that's a tired argument.  Maybe it was true 10 years ago but modern JVMs are pretty darn fast (comparable to your favorite language most likely).  There are certainly desktop Java perception issues due to JVM startup times and they are working to address that with the Java Kernel work.1. +1 - completely agree
2. You don't have to know.  In general, just use valueOf().  In cases where you are optimizing something for extreme performance, you might possibly need to know this.  But, I'd argue that that is typical of extreme optimization - you need to understand the abstraction at a deeper level to extract maximal performance.  

I don't think you want the JVM to know about Integer.java.  I'd much rather have a JVM that works with bytecode from any language.  What it could know about is detecting and caching immutable objects.  However, this gives you differences in identity vs equality that may have meaning in some particular application.Ah, of course. Given that C enumeration labels auto-convert to integers, that makes sense.I actually do like -- sometimes "conversations" seem to go on that are just A-B-A-B-A-B responding to each other. (I would say that one caveat I would add is that don't do this at the "root" comment responses, do it with minimum level to start at 1 indentation. Otherwise it looks (at first glance) like a lot of root level comments, and I tend to scan through and read root level comments sometimes to see where some of them might lead before reading all the nested; so root comments should (imho) stand out a little more.)

(edit: never mind, I see this is exactly what you are already doing, and it isn't possible to do it in the broken way I was describing. I was confused by some of the earlier graphics which showed a gray box directly above a blue box -- the later example shows the first reply being indented by the little green arrow thing which is the kind of behaviour I would like.)

The problem that remains with this is that this still becomes vertical spam to someone who doesn't care about this side thread going off into "conversation" mode. A feature to collapse/hide this side thread (or have side threads of depth N be automatically collapsed) along with of course and indicator that there is a collapsed side thread and the ability to expand it would go towards helping this.

e.g.: A, B, and C are root comments. A has a long "conversation" thread which vertically pushes B and C far, far down. If I could configure that "when conversation threads exceed 3, only show the first 3 by default" then the vertical pushing is limited, and by 3 posts I would know whether I cared to expand and read the rest of the conversation.Shoulder surfers have a hard time tracking fingers typing password at even as slow as ~40 wpm. Eavesdroppers, however, will have little problem with that (they could even record it if necessary).Point and shoot. The craftmanship end of photography isn't quite on the scale of sculpting. Mainly, you just need good taste and the willingness to position your body in order to frame a shot. That latter is where news photographers really earn their pay, either by going to war zones, or fighting with crowds, etc.Thank you sir! While i agree that it is the more morally sound route to 'teach a man to fish (google)', i'm not sure if commending my righteousness is warranted.. i did what any pious hacker would do in that situation.. not for reward or recognition, but simply because i knew in my heart that it was the _right_ thing to do.

Perhaps you meant to say self-righteousness? If that's the case, feel free to piss off. While you might be right, can you see the irony in pointing out another's self-righteousness? 

Unless you were being ironic from the beginning,in which case you sure got me good ;)whoa!

I would really like to know how this is going to pan out in terms of votes and comments..

The site is (pro) religeous
and has an article that is kind of anti (Microsoft) Vista


I can almost smell the smoke of confusion on reddit today.I have no problem with auto-boxing, I have used it in many languages without ever having a problem. But to actually specify something like that instead of making it an implementation detail seems really dodgy to me.That would have happened regardless&gt;The discussion in the spec actually suggests that some memory-rich implementations may wish to cache integers from -32K to +32K.

Thank goodness they didn't do that!

&gt;Now if you want to argue whether autoboxing should exist at all, I think that's a great question.

I think autoboxing is a good feature.  One of the major slow parts in Java is the allocating and deallocating objects.  To be fair, thats a problem with most programming languages.  My personal belief is that they should have done object pooling, where a certain number of Integer objects are just not GC'ed immediately but instead reused when needed.  A smart GC could scale this up and down depending on the demand frequency.  That would be optimal but would take some serious work to implement.  Thats just my 2 cents though.Emacs has some fundamental flaws that prevent it from performing well. See [The Art of the Interpreter](http://library.readscheme.org/servlets/cite.ss?pattern=Ste-78a) for a good explanation of general interpretation techniques around the time emacs was written (you should notice some of the mistakes emacs made).

Dynamically scoped variables incur heavy runtime costs (either looking up through alists or using value cells and making function calls expensive). Emacs also allows functions to be passed to functions by symbol or '(lambda ...). The bytecompiler can't tell if an argument is code or data (elisp has #' that lets the bytecompiler compile the argument, but the ' form is still allowed and more used).planner-mode lets you do all of this.

planner, bbdb, remember, and gnus (or VM if you want a traditional mail client) all integrate wonderfully. It also has support for erc and some other stuff, but I don't use that (yet).

emacs does a *bit* more than basic plain text editing :-)1. Most of the time I am using ints for database keys, often for things like hash tables. So other than some basic lookup tables, they almost never fall into this range.

2. Speed is always an issue when talking about core components like this. 

Memory, on the other hand, is essentially umlimited as far as most applications are concerned. The main reason to limit memory consumption is to avoid the performance robbing page faults and cache misses.

3, 4. Can you post a copy of the Sun implementation?
Pretty decent, makes for easy reading.

_"Now would be a good time to familiarize yourself with the Erlang reference documentation to figure out what ~.10B means. Hint, check the modules page, and find the io module. You are certainly welcome to create a more easily navigable API document."_

I'm not sure when this was written, but more easily navigable documentation exists [here](http://erlang.org/doc/doc-5.5.3/lib/stdlib-1.14.3/doc/html/io.html). Too bad I couldn't find it on google using 'erlang io reference' or even 'erlang reference'. The [library documentation](http://erlang.org/doc/doc-5.5.3/lib/stdlib-1.14.3/doc/html) seems kinda hidden away.I agree with you whole-heartedly. (I may even be convienced that caching isn't automatically bad if the Sun implementation is less beain-dead.)What about the cost of looking up the Integer object?

For expensive objects like database connections I am all for object pooling. But I have seen too many projects suffer huge performance hits by trying to use object pooling technology like COM+/MTS for trivial objects.
For crying out loud, stop selling Vista already!! You are retired! :))

The only funny bit was about him asking kids stupid annual job review questions.

Did you read the comments though? Apparently, some Macs in Mac stores are running Vista. Probably just to show that they can but..I vote for articles, not websites.glad to hear that, you are one of a small group.Nice historical piece.  I found it interesting that they did not mention Windows at all, even though 3.1 had been out for a year by that point, and was becoming rather popular.I must've been thinking of Blender. Silly me.Agree.  Object pools are usually not worth it for trivial objects or even for most objects that don't hold expensive external connections.  Object creation in the JVM is now very fast.This would require having some well-known notion of an immutable (and swappable) instance in the JVM I think as the JVM has no notion that Integer objects can be reused in this way.  Allocating objects is actually very fast.  GC is the slow part.1. But are you autoboxing or calling valueOf() with these?  Maybe so, I don't know.  
2. But saving the object creation is saving you a few nanoseconds per call.  Seems to me like you're more likely to see the GC overhead than the creation difference.  It's certainly not my experience (in the Java world) that memory is unlimited.  
3,4.  It's the first example in the blog.Bascially what Ricercar said, but in more detail:

Pointers are integers.  Operations on pointers are provided to read and write data of varying types from a store.  In C, these operations are expressed by combining (T *) casts and the indirection operator.  Further, it's generally accepted that using pointers for much more than low-level operations is a bad idea.

My example demonstrated that it's possible to reproduce all those characteristics (except the backing store) in purely function Haskell and that, in fact, it's still a bad idea. (-:  (And, hopefully, to convince people to laugh in the process.)That's the problem. The specification is missing so much information that you MUST look at a previous implementation like OpenOffice.

And who is to say OpenOffice got the spec right and say KOffice got it wrong. The spec needs to be complete enough to arbitrate differences in implementation, otherwise we are right back to having one application dictate the rules.

i don't even know how to vote yet...only comment. is it something to do with the arrows? probably should've read some 'howto' file on the site...**Virtualization**

File and registry virtualization helps standard users who have restricted access to the registry and file system write to these protected areas by creating a ‘per-user’ copy and redirecting subsequent data operations.

This means that if a regular user, or non-elevated administrator runs a program, and that program tries to write to a folder under Program Files, those writes are redirected to a folder to which the user has permissions to write.

Virtualization is enabled under the following conditions:

* The process token is for an interactive standard user. 
* The process is a 32 bit process (64 bit process have virtualization disabled).
* The process is not running elevated (This is disabled for administrators).
* The process does not have “requestedExecutionLevel” element in an embedded manifest. 
* The process is not marked for ‘NoVirtualization’ in the Application Compatibility database.
* The computer is running in Normal Mode (virtualization is disabled when booted into Safe Mode). 

**Registry**

Writes to HKLM\Software are redirected to HKCU\&lt;SID&gt;\Software\Classes\VirtualStore.

This virtualization applies to all registry keys in the path above, except for keys marked with a special flag to disable virtualization. You can check the registry flags to determine if a particular key is marked for virtualization by running the following command: 

reg flags HKLM\Software\&lt;key or subkey&gt;

The output of this command shows whether the key is virtualized:

HKLM\Software\Microsoft\Windows\CurrentVersion

        REG_KEY_DONT_VIRTUALIZE: SET

        REG_KEY_DONT_SILENT_FAIL: CLEAR

        REG_KEY_RECURSE_FLAG: SET

The operation completed successfully 

In the above output, “SET” means that the particular flag is configured and “CLEAR” means that the flag is not set. 
Keys that are protected by Windows Resource Protection are not virtualized.  This means that if ‘Trusted Installer’ is listed as the owner for a file or registry key under the security tab, virtualization will not be applied.

**Files**

Writes to: %programfiles%, %programdata%, and %systemroot% are redirected to %LocalAppData%\VirtualStore. 

Through Explorer, if a folder contains virtualized files, those files are viewable via the ‘Compatibility Files’ button that is displayed in the window toolbar. 

**Limitations**
The following limitations apply to virtualization:

* Impersonated callers are not virtualized, This means that files that are EFS encrypted or using NTFS compression are not allowed in virtualizable areas.
* Directory Junctions, Symbolic links, and Reparse points are not supported in areas of the file system where virtualization is applicable. (%programfiles%, %programdata% and %systemroot%)
* Specific file extensions like .dll, .exe, .sys are excluded.
* Specific system folders are excluded from virtualization.
* Non-System volumes do not have virtualization enabled.Hello sblinn,

the expansion trick is nice, when collapsed it may become like gmail's collapsed conversations. Thanks.Common Lisp, as generally used, is not operating in a functional way. Even Scheme in its relative purity has set!.  Python is used idiomatically in a way that is less-functionalish, but that doesn't change the fact that one easily can have this shadowing problem...this is why most of my input "list" are called "seq."

It is true that I sometimes operate on lists for lists sake, I'm suggesting it's a minority case in most parts of most programs I write.

Packages are a good feature of CL, although many people have qualms with CL's implementation. Even fairly well known CL codes like the AIMA sources don't use packages for this reason.[removed][Allocation and deallocation of short lived objects is FAST](http://www-128.ibm.com/developerworks/java/library/j-jtp01274.html)&gt;think as the JVM has no notion that Integer objects can be reused in this way.

The code in this article reuses 256 Integer instances.  I am guessing you may mean that the primitive wrappers in Java are immutable?  Yes, another bad design issue with Java.

&gt;Allocating objects is actually very fast.

Compared to what?  Is allocating objects faster than not allocating objects?  Think about it.Besides the obvious question (how are ordinary people supposed to learn that Vista works in such a radically different way?), I was struck by the observation that

"We opened the file with half a dozen different editors, which lined up neatly into two camps: MS apps (Notepad, Wordpad, MSWord, Visual Studio, even the DOS type command) that lied about the content, and /every other editor/ that told the truth and showed me my changes."

I'd love to have been able to listen in on the product development meeting where they decided that that was a perfectly fine way for the system to work.  It's almost a WTF.&gt;What about the cost of looking up the Integer object?

Stack.  Just pop it.

&gt;But I have seen too many projects suffer huge performance hits by trying to use object pooling technology

Pooling is the easy part.  The hard part is knowing how large the pool should be and when to reduce or increase its size.  See [Javolutions](http://www.javolution.org/) object recycling or its FastMap implementation for how this works.  I am only suggesting pooling as a replacement for holding 256 Integer instances when I first call Integer.valueOf().Discussed here
http://ifacethoughts.net/2007/02/02/translating-from-openxml-to-odf/
Translating From OpenXML To ODF on iface thoughts[But allocating a large number of small objects is SLOW](http://www.jelovic.com/articles/why_java_is_slow.htm)Silly idea bouncing in my head which is probably not suitable for implementation: I sort by "new" but often scroll to the bottom to get the oldest, but then have to then scroll back up searching through the reply tree to find the "original root reply". So maybe have stacking of replies go on *top*...[removed]&gt; std::map&lt; int,StackAllocatedString,std::less&lt;int&gt;,StackAllocator&lt; std::pair&lt;int,StackAllocatedString&gt; &gt; &gt; stackItAll;

that sure is beautiful code. remember, the spaces between the angle brackets are significant! (if i remember correctly)Because the religious are blindingly pro-Microsoft?There are some important ways in which monadic programming is different from imperative programming though, even in the I/O monad. Most imperative languages don't give you such ability to glue actions together in new ways. Representing actions explicitly gives you the ability to write your own control structures whose definitions are expanded at runtime. The Control.Monad module is full of such things that are by no means built into the language, and will generalise across all (or wide classes of) monads.

For example, let's consider the function 'sequence', which takes a list of actions and combines them into a single action, producing a list of the results.

Despite the higher-order nature of what it's doing, we can write it in a sort of imperative way:

    sequence [] = return []
    sequence (x:xs) = do {v ≺- x; vs ≺- sequence xs; return (v:vs)}

Or we can avoid the do-notation:

    sequence [] = return []
    sequence (x:xs) = liftM2 (:) x (sequence xs)

Or even more functionally, removing the explicit recursion, highlighting the transformation replacing the list structure with program structure:

    sequence = foldr (liftM2 (:)) (return [])

The ability to write higher-order devices for combining the programs you write is a major part of what functional programming is all about.

In some sense, yes, when you're working in the IO monad, you're writing an imperative program. If you choose to, you can stay on that level. However, you're also writing a program that writes an imperative program, and that's the functional part.

This ability is in no way diminished by the abstraction provided by the I/O monad. The only difference between the I/O monad, and what you'd designed is that the I/O monad prevents you from making a mistake in threading the world parameter (and there essentially is no option there in the case of the I/O monad, because duplication of that parameter would mean that you'd have to duplicate all the resources which the I/O monad has access to, including the network, disk, user, etc.).

I don't really like viewing the I/O monad that way though. It's far better to reason about it in terms of abstract programs which when run will produce values and the monad operations as various concatenative operations on those programs, or else in terms of its Kleisli category: thinking of functions of type s -&gt; IO t as side-effecting functions from s to t.Thx for that. I was searching for such a doc.[removed]It's C++! You expect it to be pretty?
Yes. I don't think packaging is a bad idea, but I don't think that Lisp-1ness is the abomination it's made out to be. I suppose I'm speaking for moderation.

I like both Scheme and Common Lisp and have spent more than cursory time with both.this I agree with. internal defines are not that appealing to me. Let and letrec are much better.

I also liked the named-let iteration construct.How hard is it to disable this nonsense?I'm commenting on this ONE MONTH LATER :-)  just to say that I was absolutely wrong in my prediction.  My projects now uses JSONObjects extensively.   I just wanted people to know that I am willing to admit when I am wrong! ^^

Great writing. I loved the mechanical bear analogy.Yeah, there are certain cases where threads are a bad idea.  Something like what you're working on might be one.  I'm just sick of the clueless people saying things like "threads considered harmful" without considering all the applications where they're almost a must-have for scaling and without considering that some coders find them just fine to work with.
Libraries.
Speed.
Legacy code.

That order.
Vista did not evolve from XP. That's like saying we are from apes... 

Vista WAS created in the image of Bill Gates penis..Correct or not, but still beautiful. As you see, YMMV :-]I vote however I feel like.. Sometimes I just 'up' randomly. Is this a moral issue? Do I have a duty to others to only 'up' links valuable to me? 

Sorry,  I just had my third beer... I could talk about current insurance rates for hours right now...It's not really any worse than the documentation for upvars() in web.py:

    upvars(level=2)  : Guido van Rossum sez: don't use this function.

(Incidentally, that's a terrible doc comment, because it doesn't tell you *why* you shouldn't use the function.  The problem with upvars - or any form of dynamic scoping - is that it limits the contexts where you can use the function.  For example, if you want to factor out common calls to web.render(), you can't just pull the call into a new function, because the default behavior expects to find template variables among the locals() of the caller.  On a new web.py project, I've imposed the coding constraint of "always specify vars instead of using the default web.render() functionality" for this reason.  Upvars() can be useful, however, if you *know* you will always be calling a function in the same way and want to save some uglyness in the caller.)If you mix up the letters for Vista it spells donkey. Try it![deleted]Why was this voted down? The feature in the article is poorly documented, otherwise it could be actually be useful. Posting it is positive.Thats certainly very weird. This ( http://www.hanselman.com/blog/VistasShowCompatibilityFilesAndTheScrumptiousWonderThatIsFileVirtualization.aspx ) explains the behaviour much better. Not sure I agree with their logic there, but I now understand why they are doing this. Its an interesting idea, keeping peoples app modifications under their home directory. 

Personally on my personal machine I would find it annoying, but in a corporate environment far too many apps write to places like d&amp;s/username/local settings , which is *never* copied with a roaming profile (outlook does this with its message stores, to name but one app)(yes there are workarounds).I'm getting closer.  [DejaVu Sans Mono](http://dejavu.sourceforge.net/wiki/index.php/Main_Page) seems to have pretty good coverage of the mathematical operators, but the combining characters don't seem to be well supported.  And on my xterm, the underscore character disappears when any character gets drawn below it.  I don't know if that's a problem with my xterm, or the font.Went back and read the comments, found the one you mentioned.

also found this:
"There are a million little nifty things that it does, and it hurts to go back to XP (even more to use a Mac)."

XP &gt; Mac OS with respect to little nifty things? What the..I see the impending onset of multicore CPUs as something that may finally drive C/C++ out to pasture.  Without major rewrites of your code, there's no guarantee that your C or C++ program will be using all the available cores, whereas languages like Haskell, being purely functional, can easily adapt (and really, have already started adapting) to these new systems.[removed]Thank you for posting this.   As a developer who will be forced to deal with Vista this is the info I needed.  

The spaces between the angle brackets are not supposed to be significant.

However, there are some C++ compilers (MSVC++ 6, for sure) that have problems compiling templates when you have "&gt;&gt;" or "&lt;&lt;" (less common), since the compiler will interpret that as a bitshift operator.  This problem in MSVC++ 6 has been fixed in Visual Studio .NET 2003's C++ compiler.and I think you're trying too hard to be dumb.And instead you want to rewrite legacy C code in Haskell? Are you mad? I work with PhDs who can't correctly write 3 lines of C++ code and you expect this kind of people to learn functional programming in Haskell? We still use C for a reason: it's easy to understand and to apply the usual concepts to this language.Yeh, it is a good idea, but I can't say I like it because I'm the only user on this computer.

But let's see. Maybe one can get used to this new things. I only wish someone would compile a list of all these new thing (I guess there are books though).I think the non-obvious thing about the Sun implementation is that they are using the static class creation trick to avoid synchronization and in so doing they do not have the option to lazy-load the cached instances.  They *must* pre-populate the full cache or else they are forced to incur synchronization at the time you ask for each value (which definitely seems worse).  This is the choice the gcj/harmony impls made.yes, it's hard to know whether it's a booster or, that the features take longer to make themselves felt than the reviewers spend.Don't you hate the way idiots take an interesting word, and use it for its effect, wrongly, and eventually mash its meaning to nothing special? You should have to get a license to use cool words.But he is right in that while C did fit the computing model 30 years ago, it doesn't so much today, and it is much easier to use a language suited to the new model vs trying to retrofit a jet engine on a horse.

Though...I'm not so sure Haskell is the future either. It really is too much for most programmers to handle.&gt;Tending to an itch is more about focusing on the symptoms versus what the article speaks of which is looking for a cure. 

Don't get trapped in the metaphor; here "scratching an itch" means the original goal a tool, application or platform is looking to solve.

&gt;I would like to know if the tasks involved in management are really NP-complete or whether they can be broken down into areas that can be automated. 

I suppose you could possibly conceive of a Managatron 2000 that could deterministically resolve problems ("problem: my chair does not recline. solution: new chair"), but then I think you could make the same argument for the programmer. Why don't we have machines writing our code now? For precisely the same reasons: there are creative, human aspects to the job that machines simply cannot replicate. Yet, anyway.Maybe python-style syntax could be added to C: a, b = b, aBecause C maps pretty directly to how current CPUs work.  If processors had "instantiate object" and "send message" opcodes, maybe we'd be writing our low-level code in Java or Smalltalk.I think Haskell is pointing the way towards the future, but is not the future itself.  There're too many annoyances with Haskell, and it's frankly too unfamiliar for most programmers.  However, I think the basic ideas - purity, monads, higher-order programming - really will have a large impact on computing once people understand all their implications.Ruby is probably the future for some types of high-level coding, but I'm skeptical of C going away for bare metal stuff.

Ruby gives you at least some of the power of a functional language but with a syntax that's quite approachable and beautiful.
Um, many of them do.  You can't expect everyone to work at Microsoft.There's always [Jekyll](http://jekyllc.sourceforge.net/index.html) and [Cyclone](http://cyclone.thelanguage.org/).

The article's right though.  Much programming language research focuses on the high-end, where people work in more abstract domains and can trade-off processing power for productivity.  There really aren't many alternatives at the low-end, yet C programming is quite painful.[deleted]Anders Hejlsberg? Your point has been refuted:)I like the idea a lot. Combined with a Kuro5hin-style ability to hide a whole tree of responses, I think it would be just about ideal.small simple image gallery/show using mootoolsConsidering that the author quoted this passage, 

&gt; The concrete syntax of Standard ML and Haskell are every bit as bad as C++.  It is a curious measure of the programming language community that nobody cares.  

I seriously doubt that Haskell is an option. And note that he is talking about the syntax, not the feature set, as being the issue; thus the "purely functional" argument doesn't really apply.I think Ruby's future will be determined within the next year or so. If a solid VM can get off the ground with Unicode support, decent speed, and everything that should be expected of a general purpose scripting language at this point, then yes, I could see it gaining serious traction. However, if it cannot, Ruby will stay firmly tied to Rails and will decline in usage over time with it (as I think Rails has hit the apex of its hype).

I think Python already has the mature implementation, but its not going to create a new wave of hype on its own. Its acceptance will be slow and steady, and as it is now the only language I can see slowing it down will be Ruby (and I gave the outline for Ruby's success/failure above).

I cannot use Ruby in its current form, even though I think overall it is a cleaner language than Python. Some apps I really need Unicode and most apps I do need decent speed. I hate to say this too loud because I love Python, but I'd switch (back) over to Ruby quickly if it got a decent VM.

Okay, sorry for the long off-topic response. C is it? :)An interesting idea. It failed in the past, but maybe we are at a point where it can succeed. But who will write the new OS from scratch to support it?[removed]Looks like you haven't gotten commenting down yet either ;) If you want to reply to a comment like perfectpussi's, there's a little 'reply' link under his.  That makes threading work, and that separates this site from metafilter.

The voting has very much to do with the arrows.  You click them.Um, why is the banner image for the ".NET Addict's Blog" a close-up photo of a Mac tower?True... I agree.

I tried to use Ruby as a backend for my evolutionary computation work.  The VM just wasn't up to it at all... Ruby ended up using 98% of the CPU just doing some simple stuff and returning a 64-bit integer.  It sorta resembled a dog tied to the bumper of a car.
Yes!  Good managers surround themselves with good people, ask them the right questions, listen carefully to them, and understand enough about technology to build up a series of good decisions.

The manager needs to know what is possible, who can do it, and whether some schmoe is dragging down the team.

It would be good if the manager has done enough programming to understand that there are always complications and that you can't save a late project by adding programmers.  Without experience, the manager must pick up this understanding some other way.
ok ok .... but if GNU smalltalk is on the list, Smalltalk/x should be there as well. ;-)You could certainly see how this feature would create some confusion, but it's also pretty bizarre to be storing your own files in "Program Files", which it sounds like this guy was doing.  And he doesn't seem to acknowledge that that's a strange thing to be doing...
This is a great article that misses the point. :-) Mainstream adopters certainly *care* about interoperability or devtools, but that's not going to be what brings a language across the chasm.  Rather, it has to address a certain "pain point" that's currently so painful that mainstream users will put up with just about anything to deal with it.

C did this with stack &amp; register allocation.  Assembly language programmers need to spend much of their time fiddling with memory maps and calling conventions.  When I had to write assembly code in college, my programs had more comments than code, most of it bookkeeping details about how memory was laid out, which registers I was using for which purposes, and how I would pass arguments between functions.

C++ did this for data hiding and abstraction.  It's not really that you couldn't do this in C: top C programmers write very well-organized programs that's easier to read than much Java crap.  However, not all programmers are great, and when C became widely used in industry, companies ran into significant pain when organizing and managing larger projects.

Java did this with garbage collection.  Memory management in C++ is so painful, particularly in multi-developer projects, that virtually all desktop software contains memory leaks.  Notice that Java really crossed the chasm first on the server, the one domain where memory leaks are absolutely unacceptable.

The problem with Ruby is that it's main advantage over Java is "less typing".  Typing is rarely a pain point for software organizations as a whole, though it's certainly annoying for programmers.  Java EE, however, has become so ridiculous that it's opened the door a little for Ruby.

If I had to pick a couple pain points that are likely openings for a new language, they'd be:

* Concurrency.  It's virtually impossible to write a correct multithreaded program that relies on several third-party libraries.
* State.  OOP hid state for us, but now that most important apps have to run over multiple computers, that abstraction layer has been cracked wide open.
* Bugs in other people's programs.  Most major software nowadays isn't written so much as cobbled together from other libraries.  Any bugs in the libraries get magnified as they're used by other libraries, so that by the time you get a full app, it's essentially impossible to guarantee that it performs correctly in all situations.  Anything that a.) fully documents the specs of a library and b.) ensures that a library meets those specs would be a great help.

You can see why I'm interested in functional programming. ;-)I don't see how my comment has anything to do with being "dumb".  The story has too many elements that just cannot be true: leading a crowd from Apple store to buy Vista?  A spam filter that works this perfect?  They are obviously attempts at humor, unfortunately they did not work for me.  

Sorry if my comment offended you MS fan boys.I've just read [this document](http://portal.acm.org/citation.cfm?id=606670&amp;dl=ACM&amp;coll=&amp;CFID=15151515&amp;CFTOKEN=6184618) written by two of the Clean developers. On page 26 there is a paragraph which I want to quote (typing errors and the part in [] are from me):

"This system [monadic I/O] is simple but has the disadvantage that all objects to be destructively updated must be maintained by the system in a single state which is kept hidden for the programmer. Clean does not have this restriction. One can have arbitrary states which can be passed around explicitly. Such a state can be fracture into independent parts (e.g. distinct variables for the file system an the event queue)."

This is a big disadvantage because it reduces abstraction: Instead of being able to write code which works on certain parts of the 'world', with monadic I/O it's an 'all or nothing'.

And if you read Wadlers "How to Declare an Imperative" (available [here](http://homepages.inf.ed.ac.uk/wadler/topics/monads.html)) where he compares different methods of doing I/O in a functional way you see another problem: While the monadic I/O approach is quite simple to use, it has the disadvantage that it can't emulate the other approaches while those can be used to implement monadic I/O. 

So monadic I/O is easy to use but also quite limited. Now I'm in principle a 'fan' of certain kinds of limitations and higher abstractions in programming languages, but things are different here: Monadic I/O simply is no 'higher abstraction'! It nothing else but the imperative model of computation implemented by the concept of monads. So it's a step back and no step forward. I would like to see advances in the other direction.

The reason why I see problems with monads in general is a different one: They allow meta-programming and I don't think that meta-programming is a good thing for application development. It's a different thing for certain other domains, but I'm primarily interested in languages for application development (because this makes up the major part of programming).

Man, I love that particular Joel article.  Java vocational training is NOT computer science.
[deleted]They're kicking [this](http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6336351) around for Dolphin. I love how all the assholes who don't know what the difference is between stack and heap are voting against it.&gt; it's also pretty bizarre to be storing your own files in "Program Files"

Not if you're the guy who's developing the program, and you need to change the program's data.Very nice!http://programatica.cs.pdx.edu/House/ These guys?[deleted]And you wonder why there's a stereotype that nerds never get laid.  There's even an article about it!The blog you cited is appalling. The author comments at great length about advanced language features when he clearly has no understanding of the basics.
[deleted][Pretty easy.](http://www.ubuntu.com/)This was from a document received during Vista training for Technical Support, I looked but could not find it available on the web. Hopefully, Microsoft will help us out by publishing this - it is quite clear information.haha yeah if people made it to the end.. that was hilarious[deleted][deleted]Oh, look, manual memory management :-).
What's with all the PDFs lately? Can't they just make a simple HTML page?
Squeak may be the best thing since sliced bread, but every time I try it out it makes makes my eyes bleed.

So Alan, please: Hire a graphics designer.
Actually, it's fairly simple to create restrictions of the IO monad. In fact, using typeclasses, one could even make them fairly convenient to use together. The only reason that it's not broken up in the current Haskell prelude is that typeclasses were fairly new in Haskell 98, and certain approaches would involve MPTCs and functional dependencies, neither of which was around when H98 was written. The problems they're discussing only seem to be there due to a lack of imagination, I suppose. Consider newtyping the full IO monad and simply providing only file operations in the newtyped monad. The data constructor for the newtype can be hidden by module boundaries, and then you have a monad in which only file I/O can be done, together with a very simple projection back to full IO. Altogether, it would be somewhere around 4+n fairly trivial lines, where n is the number of file I/O primitives you want to provide.

Actually, the monadic approach can be used to emulate other approaches to I/O. Wadler was either wrong there, or else he was saying something other than what it appears taken out of context. Look at the 'interact' function in the Haskell prelude for a trivial example of that. One could extend that idea out to provide the old stream/continuation-based model of IO which Haskell once used. Claiming that it couldn't is absolutely ridiculous.

Quite simply, it's not limited. Every approach to I/O is equivalent once they get to a certain level of expressiveness regarding the system, but some are safer or more convenient than others.

Metaprogramming can be extremely practical for application development. Constructing a domain specific language to solve problems of the type that your application is going to solve, such that your application becomes an easy consequence of that language gives you incredible flexibility in the face of future changes to design requirements. You've essentially solved an entire class of similar problems. This pays off when you actually have to maintain the code that you write.A good story here.
Strange religious setting.
Capital I's please.Ah - new version of Windows, new list of "features" to switch off.  With XP it's a fairly short list: personalized [sic] menus, extension hiding, menu animations, the new Start menu layout, the new Control Panel layout, and so on.  Looks like Vista will have all these and more.

Gotta love that Ubuntu.I've been developing software for quite a while, and have in the last year gotten with the new Microsoft SharePoint stuff, and all the attendant Visual Studio accoutrements that go along with .NET programming.

I often kvetch to my cow orkers that there's something about the high-level, general approach that Microsoft takes to creating tools for developers that just sits wrong with me, but I have difficulty describing what that approach is that offends me (a unix-y, emacsy-type guy.)

The closest I can get to describing the feeling is calling it condescension, and this Vista anecdote is a crystalline example of the typical Microsoft sort of thing that drives me nuts several times a day.
You see though, this is the wrong way to do *anything*.  It's one thing, like in XP, where if you don't have it set otherwise it gives you a warning about not editing the files.  But this, to give crazy copies of the files to certain apps, is just ridiculous.It _may_ push C/C++ further out of application space but the OS will be written in C and assembler. That is unless an OS based on a safe language (obviating the need for a protected kernel and allowing user space threading for example) takes off.*Why do we still use C?*

Ever used a TV, radio, MP3-player, car, train, plane, ATM, alarm clock?

The keyword is "embedded" and embedded software makes up about 70% of all software produced, you fucking Java-monkey.The idea of a virtual file system was implemented by Sun Microsystems back in about 1985.  They did it so each user would see their working source code. It was strange how you could look at the same file in two different windows (logins) nad see different content. I hope M$ is not trying to claim this as an original idea.I did. It was charmingly written throughout—great topic matter, and witty presentation.This is why I look at D with interest.But note that if you care at all about speed, you [really, really, really](http://shootout.alioth.debian.org/gp4/benchmark.php?test=sumcol&amp;lang=all) want to use ByteString.That developers even have to think about this is really indicative of a big part of why I sometimes dislike working with Java.  It's not the basic design issues (ie functions not first-class objects, strict typing, lack of introspection) that set it apart from languages like Ruby or Python.  It's the utter lack of elegant, clean ways to code certain things.  The fact that you have an int primitive and an Integer class--annoying enough at first--is made worse by the fact there are multiple, subtly different ways of converting between the two.

I mean, things in Python just work (unless they don't, when they misbehave in unpredictable and hard-to-debug ways).  Want a dictionary in python with integer keys? Do my_dict[i] = 'foo'.  It works.  In Java you need to import the Hashtable class and then convert (cast? int's not an object though) it to an Integer first, then set assign the key.  All in all, you need maybe 2x-3x as many lines of Java to get things to Just Work.  When dealing with new libraries you haven't used before, I've found it takes far longer to figure out how to use a Java library than a Python one.

Don't get me wrong,  I think Java is pretty good overall, and it certainly has its advantages over Python.  But things like this get really annoying.What?  No.  It's actually very difficult to write correct lines of C code.  Most people will miss loads of funky stuff that could happen.  For example, declared but uninitialized vars are filled with random data, pointers in general, null references, memory allocation, etc etc.

If those PhDs happen to be mathsy, haskell would practically be natural.  The main reason that C++ would be difficult for this type of person is that they haven't grasped the computation model - because it makes no real sense.

Now that you have ingrained the model in your mind, it does seem easy to understand and apply.  Starting from blank it's quite convoluted.

I switch back and forth between haskell and C++ every day.  I know.

Not that I'm saying that Haskell is a good to-the-metal language.  It's too abstract for that.I use C to write small amounts of low level code, and that's all. It does the job fine. I'd prefer something with simpler syntax and a better macro system, but its not really important when you don't have a lot of code.No, no - the '&lt;&lt;' token is indeed a shift-left operator, but the other compilers just were smart / helpful about allowing it in templates (which MSVC 6 didn't). But, allowing it has only recently been made official (or officially destined to become official) but MSVC 6 wasn't wrong, just, the others were extra helpful, ahead of time.This is a similar effect you can get in FreeBSD by playing with "Union" type of filesystem mounting points.  Its a great way to send yourself crazy.

eg. What happens if you want to delete a file you don't have rights to?  Does it record the deletion in your private directory so you can't see the file any more but other users of the machine can?

I know Windows is designed with DEUs (Dumb End Users) in mind, but I also think its important to behave consistently and in a way that people expect.  This feature does neither.
To quote Wadler from the mentioned paper:

"From monads to streams. One may also ask whether the stream model can be
implemented in terms of the monad model. The answer is yes and no. One can
write such a function. but it turns out to be incredibly ineffcient. So monads are
easily defined in terms of synchronous streams, but not conversely."

and

"From monads to linear state. There is no obvious way to make the converse
definition of linear state in terms of monads"

The paper is a bit old (around '95), so maybe something has changed since then.

But you said yourself that the I/O monad "prevents you from making a mistake in threading the world parameter". With a direct streaming approach this wouldn't be the case. 

Also you wrote that "duplication of that parameter would mean that you'd have to duplicate all the resources which the I/O monad has access to, including the network, disk, user, etc.". This is only a problem because the IO monad is monolithic. If you split the 'world' in little parts, it could become quite useful to duplicate such a small part of the world to make certain computations more easily (for example to automatically undo file-changes in combination with backtracking). Even restricting it by using typeclasses won't change this unless the compiler uses this information - but in Haskell this is no problem because it's simply impossible.

And I still don't see what so 'expressive' with monadic I/O. If we could for example copy or rewind the 'world' this would provide an additional level of abstraction. But thats not possible. So what's the advantage here compared to every run-of-the-mill imperative I/O?

BTW: Any clue why WinHugs often hangs-up if you try to do something with I/O? I find it much more convenient than GHC to try out small things.
An ad for an F# book.Re: multi-core processors driving out C++: No, that's exactly wrong. For anything other than 'embarrassingly parallel' programs, it's _hard_ to get 2x 1 CPU performance from 2 CPUs. GC'd languages (which [being fnal requires](http://en.wikipedia.org/wiki/Funarg_problem)) are never more than 1/2 as fast as C++ (comparison between C &amp; [Java](http://shootout.alioth.debian.org/gp4/benchmark.php?test=all&amp;lang=java), [Ocaml](http://shootout.alioth.debian.org/gp4/benchmark.php?test=all&amp;lang=gcc&amp;lang2=ocaml), [Haskell](http://shootout.alioth.debian.org/gp4/benchmark.php?test=all&amp;lang=gcc&amp;lang2=ghc)). If you could get 2x performance by writing something in C++ instead of a magically-parallel fnal language, you'll do that. So, I claim that if you want more performance, and it's not 'embarrassingly parallel', the first thing you'll do is re-write it in C++. 

Then too, many algorithms are simply not parellelizable, or the communication overhead tops performance off at a certain number of CPUs, and you're back at squeezing out every drop of performance you can with C. 

So no, as [Moore's law flattens](http://www.gotw.ca/publications/concurrency-ddj.htm) (but maybe it's gotten straightened out with IBM's discovery), C++ and variations will continue to rule the roost for performance. 

-- (I've edited this heavily to include points made below).awesome.  
great reply.
thanks&gt;...no option there in the case of the I/O monad... ...duplication of that parameter would mean that you'd have to duplicate all the resources which the I/O monad has access to...

Well, there's always [forkIO](http://haskell.org/ghc/docs/latest/html/libraries/base/Control-Concurrent.html#v%3AforkIO) and friends.
how about some substantiation that this approach is really faster?don't know about 'we' still use it.
_i_ still use it because when i get home from my job doing maint. coding a powerbuilder 6.5 app, and developing this stupid c#/asp .net app, i really don't want to have to fool around with languages that i loathe.  i want to write what i want in the language that i enjoy using.&gt; You're exactly wrong. It's hard to get 2x performance from 2 processors.

I'm not seeing where he made this claim to begin with.

&gt; If you could get 2x performance by writing something in C++ instead of a magically-parallel fnal language, you'll do that.

I don't know Haskell, so I have no idea what kind of interfaces it has to C or C++, or how easy those are to use. I'm guessing though, that as time goes on and we, as an industry, spend more time employing parallelization to solve problems languages with better support for it will dominate, and "everything else" will be reduced to point-usage where it is specially suited to the problem.[CCured in the Real World](http://www.cs.berkeley.edu/~smcpeak/papers/ccured_pldi03.pdf)(pdf). CCured is *fast* and usable for system programing. See the chart on page 9 for more info about speed and the programs modified.Yes, users shouldn't be modifying it, but you shouldn't lie to the user either just because you think you know what's good for them.  That's the height of user UN-friendliness!A pretty darned good list.  A must-read for anyone considering performing contract work.

I've been bitten before (but it was on my *3rd* project.  My first two went rather well, actually).  I spent 10 hours on a $100 project I thought would take 2 hours, and didn't even get paid for it in the end.  You learn quickly, and a couple of lost days isn't the end of the world.Nice! It's impressive how much optimization you can get with GHC's rewrite rules.Parallelization as an end in itself?I generally agree with you, but don't mind serving as a counter example.  

I actually use this all the time.  If you are updating an existing bash application, and need to look up how to do something new ...  it's worth giving this a look, instead of adding the overhead of launching a perl interpretter.  

Not a common use case, and yes, occasionally it is easier to use perl ...  but the version of perl on the installed systems I'm working with versus the version of bash makes sticking bash easier and more efficient. 

Since half of the solution already existed in bash and the destination systems are known, there has been no reason to change scripting languages. Further more the initial time constraints did not provide time to learn the more capable, but more complex perl or python.  

This temporary solution also does not justify transferring everything to perl or python, as it's already scheduled to be automated in one of the java apps for the next release.  
No, he doesn't make that claim but if the point is efficient use of processors, C languages beat out automagically parallel, fnal ones, _for even single processors_, handily (see my links to GC'd, and 2 fnal ones, above (actually Ocaml comes out quite well, but it's not purely fnal)). Then, parallelization is hard on a program, it takes extra programmer effort - parallel algorithms are written differently than serial ones (eg, quicksort). Add, that communication between processes slows everything down, and that not all algorithms are even parallelizable, and 'lots of cores' does not become a solution to the loss of continual doubling of performance of single cores (except for easily parallelizable problems). 

I see all kinds of talk about how multiple cores will save us, but, if you've ever taken a parallel programming class you'll realize the first thing to do is to avoid having to do that if you can. Of course, very parallel problems are nice &amp; fun for it, and sometimes the problem is so big that you can't avoid it, but, it adds pain and costs. Given the numbers and facts above, say you want a 2x performance increase in some complex app - which is cheaper, going from 4 to 8 CPUs (which may not even be effective, you may have 4 tasks but not 8 to make use of them all), or re-writing in C? 

Not to be nasty to you (more an aside to myself) but, in programming forums I haven't seen a lot of people who are familiar with parallel programming. 

Maybe you're right though; fancier graphics will be done by fancier graphics chips, speech recognition by speech chips, and physics by physics chips. In which case you're probably right. But in general, N x 1GHz CPUs &lt; 1 x NGHz CPU.It only takes one graphics designer anywhere to change a Squeak image and publish it and her changes.That's not quite the same. You actually have to duplicate *the world* if you want things to remain referentially transparent.  That means the entire network, the user, and so on.I agree, it should just deny permissions instead of making virtual copies.Moore's law is about transistor count. It's still on track. 
What's stopped is increases in clock speed, killed by heat generation. That's why we're talking about 64-core machines and beyond in the "near future"; we've got all these transistors, but you can't keep throwing them all at making a single core go faster and faster.

(I've got myself a sneaking suspicion that once we start going multicore, we're going to see the average size of a core go _down_ from the current size, so we may indeed get to 64 really quickly, but all I've got is my intuition on this; simply replicating cores designed to be fast single cores seems likely to not be the optimal speed decision when your trying to choose between 32 really efficient cores or 128 slightly-less-efficient cores that are 1/4 the size.).. ok; "the Bastard, Non-Version of Moore's Law Which Says Proc. Speed Goes Up Every 18 Months" then :)The sad thing is that everyone must now absorb this and deal with it. It's in there, it's not going away, so add it to the support database, and expect calls from irate and confused users for the next 5-10 years.

Microsoft needs to remove these layers of cruft, not build more of them. This is yet more code for them to maintain and write countless support articles about.&gt;BUT there is logic to the madness.

The complete technical history of MS operating systems in a nutshell.Two things that are missing that I've found to be very important.

First, identify the single point of contact in the organization who will be responsible for enumerating deliverables and accepting those deliverables' completion.  In other words, don't get trapped into trying to satisfy  multiple people or groups within the client organization.

Second, clearly define any and all review processes for acceptance of completed work.  This includes all intermediate milestones as well as delivery of a completed project.  Codify what happens if the client cannot complete one or more reviews (technical reviews, acceptance reviews, what have you) in a timely fashion.

As a contractor an important goal, indeed an important  prerequisite, is to ensure that you don't suffer at the hands of the normal tendency of organizations to institutionalize responsibility avoidance.&gt; GC'd languages (which being fnal requires)

assuming fnal = functional:

I don't believe that this follows.  In a pure language, the scoping of each unit of storage is explicit, and can therefore be deallocated when it goes out of scope.  Therefore there is nothing in the maths that requires use of a garbage collected heap. (You might still put things on the heap, but with direct deallocation criteria, rather than a GC.)

Now, actually _writing_ a compiler that does this analysis everywhere, completely, and correctly is damn hard - which is why a GC'd heap is used.  This practical concern is different, and ultimately surmountable.

In practice, I can see improvements being made to scoping analysis until the use of the heap and GC has non-noticable impact, in most cases."It sorta resembled a dog tied to the bumper of a car."

Funny.  :)  But in which sense do you mean this?  Front bumper or rear?  Tied tightly (touching the bumper) or loosely (a few feet away from the bumper)?And render standard, non-privileged accounts useless so people create administrator accounts by defaults just so non-standard-conforming programs work? Nah. Virtualization is good, what's lacking is there should be some kind of warning (system tray popup, perhaps) when a virtual file is created instead.Well, actually I don't know this stuff first-hand, but I was going by [this wikipedia article](http://en.wikipedia.org/wiki/Funarg_problem)...*Just thinking about all the code I have in RSS Bandit that tests URLs for equality, it boggles my mind to think a standard library could have such a craptacular implementation of the equals() method.*

Yes, after all, who cares that it is clearly stated in the Java-docs, because modern webmonkeys are way to hip to read pesky documentation.

Fucking asshats.[removed]Which it won't, because nobody likes an operating system that restricts their choice of programming language.Ah, right.  You were referring to the special case of closures, rather than general objects.  

However, all the funarg problem shows is that you need to use the heap - not that you need garbage collection.  It's still possible to do it all with static analysis and scoping.  Work out how high up the call chain the funarg can get to, and then deallocate on exit from that scope.  The performance hit then comes once, at compile time - which is, when performance and saftey both matter, probably the best place for it.

Although, ugh, that's really unpretty to actually do...[removed]Hmm very cool.  I wonder if this would work in combination with KVM to get a state snapshot of a virtual machine.Actually I just had a half-informed impression, and got carried away in my claim. I figured there had to be _some_ reason that functional languages are slower. Thanks for the education :)Apparently I'm the only one who thought, "wow, that would make it really easy to install legacy applications (ie, designed to run as admin) as a normal user."

In an ideal world, a kludge like this wouldn't be necessary, but there's a boatload of programs out there that assume admin privileges, and this will help ease the pain a little bit for end users who just want to use their software.Its my PC, I ought to be able to store files wherever I want. All that directory should be is a naming convention, not a super special directory with secret virtualization powers. 

Its a crappy naming convention, at that. I wish they wouldn't stick a space in the middle of one of the most important directory names in the system.[deleted][deleted]$1 million dollars/20000 lines of code == $50 per line of code.  Not bad work if you can get it.[deleted]Yeah, I'll give 'em that.  They continue to excel at inventing new levels of "awful"...So I guess we can conclude that all graphics designers hate squeak (...or don't know how to program)."joke becomes true" versus "improbable joke situation turns out not to be impossible"In the interest of full disclosure, yes I released this project. No it does not have ads, nor do I charge for it.[deleted]When I think of Squeak, I am reminded of this quote from [here](http://www.90percentofeverything.com/2006/11/23/why-the-olpc-needs-lots-of-usability-work/):

&gt; While lots of hard work has gone into the UI design so far, it seems they are getting ahead of themselves and chasing their own dreams. The whole ‘breaking away from the desktop’ smacks heavily of academics who have finally found an outlet for their wacky ideas. Creativity is of course very important, but it has to be tempered within the requirements of the target audience.I could not agree more with your advice to identify a single point of contact within the organization.  I hadn't learned that yet when I did some design work for a non-profit where different people would call or email with contradicting demands less than five minute apart.  It was hellish.It's hidden away on erlang.org, below the 'documentation' link and the 'downloads' link, in HTML (your 'library documentation' is just the documentation for stdlib, which isn't everything -- which isn't even the core of everything, contrary to C naming: you'll find the core of everything in the 'kernel' docs) and as partially-similar-but-not-equivalent manpages.  "I can't find the documentation!" is a whine about Erlang that I still haven't the faintest understanding of.  To break it down:

0. http://www.erlang.org/

1. 'Documentation': http://www.erlang.org/doc.html

2. 'Erlang/OTP R11B documentation': http://www.erlang.org/doc/doc-5.5.3/doc/

&gt; Online www documentation for the run-time system as well as all the libraries. Also available as an archive of HTML documents from the download page or as separate PDF documents.

3. On the left, you'll see 'Erlang/OTP', 'Erlang Programming' (start here), 'Working with OTP' (save this for later studying), and 'Applications'.

4. Under 'Applications', you'll see the already-expanded 'Basic Applications' (including 'kernel' and what you called 'the library documentation' -- stdlib), and also 'Database Applications', 'Operations &amp; Maintenance Applications', 'Object Request Broker &amp; IDL Applications', and 'Tool Applications'.

5. Each list of applications contains -- as you might guess -- an application.  Each application contains, most obviously, a list of documented modules.

6. Each HTML link under each application HTML link under each 'Basic Applications'&amp;c points at the Reference Manual for the application.  What you called 'The library documentation' is the Reference Manual for the 'stdlib' application, as the title of that page might suggest.

7. If you look at the top-left of a Reference Manual for an application, you'll see related documents: the Release Notes for that application, an 'Off-Print' link, a 'Top' link back to where we started, and -- for some modules -- a *Users Guide*.

8. So, starting at the start, let's find out how to create GUIs in Erlang.  Well, the 'I' of GUI means 'Interface', so... yes, 'Interface and Communications Applications' has a 'gs' link.  Follow it.  OK, this is the GS Reference Manual, and it has... one documented module, 'gs', which tells us nothing about how to create GUIs in Erlang.  Now where would we-- ah hah!  A User's Guide!  Following *that* link, we find: [GS User's Guide.](http://www.erlang.org/doc/doc-5.5.3/lib/gs-1.5.5/doc/html/part_frame.html)  Unshockingly, it contains numerous examples that show off basic GS features as well as different ways (e.g., build parts of your GUI as a big nested data structure) to manage things in GS.

9. Some basic documentation you'll find in stdlib, some you'll find in kernel.  Some things you'll find under 'Tool Applications' and 'Database Applications' should interest you greatly.  Erlang has *excellent documentation*, especially in contrast to other languages, and you should familiarize yourself with it instead of trying to search the entire web by keyword.

10. Me, I compile Erlang/OTP myself and install it in `/usr/local/lib/erlang` , and then cd to that directory to untar `doc_html` and `doc_man` both.  I then open 'links' and make a bookmark of `/usr/local/lib/erlang/doc/index.html` (IIRC -- not on my machine, now).  'links' has a hotkey of 's' that opens its bookmarks, and it always highlights the last bookmark you used.  So I just hit 's enter', search down to e.g. `Basic Applications/kernel/mod_tcp` , hit 's enter' again, and find something else.  This method is quite speedy -- for web browsers like Safari and Firefox, it works for me to have a tiny button below the addressbar named 'e' that points to this same localhost documentation.

11. This method of untaring the HTML documentation in the installed `otp_src` root has a nice effect: the HTML docs mirror the source tree, and just get dumped in a subdirectory of `stdlib-$version/` , next to `stdlib-$version/src` (where the Erlang sourcecode resides) and `stdlib-$version/beam` (where the compiled-at-buildtime .beam files reside), and `stdlib-$version/include` (having e.g. record definitions).

12. A warning: don't take newer documentation and untar it in this way over an older source tree.  Erlang nodes, when they load an application for the first time, seek newer versions -- and will fail when it happens that `stdlib-$docversion/` doesn't have any compiled .beam files in it.Not a bad start, but they are going to need a lot of support. A commerical-grade printing subsystem alone would be enough to overwhelm them.Are we graphics designers?  What do we know? :-)  Me, I find the environment soothing.Hey clueless, did you bother reading the title, let alone the article? The whole point was that he uses C for systems programming, but wants something better.Does a mechanical bear shit in the woods?
ByteStrings have different complexity, and different strictness properties to [Char], they also don't encode the same character sets.That's pretty interesting. I bet you could build a good number theory curriculum around that idea; giving number theory a REPL couldn't be all bad, given the abstraction of the topic. And it would make for some interesting homework possibilities that might feel a bit more "hands on" than normal proofs.I'll buy that. :)

(I feel it's worth pointing out as a lot of people are formulating it that way, and "meaning" it. The situation we'd be facing as programmers would be totally different if weren't going to be advancing _at all_.)Yeah, I find it much easier to reason about math if I translate it into Haskell programs.

There's actually a very nice math textbook based on this approach; search for "The Haskell Road to Logic, Maths and Programming." It covers logic, relation, functions, and a lot of other cool stuff--including Babbage's difference engine (well, the theory behind it, at least).mcafee site advisor says that site has spyware/adware.[The official homepage](http://homepages.cwi.nl/~jve/HR/) has a sample chapter; nice. I can't argue with the applicability of Haskell to that task!

That book doesn't get into Peano arithmetic based on the table of contents, but assuming Haskell is consistently used throughout the book as it is in Chapter 1, it's certainly a good proof-of-concept of the idea.There is an implementation of the Peano arithmetic in chapter 8, but I haven't read that far yet. :-)

The average chapter is about half math and half Haskell. Chapter 1 has more Haskell than average, because it's introducing the basics of programming.

The chapter on proofs is really good, with lots of practical advice, but it's the one chapter without any Haskell. All in all, it's an excellent book.Well, given that allocation takes one instruction (add to a pointer) and freeing is free, I can't see how it wouldn't be.

This is definitely for special cases though, not for general code.  You'd never use this in normal everyday application code.  It's for things like optimizing an inner loop in a game when you're trying to squeeze another 2 FPS out of it.

Then again, I really don't see why you'd write normal everyday application code in C++ most of the time... unless you wanted a really fast desktop app or wanted direct access to some good C++ libraries.  I'd pick Java or C# over C++ where speed isn't the #1 concern (and these languages are reasonably fast).  Server-side I'd pick Ruby or Python.
I had a pooch like this once when I was a kid... you know... poor guy probably kept up with you for a mile or so... sniff... tough little mutt...

:)
Meh. Dividing infinity by 2 (implemented in C using IEEE floats):

    $ cat inf.c
    #include &lt;stdio.h&gt;
    int main () {
      float x = 1.0/0.0;
      printf("x = %f, x/2 = %f\n", x, x/2);
    }
    $ gcc inf.c
    $ ./a.out
    x = inf, x/2 = inf
[deleted]Good additions.  Not having a clear, unambiguous means to determine when you are done can lead to much heartbreak.

I try to get requirements defined in such a way that I can write a pass/fail test for (ideally) everything.  

"Must be fast" is not an acceptable requirement.  Neither is "Must look cool."

Objectively testable requirement are not always possible, though, so the issue of trust and good faith is important as well.[deleted]With all due respect to bbdb, gnus and the rest, none of them provide the kind of "no silos" database I described above, AFAICS. Unless (or until) Emacs *does* provide such a database, it's not a super-PIM in the sense I described above.  It may make for a nice and well-integrated PIM but that's not the same thing.via
http://ifacethoughts.net/2007/02/03/format-string-vulnerabilities/
Format String Vulnerabilities on iface thoughtsMy point was (is?) that a lot of tasks could benefit (perhaps marginally, perhaps more) from parallelization, but it is generally avoided due to the difficulty. You're right, why go through all the trouble to do things in parallel when it is easy, convenient, and dead simple in-order.

As time goes on I fully expect the programming community at large to pick up a better understanding of parallelization techniques for two reasons: 1) there will be some problems where it is a better solution, and 2) no matter how nasty a parallel solution is, it keeps them from having to learn enough C or assembly to get close enough to the computation to make a real difference. (We're talking corporate programming here, where mixing languages is anathema unless at least one of them is XML)

What will eventually happen is that parallelization techniques will become part of the standard skill set, and parallelization support will become as much of a requirement for the corporate programming language du jour as object-orientation is now. At that time, all the stupid little things that are done in-order now that don't need it will be relatively simple to do parallel. It isn't necessarily that I think parallel solutions will be faster than C rewrites, I think that C rewrites will be needed less because a single processor can be dedicated to one problem and all the peripheral crap can run somewhere else. (Yes, there are obvious caveats, but in the large I expect this to happen)

I don't expect "throw more cores at it" to really be a practical answer without looking at the problem first. Then again, I never really considered "throw more cycles at it" to be a practical answer either, it just had the misfortune of often proving to work.Closer to $1 million for -180,000 lines of code.&gt; 5 years ago is already irrelevant today because technology changes so quickly

Name a fundamental advance in the art of programming that has taken place in the past 5 years.

Fashions come and go, incremental improvements do not always have staying power, in some respects mainstream practice has still not caught up to Xerox PARC. Just because something changes does not necessarily mean that it has been improved...So it's completely all right that the standard library function doesn't report whether two URLs are actually equal, because it _documents_ that it doesn't report whether two URLs are actually equal?

Let me introduce you to my new standard Java library; it implements integer addition with a method that actually adds a random floating-point number to each result, but hey, it documents that so you know that it doesn't correctly perform addition and can roll your own replacement.A tribute to Ada's Jean Ichbiah
http://www.regdeveloper.co.uk/2007/02/02/ichbiah_obit/
Programming pioneer dies
His time table wouldn't hold up in court.  It simply isn't physically possible.[deleted][deleted]I think at the time they wanted to show off that they were no longer restricted to DOS style 8.3 names.  "Look!  Now we can finally have names longer than 8 characters followed by a period followed by 3 characters, just like real operating systms have been able to do for ages upon ages!"

That and of course sticking the knife in and twisting it for anyone who likes to use the command line, since support for those long names under the windows command line is always kinda flaky.But they won't let me do that at work :([deleted]You can't connect to Exchange if you don't have Web Outlook enabled, because Evolution essentially uses the web interface to log in to your account and screen scrape it. It does not have any kind of native protocol connection, it is a last resort hack. This means it doesn't work half the time, and most of the functionality you'd expect is not there.

The proprietary "published" API is a closed-source, Windows-only, binary SDK library. So pretty much useless for the current discussion.

Just thought I'd add some fire. ;pNo!  It is the new O'Reilly killer, which is to say 'yawn'Yeah, I agree about the architecture expertise being a bit biased.

True, it is a bit chicked and the egg.  However, I think that if a popular OS is written in a high level language which allows multiple backends, cpu architecture design could finally be allowed the freedom it deserves.

I think it may be quite possible to have a high level language which allows stuff like the allowed numeric types to change.

There's always the problem of porting the runtime, though...gcc is probably not the best project to single out.  Perhaps a vendor c compiler.

I'm willing to bet, while we're interneting, that Fortran compiler writers know a hellova lot more than your stinky c coders :P

They've been putting twice as many years into optimizing numerical code.
I think (someone please correct me if I'm wrong) Ocaml is one programming language which doesn't suffer from this problem. In Ocaml, format strings are type checked at compile time and its not possible to generate format strings at run time.

Yes, this can be a bit of a PITA, but it means that a whole class of vulnerabilities disappear.
According to dictionary.com one of the definitions of shill is - a person who publicizes or praises something or someone for reasons of self-interest, personal profit, or friendship or loyalty.

Which I think is about right, he's not pushing MS Vista because he thinks it's a great product (although maybe he really does believe that) but because he has a personal and financial interest in it.I've worked as a contract programmer for years, and my companies have followed all the sorts advice in that article which I find to be good but certainly nothing remarkable. I think the big problem with software development, particular web development is that consumer expectations just don't line up with the amount of effort involved, and crucial feature changes (that weren't apparent early on) blow every project out significantly. These experiences are a large part of the background for a new framework I've developed (now in prerelease at datamagi.org) which aims to allow contract programmers to develop pure business logic quickly without any interface complications, and then to plug a generic interface layer in afterwards which infers intefaces from the business logic without the user having to write a line of HTML (or a command-line client, or a rich client - there are all sorts of plugins there or coming).
... well, this URL hates me and and my links+lynx, for the laughable alleged reason that Salon cannot give me cookies.  But, given the number of times that Salon.com has sent me 'newsletters' consisting entirely of PHP errors (more than the once you'd think it'd take them to wise up and have a let-us-not-embarass-ourselves-this-way-again test on outgoing mail), I think they have pretty crappy programmers.A friend of mine on the Chandler team told me that the main problem was, as they called it, "Too many chiefs, not enough indians." That is, too many managers for too few programmers.

Was this mentioned in the book?[Indeed](http://validator.w3.org/check?uri=http%3A%2F%2Fwww.salon.com%2Fbooks%2Fint%2F2007%2F02%2F03%2Fleonard%2F%3Fsource%3Drss).Is a mechanical pope Catholic?
subtract Zero Zero = Zero
    subtract Zero (Succ n) = 'negative numbers do not exist'
    subtract (Succ n) Zero = (Succ n)
    subtract (Succ n) (Succ n) = subtract n n

    subtract infinity infinity



I guess I know now why infinity minus infinity is "indeterminate", rather than zero like I always thought it should be.  This highlights an interesting point though, that numbers are not static entities they are *the process necessary to compute them*.Well, what I like about dead trees is consistency: the clarity that comes from a single editorial vision.  Sure, you can learn C online--but the O'Reilly C book was an excellent introduction in far less time.  Granted, Practical Common Lisp is available online, but that's not always a given.  Beginning Databases with PostgreSQL is another excellent work, which is (AFAIK) only available in printed form.

Online references are great, but they oftentimes tell too much; a well-written book tells you what you need to know, not what there is to know.I was all set to say why I thought this guy was wrong, and offer my alternative explanation, but after reading the article, I pretty much agree with him.&gt; But even more, if you had a relative who was always wondering, "What is it that you do all day?" you could hand my book to that relative and say, This is what my work is really like.

I can't say if "Dreaming in Code" achieves this goal or not since I haven't read it yet.  However, I can say that Tracy Kidder's "Soul of a New Machine" does, even though, technically, it's about a hardware project.Yeah, it looks like you can easily define 'add infinity infinity' and even 'multiply infinity infinity'. But 'subtract infinity infinity' and 'divide infinity infinity' just go into an infinite loop.

Interestingly, Haskell's "undefined" value is also the infinite loop:

    undefined = undefined

Coincidence? Ask someone smarter.2%_"Liskell is a new syntax frontend for Haskell. "
Hmmm...Then, when you look at it, you see Lisp syntax, not Haskell.
The sheer _irony_ of that...And yet, mathematicians perfectly regularly subtract infinite *amounts of digits*, to come up with ratios for repeating decimals such as 0.123123123...I think it's more accurate to say they are able to regularly *generate* an arbitrary amount of digits from processes whose definitions are *finite*.Finally, something to help me with my endless fields of Java files!Well, you can't anyway, since URL is declared final. Therefore, you' have to 'wrap' it, which is more analogous to newtyping anyway.

final class MyURL {
  private final URL u;
  ...
}

...but I was waiting for these basic things to be fixed up so I could pick on it. I'll let it slide :)I don't like the behaviour of *shopt -s cdspell*:

    shopt -s cdspell
    mkdir aaaa1 aaaa2
    cd aaaa
    &gt; aaaa2
    cd ../aaaa3
    &gt; aaaa2

Bash should only guess when there is only *one* directory name with a minimal [Hamming distance](http://en.wikipedia.org/wiki/Hamming_distance) to the typed name &lt; threshold; in other words if the typed name is still unambiguous.

Edit: #$§! markupI continue to laugh the smug laugh of Mac user when I reflect on the fact that Internet Explorer's filename is still iexplore.exe, 12 years after the invention of long file names for Windows.Anytime someone thinks, "Hey, I could implement this feature as a system tray pop up!" and then follows through, he condemns himself to 24-hours more in purgatory. Just heads up on that front.I like `CDPATH`.  Does anybody know whether it's possible to to make tab-completion work for directories found in the `CDPATH`?amenseems like a subliminal ad for the xml editor.If you are going to consistently be able to get good pictures you are going to need both technical and artistical skills. Taking a good background picture is actually usually quite easy, you can do with the simple compositional rules and just find some nice view to capture. 

But if you are going to capture an emotion, or be able to capture a moment at just the right, repeatedly. You will need experience and thought behind your photographing. 

Sure, it's not like the other art forms, about controlling you own body to do what you want it to, but to make art being about that is to make it a sport rather than art. I'd never be able to make a realistic human sculpture, in the same way that I'll never be able to run 100 meter in ten seconds. 

Since I'm interested in Photography, sure, I could go out with a good digital SLR to the English countryside and I'd produce several really nice photographs usably as screen backdrops. But send me to an African countryside village, and tell me to create a captivating story with only images. And you'll see that there's more to it than just knowing how to handle the camera.*So it's completely all right that the standard library function doesn't report whether two URLs are actually equal, because it documents that it doesn't report whether two URLs are actually equal?*

Yes. I know you lazy, stupid, semi-professional web-monkeys avoid any kind of documentation like Steve Jobs avoids vaginas, but sometimes it's nice not to be completely clueless.

I hate it if people post lengthy rants about things caused by their own incompetence.[removed]I made [one for Python](http://beza1e1.tuxen.de/files/stuff/futures.py)&gt; C is a glorified assembler, literally an assembly language made glorious.

AmenBy the way, I think APLs have the absolute most to immediately gain in many-processor systems.  For obvious reaons? :-)You mean, like: "Computer, initiate auto-destruct sequence, authorization Picard 47 Alpha Tango." ;-)The classical example of an operating system written in a safe language, the Lisp Machines, supported languages other than Lisp, notably C, Fortran and Prolog. So it's certainly possible, and since everything turns to machine code (or JIT'ed byte-code maybe), the biggest problem will be interfacing with the (hopefully!) safe system calls. Writing a C interface to a set of, say, object-oriented or functional system calls would be an interesting task.For alternative languages to reason about math in, see the (freely available) books by KEI at http://www.jsoftware.com/jwiki/Books -- you might also like a used copy of 'A Programming Language'. My copy came with a yellowed newspaper clipout of an APL ad :-) In books on that wiki page, KEI uses J as an *entire replacement* of standard mathematical notation. So, you do all of the normal mathematical reasoning you do in normal math, but you use this notation that happens to be executable. I think someone in here recently lamented that non-mathematicians regard mathematical notation as 'etched in stone'. Care to show that it isn't?

Relevant quotes (each of a separate book):

&gt; Our objective is similar, but we now have new tools: the development of computer programming has provided languages with grammars that are simpler and more tractable than that of conventional mathematical notation. Moreover, the general availability of the computer makes possible convenient and accurate experimentation with mathematical ideas.

&gt; This book introduces a new tool (J) for exploring math, and to foster its use by applying it to a variety of topics. It provides a ramble through a variety of topics rather than a systematic study of any one of them. The book assumes that you have J at hand on a computer, and will simply show examples of exploring math with it.

&gt; on the contrary, the use of simple, executable notation makes it accessible to any serious student possessing little more than a knowledge of the counting numbers.

&gt; In particular, the array character of the notation makes possible an elementary treatment of partial derivatives in the manner used in tensor analysis.The text is paced for a reader familiar with polynomials, matrix products, linear functions, and other notions of elementary algebra; nevertheless, full definitions of such matters are also provided.was going to comment on the guy's site, but the IP I'm on is blocked by "SpamProtection".  Since I'm not at home (I'm on a business trip in Japan) I can't do anything about my IP anyway.  Ah well.&gt; However, if it cannot, Ruby will stay firmly tied to Rails and will decline in usage over time with it (as I think Rails has hit the apex of its hype).

This assumes people use Rails mostly because of hype and certainly not merit. Bad assumption.It's almost impossible to write something (for humans to read) and get your point across clearly. In my experience, 9 out of 10 times people misunderstand what you say, without realizing it. As the topics get more controversial (e.g. assumptions on both sides will likely be different), written communication becomes almost impossible. Even oral communication fails quickly after. This is why people can't convince eachother in debates/arguments: human language isn't up to it.

When you write something... *anything*, people have absolutely no clue what you're trying to say. Giving instructions to a computer is a lot easier than communicating clearly to people.While I don't disagree with your statement, "When Scheme is heavy duty, it is not portable", I think the takeaway is that Chicken is a well-balanced Scheme: lots of power (if not all of it), without sacrificing portability --- and for many of us, that is a winning combination.Complexity upon complexity.Shallow article that doesn't really cover the real problems and solutions.

I would like to point out that the following ruby code:

    begin
      printf("%d %d", 41)
    rescue
      puts 'Catched!'
    end

Works exactly as expected: the exception gets caught. The author writes that _'Ruby terminates the program with an error message'_, which shows a fundamental lack of understanding of HLL such as Ruby.

Security awareness is a good thing. And I appreciate that this guy attempts to educate people by pointing out some vulnerabilities. But I think it would've been a lot better if he tried to cover less ground in more depth.Maybe he read Wadler's [Views paper](http://citeseer.ist.psu.edu/cache/papers/cs/27853/http:zSzzSzwww-2.cs.cmu.eduzSz~rwhzSzcourseszSzmoduleszSzpaperszSzwadler87zSzpaper.pdf/wadler86views.pdf)  and thought Haskell used join lists? See section 6.

Probably not, though.Funny article about aliasing without actually mentioning the term :)

Though it is a solved problem. Compilers can detect it with global optimization or you can user some keyword like __restrict in msvc (gcc has this probably too).

Oh, and yes, FORTRAN is better if you are only do number crunching (well, as long as you don't want to save the results in a binary file).Come on! Wake up your inner child! :-)
_"It's hidden away on erlang.org, below the 'documentation' link and the 'downloads' link.."_

Yeah, in the end that's how I found it. Still, it's curious that google's unable to find it, no? Ah, 'erlang io' finds it. 'reference' was a red herring somehow.

_"'I can't find the documentation!' is a whine about Erlang that I still haven't the faintest understanding of."_

Wait, so is it hidden away or isn't it? If it is, then the complaint should be easy to understand, right?

I wasn't aware this is a more common complaint against erlang. I was just making a point about my immediate experiences, is all. I was just less than thorough in my searching. But to suggest that somehow erlang is special -- so I need to browse rather than search for documentation -- is self-serving. The bigger question was why I couldn't search. The answer seems to be that erlang's vocabulary of keywords was a little different from what I was used to.

I don't think this is just me. If others are whining about this, it is potentially holding back adoption.*Click here to go to salon* isn't much content. Must be really hard to write a web app for a news site.
Any idea about whether registry reads are redirected?  I'm guessing not since it appears you can query HKLM, in which case all software that uses entries under that key are gonna break and/or have to be re-installed...  Groovy...&gt;Though...I'm not so sure Haskell is the future either. It really is too much for most programmers to handle.

If I, a 15 year old, can wrap my head around side-effect free functions and monads, I'm sure most other programmers can; you just have to apply yourself.&gt; Wait, so is it hidden away or isn't it? If it is, then the complaint should be easy to understand, right?

Of course, I mean that it isn't hidden at all.  Using your own language in conjuction with the BLINDINGLY NON-HIDDEN PATH TO THE DOCUMENTATION is *an expression of ire*.

&gt; I was just less than thorough in my searching.

No kidding: you didn't look at *erlang.org*.  (You know, that 'google is the new `http://`' is a joke, right?  'google for X, it is the first one' depends on what server you happen to hit behind google.com.  'want to search my chronologically ordered set of difficult to navigate documents?  Just use google!' is *completely useless* -- there's no way to say 'I want to find posts of yours that refer to FOO that you initially posted in umm the first few months of 2005?'.)  You threw keywords at google instead of going to the obvious source, and you excuse yourself as *less than thorough*?  How about *idiotic*?

&gt; But to suggest that somehow erlang is special

Special just like any other language with documentation.  Where do you find Perl docs?  Through the command 'perldoc' on your local machine, or on www.perldoc.org if -- I suppose -- you are yet unfamiliar with the expanse of documentation on that system and would like to search it all at once.  Where do you find docs for C programaming?  Through the command 'man' on your local machine, or at your distro's homepage, or just randomly across the net via google (and how do you do that?  Not with random keywords!  You type the 'man' command itself into google, with the manpage you want) as a last resort.

Clearly, though, you aren't even faintly an Erlang programmer -- you don't know where the documentation is, you don't know that stdlib is not 'the library', and you haven't even been to bleeding *erlang.org* to download the system, download the docs situated immediately next to the system you just downloaded, look for tutorials that are situated immediately below the web-readable documentation you could've downloaded, or -- well, anything.  You made a lame google search on something you haven't invested any interest in and then you find it easier to post this as a complaint about Erlang than to take the next obvious step?  Here's a shocker for you: people who invest nothing in Erlang can't find its documentation!  Why, it suddenly occured to me that I should get some K docs!  K, an actual language, actually named this, with an actual website, is so deeply fascinating to me that I feel that I should comment on it by asking google what it thinks.  Hm.  Hm hm hm.  Well, crap.  You know, I think you're a fool
for not finding Erlang's documentation, but I'm completely flummoxed by K's documentation.  I won't even ask: what is K's non-obvious homepage?  I'll just note in grave tones that I couldn't find it, and then call someone self-serving for getting irritated at me.

&gt; it is potentially holding back adoption.

No, people considering Erlang adoption start by going to erlang.org , or wikipedia, or dmoz, or wherever people go to get the most basic and fundamental information about any programmming language: What implementations are there?  Where is its documentation?

No gold star for you today, internet researcher.Having a permanent position with one company, perhaps a company that contracts work to you without you personally having to deal with clients will make life a lot easier. They manage the bureaucracy, you do the work. It takes a bite out of your pay per project, but the increase in efficiency makes up for it.To solve that problem it just shouldn't allow the admin to run most normal user apps, problem solved, screw non-standard compliant software makers.And with Windows you still don't have dotfiles in your home directory you can just copy to get your old settings on a new/reinstalled PC.The difficulty of writing software might be related to the fact that every line of code you write is done at a dozen or more (probably a lot more) layers of abstraction, controlling tens of millions of parts.  Computers are the most complex inventions (in terms of number of components) ever created by mankind.  Getting it all to work in unison is, well, hard.Hey, that's a feature!  It means you can get *really good* at setting up your system, on account of how you need to do a scrape-and-rape (format the hard disk and reinstall the OS) approximately every six months just to keep it minimally functional.  Practice makes... a convert to Ubuntu!You can do all sort of entertaining things with IEEE floats. :-)

Notice the mathematical oddity here: 0.0/0.0 = NaN (correct), 1.0/0.0 = inf (only if you're thinking about the limit of a continuous function), and -1.0/0.0 = -inf (ditto).

So IEEE floats are very useful for computing, but less so as a mathematical model of infinity.via
http://blogs.sun.com/theaquarium/entry/wsit_praised_in_sd_times
The Aquarium: WSIT Praised in SD TImes
[removed][removed]Very exciting!  Can anyone point me to a similarly warlike post in another context?  e.g., Haskellers plotting the death of C?  CCured maintainers explaining how they can annihilate Cyclone?  A PC enthusiast laying out the steps to rid the world of Mac?  Anything :-)  This post refreshes me with both a feeling that Ruby on Rails must be pretty neat, and also that Python has some fairly capable libraries to it.  Now, code should follow rhetoric.this code from the article is the most poorly contrived I've ever seen

&gt;userdata = {"user" : "jdoe", 
&gt;    "password" : "secret" }
&gt;passwd  = raw_input("Password: ")
&gt;
&gt;if (passwd != userdata["password"]):
&gt;    print ("Password \"" + passwd 
&gt;      + "\" is wrong for user 
&gt;           %(user)s") % userdata
&gt;else:
&gt;        print "Welcome!"

The whole point of string formatting is to avoid ugly string concatenation.I don't understand why this keeps coming in to question. It's a LAMP stack, it scales like any other LAMP stack.Well, technically it's a LAMR stack (is that pronounced "lamer"?) ;-)

Or possibly a LMMR (Apache -&gt; Mongrel) stack, which has a somewhat more fortunate, though less pronounceable, abbreviation.I think you need to redefine your notion of "tiny".Another good book is *Showstopper!* by G. Pascal Zachary about the creation of Windows NT.Actually, I don't have a problem with the documentation, but with the class, that shouldn't exist in the first place. It seems to me the only effect it can achieve is confuse new users that don't know what a symbol is.At first, considering the date of the article and book, I thought the typical "why is this news,"  but after I got started reading, I changed my mind.  This could be either considered a good treatise on graph reduction techniques in general, or a specific language-neutral backend implementation.

Having written parsers and executors in the past, I think this is useful beyond functional programming, and can be an alternative to tokenizing and stack evaluation.

So I now have this bookmarked with my "development" links.  Thanks.There was no need to define a new data type; the Prelude already has the concept of infinity:

    Prelude&gt; let i = 1/0
    Prelude&gt; i
    Infinity
    Prelude&gt; i/2
    Infinity[deleted]&gt; This post refreshes me with both a feeling that Ruby on Rails must be pretty neat, and also that Python has some fairly capable libraries to it.

What kind of take-away is that? Get off the fence and get frothing for one side or the other!Right.

But denial of service attacks are possible at every level. Every input you accept, and every output you write is a potential vulnerability.

This isn't really an issue with Format String Vulnerabilities, it's far more generic. You always have to consider the exceptions that can be thrown by functions. If you don't, you may indeed compromise the security or reliability of the system.&gt; But unlike computer hardware -- the microchips and storage devices that run programs -- software isn't rooted in the physical world. It's still written, painstakingly, line by line and character by character; essentially, it's all made up.

Hey, waitaminit. One of the reasons that software is screwed up is that shit flows downhill. The easier it is to change the software, the more you're expected to fix it.

When Intel screws up a chipset, or a CD-ROM vendor forgets an interrupt line, or a motherboard vendor crosses two traces, the *software* is expected to work around the problem. When Microsoft hoses an API for Windows XP, we have to work around it until it is fixed in Vista. Then we have *two* code paths, the XP path and the Vista path. So our code is more complex, and their code is more complex too because they support the hosed-up XP API for backwards compatibility plus the new Vista API.
yes yes yes.  i feel like a big nerd even caring about this, but, damn what a great post.I think he misses the boat on why ruby is an acceptable 'lisp':
    (puts "sexps", (puts 'crude', (puts 'support', (puts 'only' , (puts 'need', (puts 'lisp', (puts 'acceptable', (puts('an')))))))))&gt;"How to Beat Rails" 

Better left to Microsoft when and if they feel threatened by Rails

&gt;Section 1: The Model #1. Use SQLAlchemy .... #2. Fix SQLAlchemy’s configuration

"If you don't have anything sufficiently thought out to say, go ahead and say it." -- George "Groucho" Jempty"This is why people can't convince each other"

I'll disagree with that: Ambiguity is why people _can_ agree on things. We all are in favor of "freedom" because we all mean different things. First amendment vs. legal marijuana vs. pornography vs. ability to drink out of any water fountain, etc. Some people say "freedom" and mean: The freedom to bomb foreign countries and hence spread _more_ freedom.&gt;Ballmer swears that there will never again be a five-year gap between versions of Windows.

5 years between new versions is good, not bad, except for microsoft and other vendors who want to sell new stuff. It is a gigantic expense for business to move to a new OS version. I guarantee that 95% of businesses would have been fine with another 5 years of XP.
It has a touchscreen???[removed]If two people are thinking of something different when they come to an agreement, it's not much of an agreement is it? Because if they continue talking, they might eventually figure out they're not on the same page. Therefore, I'd say a meeting of the minds is a necessary but not sufficient requirement for an agreement.

When you convince somebody, his opinion shifts from whatever it was to what you yourself believe. I don't think it matters whether people _think_ they've come to an agreement, but whether they are _in fact_ on the same page.

Also, it seems like your first thought was one of disagreement, before considering we may actually have different opinions of what 'agreement' entails.

I suspect you could've guessed from the first post that 'agreement' for me requires that people actually share the same view (more or less).I think the point is that other frameworks can learn from Rails and cross-pollinate all the good parts of Rails onto a faster underlying platform.&gt; Python’s Unicode support is second only to Java (in which every string is Unicode)

He forgot that Tcl has had that same feature for something like 5 or 6 years.  Of course, everyone forgets about Tcl because its core team couldn't market its way out of a wet paper bag.&gt; This post refreshes me with both a feeling that Ruby on Rails must be pretty neat, and also that Python has some fairly capable libraries to it.

...And I say that your three percent titanium tax doesn't go too far enough.Whereupon you'd discover the fundamental problem starts in people's heads and thought processes, and that ambiguity in language is an effect, not a cause.It ain't funny if you need to say it is in the title...Ok, so does anyone want to tell us how Java can beat Rails by cherry-picking the best features out of all the projects out there?

Seriously, this kind of discussion actually hurts the Python web world. Assembling these kinds of resources would be downright impossible, since some of the resources involved are members of development teams and not assets of the code itself.That's just the IEEE floating point number idea of infinity, though.I don't think that Python has much to do with any restrictive nature to the templating systems. Specific implementations (like kid) take a strong stance on this, but even kid allows you to put raw Python in your templates. Last I knew Genshi was working on adding it.

That said, Worse is Better usually means that the *worse* solution (e.g. the *less* elegant one) is the one that wins out. For some reason I don't think that follows from the comments here.And still, it scales just like any other LAMP stack. Fascinating subject.Well, most of the strenght and elegance of Rails comes from Ruby, so no, a lot of what makes Rails cannot be be ported to any other language. This is obvious when you look at the attempts att porting Rails to other languages. The elegance just does not translate to other languages.Then how come that people start to express themselves far more conciously and accurately when they're faced with an inflexible and ruthlessly demanding programming language?That that use Macs?Riiiight. There's "a Windows machine on every desktop" because Microsoft has consistently said "screw non-standard compliant software makers". Not.

The point is they were screwed either way. They *needed* backwards compatibility. A better solution would have been to only run legacy apps in some kind of explicit emulation mode. Mostly transparent to users, but explicit to developers, I mean. That way a dev would choose to either have their app respect the new rules, or be forced to run in explicit (not this piece-wise implicit) emulation mode."Requirements engineering?" Sheesh. Next on Reddit: "Why Sanitation Engineering is so hard." Followed by: "Starbucks employees hate the title 'barista', demand to be called Beverage Engineers."

Engineers may rule the world, but it's only because everyone and his damn dog is an engineer these days.
[removed]Web services &amp; SOA. The tools I use today are drastically different then the tools I used only 2 years ago. New frameworks, new methodologies... For example, I use spring, xfire, wicket and other projects, non of which existed 5 years ago, all of which considerably change the design and content of my code.Wow. I wasn't irritated before, but I am a little more than that now. Two of the largest and most unhelpful comments ever. Sure, you're right. I'm a fool (but I wasn't trying to [put any language down](http://weblog.raganwald.com/2007/01/what-ive-learned-from-sales-part-i.html)), there is absolutely no way I could have gone to erlang.com first (and many times before) and left intimidated by the layout, searching google first is just absolutely outrageous, the erlang documentation is *perfect* in its organization ("/OTP 11B!! Were you not listening at kindergarten?"), all people should consider their smallest remarks for the faintest whiff of slander towards erlang, and you are God standing in heavenly judgement. Good thing I'm an atheist; I'm done talking to you.

_"Clearly, though, you aren't even faintly an Erlang programmer."_

I never claimed to be; I'm at an erlang tutorial, aren't I? May others like me not run into ultra-sensitive ultra-defensives like you.

Keep insulting people for trying to research on their own; it's the perfect way to keep them asking stupid questions without doing their own research.[removed]This is total bullshit. Only the Rails *newbies* are also *Ruby* newbies. The Rails experts are all very well versed in Ruby.

Furthermore, you basically can't get anything working at all if you can't program. The author's claim that you don't have to code to produce something with Rails is complete bollocks.And it would be much less complex if they just supported two wildly different APIs.  Look at all the Mac software coming out for both classic and OSX as an example. Oh, wait..."People"? I think you mean "Programmers on the upper end of the intelligence curve who like to explore alternate programming languages."

If you think the latter group is representative of "people" in general, you need to get out more.

I've seen plenty of conceptually-fuzzy code, so even being forced to do it in a "programming language" is no guarantee of true clarity!Hahaha... business that use Macs.If Vista was not today available, would anyone care?

Please correct me if I'm wrong -- in the protracted period between the releases of XP and Vista, nobody but nobody was even slightly concerned that Vista was overdue.

Vista delivers nothing, with the exception of a meretricious GUI update, that can't already be obtained from third party sources.  Moreover, Vista's new "features" do little except complicate an existing usage model that, while imperfect, was fully meeting the needs of both home and business users.

The fact is, Vista is neither needed nor wanted.  It was and is designed to serve the needs of no one but Microsoft.I only illustrated that the way people think is (partially) governed by the medium through which they express themselves.

It seems your argument is basically that because a (spoken) language can not completely fix the unclear thoughts we have, all languages are therefore equivalent.&gt;Without that discipline, too often, software teams get lost in what are known in the field as "boil-the-ocean" projects -- vast schemes to improve everything at once.

I hadn't heard that term used before. Ringworld reference?Vista is a complete failure - all the news is about GUI and security and anyway we shouldn't rely on MS security tools - an independent AV/FW is mandatory to get some security on our machines. And we can run these tools on XP machines. 
So, who wants to buy Vista just for GUI, you can!Aren't you just copy-pasting this comment from five years ago, replacing "2000" with "XP" and "XP" with "Vista"? Or, if not... why not? Seriously, if you're already jumping on a bandwagon, may as well make the ride lazier.

Vista is an improvement on the previous version of Windows. When all is said and done (and upgraded and installed), it will be easier to maintain, support, and use. There is no reason to resist its adoption over XP other than plain old conservatism.

"My old shoes feel great, and these new shoes feel like crap!"

"That's because they're not broken in yet. They're better, trust me."

"Ah, fuck ya. I'm going out for a walk in my old shoes."There are quite a few, and I'm very surprised -- to be honest.&gt; In conclusion, don’t say your language is a functional one just because you borrowed a few ideas from Lisp. If you want a real functional language, try OCaml, Haskell, ML, or Scheme. Calling imperative/OOP languages functional just makes the term meaningless.

The entire argument falls down as soon as he mentions OCaml, ML and Scheme which are technically imperitive languages that strongly promote the _functional style_.

While Python, Ruby and Javascript don't make a functional style as easy as the languages he refers to, at least Ruby and Javascript supports functions as first class values. Someone with a greater knowledge of Python could answer for it as well. 'Functions as first class values' should jog the memory, as it's the time honoured definition of functional languages.

Also as an aside being OO doesn't preclude being functional either (which seems to be implied in the article).No, everyone forgets about Tcl because if they didn't block out the memory they'd end up going insane and clawing their eyes out at the thought of it.
You know, I know people still using Windows 2000. Frankly, I still don't know the added value of XP over 2000 as it is. If Microsoft would have just kept the drivers updated, we could still be on W2k. Consider an alternative universe, one where Microsoft realized that upgrades to the OS make it very little money for the cost, and that the true value is in a stable OS.

2000: Windows 2000 comes out. 
2001-2005: Multiple service packs arrive, including new drivers and refinements to the USB recognition. 

2005: OS X 10.4 releases, and people start really paying attention to it.

2006: Microsoft, in a release pack, adds desktop search, 3d acceleration, and a few "me-too" features... like Vista really is. Apple fanatics complain, but mostly because they've had to upgrade to each new version of OS X. Microsoft looks like the good guy. The version they bought in 2000 still is keeping up with the Mac. (Oh, and not everyone can use the new eye candy, but people understand that this is really asking too much.)

2010: Windows FS, and all of the nice things coming from R&amp;D are ready to be deployed, and a new version of Windows, rewritten from the ground up, is released. Included is a virtualization copy of Windows 2000 that opens up older programs. Everyone whines for about a year or two, but the ten year cycle progresses.

Who's really upgrading to Vista? Who really upgraded from ME to XP? Not many, I'd suppose - they just bought new computers. Those who hated ME usually just pirated a copy of Windows 2000 anyway.[deleted]I've never read his blog but he reminds me of someone who tries desperately hard to make himself sound like a geek because he thinks it's cool, but really just ends up sounding really annoying.With a big effin' STICK.Soothing?! Holy god. I *love* Smalltalk and can hardly stand it. It's like having a million Skittles hitting you in the face.Seems like it wouldn't be all that traumatic to have a "user's environment" (for both of them :/ and a "developer's environment"...

Wishful thinking; I know.
Sigh, this problem was dealt with in the C99 standard. Can we all update our knowledge to where things were seven years ago?Yes, you're right; as all systems become multi-CPU, that knowledge, those options must work their way into people's consciousness and it'll be part of what you have to know to be a programmer. I'm looking forward to that.[deleted]&gt; Ok, so does anyone want to tell us how Java can beat Rails by cherry-picking the best features out of all the projects out there?'

No, even if Java had all of the pieces out there to put such a framework together (and it does) Java can't beat Rails because Ruby and Java aren't really comparable languages for many different reasons, some benefiting Java, some Ruby. On the other hand, people are always asking 'Python or Ruby?', which makes sense because the languages are more similar than they are different.

&gt; Seriously, this kind of discussion actually hurts the Python web world. Assembling these kinds of resources would be downright impossible, since some of the resources involved are members of development teams and not assets of the code itself.

I disagree. Assembling those resources really wouldn't be that difficult because they are already out there. Python frameworks surprisingly aren't really too far from this, and as you can see, he mentioned plugging existing pieces of code together, its not as if he is saying 'someone needs to go out there and write something completely new for Python'.

I think something like Pylons can already do this. Take the pieces that the author described in the article and wrap them up into a Paster template and you've got the framework he is talking about. The hard part would be documentation and marketing, which is where Rails and Django really win.[So?](http://validator.w3.org/check?uri=http%3A%2F%2Freddit.com&amp;charset=%28detect+automatically%29&amp;doctype=Inline)&gt; Very exciting! Can anyone point me to a similarly warlike post in another context? e.g., Haskellers plotting the death of C? CCured maintainers explaining how they can annihilate Cyclone? A PC enthusiast laying out the steps to rid the world of Mac? Anything :-)

I think it's important to note that he is a high school student. Sounds like a damn smart high school student, but that could explain the Rah! Rah! aspect of his post.So... they thought up lots of new innovative things to do, got half way through them before realising they were all crap, then threw it all out and decided to copy Apple they way they always have...*I can't tell if my comment was eaten by the blog, or if it's just pending moderation, so I'll duplicate it here.*

I was just playing around with a function of order 5, and saw this blog post, so I thought I'd comment.

Call-with-current-continuation in a language with first class continuations (making the type system correspond to classical logic) has type ((p -&gt; q) -&gt; p) -&gt; p, and so is order 3. However, without first class continuations, this type becomes uninhabited.

The natural analogue of call/cc when you don't have first class continuations has type:
((((p -&gt; k) -&gt; k) -&gt; ((q -&gt; k) -&gt; k)) -&gt; ((p -&gt; k) -&gt; k)) -&gt; ((p -&gt; k) -&gt; k)
making it order 5. The translation from classical to intuitionist logic increases the order by 2. I think it's reasonable enough to conclude that it's important, given that one form of it is distributed with most Haskell implementations as Control.Monad.Cont.callCC, in the instance of MonadCont for the type Cont k. Whether this qualifies as something having an analogue at a lower order is an interesting question. In order to lower the order of the function to make it more comprehensible, the libraries introduce the type constructor Cont, such that Cont k a is isomorphic to ((a -&gt; k) -&gt; k), which brings the order back down to 3. (Cont k) is then made a monad instance, and callCC is implemented for it in the instance of MonadCont. The implementation needs to actually use the structure of the type of course, making it 5th order, at least in terms of how you have to look at it during implementation.I have a hard time believing the author takes himself serious. The bunch of articles from his blog that were submitted to reddit today all sounded like convulsive Ruby bashing, in favor of Python.

Who cares if he twists his definition of "functional" until Ruby no longer comes out better than Python?[deleted]What is LISP? Is it Common Lisp? InterLisp? MACLISP? McCarthy's original Lisp?Yet bugs still happen.[removed]I think it's more design philosophy than anything else. Some people lean toward a PHP-style "programming language embedded in HTML" (Rails), others lean toward as little HTML and as much code as possible (Nevow), and still others fall in between.

Django (shameless plug, since I help develop it) is one of the ones in the middle; the template system is extensible with custom tags, but forbids direct use of Python code and encourages you to keep your application logic elsewhere.While the blog post itself was content-free, there are plenty of interesting comments below.I posted my reply at [Countable Ordinals in Haskell](http://japple.blogspot.com/2007/02/countable-ordinals-in-haskell.html).

Summary: I think we should represent countable ordinals as the limits of increasing functions from the naturals to the countable ordinals.[deleted]what shit and this blog entry have in common: they both stinkActually, [shit](http://programming.reddit.com/user/shit/) often has some fairly insightful comments.  I have no information on his hygiene habits, though.Software still seems to work on 2000, that I'm aware - I haven't had any problems...Maybe people using apps from AutoDesk?[META MATH!](http://arxiv.org/pdf/math.HO/0404335)Lisp is a Turing machine made of cons cells and symbols. Anything more or less is up to the individual.Understand the difference between client's *requirements* and *expectations*.This is from the author of the io programming language, and is included in the io distribution, which is where you should look for the most up to date code. I'd be really interested if anyone has links to any other interesting minimalist databases, preferably with benchmarks.I think Balmer means he wants to work like a linux distro. Continuous improvements instead of a big bang every x years.

That should mean less expenses for a new version but more frequent. If the total expense will be bigger than with the current scheme ???

L.
[deleted]&gt;But unlike computer hardware -- the microchips and storage devices that run programs -- software isn't rooted in the physical world. It's still written, painstakingly, line by line and character by character; essentially, it's all made up.
Erm... so there's some sort of chip-mines in Silicon Valley, eh? No engineers, painstakinly designing 'em, gate by gate?Awesome.  I love APL.Why did they have a design team separate from the programming? No wonder this project never got finished.[deleted]It seems to be an academic thing. You're not taking it seriously unless you produce something that looks like it was published. The precise formatting of your block-justified text is *important*, dammit! You arranged that graph just so! HTML just looks so very... Wikipedia.That reads like the first quarter of a decent analysis, but ends abrubtly with hand-waving. Color me unconvinced.I completely agree. A valid point, don't understand why it was downmodded so much.Well, the whole point of a standard library is to, you know, get common useful things implemented correctly so programmers don't waste time reimplementing them over and over again.

Having a standard library method which incorrectly implements something -- even if it's documented -- makes that method useless. Again, it's as if someone had written a library with an `add` method that was documented as "Performs integer addition. Note; due to implementation details, return value will be incorrect." What good is that to anyone? If you don't read the docs you get a buggy program, and if you do read the docs you find out you have to roll your own replacement because the craptastic standard library couldn't be bothered to get things right.Anybody but me initially think "pointless" when they saw "point-free"? (based on the word, not based on any facts about what point-free actually means)Pretty smoking performance there.
EDIT: This is a nice list of minimal dbs:
http://en.wikipedia.org/wiki/Dbm&gt;So, I claim that if you want more performance, and it's not 'embarrassingly parallel', the first thing you'll do is re-write it in C++.

I think it's important to consider that all NP complete problems boil down to the [subset sum](http://en.wikipedia.org/wiki/Subset_sum) problem. The subset sum is embarrassingly parallel, then NP complete problems will benefit from "magically parallel" languages/compilers.Wow.This isn't, strictly speaking, Microsoft's fault. 

Not that there isn't a lot that *is* their fault, but they get a "pass" on this one.

Why? Because they're trying to do the impossible. The "operating system" (consisting of the actual operating system *and* the user environment) has gotten too complex as a single entity to design and build all at once. "The Mythical Man-Month" tells us that software dev teams don't scale, and we should listen. You can't do anything that employs 8,000(!!!) devs without enormous levels of inefficiency. 

Open source systems long ago took the sensible approach of evolving as a series of separate, designed pieces. Thus, team size and complexity are kept to a manageable level. But Microsoft believes it can do everything, all at once, that they are too big and too smart to fail. 

They're wrong. O(n^2) is O(n^2), no matter who you are. 

So they shouldn't be blamed for failing to do it. Only for being short-sighted enough to try it in the first place.The comparison also is comparing different levels of hinting. The ClearType (and probably FreeType2) example seems to be set at the lowest level [(cf. the online ClearType ActiveX tuner)][1], so the comparisons to OS X and between each other are unfair.

A better comparison is white-on-black (or light-on-dark). I know FreeType2 struggled with hinting that while ClearType and OS X did fine when I used Linux back in 2002 or so, and Ars Technica's old design was my go-to example of unreadable text.

[1]: http://img153.imageshack.us/img153/382/clipboard02il5.pngActually I still like C++, even for very parallel problems. Say the problem is large, and takes a month to run as it is. To get it down to 2 weeks, would you rather re-write it in C++, or double the number of processors? You'd rather double, you say, from 16 to 32? How about from 32 to 64, or 128, or 256, or ... ? At some point it gets expensive, whatever your budget, all of which puts pressure on, for it to be written as efficiently as possible, per processor. 

But I don't doubt that there are cases where easy coding will fit, when 8 processor chips get cheap.&gt;  9) Team entries will be allowed. Please indicate with your entry if this is a team effort, and name the particpants. At least one member of the team must register for at least one day of the conference.

Anyone interested in forming a reddit team? I live in Belgium, so _if_ we make a chance at winning, I can easily make the trip to Cambridge, UK.[deleted]&gt;  9) Team entries will be allowed. Please indicate with your entry if this is a team effort, and name the particpants. At least one member of the team must register for at least one day of the conference.

Anyone interested in forming a reddit team? I live in Belgium, so _if_ we make a chance at winning, I can easily make the trip to Cambridge, UK.[deleted]Hate Windows?  [Try Ubuntu](http://www.ubuntu.com/desktop).  It's free and good.Well in linux I enjoy the freedom to store files wherever I want as a regular user-- no wait, I can only use ~/ and /tmp.
Perhaps those who misunderstand it, or have notions of Tcl circa 1994.  My friend Salvatore does a good job of setting straight some of the misconceptions people have of the language here:

http://programming.reddit.com/info/2t7m/commentsWhen Apple did away with backwards compatability, they did so not only because it needed to be done, but because they had nothing to lose. (because they had such little market share)

If the next version of Windows isn't backwards compatable, what reason will people have to buy it over an Mac?  Backwards compatability is one of the things that keeps money flowing into Microsoft's pockets... therefore they'll never get rid of it.One advantage (over XP) you didn't mention  is 64-bit support.  Of course this has been available in Linux for a while.Going with what you said, the hard part of programming is taking this idea that's a jumble in your head and turning into these really concrete, non-interacting abstractions that accomplish the same thing as the blob.You're preaching to the choir man.

Most people here are already using Linux or OSX.&gt; eg. What happens if you want to delete a file you don't have rights to? Does it record the deletion in your private directory so you can't see the file any more but other users of the machine can?

The partition appears identical to all users. Not confusing at all.

BTW, linux has unionfs also.What was the point of writing reddit in XHTML if they weren't going to do it strictly? The whole idea of XHTML is discipline! If you want to slack off, there's no shame in using HTML 4.True, but occasionally dusty travellers stagger in off the Internet.  

It's simple hospitality to offer them a tall glass of  refreshing Kool-Aid.I'm really anxious to see what Microsoft does in the next few years.

First of all, I wonder if the people who don't really know any better will slowly start to use Vista.  I suspect they will because it is already bundled with all new computers by major manufacturers.

Second, I wonder what Microsoft will do with their next release.  We just witnessed Nintendo go from pretty good product (N64) to kinda crappy product (GameCube) to rediculously awesome product (Wii).  Microsoft will be in a position to do the same thing having just released their kinda crappy product.  But will they take the ball and run with it?Macs with their BSD base have quite a few abbreviated names in the file system.&gt; I disagree. Assembling those resources really wouldn't be that difficult because they are already out there.

Ok, you convince the Django team to drop their platform and go provide docs for this new system. I'd love to see it happen. I work on docs for TurboGears (which are now much better than they were when I started, assume from that what you will) Documentation is damn hard. Good documentation is harder.I'm on Ubuntu, after getting fed up with Windows, but I often feel I've just trade one set of annoyances or another.

The win for me is for Ubuntu because it won't (likely) freeze or blue screen, even after I've mucked with things.

But for people who use a PC as a basic E-mail/Word processing/Web  appliance, WinXP is as good or better than Ubuntu.  


Actually, "_partially_ governed" is my point exactly. Merely giving people an unambiguous language will only get them _partially_ to perfect logic. The rest of what is left is the fundamental fuzziness and ambiguity in the human mind.

"It seems your argument is basically that because a (spoken) language can not completely fix the unclear thoughts we have, all languages are therefore equivalent."

No, my point is, well, allow me to quote myself: "the fundamental problem starts in people's heads and thought processes". You see? You brought your own fuzziness about how I was making a claim about the equivalence of languages. There is nothing in the source material about comparing languages, so you can't even blame that on English. It came out of your own brain. This happens all the time, and I'm not blaming you for it. Human thought is fuzzy. We bring our own biases and thoughts to other people's words all the time.

_Even if_ you could wave a magic wand and force everybody to speak perfect Lojban, almost immediately, Lojban-prime would develop that would be just as fuzzy and ill-defined as English. It's an effect, not a cause.That scenario is in your head and far too open ended. Can you give me a real world example of such a scenario? If you were in the business of writing physics simulations, I suspect you would likely be using Fortran. Rewriting it in C++ isn't necessarily going to help there.This article would be much more convincing if it contained any actual code. I guess nobody does this because it would involve heavy amounts of plumbing to get all of those libraries working together to form a coherent whole. Then you'd have to document it. Also, you'd have to foresee all kinds of strange configurations people would love to have to combine these libraries in useful ways, and then implement them before the whole framework degenerates into a bunch of ad hoc hacks that people are using. It's a bit like saying "Here's how you could build the best house in the world! Just pick parts of the best 100 buildings and put them together!"Because it is far more complex than chess.  Because it has an infinite number of degrees of freedom.
[deleted][deleted]I wouldn't be so sure.

Linux doesn't [get crufty the way windows does](http://www.ddj.com/184405140).  Non-technical home users often lack the basic skills necessary to keep windows from turning into an adware-crippled, virus-ridden DDOS zombie, skills we take for granted.  Also, home users often have older machines which perform poorly under late model windows, but zip along on Linux.

I think Ubuntu is poised to become the OS of choice for parents of geeks.  :-)You're saying that any language spoken will eventually degenerate into something that has a comfortable level of fuzzyness. That is, 'as fuzzy and ill-defined as English'.

That's very plausible.&gt; Ok, my lisp implementation includes SB-BSD-SOCKETS as well.

Right, but unlike C's API, they're not (yet) standardized.


&gt; That's right, sockets are not in the standard [...].

No, they most certainly are in POSIX C.

&gt; Wishing it were so, [...] is not going to magically change the ISO or ANSI definition!

POSIX *is* ISO.  (ISO/IEC 9945, to be exact.)

&gt; Here is a copy of the C standard : [...]

That's ISO/IEC 9899:  the core, but not the whole.  Declaring that's C socket API is not standardized because they're not in 9899 makes about as much sense as declaring that Common Lisp's hash tables are not standardized because they're not in the first half of ANSI X3.226.Although I agree with both you and the article, shouldn't good C bindings (Python?) obviate at least your first two (and the third, if you don't have to make changes, ha ha...)

(And you left out space/memory :D)Jeeze, all these software people seem to be members of the "piss and moan about everything" club. Software is, "still written, painstakingly, line by line and character by character; essentially, it's all made up"? Yes, well hardware is too, and they don't seem to be having too many issues.&gt; at least Ruby and Javascript supports functions as first class values.

Like Java, Ruby's aren't quite first-class:  you can treat them as values, but only by wrapping them and calling the wrapper through a different interface (`f.call(...)` instead of `f(...)`, like (a much less verbose version of) `java.lang.reflect`).

&gt; Someone with a greater knowledge of Python could answer for it as well.

Python's functions are first-class.Conway's game of life in 6 bazillion lines of _runnable_ Java:

http://www.ibiblio.org/lifepatterns/


[removed][removed]Of course, not against Fortran. The discussion was originally about how cheap multiple CPUs, and the difficulty of parellelizing code by hand, would make C++ (and other low-level languages) obsolete, to be replaced by functional languages in which the runtime could automatically dispatch work off to other processors (which isn't realistic, the programmer always has to pick and choose, or else some of the things chosen automatically might be too trivial to be worth it). I'm arguing that low-level languages (for me, C++, but Fortran too) will be more important than ever.I feel really, REALLY dumb. Why's this one so funny?

*should probably pay attention more in calc class*[Not really, no.](http://reddit.com/blog/browser_stats)You are ignoring the momentum and community behind Rails.  Web frameworks don't have to be the best, technically and performance wise. "Good enough" plus vibrant community == success.  Rails is far beyond "good enough" right now, and the community is only growing.  Look how long Struts has lasted in the Java world, despite there being a *ton* of superior frameworks technically.

Besides, for most things Rails is fast enough as it is, and faster vms (jruby and rubinius, particularily) are going to "arrive" in 07.  Heck, jruby is almost there as it is.Hmm... *Even* Perl, but not APL. And how are the languages sorted?"Even Perl" was a joke, of course.  Not sure about the sorting -- it may just be the order in which they were added._"Composite keys (multi column primary keys) make any kind of Object/Relational mapping and persistance in general harder."_

Bah, use a [capable system](http://www.sqlalchemy.org/), instead of complaining.See also [Life in the Stencil Buffer](http://www.rush3d.com/reference/opengl-redbook-1.1/chapter13.html) (scroll down).&gt; Then how come that people start to express themselves far more conciously and accurately when they're faced with an inflexible and ruthlessly demanding programming language?

Because computers understand unambiguously.  People, on the other hand, are constantly moving targets.I took that not as him literally meaning that the Django developers need to do the docs, but that the docs need to be of that quality.Ever actually done any web coding in Python? It's surprisingly easy to glue together: http://bitworking.org/news/Why_so_many_Python_web_frameworksThis is one of the few minimalist programmer web sites I've seen that actually manages to look clean instead of slapdash.Until you finish the sentence...

&gt; ... who absolutely hated every moment of their day and brooded over why they couldn't have the cushy secretary jobs. It was terribly noisy and terribly boring and the supervisor stood over them like a slaver on a Roman Galley.[deleted]By size (google for "code golf"). Some of these sizes I don't believe!
Edit: misread - the languages appear to be sorted in an arbitrary consistent order.Is this even a good idea? The Firefox debuggers (Venkman, Firebug) on standard machines have barely acceptable performance as it is, and they aren't RPC'ing on every "step" command. Is it even _possible_ to make this performant on a modern machine?

(Note, this hasn't got anything to do with Ruby being slow. It's the _browsers_ being slow with all that data, and the RPC'ing for every significant interaction.)[deleted]I definitely can appreciate this. I had previously tried the usim emulator and disk image, but couldn't figure out anything to do within the emulator as I lacked the basic HOWTo documentation.[Not so.](http://www.microsoft.com/windowsxp/64bit/default.mspx)Many devices don't even have (working) 32 bit Vista drivers.Rock On!&gt; The developer might say, I could take this thing off the shelf that exists already and plug it in, but it's going to take almost as long for me to learn how to do that, or maybe even longer to learn it, than to write it myself.
And programmers, given a choice between learning someone else's code and just sitting down and writing their own, they will always do the latter. And the programmer who says, it will be faster for me to write it, rather than to learn it, is usually correct. Except that what he will write, most likely, is something that will work but will not have its rough edges worked out, will not have the benefits of a piece of software that has actually been used for a few years, where the bugs have been found and the users have given feedback and have helped you figure out where the problems are. So what they will often be handing you at the end of that I-can-do-it-faster-myself thing is something that works, but that is kind of a mess in certain ways. Whereas the thing that you were going to pull off the shelf, maybe it will take the programmers a while to learn it, but once they learn it enough to hook it up to this project you are creating, what they are hooking up will probably have a lot fewer problems.

Good quote. I'd like to read the book.

I think a very pervasive situation, a "pattern" even, is that libraries and code are released that are impossible to use.

To the author who has toiled for months, it seems as obvious as "just plug it in". But to someone who looks at a library for the first time, it's a big glob, with no obvious sense of its scale.

Programmers should learn psychology so they can sell their stuff. A nice counter-example is the Expat library - it was dead-easy for me to pick up when I once needed to parse some XML, because virtually the first sentence in the tutorial was "you only need 3 functions to use Expat". How many libraries have that kind of reassuring welcoming experience?Clever. But it's not true of the applications that the average Mac users interacts with. 

It makes sense for command line utilities to have shortish names, since they need to be typed out by people using the command prompt. (Yes, there's tab completion for names, that only saves so much typing. You can't use tab completion in a script file, for example.) Plus, it makes sense to preserve the usual naming conventions of Unixes of the past in order to be interoperable and not break script files, the muscle memory of old Unix heads, etc. 

However, none of these conditions in which it makes sense to use a short file name applied to Internet Explorer when it was written. (It was created for Windows 95.) It makes no sense for applications with graphical interfaces to have short names. No one is going to be typing them out repeatedly. They can't be scripted with text files. They don't inherit from the Unix past. Hence, on the Mac, `ls` is a short name, but `Dictionary.app` is not. Internet Explorer was even called `Internet Explorer.app` back when they made it. It's just a sad fact of Windows development that Microsoft still isn't completely comfortable with long file names.[deleted]You are of course right; but the essential thing is that abstractions don't work 100%, so there is that very "crosstalk" in software.[A Vector Magazine article](http://www.vector.org.uk/archive/v213/cliff213.htm) talks more broadly about various implementations of Life in J (a successor to APL).  It assumes familiarity with the language, however.It isn't the character set: it is the operator.  There's an analogue of Greenspun's Tenth in that you can have an unreadable bastard (Unicode would make it less of both) of APL simply by implementing it as functions that operate on arrays.  As APLs get great performance out of very carefully optimized functions+operators (in APL terms) and almost no effort whatsoever towards optimizing APL programs, you wouldn't even lose what you would with Greenspun: a well-tuned native-code compiler.Fortran is indeed low level, but it doesn't fit in with C and C++ here. The revisions done in the 90s and 00s mean that contemporary Fortran has a lot of the [implicit parallelization](http://en.wikipedia.org/wiki/Implicit_parallelism) opportunities that functional languages have. For example, intrinsic set-based operations like 'FOR ALL' and 'WHERE' keywords. These are keywords and built into the compiler so the compiler can decide whether to vectorize or parallelize the operations. C++ uses libraries for set operations (foreach, foreach_if) so it's set in the library how things will be done.

Fortran has already picked up some of these functional-inspired benefits. C++ hasn't, and I'm not sure it ever will.&gt; Why is writing software so hard?


Why is my penis so hard?

Answers to both questions involve something or someone named "Chandler".

Wow.  Haskell can do *every*thing...

/sarcasm
Coming from a comp sci/ comp eng background, you need to examine the total complexity.  The scope of most hardware is similiar to the scope of a single application.  It may be a complex application that few people can understand, but it's probably not much worse then the linux kernal.  

Windows has an overly complex design, which gets to be combined with a rediculous amount of backwards compatability.  Combine that with the pressure to release major new features rapidly, and you get ...  well Vista, with most of the major features cancelled or delayed.&gt; Two of the [...] most unhelpful comments ever.

They're called flames, dear.  -- Or rather the second is.  The first is made out of the pure stuff of helpfulness; I think you must've already forgotten it.

&gt; Sure, you're right. I'm a fool

&gt; Keep insulting people for trying to research on their own; it's the perfect way to keep them asking stupid questions without doing their own research.

So, you're an unapologetic fool?  You didn't do any research of any merit, complained about your results, were given: a gently snarky response that lamented aside at a recurrence of this silly complaint, then offering an analysis of what the Erlang documentation has to offer and only a theme of 'stdlib is not special' for an unapologetic fool to possibly take offense at.

You then responded with something possibly classifiable as a 'most unhelpful comment': sulky objections that I am 'self serving' for suggesting that people 'look for Erlang documentation in the Erlang documentation', finishing with this evil little cliche:

&gt; If others are whining about this, it is potentially holding back adoption.

Could that have possibly been sincere?  I am only twenty three, and already I am an bitter old man when it comes to self-serving tripe like this -- from its first radar-bleep of 'If the devs do not make/unmake the changes I have referred to, *nobody will ever buy this game.*'++1 to 'If you add this feature, *people will surely flock to your product*.'++2 to 'So long as Lisp has parentheses, *it will never be popular*'.

These assertions always come at the very end of a particularly-toned message from someone who has never invested anything in the object of improvement.  Sincere people, and sincerely-interested people, just don't use parting-shot language like this.


++1 On a Subspace forum, before Subspace Continuum, before Subspace hit the shelves and after it was apparent that VIE intended to push it to the shelves after years of live testing.  I was a very early teen, played the game regularly without talking about it much on the forums, and will never forget the alternately sneeringly and sulkingly poisonous atmosphere of those forums, consisting of posts pushing last minute changes or unchanges, each promising (or declaring, or snidely predicting) individual or group or worldwide abandonment of the game if these suggestions were not met.

++2 An unusually adept response to one of these: http://www.mercury.cs.mu.oz.au/mailing-lists/mercury-users/mercury-users.9904/0038.htmlOpenBSD has impressive regularity: they release every six months.You are reading the Washington Post, dear.*blink*

So, Nintendo-that-does-not-make-money-on-its-systems throws away backwards compatibility once and thereafter has a simple enough system that pretending to be the older system is relatively effortless, should be the measuring stick for Microsoft-that-does-not-receive-third-party-licensing-fees, that must maintain backwards compatability with such a baroque history that Windows 95 runs someone else's Sim City in its own special bug-insensitive virtual memory?"Frankly, I still don't know the added value of XP over 2000 as it is."

XP was 2000, updated for home use. At least at the time, it had better support for Win9x programs, especially games, and especially DirectX. It was intended to be an upgrade over Win 9x, not 2000; if you're happy in 2000 there's probably no reason to upgrade to XP, but there were many things that didn't work in 2000. 2000 was an upgrade of Windows NT, XP added back in all the 9x stuff that NT threw out so home users could use it.

For all I know, all these things have been "fixed", and 2000 may now be nearly indistinguishable from XP; I'm not in the 2000 loop. But there was a clear distinction between 2000 and XP at least for a while.[You're reading a newspaper, dear.](http://www.dailyhowler.com/)Doh!  Yes, that's a good point.  I had overlooked the fact that that's what the author was doing.

But still... This really is an important part of moving Windows towards a more UNIX-like model where programs (and users) don't go storing per-user data in Program Files, while making sure that lots of existing applications don't break.  But it is a kludge...
I always knew that mathematicians were impossible!

People more intersted in the pervasiveness of metaphores in language should read *Metaphors We Live By*, by [George Lakoff](http://en.wikipedia.org/wiki/George_Lakoff) -- who also takes his analysis into American politics, in *Don't Think of an Elephant*.[deleted]What do you mean by 'three precent titanium tax'?Or at least, it couldn't market itself as effectively as the Perl community could market contempt for it.  Post-PHP, I don't think Tcl has any active enemies anymore.  FWIW, I enjoyed the Xircon IRC client on windows, which used Tcl as an effective scripting language.I'm sorry my ill-considered near-cliche statement got your hackles up. I didn't say 'surely', I said 'possibly'. And I didn't start with that statement, my first 1.5 comments on this thread focussed entirely on my own experience (including detailing my own missteps which you seem to care about only to be derogatory). Only when you mentioned you were sick of it coming up did it occur to me to point out that if it keeps coming up perhaps the newbies you meet have a point.

This isn't a pure erlang discussion forum; I have neither that much of a sincere interest in any particular programming language nor any sort of agenda that I'm constantly furthering by manipulating others. I'm here to have good conversation; that I *am* sincere about.

Next time you are inclined to flame somebody I sincerely wish you will try to exercise more restraint. A flame too is sneering and poisonous, just like the cliches you're perhaps a little too trigger-happy to rush to the defense of. It's not helping the atmosphere to respond to what you think are sulking children by turning into a [bitter old man](http://reddit.com/info/bv2c/comments/cby3x?context=5&amp;style=nested#cby3x). You're escalating my perhaps-incorrect but minor criticism of something into a major criticism of somebody.

I'm sorry I said 'self serving'; you said I was whining. The right way to do support and documentation is one of my long-term interests. This world still doesn't know how to foster a constructive conversation between users and 'providers'. There are pathological behaviors on both sides. The dual of the cliche you mentioned is for developers to get defensive about their creations and increasingly tone-deaf to the criticisms of others. I was too quick to respond to what I thought was a case of this pattern.

\-----------------------------------------------------------------------[deleted]If there was a bug in code like this, would it be too cleverly written to debug?

IMO software should not need "sci-fi explanations."It's a bad futurama joke. There are two presedential candidates that are clones of each other and are having a debate. Here is the text of it:

JackJohnson&gt; I say your three percent titanium tax goes too far!
JohnJackson&gt; And I say your three percent titanium tax doesn't go too far enough!

Hopefully that context helps it make sense, otherwise I'm probably the only person who will think it is funny, please carry on.It is actually Javascript. SVG is only a representation of graphics (more abstract than an array of pixels).Um, it's fairly easy to set up a home system for a non-technical person so they won't turn their computers into zombies.

Aside from the typical Spybot/Antivirus/etc stuff, one of the big ones that people don't usually do is to run on a limited user account.Really?  I've found it to be an annoying hassle.

Also, have you ever actually tried to *use* a limited user account under windows?  Doesn't work so hot.Yup. It's not so bad unless you're trying to install applications.[removed]It's just describing an algorithm. Some algorithms are hard, and need metaphors.Uh, it doesn't even seem to display the submissions, just statistics about them.The submissions aren't displayed because it's a competition! Take a look at sites like [codegolf.com](http://codegolf.com/) or [Fonality Perl Golf](http://www.fonality.com/golf/) for a better introduction.What is this doing on programming reddit?Ah, I'm afraid that Haskell is too low-level a language to offer [practical time-travelling to programmers](http://www.elf-emulation.com/funge/funge.txt).

The instruction set (search for 'TRDS' on that page to find the extensive commentary):

    "TRDS" 0x54524453                                                                                        
    C   ( -- )              Continue normal time                                                             
    D   (V -- )             Set absolute destination space coordinates                                       
    E   (V -- )             Set relative destination space corrdinates                                       
    G   ( -- n)             Get current time point                                                           
    I   ( -- )              Set inverse settings                                                             
    J   ( -- )              Initiate a jump                                                                  
    P   ( -- n)             Maximum distance in the past ip can jump to                                      
    R   ( -- )              Reset all tardis controls to default                                             
    S   ( -- )              Stop time                                                                        
    T   (n -- )             Set absolute destination time                                                    
    U   (n -- )             Set relative destination time                                                    
    V   (V -- )             Set destination vectorMy favorite is still:

    class Array; def rand; self[super length]; end; endHow about we blame them for being short-sighted enough to design a monolithic operating system rather than a small operating system that could be easily and incremental improved in parts? They went out of their way to integrate the browser with the operating system, and integrate the chat tool with operating system, and so on for purely business purposes, so they could refuse to separate the two when Netscape complained. Then you're saying we shouldn't *blame* them when it blows up in their faces and they can't modify or fix a darn thing?

They designed this unfixable beast in the first place, despite three and a half decades of evidence that tight monolithic integration is not the right way to build an operating system.
No, the nicest curry in a Lisp is (unfortunately requiring a code-walker):

    (foo a _ b)
    ;; evaluates to a function accepting one argument,
    ;; that then calls (foo a that-arg b)

The key requirement is that currying cannot happen except on functions with explicit arities; I used to think that languages like Lisp (with N-ary functions) and Erlang (with named-by-arity functions) simply couldn't have nice currying -- the `_` syntax in function-calls was a pleasant surprise.&gt;How about we blame them for being short-sighted enough to design a monolithic operating system rather than a small operating system that could be easily and incremental improved in parts?

Isn't that what I just said?

I said:

&gt;So they shouldn't be blamed for failing to do it. Only for being short-sighted enough to try it in the first place.

The problem isn't that they can't do it. The problem's that it can't be done."But for me, a young developer fresh out of college, the ability to iterate (not to mention define an array or a hash) in a single line of code was a big draw away from my world of statically typed languages."

Boo! Come on, statically typed languages are perfectly capable of doing this. Just off the top of my head, the equivalents of iteration could easily be done in many statically typed functional languages through folds.How is it that displaying the submissions would ruin the competition?&gt; If the next version of Windows isn't backwards compatable, what reason will people have to buy it over an Mac?

Right, it'd actually have to be *better*. Microsoft knows they'd lose horribly in a game of competing on merit.

&gt; Backwards compatability is one of the things that keeps money flowing into Microsoft's pockets... therefore they'll never get rid of it.

This will also possibly be their un-doing as Vista has shown. Backward compatability, at least as MS handles it, builds up over time like harmful chemicals in your body.&gt; 9.67% linux

And it's probably gotten much "worse" since August judging by the comments.
I see it was written as a 'test' for Mozilla. Works fine in Opera 9.1 as well.Yeah, that's just a silly statement. The type system has *nothing* to do with syntactic brevity. E.g. in Haskell:

    map (+1) [1..]

Wow, Iterating in a single line, and statically typed too!!

I guess if all you've done is Java, then anything else looks good.Campus Propaganda:

    int *arr_add (int *a, int *b, size_t len) {
      int *r;
      size_t i;
      assert(NULL != (r = malloc(len * sizeof int)));
      for (i = 0; i &lt; len; i++)
        r[i] = a[i] + b[i];
      return r;
    }

    (defun array-add (a b)
      (let ((r (copy-seq a)))
        (loop for n across b
              for i from 0
              do (incf (aref r i) n))
        r))

    let add_vec v1 v2 =
      let len = Array.length v1 in
      let res = Array.create len 0 in
      for i = 0 to len - 1 do
        res.(i) &lt;- v1.(i) + v2.(i)
      done;
      res;;

    a+b
&gt; Indeed, the 2001 Release 5.1 was an Intel-only affair. How soon before Apple goes the same way?

...

From [wikipedia](http://en.wikipedia.org/wiki/BeOS):

&gt; To further complicate matters for Be, Apple refused to disclose architectural information about its G3 line of computers-information critical to making BeOS work on the latest hardware from Apple.
Did you give this as a presentation to Ruby users or something?

I tried to explain evolutionary computation and their jaws dropped at how stupid they thought I was when I talked to a Ruby user group.I keep getting compile timeout when I submit my Haskell program.It amuses me that his primary objection to Ruby is that it requires #'FOO and FUNCALLMake the obvious thing easy, and let the programmer/user have options to do everything else.  An anti-pattern is to show everything that is possible in the documentation, without specifying the common options that everyone basically wants to use.  For instance, if the `tar` and `rsync` programs didn't have those examples they added at the beginning of their man pages, they would be unusable for most people.Beyond the obvious, I'm not sure how to answer that. It's standard in these contests to keep the submissions secret until the contest is closed. I don't know if you took a look at the Fonality site I linked to, but they have a [post-mortem page there](http://www.fonality.com/golf/post_mortem.cgi?id=1) which allows you to see the submitted code now that the game is over. At Codegolf this issue is [still being discussed](http://codegolf.com/boards/conversation/view/81). And at the moment I believe that the shinh.org site listed here doesn't retain the code at all.&gt; Be prepared to get screwed if this is your first contract job. If you don’t know the ropes, you will underestimate the project difficulty, underestimate how often your client will change their mind, and miss your deadlines.

Priceless. That's the way clients do thing. That's also a good reason why, if you're doing a long-term work, you should design your program to scale (this cannot be emphasized enough) or you risk hurting yourself in the long run.&gt; The entire argument falls down as soon as he mentions OCaml, ML and Scheme

Ah, but not Haskell?  Mentioning *O'Caml* causes his argument to instantly self-destruct, but mentioning *Haskell* does not?  When you get to the point that the sacred title of 'functional language' only settles on Haskell, Clean, and Mercury, you've only succeeded in destroying the term.  Would you allow 'impure functional language' to apply to O'Caml?

&gt; Someone with a greater knowledge of Python could answer for it as well.

Sure, it supports first class functions -- and so does C.

&gt; Also as an aside being OO doesn't preclude being functional either (which seems to be implied in the article).

It doesn't?  What language do you think of that does not immediately making his argument fall down, that supports OO?Ah, no: it now amuses me :-)What NOT to do to when you're making your website search engine friendly.C: Bad use of assert and possibility of integer overflow.

Lisp: (map 'vector #'+ a b)Hmm, not sure we should be taking advice from this guy:

&gt; I personally don’t like creating functions. While they make coding easier, they really don’t speed up the script or make the script shorter. I prefer to use includes...
[deleted]i couldnt agree more. man, i hate to code. i have the ideas, the concepts, and the problems already solved in my head. and then i have to sit down and code it together - the worst and most tedious job around. it can be nice getting an elegant function together, but mostly it's tedium... for me anyway.So true.ah, but people *have* already done this, and there's a reason we haven't heard much about them.&gt; no shame in using HTML 4.

continue and you will achieve an understanding.The comparison with chess really captures the cause of the difficulties in the most challenging software design problems.&gt; Bad use of assert

Intentional use; it'd be rude of my function to determine how the program should handle such a terrible event.

EDIT: eh, but looking at it that C again, I see that I originally threw away malloc's return.  At least my program would've helpfully crashed -- probably -- at runtime.If windows was the first to have this feature, I'd seriously consider switching back after all these years. This is that awesome.It would be "rude" of your function to dereference an uninitialized pointer if NDEBUG is defined.  When NDEBUG is not defined, your function will crash the whole program if malloc fails.  I know it's difficult to say anything right in C.He rejects Spolky's point entirely: "Spolsky gets it right that “It’s harder to read code than to write it”, but unfortunately reaches for the tired “people (programmers) are stupid/lazy/undiciplined”"

But I know for an absolute fact that Spolsky is dead-on right -- at least regarding some people who are professional programmers, but who aren't good enough to be working in the same environments as Rentzsche. If Rentzche could spend a few years working in the programming section of a marketing department at a large company (where they store, retrieve, and summarize marketing data), he'd see the behavior Spolsky describes.

The people Spolsky's talking about will literally replace very good working code with their own crappy, buggy versions because they found it a challenge to understand the original code. Not only do they decrease the code quality, but they spend a lot of time writing code that simply doesn't have to be written --yes it takes time to understand, but not nearly as much time as to rewrite from scratch. The problem with understanding is that it takes more focus and effort, and truly crappy programmers don't have the inclination/skill to go there.Whether a number repeats like this depends on the *base* used to write it.

Some repeating floats in decimal are non-repeating floats in other bases;)I'd like to quote [cgibbard](http://www.haskell.org/pipermail/haskell-cafe/2007-February/022011.html) here, he is fond of pointing out that *lists are loops as data* :

&gt; Lists are, in a sense, loops which haven't yet happened, and much of programming is done by transforming them. Due to laziness, those elements not needed won't be computed, which is a similar capability as that of being able to break out of a loop early. This inversion allows you to essentially extend the code which would be in the "loop body" after the fact (that is, without modifying the existing code), which is not something you could do in a strict imperative language unless you'd specifically provided for passing in a continuation in the loop, or were modelling lazy lists somehow.  Eventually, you come to mostly forget about loops and just think in terms of the lists themselves, but it's a very useful alternate view to have handy.
&gt; 
&gt; Of course, lists aren't the only data structure just as loops aren't the only kind of recursion. Laziness basically makes data structures into tools giving you the ability to "reify" recursion such that only those steps which end up needed later on are actually carried out.
[removed]tab-completion depends on readline. I'd check there.[removed]I think you're not being sufficiently challenged.Look at the big picture. If I pass a variable to a function, there are a minimum of three processes to complete. If I pass a variable to a routine that isn't a function, there's a minimum of two processes to complete.I had this problem too; I contacted the author and he increased the timeout. Seems usable now.I couldn't agree more. I believe it was Minsky who said that "programming is good medicine for sloppy ideas."

Was it Knuth who said "programs are written for people to read and only incidentally for machines to run." ? 

I highly recommend the epigrams of Perlis for good insights into the nature of our craft

http://www-pu.informatik.uni-tuebingen.de/users/klaeren/epigrams.html

Cheers.Nice article. The analogy with the Mythical Man Month is also good.I for one like to code.  Hehe..I have coded my own HTTP client and I do love to write in assembler!   Of course, I also enjoy problem solving.
I feel sorry for guys who are working in a field where a large part of their work is a chore for them.  What a grind their lives must be.[deleted]It's really slow on my ibook running FF2 :PAlso quite a few in science.This is possibly the dumbest thing I've seen posted to the programming subreddit.It would be interesting if you explained the reasons from the point of view of a switcher.[removed]I partially agree with him.  I love programming, but I don't want to spend time working on a problem that's already been solved (like writing my own XML parser).

On the other hand, there are times when I code for coding's sake, just to see what new language oddities I can discover (especially with a complex language like C++, or an interesting one like Haskell).Yes, I gave this presentation to the Melbourne Ruby users group: http://groups.google.com.au/group/melbourne-ruby
It was quite different to the usual talks, which are often Rails focused, but people seemed to enjoy it. I tried to focus in the talk on the aspects of game theory and my work which would be the easiest to explain and would capture the interest of the audience, rather than the bits which I might be most interested in, but would be difficult to explain. For instance, there were no mathematical equations in my talk, and the simulation demos that I ran were graphical in nature. 

I was a bit concerned before the talk that people's eyes would glaze over and they would would be wondering what the hell I was talking about, but luckily I don't think that was the case.Unless you're maintaining the code, it isn't so much being able to read it as being able to understand the api.  It's impossible to overstate how bad most api's are.  Even on some fairly simple ones, you can spend a lot of time experimenting with them to learn their behavior.  Forget about the bloated api's that today's applications are based on.  You will never fully learn them.  So in a lot of cases, you may rewrite something just to get a simpler more useable api.In PLT Scheme, you can solve the problem without a code walker, like so:

1. Define an macro my.apply to replace application.
   That is (my-apply foo a _ b) expands to (lambda (x) (foo a x b). If there is no _ in the macro application, then (my-apply foo bar) just expands to (foo bar).

2. Write a module named, super-scheme, exporting all builtins except #%app, and export also my-appply under the name #%app.

3. Use the above module as the base language. The PLT Scheme expander will expand an application in (#%app ...), and since we exported my-apply under the name #%app, we can now curry without any problems.


YOU do that. You're probably a good programmer. I'm talking about people who don't instantaneously understand something, and therefore decide to rewrite it. You may not know any such people, but believe me they exist.Until he said that programmers write code because they like to learn, I was about to reject it.  That is precisely the main reason.   They want to -understand- the problem, else they have failed to solve it.

People who write code merely to glue other peoples' libraries together are just hacks.  Use all of the libraries you need, of course.  Let them be 99% of your project, if necessary.  But let the core functionality be your own, else you have not contributed to the world at all.

Could it be that they're both right?

I know some programmers who could be called "ultra practical hackers". They really will leverage any tool that abstracts away a small problem so they can get busy with the big problem. These people tend to have very few hangups and dodge through problems like a boxer.

I also know "not-invented-here hackers" who dilute themselves constantly into rationalizing why some library, that works for millions of others, just isn't good enough for them. They rewrite things like CGI variable handling code, HTTP clients, file system traversal code, etc. The only thing they're consistent on is never doing as well as the library everyone else has debugged for years.

You don't hate coding.  What you hate is verbose, bureaucratic programming languages that throw up mountains of pattern-driven, IDE-generated text between you and the executable form of your ideas.nice - reminds me of my old flash tetris:
http://home.arcor.de/fduffner/noTWebApp_tetris.swf
Isn't engineering in general about solving problems? Also science, chemistry, medicine, material technology - what not, if you count out basic service and prodction tasks?

You're totally impractical in software development if you don't like coding, too; if you're not interested in the technicalities involved. That's how you implement the stuff and before that you don't have anything in your hands. Only in your head.

If you only have wonderful ideas and glorious concepts, you better make a career as a philosopher, not a programmer.
&gt; ... I have coded my own HTTP client and I do love to write in assembler!

I just hope (if you're on my team) you like to do these things for fun and learning. Not in the middle of a real project that needs to move as fast as possible.

Nice news. They added OpenCOBOL! :-)
It's really fast on my Acer Travelmate 4501 running FF1.5, which is rather odd since I'm fairly sure your computer is better than mine.

Edit: Definitely not FF2's fault.  I just upgraded and it's even faster.  So either Macs just suck or you're used to it going really, really fast.you are partially correct. i do a lot of coding to pay the bills, and not enough for my own ideas. i need to work on that. 

:)But how many people use reddit at home vs at work/school where they don't get a choice what OS they're running?&gt; i have the ideas, the concepts, and the problems already solved in my head. and then i have to sit down and code it together - the worst and most tedious job around.

I always find that when the code hits the real world the real world hits back.

It only seems smooth and elegant in my head when everything is theoretical. When I can gloss over the details and be ignorant of reality.

It can be tedious though, when there are lots of corner-cases and their workarounds in place. It all starts feeling so un-clean, I sometimes pine for the beginning-phase when it was still clean in my head.I like to code.

Writing code using just debug.exe without mnemonics is a beautiful thing.

So is writing C code... but then butchering it into the smallest number of characters possible.

And converting every program you write so that all iteration consists of recursive calls to main() is practically orgasmic.They run just fine in parallels !I think the author pretty much nailed it.  Coding for the sake of coding is NOT interesting.  I want to know that I'm working towards a goal or solving an interesting problem.Hey I like Rails and Ruby.  But I agree with the notion expressed above in this thread that the long term success depends on the improvement of the underlying platform.These guys have a lot of programming to get that sound.I don't really get what this is. Some kind of standard library for D? What's wrong with Phobos?Using libraries to build systems is the smart way of doing things. Hacks are the ones who think something that takes them a few hours to write can stand up to something developed and tested by a community. This is rarely the case. 

from wikipedia:

The OpenDocument standard was developed by a Technical Committee (TC) under the OASIS industry consortium. The ODF-TC has members from a diverse set of companies and individuals each with an equal vote. The standardization process involved the developers of many office suites or related document systems. The first official ODF-TC meeting to discuss the standard was December 16, 2002; OASIS approved OpenDocument as an OASIS Standard on May 1, 2005. OASIS submitted the ODF specification to ISO/IEC Joint Technical Committee 1 (JTC1) on November 16, 2005, under Publicly Available Specification (PAS) rules.

This isn't about Sun forcing a de facto standard. But I don't really know what are your sources to state that OpenDocument isn't really open and the specs ambiguous.I dunno why you got into programming then.


I totally love programming. It's the best thing when you can build something from nothing.

If anything, I can't stop programming, it's just too much :D

Here, since you have so many ideas, what else should I add to my sites?

beersex.net

newsique.com

newsique.com/forum (i made the forum too)


(i've got a lot more sites, but I'll probably get modded down for spam or something if i list them all)

*edit* yup, why do i bother asking reddit for any feedback, everyone's so jaded here, when you ask an honest question you get modded down.. oh wellYou sound more like a project manager than a programmer..

I like both aspects..[deleted]Yes they're both right -- it's called an opinion.  I like to code...obviously this author doesn't.  Why does he feel the need to generalize all coders?

good programmers vs. bad programmersUsing nothing but libraries will also make your code significantly slower due to all the overhead.

very good programmers will use libraries and strip out all the crap they don't need in order to streamline the code.I see coding as a the whole process of design, solving problems, implementation, debugging and testing. Take away one of those elements and you're not left with any kind of meaningful activity.Horrible code!There are all kinds of different programming jobs, some can rely on bolting together tested libraries more than others. But I wanted to make the point that using tested code from other people is in no way hacking.Without question, but relying on libraries to do everything (which is very possible due to the number of libraries out there) is not good practice..How is it difficult to tell when things get evaluated?  This is lazy evaluation, not random evaluation.

Elements in a lazy list will not be evaluated until you examine them.  Never examine, never evaluate.  Examine only partially, evaluate only partially.  

Perhaps he was talking about lazy lists embedded in a strict language? 
The template engine CL-EMB allows you to embed Common Lisp in the templates, or just use a minimalist template language.

(From the shameless plug department.)
So you pull news from various sources and toss it into a digg-ripoff interface, right?Note that this tutorial is for syntax-rules macros, which is in the current R5RS standard.

The new R6RS standard will "extend" syntax-rules to syntax-case. The "new" system have ripe all the benefits from both syntax-rules (pattern matching, hygienic introduction of indentifiers) and CL style macros (using full Scheme to define macro transformers).

Just in case, you read the tutorial and think Scheme macros has limitations compared to other systems.

Jonathan blogging about Andrew interviewing Scott quoting Larry and that was only the first two sentences. :)I'm fairly sure it uses WebDAV to connect - it doesn't screen-scrape - which isn't really a last resource hack - but sure, I imagine the connection ain't great. Punctuation boy isn't saying 'Ain't great' though, he's saying 'don't work at all nohow because MS deliberately impossible.'The paperless office will be here about the same time as the paperless bathroom.While the title is a bit misleading (it really should be called "The Death of the Computer Science Department"), this is a good examination of the struggle higher education faces to try and remain relevant. Colleges and universities aren't keeping pace with technological change. A CS degree is a sign you've received an outdated education.The type of programmer he's describing is the worst kind of programmer that I have to deal with in my everyday work environment.  The next step up is somebody who would cherish the thought of being able to write an XML processor.  Not only for the learning experience but for the pure sense of accomplishment.  But that could get you in trouble as well with a NIH syndrome.  The next step up from that is the coder who would not only enjoy programming a new XML implementation but is also pragmatic enough to use a pre-existing, and field tested, library.

Dreamers are a dime a dozen.  People who not only have the will to face the reality of the situation but also enjoy the craft itself are the true heroes.Really? I don't know. I've seen it time and time again: As soon as people inherit a code base, they want to tear it down and start again. Yes, they make excuses, but it's always the case.

It's not so much because they like to code, as that they like to own the code. And you automatically own code that you wrote.I'm sorry, toss it into an digg ripoff interface?

A) I coded the entire site (no "interface" used, whatever that means)

B) digg did not invent any aspect of my site, if anything i 'ripped off' slashdot.

But I guess you're probably one of those people who hates on google for not being the first search engine, and therefore is a "rip off" of yahoo.. right?[deleted]Here's a link to the ['commenting' help section](http://programming.reddit.com/help/commenting) on Reddit so's you can use markdown to get quote bars instead of using quote marks.

&gt;MS has made sure of that and they will continue to make sure of that.

I can just see the meeting.
"Bob, have you made sure the OSS guys can connect to public folders?"
"Sue, we discussed this. We already devoted 6 months dev time to it but at the last minute we deliberately decided to take out the open API and make it as hard as possible to use."
"Oh, hey, I remember now. And you put in those threading issues just to fuck over the Xinian guys, right?"

It doesn't have to be a god damn deliberate strategy. 

&gt;Look I have nothing against homosexuals but I don't think it's appropriate for you to come on to me on this forum.

couple hints for you. (1) there are women on the internet too. (2) there's 'a come on' and there's 'motherly condescension' - truly sorry you can't tell the difference, you probably need to talk to your therapist about that.[removed]If they're rewriting Perl's CGI.pm, then it's because CGI.pm is a bloated memory hog and should logically be split into about a dozen different modules.  Sure, it works for millions of people, but I'm not going to touch it with at 10 foot pole.  Then again, I just use the mod_perl Apache::Request object, so I'm not exactly reinventing the wheel either.  Anyway, my point is that not using accepted standard modules is sometimes a good thing.  Sometimes what people think is "good enough" will cripple a high-traffic site.Yes there is no pleasure quite like debugging someone else's "optimised" regular expression engine.

Optimised not to work...Wow, mighty friendly round here. ;-)Just use a reader macro. `[foo a _ b]` looks just fine.Yes, Yes and Yes.

Rentzsch's lines in both this post and the washington post article irked me to no end.  He's so close to the truth, but then throws out a turd like the line rebutted here.

I'd say that there are three parts to every programmer.  The artist, the engineer, and the idiot.  The artist delights in the beauty of a solution, or the art of lisp.  The engineer is precise, bolting together the parts of a frame to get to the real problem that they want to solve.  The idiot is responsible for choosing between the other two at precisely the wrong time.

Programmers want to solve problems (unless they are learning).  The artist comes up with the beautiful solution, the engineer creates the support framework for that solution, and the idiot gets in the way.  The idiot says well, this isn't quite the problem but, I have a beautiful idea of how this wheel thing should work, so instead of just using the design for wheels that countless other people have successfully used, lets build my beautiful design.  In the end the programmer wanted a beautiful car, and instead they end up with a distorted result of what was in their head with square wheels that explode when you turn left.

The trick is developing the Zen like guidance to know when to let the idiot get away with his prattling, and when not to.  Once you throw in idiot managers it becomes even harder and you wind up on a death march like Rentzsch talked about.The hack that makes the comma delimited script specification work is quite neat, but the overall approach is flawed.

Your scripts aren't going to be changing on the fly, they only change when you update your site.  So putting a load on your server each and every time a page is accessed is silly - you only need to concatenate and compress once every time you update your JavaScript.

This is fairly simple to incorporate into your upload scripts (and if you update your site by hand you really should think about changing that) so it's no maintenance burden.

From an implementation perspective, there are a number of things wrong too.  The HTML is wrong for silly little reasons.  He forces the script to run on every page load.  He doesn't use the PHP configuration functions to check if ob_gzhandler should be used.  The caching is flawed in a number of ways.

The latter is merely a symptom of the overall approach.  If static compression on update were used instead, your web server would be able to handle all the caching and serving automatically.  But because you're doing it yourself with PHP, you have to reinvent the wheel, and nine times out of ten people miss lots of little things, meaning it will work *most* of the time, but not *all* of the time.

It's more efficient and simpler to compress in your site update scripts and let your web server handle serving files - that is, after all, what it was created for, and it can do a much better job than custom PHP scripts.
The ending is awesome.1. While impure functional language would be valid as far as it goes, the point of contention is what is meant by just 'functional language'. Referential transperancy (what allows Haskell, Clean and Mercury to be qualified as pure) is not a necessary or sufficient condition. Historically, first-class functions is the criterion.

2. C does not support first-class functions. Function pointers aren't enough.

3. OCaml, Lisp and a few extensions of Haskell.This is one of the best explanations I have read.  Unicode can be quite unintuitive at times, and this explains the underlying reasons for why it works this way in a quite simple fashion.

There are flaws in his explanation of HTML though.  He suggests using a "&amp;lt;meta&gt; tag [sic]" to specify the encoding you are using.  In fact, that's not reliable, and you should always specify the character encoding at the HTTP level as well.  What is specified in the Content-Type HTTP header is canonical, and typically you would want to include the meta element for fallback if you intend on opening the file in a non-HTTP context (e.g. from a local drive).
This article is one of the biggest crocks I have seen in a while.  If you continue to conflate IT and CS, then you will wonder why CS courses aren't "up to speed" with industry.

They're not supposed to be.  Industry is any crap that makes people money.  Computer Science is a mathematical discipline.  When did so many people get the two confused?

CS departments aren't out of touch, they're doing what they're supposed to be doing: thinking.  What the hell does it matter if it's immediately useful or not?  Did anyone think a bunch of mathematicians in the early 20th century contemplating the decidability of statements in axiomatic logic would have such a resounding effect on the modern world?

Did people stop practicing mathematics after the Greeks?  

"Nothing new in 50 years, must be dead!"

"Geometry is not helping me harvest my wheat, must be a bunch of hopelessly lost academic nonsense."

The only thing limited here is the imagination of Mr. McBride.

Oh, he's a lecturer.  From UK.  You mean, like [this guy](http://www.lambdassociates.org/blog/decline.htm)?  That explains a lot.

Until version 3, Unix files stored a creation time.  Isn't that surprising?
the author is also a mercurial developer...depends on several things, and who you ask, but phobos is by many considered to be less than optimally designed, and certain parts (the D runtime) are also of lesser quality (looking at bug counts at least). It also receives a lot less development these days, and is far from community driven.I agree with you. In school, I spent most of my project time trying to find out if there was something to make my projects less coding and more robust and complete, such as existing libraries and so. 

I never wanted to program, but just get the job done efficiently and quickly. Only when nothing currently exists, and it has to be done, do i program. 

However, I never get tired of UI stuff. With web design, I could spend hours toying around with CSS and javascript to get it just right, cross-browser compatible, etc.Not programming.
The problem I have with Jonathan's point of view is he's talking in absolutes -- programmers like problem solving, period.

Bunk. I like learning new languages. I like solving problems neatly too. I was reminded of Linus Torvald's ancient quote about [when men were men and wrote their own device drivers](http://www.google.com/search?q=when%20men%20were%20men%20and%20wrote%20their%20own%20device%20drivers).  Systems programming is about so much more than mere problem solving.

Programming is also about craftsmanship, skills with languages and systems, accumulated experience and so much more. If programmers really only cared about problem solving they'd stick to crossword puzzles.&gt; I think it's more design philosophy than anything else.

I think that it's amusing that one of Python's core design tenets &amp;mdash; "there should be one—and preferably only one—obvious way to do it" &amp;mdash; is actually completely the opposite when it comes to web development.  We've got many templating approaches and many frameworks, compared with the relative scarcity of, say, Ruby.ok, like we all knew that already.  What's your point?  Saying something is or is not compiled is not saying much at all.I think another part of it is that people view problems differently, and so their preferred implementations of solutions will vary.  It's not comfortable to get into the original coders mindset to maintain their code base, so we like tweaking it to fit our own view, or in the extreme completely rewrite it.I dunno. I think programming is more than just coding.  To be a good programmer, you have to have some ability to see the goal of the project too.  Otherwise you could end up coding less effectively than you might otherwise.

I remember seeing an app written by 3 programmers for a middle school as a college project.  Each person's piece was designed without any thought or input from the other pieces.

It was pretty horrible.  Every week they had to reprint all the student records for attendance because the maintenance code reassigned student ids every week.
Yes, Help -&gt; About -&gt; Goatse can piss off your users.That's exactly the reason why I don't have any interest in what a non-coding architect has to say.
Without coding experience, you can be an architect that draws boxes representing systems, which is as important as having decent coffee.Spolsky is *not* dead on right.

He's right *only* if the code doesn't need to change. If it does, as it does in any long-lived project, there is a point where it's too hard to understand it. Sometimes there just isn't any "why was it done like that"/"why is this here" knowledge anymore. Necessary changes become a huge liability.

Where I work, we have hugely problematic portions of code ( who does not, lies ;-) ). Typically, we work around not changing it: we rewrite a part that need the change and isolate it. This gives us duplicated code, but it's much cheaper in terms of money (or time) and regression risk. So, ultimately, we seldom rewrite (and only parts), so we +- do what Spolsky suggests, but at least we feel bad about it and do not brag like him ;-)

Sposky's argument is one-sided and hence ultimately wrong. There is a balance and he fails to mention it.

He's dead on about total rewrites, though.I love to delete code, absolutely love it.

Means less code to understand &amp; maintain.  Less places for bugs."Pick another language that you can reasonably implement an OS in, ..."

Um, exactly how often do the vast majority of programmers need to implement OSes?If you're using XHTML, it's probably better to specify the encoding in the XML declaration, instead of a meta tag.I think the lesson here is simple...don't read Salon.comNah, for me it's definitely both. I don't code all that often, but when I do I relish solving problems and coming up with efficient algorithms and stuff. However, there's a lot to be said for the nice clickity-clack of the keyboard, the pretty blocks of code, and all that. I really do actually like the *mechanics* of coding. I'm learning Emacs and I have fun just moving the cursor around the screen, having it fly around at my beck and call. I love to see the text appear, the braces open up and close around things (or parentheses if I'm using Scheme). I love the little rhythms that come out of my fingers on the keys.

I'm sure carpenters like building things, making solid chairs or houses or whatever. But I can very easily see them enjoy the visceral feeling of pounding nails into wood, hefting around heavy objects, and all that.

So yeah, I like both the problem solving and the actual coding part.Who the hell downmodded this?
Pools can be really, really fast.  That much is agreed.  The problem with pools is that system-wide pools become a bottleneck in multithreaded code.  This stack-allocation technique avoids having any allocation bottleneck at all.

The problem is that any memory management at all is a threading bottleneck.
BSOD, mainly.  WinXP pro had been quite stable, but after about a year it was blue screening. 

Mind you, I kept installing all sorts of things (Audio/video stuff, code editors, graphic tools), but I don't think that should make an OS so unstable that it just up and dies with cryptic driver errors.

And it was suffering the usual Windows bit rot.  It seems that anytime you add or remove a program or driver or whatever to/from Windows, it ups a counter in the registry that tells it to run slower and slower.

Plus I code, and coding on WinXP but deploying to Linux was quirky.  I live in the command line anyway, and cygwin only gets you so far.

Since I was using fewer and fewer Win32-only apps,  I decide to make the switch, expecting that Ubuntu would be more stable.  Well, it hasn't blue screened, but X has locked up on one or two occasions.  Wireless was a pain to get working.  DVD playback with VLC is not as stable as on WinXP.  And I miss Paint Shop Pro.

(BTW, WINE is quite slick.  It failed me on PSP, but works like a charm for DVD Decrypt and DVD Shrink, and I have IE6 running as well.  For testing. :) )

I've yet to find a file manager that is really good and intuitive; I've settled for Konquorer, but KDE makes the same error as Microsoft in coupling the local file manager to a browser, and it has assorted annoyances  (such as how it prefers to associate apps with mime types rather than file extensions, often guessing wrong about what sort of file something is.)

The upside is that I believe, on balance, that I have more control over my files and apps, and that getting off the MSFT upgrade cycle is a Good Thing.
[removed]Yeah, but he was interviewed with the washington post and some other publications touting the same nonsense.[removed]Exactly, its easy to think of how you would write an XML processor, but its also all to easy not to see the horrible problems that you will inevitably run into, which the polished and published implementation has already solved.Sorry, the message `wakeUp:` to `child` got routed to `doesNotUnderstand:`. Now creating nuclear winter object and initiating launch sequence.That guy stole his icon from the Stickies program on the Mac!I agree with the comments, especially MoronHunter.
Yeah what about them? I'm talking about Rails here. The author is implying that RoR is a framework that does not require programming. This is false. You can't produce anything worth a dime using Rails without non-shallow programming abilities.

Newbies *used* to come to Rails with that faulty perception, due to the scaffolding feature showed off in screencasts, but that's no longer the case. Even back then, newbies quickly realized that RoR involved actual programming, at which point they returned to using phpNuke or similar tools which allow you to build websites and webapps without doing any substantial programming.[removed]Why not? That's the ideal, for efficiency.I hope there will be a manual for the three seashells.I love deleting code too.You are forgetting all the times we don't want to tear it down and start over, because we say nothing then.

There is a lot of bad code out there.   There is some good code out there.  A lot of the code I write needs to be torn down and re-written, in large part because I have to get things done by a deadline so the company can make money.   More than once I've known there was a better way, but we didn't have time to do it right.  

I have inherited code that was good, and I didn't want to tear it down.  Even there I could find places to improve, but overall it was good code that I was able to read and understand faster than I could re-write it.    I have also given other people code that I wrote, and had them tell me it was good code (In fact better than they could re-write it once they got around to understanding it (In one case the other guy told me that once he realized all the requirements my code met his re-write would have been 5 times as many lines without meeting all the requirements).

The real world doesn't encourage good code, so there is a lot of bad code that needs to be re-written.&gt; Unicode strings are not made of bytes.

What are they made of?  Fairies and pixie dust?

&gt; You should never know, or care, how many bytes it takes to store a Unicode string.

What a bizarrely backward concept.  When I'm sending data over the network, I obviously want to know how many bytes I'm sending.

I love commenting out, then using embedded comments /* /*, then using #if 0, then moving it to the end of the file, then putting it in a new file.
&gt; What are they made of? Fairies and pixie dust?

The whole point of the articule is that Unicode strings are a *concept*. They aren't "made" of anything.

&gt; When I'm sending data over the network, I obviously want to know how many bytes I'm sending.

Again, you missed the point: you don't send Unicode over a network; you send a *representation* of Unicode -- say, UTF-8 -- but not Unicode itself.&gt; Not in the middle of a real project that needs to move as fast as possible.

You mean rushed projects that never get the bugs out?
No, a unicode string that exists in memory has a definite computer representation.

You do not manipulate concepts.

You manipulate bytes.

Depending on the encoding you may be using 2 bytes per symbol or 4 bytes.
&gt; &gt; You should never know, or care, how many bytes it takes to store a Unicode string.
&gt; 
&gt; What a bizarrely backward concept. When I'm sending data over the network, I obviously want to know how many bytes I'm sending.

Yeah, “never” isn't the right word. His point, though, is that you shouldn't assume that one byte=one character, as so many ASCII-trained (or ISO-8859-1-trained, or codepage-1252-trained, etc.) programmers do.

[Edit: Formatting fix. Apparently I needed a blank quoted line between the double-quoted paragraph and the single-quoted paragraph…]That's exactly what I was thinking while reading this. He seems concerned over page performance, but then he wants to dynamically concat and compress his JavaScript on each page load? How about just building this into your deployment scripts?Yes, but working directly with the bytes in $encoding-of-the-week is exactly the sort of problem Unicode seeks to avoid. The idea is that you work with characters rather than arbitrary clusters of 1 ≥ n &lt; ∞ bytes.

[Edit: Formatting fix.]
[Edit #2: s/≥/&lt;/. Who knew you couldn't have more than infinite bytes?]In most cases, [srfi-26](http://srfi.schemers.org/srfi-26/srfi-26.html) is the right thing to do for "currying" in Scheme.&gt; you send a representation of Unicode -- say, UTF-8 -- but not Unicode itself.

That's as vacuous as saying you don't send numbers, you send a *representation* of numbers.  You don't send English, you send a representation of English.  What of it?

&gt; The whole point of the articule is that Unicode strings are a concept.

You might want to look up the meaning of the word "code".  The point of unicode is the code itself -- a binary representation of characters.  A unicode *string* is a sequence of bytes.
[deleted]Two points:

First, it isn't really a complete program. In fact, it is so far from a complete program its laughable.

Second, I have serious problems with any code in which it takes 9 pages of commentary to explain what one line does. 

It's a bit more complicated than that.  If you are using UTF-8 or UTF-16, you don't need to specify the encoding that way.  If you are using anything else, you *must* specify the encoding that way.  If you are serving it as text/html, you *shouldn't* specify the encoding that way.

There's nothing saying that using more proper ways of specifying the encoding forbids you from using &amp;lt;meta&gt; elements as well.  The best approach is to explicitly specify it in at least the HTTP headers and a &amp;lt;meta&gt; element, and if you are using XHTML, either use UTF-8/16 or use application/xhtml+xml and specify it in the XML declaration.  But barring any real need, it's usually simpler to just use HTML 4.01 instead of XHTML.

&gt; meta tag

You mean "meta element".  The only use a meta *tag* has is to signify the start of a meta *element*.
I love not commenting.&gt; His point, though, is that you shouldn't assume that one byte=one character

That is the usual point, but not the point of the author of this article.  His point is that Unicode has nothing to do with bytes at all.  To stick your head in the sand  -- "neither know, nor care".

THAT is NOT informative.  If you want to understand Unicode, the whole POINT of it in the first place was to get past the limitations OF 1 character = 1 byte.
Thanks, but I don't really need help finding marketing material on ODF.

What I need is a spec that defines all of the OpenOffice-specific configuration values.

I also need the spec on the formulas for spread sheets.

&gt; &gt; Unicode strings are not made of bytes.
&gt; What are they made of? Fairies and pixie dust?

Hey, you know how the printed string 10 is not the same as the number 10 (base 10), which is not the same as 10 (base 2)?

Unicode characters are unique to code points, which is just some ordinal number so we can say U+00E9 rather than "Latin small letter e with acute".

The actual bytes used to represent that code point can be anything, subject to all kinds of constraints.  ASCII could be designed from scratch with few constraints, and so follows a normal and (to western eyes) rationale progression.  Unicode has to work in all kinds of environments and languages.  The notion of a character being a byte is an accident of history.  ASCII fit in a single byte, so the 1-to-1 correspondence meant it was convenient for library code to handle strlen by counting bytes, and similar.

Not true in Unicode.  There's way more characters than fit in to a byte, but there's all these systems that deal with 7-bit ASCII and use the 8th bit for something else, or transcode bytes in different ways due to endian-ness, or... 

So there's UTF-8, -16, -32, etc.  Each encoding is some way of describing the same code point, very much like "10", 10b10, and 1010b2 can communicate the idea of ten.

&gt; &gt; You should never know, or care, how many bytes it takes to store a Unicode string.
&gt; What a bizarrely backward concept. When I'm sending data over the network, I obviously want to know how many bytes I'm sending.

Are you entirely unable to understand context?  Sure, you can count bytes for a unicode string when it's placed into some particular encoding, but they emphasis here is that a string of 10 bytes is not 10 characters, and you mustn't confuse the two.

Clear enough?[removed]You need the encoding in order to know what the bytes represent.  I don't understand your "$encoding-of-the-week" accusation, these are industry standards.

&gt; The idea is that you work with characters rather than arbitrary clusters of 1 ≥ n ≥ ∞ bytes.

Greater than infinity?  You better check your encoding.

Yes, you work with characters, but they ARE arbitrary clusters of 2 or 4 bytes each (or 1 byte for e.g. UTF-8).
Yes, in fact, when you sent that comment, you didn't send actual characters, you sent numbers.  What of it?  Both sides got a bag of bits, and have to have a way to assign some meaning to them.  

The old way of assuming the same meaning on both ends without some way of -knowing- is dead.I'm about to start development of a personal web project, and am
not sure which language and framework to choose. I've been developing medium size applications in PHP and MySQL or Postgres for a few years. Using what I know would be faster, but learning something new would be more enjoyable, and PHP doesn't feel very 'cool'. The choice I have to make is first whether I stick with PHP, or change to Python, Ruby, Other or even .Net. And then which framework do I choose. I have my own base of code in PHP, but I'd like to start to use an established framework. I don't think I need anything non-standard from the framework - database stuff, login/session management, and inbuilt support for some fancy AJAX stuff would be a bonus.

I know 'ask reddit stuff' is frowned upon, but I don't know many neutral spaces in which I wouldn't get a biased answer.&gt; Yes, you work with characters, but they ARE arbitrary clusters of 2 or 4 bytes each (or 1 byte for e.g. UTF-8).

Any decent unicode support uses some internal encoding from unicode code points to bytes, sure, but it also hides that and allows client code to work with characters, not bytes.

len("hello!") != len(u"hello!".encode('utf-32'))&gt; Hey, you know how the printed string 10 is not the same as the number 10 (base 10), which is not the same as 10 (base 2)?

The UTF-8 string "10" is the same in all those examples.

&gt; which is just some ordinal number so we can say U+00E9 rather than "Latin small letter e with acute".

Yes, an ordinal number of 1 to 4 bytes.

&gt; The notion of a character being a byte is an accident of history

Accidental only in the sense of English not having a large alphabet.  Using the smallest convenient computer representation is not an accident at all.

&gt; Not true in Unicode. There's way more characters than fit in to a byte

Yes, I know.  That is why bytes figure prominently into the picture.

&gt; Are you entirely unable to understand context?

Are you entirely unable to understand English?  "**You should never know, or care**" in bold (emphasis in *original*) is not mincing words.

How can you defend what it's saying while at the same time disagreeing with what it's saying?  Cognitive dissonance much?

&gt; but they emphasis here is that a string of 10 bytes is not 10 characters, and you mustn't confuse the two.

In 8-bit encodings it sure is.  Like the encoding we're writing in right now.

It is not educational to hide the details -- showing some 2-byte Unicode characters then claiming bytes have nothing to do with it is both misleading and mistaken.
I think skip lists are going to become more important as we head into an era of widespread concurrency, because unlike trees which require constant rebalancing, multiple threads can be reading and writing to different parts of a big skip list without blocking.&gt; Any decent unicode support uses some internal encoding from unicode code points to bytes

I see what the problem is.  I'm trying to educate someone with only a rudimentary understanding of computers.

The above statement is like "any decent numerical support uses some internal coding from numbers to bytes".

If a computer COULDN'T convert unicode "code points" to "bytes", it wouldn't be very useful, would it?  As in, it would be *completely useless*.&gt; The old way of assuming the same meaning on both ends without some way of -knowing- is dead.

What the hell are you talking about?  You always had to know what the hell a series of bytes meant.  Is it an int?  A float?  ASCII?  EBCDIC?

&gt; Yes, in fact, when you sent that comment, you didn't send actual characters, you sent numbers.

We send electrical signals, which are interpreted as whatever we have programmed the computer to interpret them as.  It's funny how you have a little bit of knowledge and are running with it.This is a repost of something posted 8 days ago.  Why isn't *that* surprising?
&gt; I'm trying to educate someone with only a rudimentary understanding of computers.

Congratulations!  Your ego must keep you warm and fuzzy in the face of such pitiful company.Notice the qualifier "merely."  I am only referring to those people who do nothing -but- that.
&gt; &gt; Are you entirely unable to understand context?

&gt; Are you entirely unable to understand English? "You should never know, or care" in bold (emphasis in original) is not mincing words.

So, I take that as a yes.&gt; We send electrical signals

You, sir, are a first class asshole.&gt; Your ego must keep you warm and fuzzy in the face of such pitiful company.

Pitiful company, like yourself with all your undereducated responses?

How long have you been programming?

Have you ever programmed in C?

Assembly?

The difference between us is when it's pointed out that you don't know something, you blithely carry on as if you do.

You admit you don't understand English.That's exactly how I would describe everything made by microsoft. They always use libraries to do everything, never customizing anything.

Eg. They use internet explorer to do windows update. This is extremely poor. Apple was right to make it a separate application. This of course takes much more work for apple.

Also, whoever modded me down to -5 has clearly never touched an ide or worked on a large project.&gt; Pitiful company, like yourself with all your undereducated responses?

Yes indeed.  I humbly acknowledge your complete mastery of all things I have failed to understand.

&gt; How long have you been programming?

There is no right answer to that.  If it were a year, you win.  If it were 20, you win.  You are already quite confident you're superior and incapable of misunderstanding.

&gt; Have you ever programmed in C?  Assembly?

Huh?  Wazzah?  Oh, that character to the right of the X?

&gt; The difference between us is when it's pointed out that you don't know something, you blithely carry on as if you do.

Do you sell tickets to your mind-reading show?oiSDfin!@&gt; You, sir, are a first class asshole.

Look who's talking!  You try to "correct" me by being pedantic, yet can't take it when you are one-upped.  And, perhaps realizing that you are easily hoist by your own petard, you resort to pejoratives.

1. You don't know as much about this subject as I do.
2. You presume to lecture me.
3. You exhibit cognitive dissonance by defending and attacking the same point.
4. You seem to be relatively new to the field, asserting the "old way is dead", without understanding the history.
5. When your ignorance is pointed out, you resort to insults, having come to the end of your erudition.
I'd really prefer officeless paper.Considering this is a personal project (which I'm reading as: "It's ok if I break it for a while and can't fix it") I would say choose another language.

Every programming language forces you to filter your concept of the program through the constraints the language sets on expressing yourself to the computer. Since every language has a different set of constraints learning new languages helps you to understand how a given language shapes your code.

All that theoretical stuff out of the way, I would say pick what looks most interesting and run with it. No offense, but as far as unique approaches to programming goes, PHP isn't really a winner of a language. My personal choice is Python (and TurboGears for the framework) but pretty much anything you listed can teach you something. Unless .Net is somehow grossly deficient they all should be able to fit the bill.&gt; There is no right answer to that.

It's not a matter of a "right" answer.  My guess is you program in Python, since this is ostensibly a Python article.  Python's official implementation is in C, a language in which you DO have to deal with bytes.  The way you argue implies that you are unfamiliar with the underlying computer representation.

&gt; Do you sell tickets to your mind-reading show?

You already know the answer to that.Since it's a personal project I think you should use an other language (than PHP), if only to build your skillset, but it will also broaden your mind.

I'd suggest the pretty standard Ruby/Rails or Python/Django approach.

As for which one you want to choose, assuming you know neither (language/framework), just skim over a few Python and Ruby tutorials to get a feeling of the languages, then pick the one you like best.

You may also skim over the featuresets of the frameworks. Basically, Rails has a very simple but slightly bloatifying ORM, templates that are powerful to a fault (potentially too powerful), very good test integration, very good JS integration and -- with the last release -- a strong REST support, a fairly complex but quite clean directory structure (easy to get lost in it though) and a lot of magic. Django on the other hand has a quite good but less simple to use ORM, templates that are simple but powerful enough and wicked fast, a much higher speed than Rails (rendering stuff), AutomaticAdmin (admin interface can be autogenerated for you), good authentication and permission systems out of the box and a much simpler directory structure than Rails projects, but it has almost no JS or unittest integration.

I like Python/Django more than Ruby/Rails, but that may be because I prefer Python to Ruby.&gt; You are already quite confident you're superior and incapable of misunderstanding.

Ironically, your lecturing reveals that this is exactly how you present yourself.

There's nothing wrong with being confident.  The reason I'm confident about certain things is because I have a lot of experience with them.  I *don't* have extensive experience with Unicode but it's simply first principles that unicode strings used by a computer are sequences of bytes.

This is not a revelation.  Thus anyone who disagrees with it does not understand the first thing about computer architecture.
Looking to Backup or Transfer Your Horde Email Folders?  You might think you can simply transfer the folder files via FTP.  While in some instances this is possible, in most the file formatting prevents this method from being successful.  Here is an approach that should work in most versions of Horde (webmail).Good to see I'm not the only one posting Forth articles here. :)Ruby's getting there. They're up to three fairly-well-known frameworks now, and I'm sure more are on the way.wasn't me. But obviously I disagree with you. Though, I've never cobbled together an app _mostly_ out of libraries.No, he means the real world.Heh, I'd ditto all of that. You beat me to it.

I'd also say, heck, do both. Ruby on Rails and one of the major Python frameworks. Both are so fast to play with that you might as well. Both Django and Ruby have built-in development servers for easy local playing.

All these packages have strengths and weaknesses, and while I expect these strengths and weaknesses to all converge over the next couple of years, until then there are practical differences between the frameworks. Some have better templates, some have better ORMs, some have better built-in libraries. You need to figure out what your problem space needs most, then pick the framework that best offers them. 

For instance, Django's ORM is adequate, but pretty weak if you start pushing it, so if you've got a project where you're going to need wild SQL, I wouldn't recommend it. On the other hand, for the built-in admin application, much can be forgiven. It all depends on what you need.&gt; Responding to Origins: Why the iPhone is ARM, and isn't Symbian, sources from Sweden and Finland offer a revealing look inside Symbian development and how the OS is regarded at Nokia, and what that means for development on the iPhone.

Very good article describing the Symbian ecosystem.

&gt; Limited support for multi-threading. An important technique used on all other platforms but is not used on Symbian

I do not agree with this, cooperative multitasking is the way to go given the target hardware and memory constraints. The platform should provide the necessary API to make it painless though.

&gt; Bad development environment.

This one is crazy. There are several SDKs to use if you want to cover most Symbian targets. As explained in the article the target platform is extremely fragmented which makes compilation environment very complex. Build automation can be achieved with some efforts. But automating any runtime behaviour (debugging, running unit tests) is mind-blowing given the unfriendliness of the deployment process. And emulators are no help here, and sometimes makes thing worse since their emulation behaviour happens to be completely different from the emulated stuff, or requires different code paths to run.

Wow, what a mess this python Unicode support is. Tcl and Java are far beyond this, letting programmers focus on the problems instead of nitty gritty details how you have to call windows file functions just to get correct filenames.&gt; Eg. They use internet explorer to do windows update. This is extremely poor. Apple was right to make it a separate application. This of course takes much more work for apple.

Actually, Safari is backed by a library, too ([WebKit](http://webkit.org/)). I was going to say “but they know where to use it and when not to” (Software Update is pure Aqua), but then I remembered Apple Mail, which uses WebKit to display *and edit* messages (even plain-text ones).

My point is, using a library is not evil if the library is good and you're using it for the correct purpose.&gt; Select a section of code in your editor, hit the backspace key, and be done with it.

This is bad advice. Tried it in VIM, didn't work.Special case for vim: Select a section of code using visual mode, hit d, and be done with it. :)duhweb.py is the only framework I've ever used that feels like programming in an actual programming language instead of programming in the framework, rails has nice deployment and database migrations, turbogears is a kitchen sink.

basically they all suck. take your pick.Pylons is one of the most flexible of the Python frameworkds, it seems.I've really started to like Catalyst lately. I'm less enthused about DBIX-Class, but it plus Catalyst plus Template Toolkit seems to be a really solid combination.That's not marketing material. It is an extract from the wikipedia. If you want the specifications you can read the article and find the link there.

Are you trying to have a serious discussion or are you just too proud to change your opinion?The real world, where there are rushed projects that never get the bugs out :)i assume this is in the context of doing things like statistical translation and word sense disambiguation.

if you're doing learning/statistical modeling problems, computational speed is essential to achieving satisfactory correctness. that's why bullshit about representing integers as lists or something makes no sense in an area like this. if you got a computer 1,000,000 more powerful overnight somehow, you'd just try to do an instance of the problem 1,000,000 times more complicated instead of being 1,000,000 times lazier.Yes, certainly. Such is life, though.

It's nice to keep working on a piece of software until it feels perfect to you, but it's also nice to actually get it out there and get actual feedback from users and work based on that.Obligatorily: [Subtext](http://subtextual.org/)[removed][removed]Maybe more of a "light remote execution environment" that is tethered to a host Forth, but interesting nonetheless.  Tethered development is very useful for avoiding wasted time when getting a system bootstrapped, and the less bootstrap code the better!Through the grapevine we hear it is supposedly to help authorities track sources of counterfeiter money.   I don't know of any official source that proves this, though.I'd recommend Fusebox for PHP.  I've been developing in it for a few years now and I've been very happy with it.  www.fusebox.orgthe stuff about representing integers as lists is only a conceptual issue, the compiler would optimize those operations to direct machine instructions and you'd help it with defines.
"peace"?  Was that intentional?&gt; Vote this, the news is huge =)

The news is old. The crack was revealed on [October 17, 2005](http://www.eff.org/news/archives/2005_10.php#004063).

Not that I'm against spreading it. This is just to set the facts straight.&gt; Right, but unlike C's API, they're not (yet) standardized.

POSIX is not C, why do you keep insisting the two are the same. A conforming C99 implementation need not provide 9945. 9945 is mentioned once in the C specification, in a footnote:

"188) ISO/IEC 9945-2 specifies locale and charmap formats that may be used to specify locales for C."(pg 217)

The standard is quite clear on what 'may' means in this context. In fact, a program written using the POSIX library is not even a _strictly conforming_ C program, according to ISO :

"A strictly conforming program shall use only those features of the language and library specified in _this_ International Standard." (pg 19, emphasis mine)

Just to be totally clear, we are talking about the C programming language here, as standardised in ISO/IEC 9899 which "specifies the form and establishes the interpretation ofprograms written in the C programming language."

POSIX/SUS on the other hand, is defined in ISO/IEC 9945 as"
a standard operating system interface and environment, including a command interpreter (or "shell"), and common utility programs to support applications portability at the source code level."

POSIX happens to be based on the C language, but that does make the two somehow the same. I can use POSIX functions from my lisp implementation using FFI (although, like in C, my program will not be a _strictly conforming_ CL program), just as one links to the same libraries from C, or C++, or FORTRAN, etc.

&gt;Declaring that's C socket API is not standardized because they're not in 9899 makes about as much sense as declaring that Common Lisp's hash tables are not standardized because they're not in the first half of ANSI X3.226.

9899 and 9945 are clearly two different standards with entirely different scopes. ANSI X3.226, OTOH, is one single document which simply "is designed to promote the portability of Common Lisp programs among a variety of data processing systems."

If you want to make the claim that POSIX defines a standard for sockets which can be used from C, i'll concede the obvious point. But given that i can use that same standard to use sockets from my lisp, i still wouldn't argue that CL has standardised sockets, because it doesn't. There is no mention of sockets in the documents that defines CL, just as there is no mention in the document that defines C.

_Edit: spelling, formatting_eh... thank you, but I meant the reasons why

&gt; [...] for people who use a PC as a basic E-mail/Word processing/Web appliance, WinXP is as good or better than Ubuntu.

:)

I'm on Linux too, I'm curious as to why you think Windows is better suited for such simple tasks, where the two are pretty much identical.sure, libraries are fine, especially if they're part of the OS (there's no overhead if it's ingrained in the OS already), but coding using nothing but libraries (that you're loading separately) is often a bad way to go.Couple reviews of languages/frameworks I've programmed in:

**C/CGI**

You've gotta be on crack to use C in a webapp.  I wasn't quite on crack, but I was in high school and C/C++ was all I knew.  Anyways, the manual memory management will kill you.  Don't even think about it.

**Perl/CGI**

This one isn't too bad.  You do have to be very disciplined to make this work over the long term.  I wasn't (first job, in-between high school and college), so my code ended up as a ball of spaghetti.  But the string handling is good, there're tons of libraries, and the CGI module was pretty good.

**Object-oriented PHP4**

By "object-oriented", I mean going for the whole enchilada of templates, db abstraction layer, objects, unit tests, etc.  IMHO, it's not worth it.  If you need objects that much, you should go with Rails or one of the Python frameworks.  PHP4 just puts up too many obstactles, like the pervasively non-OO standard library, the default pass-by-value semantics, the $this-&gt; syndrome, etc.  (Disclaimer: I haven't used PHP5, that may have fixed many of these problems.)

**Pure put-data-on-screen PHP4**

By put-data-on-screen, I mean structuring the webapp as a bunch of pages that pull data straight out of a database and write it straight out as HTML.  No templating libraries or abstraction layers, though I did use PEAR DB for its utility functions, and I separated controller and view by having each controller import a .phtml file where I disciplined myself to using only &lt;?php echo $var ?&gt;, &lt;?php foreach($vars as $var) { ?&gt;, and &lt;?php if { ?&gt;.  Interfaces between .php and .phtml were all clearly documented.

This can work - it's probably the quickest solution if you just need to get something up in a hurry.  There's comparitively little bullshit, which means that you get a lot done and not much can go wrong.  It really falls down if you have more than a file or two of reusable content, however.  The interfaces become unmanageable without the formalization of real function calls and objects.

**Java/J2EE, old style**

Meaning session beans, entity beans, and container managed persistence.  Stay the fuck away; it never really worked anyway.  You will spend all your time writing XML configuration files, and then your app will be so slow that you have to rewrite the whole thing in PHP.  Does anyone actually write apps this way anymore?

**Java/JSF**

This can work as long as you're building something that's reasonably close to the Petstore example.  Basically, you're app needs to pretty much CRUD-like, with screenful after screenful of workflow screens.  JSF will move between them via POST, and of course you define navigation via XML config files.  It's a lot of work for comparatively little payoff, but at least JSF handles the state management and validation for you, which can be a win in some cases.

Stay far, far away if you're building an Ajax app.  The A4J folks have done wonderful things that are now horribly broken as of JSF1.2, because the JSF spec *can not* be adapted to an AJAX lifecycle.  JSF went out of their way to create a spec that was extensible in any conceivable manner, but the way that webapps actually changed was, of course, inconceivable.  C'est la vie.

**Ruby on Rails**

I really liked the framework, but beware: when DHH says it's opinionated software, he means it, and it's quite difficult to use (though not impossible) if your opinions are different from his.  For example, if you're in the domain-keys school of database design, tough: Rails requires a numeric primary key with the name id in your data model.  (You can override this, but if you do, you don't really get much benefit out of Rails.)

I also found there was a lot of magic involved in Rails controllers - things like it auto-detecting the view to render from the name of the controller method, or reading template variables from the instance variables of the controller - and much of it wasn't documented.  If you like this, great, but I found that it bit me when eg. I wanted to define a page component that would render as a template variable in a larger page, but also render :partial in response to a link_to_remote AJAX call.

When it came to choosing a framework for my own web startup, Ruby on Rails was a front runner, but in the end I abandoned it for...

**Python/Web.py**

This is my current favorite.  I looked very briefly at Django, Turbogears, etc, but the impression I got from them was that they were frameworks in the Rails or J2EE mold.  You don't write a Python app using Django, you write a Django app.  The language itself becomes a tool in the framework, rather than the framework being a tool to write webapps in the language.  I don't like this; I feel like I have to know enough languages already without some framework developer trying to pass off his domain-specific language as a framework, particularly since Rails or Django or JSF would probably be judged to be fairly mediocre languages if they were considered programming languages.

Web.py is basically just a bunch of libraries: it lets you access POST, GET, and COOKIE variables, it defines a mapping of URLs to controller classes, it provides HTML/URL/SQL escaping functions for security, it has a simple database query API, it integrates with Markdown, and it has a simple templating language (I don't use it though; I use Cheetah instead).  Everything else is recognizable Python; I don't see all that much magic behavior.  

**Haskell/WASH/HaskellDB**

This would be an interesting choice, but I gave up after too many installation headaches.  It was early 2005; it may've gotten better since then.  You do need to familiar with monads; just about everything is a monad in WASH (or HaskellDB, for that matter).  But it gives you an amazing amount of power and composability once you've gotten past that point.

I've heard that the latest web framework in the Haskell world is hApps, so you may want to check that out too.&gt; I'd also say, heck, do both.

Yes of course, and do also learn Smalltalk and Seaside, and Erlang and ErlyWeb, and everyone else of these wonderful languages and frameworks out in the wild (too many of them and not enough time dammit!)

But learning a new language correctly takes time, and so does learning a new web framework.

He's got a project to do, so he needs to pick one, nothing prevents him from trying something else for the next project or the one after ;)

&gt; For instance, Django's ORM is adequate, but pretty weak if you start pushing it, so if you've got a project where you're going to need wild SQL, I wouldn't recommend it.

I agree, I didn't mention it because it wasn't really relevant (even if it's fairly weak, Django's ORM is often more than enough) and because [it may become completely irrelevant in a pretty near future](http://code.djangoproject.com/browser/django/branches/sqlalchemy)Color laser printer print this tracking information.  BW laser printers do NOT.  The common explanation is that it's to catch potential counterfeiters.Now that we've hopefully got the flame war settled, and after a day of trying to be objective in my post mortem..

_"The first is made out of the pure stuff of helpfulness; I think you must've already forgotten it."_

No, I didn't forget. I appreciate the sentiment and motivation, but there's a difference between throwing untargetted information at someone and actually being helpful. If I ask how to *easily* find something, how does throwing a 10-point list at me help me? Perhaps you're telling me about something basic or about etiquette; I'm not convinced I merited such a response. Untar html documentation locally? Why should I care? And when I'm looking for a reference, the user guide is momentarily irrelevant, no matter how good it is.

Point 7 came closest to answering my question, but I had already figured it out by trial and error, that's how I found the links in my original comment. You showed me the sequence of steps without telling me why it had to be so long or how to make it shorter.

---

Compare searching google for 'python reference', 'haskell reference', 'erlang reference', 'common lisp reference', 'ruby reference', 'c++ reference'. Just about every single language but erlang will immediately show where the libraries are. The [link returned for erlang](http://www.erlang.org/doc/doc-5.5.2/doc/reference_manual/part_frame.html) is just the language, not the libraries. This was what I was talking about.

But ok, perhaps that's not erlang's jurisdiction. Let's ignore search engine issues for a minute and focus on browsing to the documentation. I already mentioned the cryptic name "/OTP R11B", but let's cross that hurdle. Next there's two levels of misleading indirections: Why are the *libraries* under Applications and then 'Basic applications'? Why is the first link under Basic applications called compiler? (I can't think of anything farther from basic.) Why does this link contain basically nothing? Why is the standard library just two of the other links -- kernel and stdlib? Why is kernel right at the end? Why is the list not even alphabetical? Why does the entire page not contain *one* instance of the word 'library'?

So is my complaint well-founded?

---

_"Erlang has excellent documentation, especially in contrast to other languages.."_

1. Is this relevant? When someone says he's having trouble with something, consider whether telling him it's all excellent is helping him. Especially since the question isn't addressed at you. If it's obvious to you you're in no position to help. Move on.

2. I'm not going to respond to the comparison without more substantiation. In what way are other mainstream languages deficient in documentation?

---

The dictionary defines a whine as a 'feeble peevish, self-pitying complaint'. I don't think any of those adjectives applied to my original comment. Perhaps you intended it to mean repetitive, but I had never said this before. This subject hasn't come up on reddit in my experience (references welcome); so why be snarky, no matter how gentle you think it is? Exercise restraint.

I realize all this is just from my side, and I would like to hear your side. Will you at least grant that I merit civility?That's OK then, as long as we don't counterfeit, no one will track our printouts.[deleted]&gt;cooperative multitasking is the way to go given the target hardware and memory constraints

Ugh, are you serious? Pre-emptive multitasking does not require much in the way of resources, and modern mobile phones can easily handle this...You're a sick, sick person.to the title, i reply: 
then i must be a bad programmer.
but the rest of the post is reasonable.
would be better titled as "good programmers only code when they must"Those pages don't explain the line -- they teach APL by it.GrrrrI've developed web apps in Django and have liked the speed at which they can be developed, but hated the fact that there was sooo much magic going on (even after the magic removal branch was merged into trunk). It just doesn't feel like you're doing any work. If you're looking for something super quick pick django or rails. Your productivity will flourish, but you'll feel dumb afterwards.

If on the other hand you wanna feel like you've acomplished something, use web.py or something that doesn't get in the way... something that just "helps" a bit.Easier to find others who can help you.

Plus I find Word to be nicer than OpenOffice.  YMMV, etc.

And overall font rendering is better on Windows.&gt; POSIX is not C

POSIX C is not ANSI C.  Both, however, are standardizations of C (with the former including the latter).

&gt; If you want to make the claim that POSIX defines a standard for sockets which can be used from C, i'll concede the obvious point.

POSIX standardizes the language-independent socket interface *in addition to* C's API for them.

&gt; But given that i can use that same standard to use sockets from my lisp, i still wouldn't argue that CL has standardised sockets, because it doesn't.

Exactly.  Unlike C, whose API to the language-independent interface *is* standardized by POSIX.

&gt; There is no mention of sockets in the documents that defines CL, just as there is no mention in the document that defines C.

9899 is *not* the only document that defines C.

The fact that 9945 standardizes a whole lot of stuff in addition to the C libraries in question doesn't make it any less valid as a standard.  The only way you can reasonably exclude it is by declaring that the definition of a language excludes its standardized libraries, but then you've simply succeeded in making the concept of a "standardized socket library" impossible in principle.

The fact of the matter is that contrary to the original post, C's socket library is officially standardized, and Common Lisp's is not.  Does this mean Common Lisp is any less practical?  No.  Would some official standardization of Common Lisp's socket API translate into any measurable real-world benefit?  Highly unlikely.  Does this whole question, in fact, have any real significance at all?  No.Yes, that's a good but rare thing.  

I suspect people fail to cite where they learned stuff either because of laziness or because they think it will make them seem less clever.  

But when someone is quick to cite a reference or offer thanks for a tip, it makes me think they are more astute, more professional, and more to be trusted.  It suggests a confidence that only comes with experience.Yeah, the title was more playing off of Jonathan Rentzsch's original post title.I would really like to know if there are any real instances of people doing real counterfeiting with a cheap ass laser printer.   Really, I doubt it's ever happened.  The real reason is because the government wants to be able to use printer evidence against you on other cases.  

Did any printer manufactures tell the gov' to stuff it on this shit?  I'd like to know if there is somebody I should be willing to spend an extra 10 bucks with because they aren't being evil.Darwin is a BSD derivate, isn't it? As such it should be quite portable.That's okay, then, as long as we don't print anything the Bush administration thinks is dangerous or "emboldening" to the enemy and then try to send it anonymously.This is a really cool idea.Mathematicians like proving interesting theorems elegantly, even if they've already been proved. They don't like doing arithmetic for its own sake.[deleted]That was sarcasm, right?Yeah, Catalyst(http://catalystframework.org) is like RoR but without the opponionated bit.  DBIx::Class is optional, as is Template Toolkit.

Top knotch dispatcher, model and view agnosticism, built in server and excellent test harness makes this one a goodie.I've always wondered why Frank didn't do a two instructions monitor instead of three (as his goal was to write a minimalist monitor). You don't *need* the read instruction, as you may well implement it *afterwards* as soon as you can write to memory and jump there.In Soviet Russia, all typewriter must have typing sample on file to catch samizdat running dogs. Really.

Could one return these printers to the manufacturer under warranty, citing this ink-bleeding "defect"? The reasonable consumer, unless told otherwise, could assume that the printer does not randomly print rubbish on the page.

Ahh, the land of the free.But a terrorist would be smart enough to buy a printer for cash and not register it.You should check out Sean Barrett's IOCCC winner from 1992, which implements a very simple Forth-like language, called FIRST, and then builds a much more complete Forth-like language, called THIRD, on top of that. The design overview is here:

http://www.ioccc.org/1992/buzzard.2.designI wonder to whom he thinks they should outsource their IT functions?It's repeated in the article, so I'm not sure what to think. My best guess is that it's just a typo which has been repeated verbatim.

Either that, or I'm missing some subtle but brilliant joke.Why exactly is this huge? Your printer still prints those marks, the NSA will still get you.This guy never heard of restrict... sigh...
[deleted][removed]Of particular interest, perhaps, to those of us trying to figure out the wizardly art of project management.Niemöller ftw!IPhone definitely has all of Darwin. The question is how much of the Carbon and Cocoa APIs it has, not if Darwin will be used. Anyone can get all of Darwin onto their Intel or PowerPC already for free, since it's (semi) open source.TT makes it all work, to me it feels like one without the other would be incomplete.

DBIX-Class is optional, of course, but still feels like it's getting in my way as much as it helps. No idea what we'd use instead though. The thing that worries me, is alot of that code is now written in such a way that it's no longer backend agnostic, so if we do decide to switch to Postgre or some other sane database, it's going to take alot of work to make the transition. MySQL pisses me off most days.

For those interested, there is an O'Reilly TT book which is good, but none on Catalyst (yet?). There needs to be one, and I nominate the basselope as the animal on the cover.&gt; Why is the standard library just two of the other links

Those *aren't* 'the standard library' in any typical sense of the word.  They are collections of modules basic to an Erlang node's operation.  There is only one module that is even automatically included in Erlang modules -- the module 'erlang', and not all of *it* -- and Erlang modules can still override its functions without remark.

Or if you'd prefer to identify 'standard library' as the body of documented reusable code that comes along with a direct installation of the system, then everything on that page is 'the standard library' -- and if you don't, how do you think about what Python and Ruby and Perl provide?  All of them likewise have always-there code (Perl identifying it as in CORE:: , and allowing its redefinition) and also an expanding generally-always-installed code.

It may also amuse you to search for other or more nuanced terms in some of the languages you mentioned.

&gt; So is my complaint well-founded?

You haven't updated your claim from 'I could not find the documentation through google', and by these comments you still adhere to it.  Well, I still adhere to such fine responses of mine to it as "how about idiotic?"

...

&gt; Now that we've hopefully got the flame war settled,

&gt; The dictionary defines a whine as

&gt; Will you at least grant that I merit civility?

No, let us please continue to wait for the 'flame war' to settle.Apple emulated Classic in OS X. Anyone can install DosBox on their computer and emulate DOS. Microsoft has the resources available to make a break with the past and paper it over with emulation, but they choose not to do it.Is it illegal to sell printers that don't make these?Using appropriate libraries are the best way to reuse code. It does not make your application slower than using your own functions. It's fundamentally best-practice, not "a bad way to go".

Abusing libraries to do what they're not designed for is usually bad though. But then you should often write a new library instead.But "a terrorist" isn't who they want to track.
@davidreiss666 is right, but it's also hard when money is cut, unless they get all the bills from a single sheet.

From a technical standpoint, this is really, really cool, but from a privacy standpoint, it's creepy.  Actually, I always thought that making a databases that tracks everything about a person (CC purchases, location via cameras, etc) to detect weird behaviors (is that a bomb or promotional material for Cartoon Network) and possily terrorists would be very, very fun, but also ethically messed up.

Here's one way the EU and US protect against it:
http://en.wikipedia.org/wiki/EURion_constellationThe same reason revealing warentless wiretapping is huge.

If you want to circumvent this, printing printing similar codes that have a random time and serial number on top of the existing code might confuse the CIA.

Or, and get this, this idea rocks, remove the yellow ink.  Too bad that makes it hard to counterfeit money (cyan + yellow = green).  You might be able to switch the magenta and yellow inks and channels to get something pretty close.Nice overview.

Perl isn’t limited to CGI anymore, though. As usual, “there’s more than one way to do it (but not all ways are created equal).”

#### mod_perl

Has been around for *years*. It isn’t just a way to make CGI scripts much faster, it also makes it natural to structure a web app in a much better fashion than CGI scripts generally are. Writing raw mod_perl handlers gives you *excellent* performance, however deployment is demanding.

#### Mason

This is an older-fashioned web framework, which is basically a template engine (where Perl is embedded in the template) with lots of app-writing infrastructre. You build apps in it basically by writing templates that inherit from each other.

It’s kinda like put-data-on-the-screen PHP, except the underlying language isn’t horrible and code ends up more much naturally structured and reusable. It’s kinda fun actually, though these days it has a distinctly stale taste.

Much of Amazon was built with it at one time and they’re still using it extensively. There’s an O’Reilly book about it.

#### [Catalyst](http://catalystframework.org/)

This is a modern framework along the lines of Rails (ie. not just a bunch of libraries you use, which you wrote you prefer). It’s older though, has much more powerful and convenient routing facilities, and is much less opinionated. You can override just about everything to taste using built-in facilities, including things like how the view render template gets determined.

In the same vein, the accompanying [DBIx::Class](http://search.cpan.org/dist/DBIx-Class/) ORM has excellent many-to-many support and understands any sort of primary key you can throw at it – it is in fact the best ORM I’ve seen *anywhere*. It’s sort of misleading to even describe it as an ORM: it’s more of an object-oriented resultset-oriented database API that plays to SQL’s strengths rather just using the database as a souped up flatfile.

The template engine most people use with Catalyst is [Template Toolkit 2](http://tt2.org/), which has spun off clones in many languages and is generally praised. Personally I hate to use it for outputting HTML, but you can plug any template engine you want into Catalyst; it never feels like a second-class citizen. (The largest non-Template-Toolkit minority consits of people who use the Mason template engine with Catalyst.)I disagree. I only have the urge to do that when the inherited code base is absolute crap. Most of the time I tend to work with what I get. For instance at my workplace I have not yet proposed to rewrite any major subsystems from scratch because most of what we have is pretty good. Now, there are chunks of code that we will rewrite this year because the new requirements cannot be accommodated given those old designs but we rarely scrap everything and typically try to salvage as much of the old code base as we can. The secret to such efficiency is to hire good programmers (who get stuff done right on the first try) and emphasize technical competency first and foremost, something that my employer has been exceedingly good about.There’s a book in the making. Unfortunately, O’Reilly won’t be the one publishing it. When the Catalyst devs approached them about a book, O’Reilly basically stated that they want only a single web framework in their lineup, and they’ve settled on Rails, and that’s that. Short-sighted if you ask me (and I’m not just saying that because of Catalyst, there’s also Django), but there ya go.They have a new label for undermining, through propaganda, anyone who threatens their power? What is it this time?I hear you. A lot of stuff ends up being this way. Hence such overwhelming urge to rewrite stuff. Sometimes the rewrite is the right decision, usually when the first design is flawed in  a major way. Most of the time however, a rewrite ends up hitting the exact same corner cases and being not much cleaner than what the stuff it replaced.Avoid Zope why? And how is Django "crap"? Explain.Alright, this is pointless. There's only one of us being sarcastic and making ad hominems. There's also only one even trying to be introspective. I'm going to stop wondering what I did to deserve this.

My conclusion is that you seem to think (and express, with extreme prejudice and defensiveness) that it's sufficient just that it's all there, and that organization is not important, and that anybody who can't handle it is a moron. That's some kind of closure, I suppose.

_"You haven't updated your claim from 'I could not find the documentation through google'.."_

What about my critique of browsing the erlang docs?

_"It may also amuse you to search for other or more nuanced terms in some of the languages you mentioned."_

It's painless everytime I search for python or ruby or haskell or lisp or scheme docs. Do you have any specific examples? Or *any* reasonable search that leads to the top-level erlang documentation page? The best I can find is 'erlang documentation' which leads to [this](http://www.erlang.org/doc/doc-5.0.1/doc) older but much better-organized version.

Desiderata: you seem to accept that your statement putting down other languages was ill-considered, and that your original response wasn't as helpful as you thought. You're just not man enough to say so.

Over and out.

\----------------------------------------------------------------------------I'm thinking of a different lesson.. don't generalize.I think that's an apt analogy.  I'm a programmer, and I'd have quit a long time ago if coding was something I disliked.  Even so, the reason I continue in my work is the elegant things I'm allowed to create with my craft, not because of the mundane things that must be done time and time again.

edit: Thinking about it a little more, I'd say what I like about Programming is the I like Making things, I like Creating. I like working with Code, but it would be a mistake to say that the means are the ends.  I doubt any Carpenter who enjoys his work would be putting nails into wood just to do it, without something to build.  I could be wrong of course, but I think most of us like working with code because of what it lets us do, not just because we're typing instructions to a machine.This article triggered me to donate to the EFF, something I've been meaning to do for a while.  You should too.  

While you're at it, donate to the ACLU as well.Nice link.. I don't know why they bother with such weak protection though.[deleted]&gt; At Lotus, the source code for Notes include many lines of code removed with “`#ifdef LATER`”, under the (correct) assumption that there was no preprocessor symbol called `LATER`. This is a very weak form of documentation; it indicates that the code isn't ready to be compiled yet, but that it will be later. But when? A running joke among the developers was that we should define `LATER` and see what happened!

I love it. `:-)`If I could choose Django's priorities, that branch would be #1. What they're actually doing now isn't bad or a waste of time, but I think it's less important. 

Obviously, you can't learn _every_ framework, but you should try out a few of the ones you can play with and get a sense of in three or four days. That's an option I wish I had ten years ago.Why that? That's like saying they only want a single language in their lineup. I'm more impressed with Catalyst than anything else, RoR included.

When the boss first said that's what we were using, in the days before my hiring was official, I'd never heard of Catalyst. I thought it was a bad move the first couple weeks. But it clearly was a good decision on his part. I recommend it to anyone, whether you're building a very simple and small webapp, right on up to the largest.No matter what option you go with, keep the option of switching open as long as possible. If you design with that constraint in mind the decision seems less intimidating.and maybe the FSF or OLPCIs there a good reason why 0/0 = NaN? I would think that if you have nothing, divided by *anything*, leaves you still with nothing.I wonder if CD/DVD labeling printers do the same ;)
I agree with this article.  Also, I'd like to point out that Google does not *just* have more machines.  You have heard of some of their tools (GFS, MapReduce, and Bigtable).  If you guessed that this is not the limit of their infrastructure and that there are many more tools available to increase productivity in a massively parallel environment, you would be right.Shame on you! Your username comes from the same text as that quote-- I believe the people-to-read-incidentally-machines-to-execute came originally from the preface to the first edition of the Structure and Interpretation of Computer Programs, in the exercises of which there is also a character named Ben Bitdiddle. 

http://mitpress.mit.edu/sicp/full-text/book/book-Z-H-7.html

So the quote is due to Abelson and Sussman.Reddit could do with a forum, really. Or a 'forum' subreddit. (Far from bitching, I actually quite enjoy 'Ask Reddit's, and it would be nice to have a place for them to live.)Why stop in the middle ? Go full length. Go lisp!
It has a range of frameworks from very powerful component based like UCW to very simple ones like WebActions or Hunchentoot
&gt; If I could choose Django's priorities, that branch would be #1.

You know what's cool about open source? If you think something's important, you get to work on it! We'd love some help on that branch, so feel free to jump into django-dev and give us a hand.&gt; and Django is basically crap in my opinion.

Please elaborate; without specifics, this is just FUD.I have throughly botched it several times on [this comment](http://science.reddit.com/info/12rtl/comments/c12vvc)This is Scott Rosenberg, author of the book and target of Rentzsch's ire. I'm going to respond here because Rentzsch doesn't seem to offer comments on his own blog.

Here is the original quote from Constantine. I don't believe I've "butchered it" at all: 

"Unfortunately, most programmers like to program. Some of them would rather program than eat or bathe. Most of them would much rather cut code than chase documentation or search catalogs or try to figure out some other stupid programmer's idiotic work.... Other things being equal, programmers design and build from scratch rather than recycle." 

Constantine offered this as an explanation for why we haven't yet arrived at the golden age of "Lego-like" code reuse. I found it persuasive and presented it in "Dreaming in Code" as part of a discussion of Brad Cox's ideas on reuse. I then referred to it once more in Leonard's interview with me. But it's rather a minor side point in a 400-page book that covers many of the other points in Rentzsch's post, including Frederick Brooks' "Build one to throw it away." I do not believe, nor did I ever suggest, that programmers "capriciously want to rewrite code for the fun of it." Rather I suggested, based on my own observation and research over several years, that programmers often have a bias toward "write it ourselves" versus "reuse someone else's code." I'm not going to try to defend positions that I never took, but I'm happy to defend those I did! 

Thanks, in any case, for the cool discussion. 

See the "help" link next to the comment button?  click it.
Don't worry, the printer probably registers itself automatically when it [uploads reports of your printing activity](http://groups.google.fr/group/comp.periphs.printers/browse_thread/thread/4426e7ac5b8bdda2/cbc2539e3c6d6594?q=%22the+program+transmits+the+printer+serial+number%22) or when Windows downloads new drivers for you.
It's really O'Reilly's loss.  The book will be published regardless, and will be just as useful regardless of the publisher.  The advantage of not going with O'Reilly is that the book will end up about $10 cheaper, which is always good.

(BTW, I'm the author, and the book will be out June-ish. :)Catalyst is pretty good in this respect.  Everything it does for you is obvious, but it speeds up your development anyway.  The disadvantage is that you'll have to spend a few hours or so getting sessions / users / templating / database / etc. set up just the way you like them.  But then after that, the framework will be working for you, and you'll just be writing code with a little bit of HTML in the template where necessary.  You'll know how everything works because everything works like it should -- and you get to be the one defining "should" :)

If you're looking to get started, take a look at the example applications on the Catalyst wiki, or perhaps try the tutorial:

http://search.cpan.org/~jrockway/Task-Catalyst-Tutorial-0.03/

Please join the IRC channel (#catalyst on irc.perl.org) if you have any questions.Maybe you simply lose some benefits if you don't implement that feature.

By the way, when was the last time a government investigated possible price-fixing in the printer toner/ink market ?
boxily is really a great app.Thanks![deleted][removed][removed]Well, Unicode doesn't have anything to do with bytes at all. The fact that certain _Unicode transformation formats_ (remember what "UTF" stands for, kids!) result in things expressible in a certain maximum number of bytes says nothing of Unicode itself, which is entirely independent of such considerations.The argument generally goes like this: pre-emptive multitasking will use a bit more CPU for every context switch than a cooperative approach, especially on an ARM CPU.  Modern phone hardware is fast enough to deal with this, but the more CPU cycles you use, the faster the batteries drain.

Personally, I think slightly reduced battery life would be worth it to have a phone platform that isn't utterly hideous to program for (and don't see how "the necessary API to make it painless" is even possible) but the market probably doesn't agree.[removed]"An encoding of Unicode" != "Unicode". Unicode is Unicode, and not any particular arrangement of bytes used to represent Unicode strings -- to think otherwise is to confuse the signifier with the thing signified and miss the point.

UTF-8 is an arrangement of bytes which can, given certain conditions, be taken to represent Unicode. Same goes for UTF-16 and other "Unicode transformation formats". However, none of these arrangements of bytes are, in themselves, Unicode; they are merely representations of Unicode which are convenient for computers to work with.Patriot.&gt; If I could choose Django's priorities, that branch would be #1. What they're actually doing now isn't bad or a waste of time, but I think it's less important.

And if I could choose Django's priorities I'd disagree ;)

ORM, to me, is not about providing a one-to-one mapping of every feature of SQL, because if I wanted every feature of SQL I'd use SQL. ORM is about something quite different, and trying to force correspondence between ORM and SQL is bound to end in pain and tears...I'm going to step in and call BS on the analogy here; Django and Rails, historically, both began development around the same time, under closed doors at the companies which eventually open-sourced them. As such, it's hard to call one a "clone" of the other.&gt; I used Pylons a bit and found it to be responsive

Please elaborate :)

&gt; have some kick-ass features like the web debugging

Please elaborate :)

&gt; supported all three of the major Python templating languages

Django includes a template language, but doesn't force it on you. Want to use another template system? Import it and call its methods. Simple as that.

&gt; supported SQLAlchemy which is the best SQL framework ever created

Ditto the above; Django includes an ORM, but doesn't force it on you. Want to use another ORM? Import it and call its methods. Simple as that.

&gt; and just was generally tastefully assembled

Subjective opinion :)

&gt; Django and TurboGears felt a lot more restrictive to me

Subjective opinion ;)

&gt; everything is connected very loosely and that definitely wasn't the case for Django or TurboGears

There are two approaches to writing a framework: one is to grab some components and write some glue to plug them together. The other is to write the components as well as the glue. The former approach was not, in the Python world, a possibility when Django began life.&gt; Django includes a template language, but doesn't force it on you. Want to use another template system? Import it and call its methods. Simple as that.

Its not as simple as that. I bought the marketing hype when I started using Django but anyone who has tried to use a different templating language know it isn't as 'simple as that'. You want to use a different templating language with Django and you'll need to rewrite render_to_reponse(), you'll probably have to reimplement tags/filters, and you'll also have to set Django up to properly give all its settings info to the templating system.

Now if we want to talk about something that really is as 'simple as that', Buffet is particularly nice and you can literally just change a line of code and get full functionality from any templating language that supports it. Buffet is used in TurboGears and Pylons and supports Mako, Kid, Genshi, Jinja (Django clone), and several other templates.

I would love it if Django would put Buffet into the framework because that really does support plug-and-play.

&gt; Ditto the above; Django includes an ORM, but doesn't force it on you. Want to use another ORM? Import it and call its methods. Simple as that.

Hasn't this been discussed before? You use another ORM and you lose the admin interface, generic views and pretty much every other feature which makes Django special.

&gt; Subjective opinion ;)

I don't think its an opinion at all. Pylons was built from the ground up to be customizable, Django was built from the ground up to fit a certain style of development.

&gt; The former approach was not, in the Python world, a possibility when Django began life.

This was true in the past, but why does Django continue it? newforms isn't something that was created several years ago; it was created in the last few months. Yet FormEncode existed well before newforms and it along with something like FormBuild more or less does exactly what newforms does. Why not work on improving existing libraries instead of redoing everything from scratch?&gt; ORM, to me, is not about providing a one-to-one mapping of every feature of SQL, because if I wanted every feature of SQL I'd use SQL. 

You are right and SQLAlchemy doesn't do that. SQLAlchemy cleanly separates what is database and what is objects, so you aren't making an overly simplistic and restrictive approach such as mapping classes directly to tables.

&gt; ORM is about something quite different, and trying to force correspondence between ORM and SQL is bound to end in pain and tears...

But that's what every ORM inevitably turns into. Programmers use the ORM and reach a point to where they can't use the ORM anymore. Then at that point they have to drop down into SQL which a lot of programmers don't know well and they know even less about how to create efficient portable SQL. Then everyone gets frustrated because they feel like the ORM is weak.

That's why the SQLA approach is so nice. You still get SQL wrapped up in a programmer-friendly Pythonic syntax but it is still very flexible and isn't going to hit places that it can't reach.That's I think important context why you have such disagreements with others, who may use higher-level tools more often than you do. Where representations and meanings are more separated.

For example, Lisp makes a big difference between lists, and the representation of lists. A list might be represented with textual parentheses and stuff, or as box-and-pointer diagrams, etc. Whatever the case is, data/code are explicitly parsed from (or serialized to) such a representation.

Queinnec's _Lisp in Small Pieces_ discusses this sort of thing explicitly in the (first?) chapter.
I've done development in a few different languages and frameworks and they've all be enjoyable to do development in.  

Ruby on Rails, Django - I think they're really good development frameworks and quite fun to use but I don't see very many companies deploying web applications on these. This website has a huge over abundance of ROR lovers that will disagree but look at job boards and there aren't too many Ruby on Rails positions compared to larger frameworks. So if you're going to spend your time learning something new, it seems to make sense to go with an established Framework.

Java or ASP.NET - I haven't really used Java for web development but I see a lot of job postings for it or similar type work and knowing the Java libraries (the language is really simple) is useful for a ton of job openings. It's also very similar to ASP.NET but ASP.NET is more popular I think for web development. 

I have done development in ASP.NET professionally and I really like it. Everything you listed for Database stuff, logins and session management are almost completely drag and drop simple in the GUI WYSIWYG designer although once you learn what's going on, you won't be using the GUI. The AJAX stuff for ASP.NET generally just involves surrounding whatever section of the page you want to be AJAX-ified with a single ASP tag and now you've got AJAX features. There are of course other AJAX features and most of those can be accomplished in a couple lines without writing JavaScript. 

You will need to use Visual Studio though but you can get the free Visual Web Developer from Microsoft for building smaller websites. It's basically the professional version of Visual Studio 2005 but with just the web development tools.

Finally, the main reason to go with an established Framework with history is that any question you have will probably have 50 or more articles written about that topic on the internet so that you'll pretty much never get stuck. If it's a really weird bug or thought you had about development, then there will be 25 articles or more. My biggest complaint when I've done development in other frameworks (ROR, Django) is that Google doesn't have answers because I might actually be the first person asking the question someplace on the internet about it.&gt; Its not as simple as that. I bought the marketing hype when I started using Django but anyone who has tried to use a different templating language know it isn't as 'simple as that'. You want to use a different templating language with Django and you'll need to rewrite rendertoreponse(), you'll probably have to reimplement tags/filters, and you'll also have to set Django up to properly give all its settings info to the templating system.

So Django is evil for having the `render_to_response` shortcut work with its own templating system? ;)

If you use another template system, you use another template system; writing the equivalent of shortcuts/tags/filters for that system should be assumed. Doing otherwise could be a maintenance nightmare: Django would have to include switches and conditional logic for every Python templating system.

&gt; Hasn't this been discussed before? You use another ORM and you lose the admin interface, generic views and pretty much every other feature which makes Django special.

`python manage.py inspectdb`

Want the admin interface with a DB generated by another ORM? Run `inspectdb` and you've got it; people are running Rails and even PHP apps with the Django admin on top of them, so it certainly should be possible to run another Python ORM with the Django admin.

Ditto generic views.

&gt; I don't think its an opinion at all. Pylons was built from the ground up to be customizable, Django was built from the ground up to fit a certain style of development.

It's absolutely opinion. Pylons was, arguably, built from the ground up to fit Ben Bangert's style of web development. So it comes right back to opinion.

&gt; This was true in the past, but why does Django continue it? newforms isn't something that was created several years ago; it was created in the last few months. Yet FormEncode existed well before newforms and it along with something like FormBuild more or less does exactly what newforms does. Why not work on improving existing libraries instead of redoing everything from scratch?

Why do people assume that the folks behind Django have never looked at what's available? Why do people ignore the fact that Django has pulled in third-party components when those components were the best fit for what Django needed (see PyDispatcher, see simplejson, etc.)?

Perhaps Django's form system needs things that third-party systems can't do -- like, say, generating a form from a Django model class or instance, and providing automatic hooks to save a new or edited instance. Perhaps having our own library means less work in the end than adapting somebody else's. Perhaps there are a lot of things that go into these choices that get glossed over when you ask things the way you're asking them.

I've said it before and I'll say it again: before trying to paint us with the NIH brush, take the time to find out why certain choices were made -- they're not always right, but they are almost always informed choices, not simply "screw everybody else, we're doing our own thing" choices.So THIS is why my yellow ink runs out so much faster! ;)That's terrible! And I'd be surprised if Lexmark was the only company that did this.I don't understand why people who comment on teh reddits hate their freedoms so much...Even if you aren't particularly bothered by the fact that anything you print could be traced back to you, think about the big picture: the fact that US government agencies can "encourage" the insertion of hidden "features" into the hardware you buy.

Think about what kinds of other wonderful surprises might be in store for you should you happen to displease the powers that be.

If you drive a recent model of car, its computer may already be [ready to testify against you](http://www.slate.com/id/2087207/) in court. Your cell phone stands ready to reveal your GPS coordinates to an official eye at all times; it could even overhear your conversations.

Traitorous machines (which rat out their owner's misbehavior, or worse) no longer arouse interest in most people, much less righteous anger. Here are some fun possibilities to think about:

Does your car come with an RF remote diagnostic system? Run for political office with a populist platform. What a shame your throttle threw open on the freeway, and the brakes oddly failed to engage.

Does your house have one of those new remote RF gas meter-reading devices? Have you reverse-engineered all of its wiring and functions? Didn't think so. Become a realistic progressive threat. What a shame the gas valve solenoid on your furnace mysteriously opened for two hours, with the ignition spark suppressed until the end.

Dismiss these thoughts as paranoid delusion and remain comfortable and unaware. The camel's nose has been let in, and the rest of the beast follows invariably. Enjoy the ride.Actually, I know someone who got a visit from the federal police after one of his employees used the office color laser printer/copier to print counterfeit money. It was actually a long time ago (the copier was one of the first commercially available models). At the time, he had to get a special license just to buy the copier. I didn't realize that they still printed the hidden serial numbers, though. It seem like a waste when you don't need to register the printer.via
http://ztrek.blogspot.com/2007_02_01_ztrek_archive.html#117062895280300702
Z Trek: Alan Zeichick's Weblog: 02/01/2007 - 02/28/2007Good. I'll get the boss to buy it, and then I can not read it and still ask you stupid questions in the channel.Scratch is a new programming language that lets you create your own interactive stories, games, music, and art. It has been developed at the MIT Media Lab’s Lifelong Kindergarten group.[deleted]Really, if Catalyst had a few webapp skeletons built up, something to get started with, it'd be even that much more useful. A bugzilla clone comes to mind. Or CMS... blog style, slashdot/reddit/k5 style, maybe phpbb style (though personally I tend to consider non-threaded discussions nearly worthless). Have the user auth abstracted out in such a way, that you can plug anything in you want. Suddenly you can add all those things to your website in a few minutes with a unified user auth.

Of course, the answer to all these things is "so go out and write the damn thing". I just haven't had time yet. (Though it seems someone's already stolen Enzyme as a name, so what in the hell would you call it?)The Docucolor isn't a cheap ass laser printer, it's a digital press. The model my company has is about 20 feet long, costs more than most cars and requires two separate computers to control it.[deleted]Dude, this is OOOOLD news.Many of us are FOR operator overloading.

But you will find plenty a coward who fears becoming obsolete and plenty a whacker who would use it for GUI creation.[removed]&gt; My conclusion is that you seem to think that it's sufficient just that it's all there

No, what I have continually expressed, with a reasonable amount of prejudice, is that *google is not a repository of documentation*, that such repositories actually exist.  I've also pointed out the absurdity of your search mechanism: in which you bounced off google and then failed -- instead of going to the obvious source, then potentially backtracking to google, and then failing.

&gt; What about my critique of browsing the erlang docs?

You also called yourself a fool.  When you did this, did your original complaint change to 'why am I a fool?'?

&gt; you seem to accept that your statement putting down other languages was ill-considered

I haven't the faintest idea what you mean by this statement.


&gt; There's only one of us being sarcastic and making ad hominems.

&gt; You're just not man enough to say so.

Please stop doing this -- it is very confusing.I do web development under Linux and only point #1 really seems true to me, and barely. One of my co-hackers uses OSX and he does just as well as me running the same code base locally. We have a small local configuration file that handles the differences in each environment.

My laptop is similar, but far from identical to my servers. It has things like X/GTK/KDE/Printer support among other things, stuff that would be inappropriate for a server.

Any problems with minor library version differences should be investigated, that stuff is likely to become a problem when you upgrade your server. Having a little Unix-diversity and making sure everything works seamlessly is healthy.

Developing in Linux is great for a lot of reasons, but I didn't find this to be a very compelling list.
"The Java Language Specification" listed twice, and *no* mention of the Java Tutorial?  Not a very well edited list!  Smells more like linkbait..._"Please stop doing this -- it is very confusing."_

Doing what? Is that sarcasm as well? It's getting hard for me to tell.

_"You also called yourself a fool."_

I can be sarcastic too. This is why I never use sarcasm online (though I need to work on my restraint when provoked) -- it's a terrible way to communicate in text where clues from tone are absent.

So to clarify: [this](http://programming.reddit.com/info/12grg/comments/c12p2d?context=5&amp;style=nested#c12p2d) was sarcasm (no I don't think you're God, sorry), but [this](http://programming.reddit.com/info/12grg/comments/c12rkq?context=5&amp;style=nested#c12rkq) was a genuine apology, trying to de-escalate the conversation.

_"I haven't the faintest idea what you mean.."_

[You](http://programming.reddit.com/info/12grg/comments/c12mmi?context=5&amp;style=nested#c12mmi): "Erlang has excellent documentation, especially in contrast to other languages.."

[Me](http://programming.reddit.com/info/12grg/comments/c12uyz?context=5&amp;style=nested#c12uyz):
"In what way are other mainstream languages deficient in documentation?"

Considering that you've not given up a chance yet to badmouth me, I assumed the lack of response meant you didn't have one.

I'm not sure why I haven't given up on you as a troll yet. Maybe you're just very good.

\--------------------------------------------------------------------------_"I've also pointed out the absurdity of your search mechanism: in which you bounced off google and then failed -- instead of going to the obvious source, then potentially backtracking to google, and then failing."_

Thanks for toning it down a bit. I still disagree, but at least we can talk about it.

I've clarified it a few times already but perhaps it's not entirely clear: I did try browsing as well even if I didn't mention it in the initial post, and I have [pointed out](http://programming.reddit.com/info/12grg/comments/c12uyz?context=5&amp;style=nested#c12uyz) how that failed as well because the layout and organization of the documentation is unintuitive. You have not responded to the criticisms.

I also stand by my primary point: if none of the standard keywords on google get me to the erlang documentation page that is a problem. You have [suggested](http://programming.reddit.com/info/12grg/comments/c12vhy?context=5&amp;style=nested#c12vhy) that other languages have the same problem, and perhaps that other keywords on erlang will work. Neither claim has been demonstrated.

You're right, all this clarification is getting tiring. That's what flames do, they add noise.

\--------------------------------------------------------------------------1. go through the 20min Tutorials of Rails, Django and Pylons
2. choose one of them
3. stick with it and don't look back

They are all good frameworks and constantly reviewing them all won't give you more fun or productivity.Me too. Some weekends I download the latest Linux kernel and then spend two happy days deleting lines. Very indulgent, but sometimes you just have to pamper yourself.[removed]How do we know that it has all of Darwin? The reason I'm not confident is because it's being used on a non-intel platform.[removed]Nice and all but he'd probably be better served with the Data.Set standard library :)I started out on Microsoft .ASP which was similar to PHP so I know where you're coming from.

I've just made a prerelease of a framework I've been working on at datamagi.org. If you have time check it out, and I'd appreciate any feedback if bits make no sense to you as a newcomer to the ideas. But if I were starting a project from scratch and needed it to Just Work I'd use Tapestry and Cayenne on Java. I'm not an emormous fan of Java, but both those frameworks are mature and as far as I know they're the most powerful tools in their field - Tapestry for templating and Cayenne for ORM. SqlAlchemy looks to be the most promising upcoming ORM but it's not yet up to a 1.0 release.

You'll hear lots of Ruby on Rails suggestions I imagine. I think Ruby on Rails is the extreme of what's possible for a web developer without really challenging the way they think. For that reason that (or one of its contemporaries) are probably the most powerful choice that you can *easily* pick up. But apps developed with RoR will tend to be unmaintainable, and the ORM system underneath it is primitive and will drive you into nasty compromises.
Do you have what it takes to beat SHA-1?What the heck, reporting news from October 2005. Get with it! Reddit and Digg need some way to penalize people who report ancient info as news!!!

I found the following on Google in a few seconds...look like you were too lazy to search Google!

http://www.washingtonpost.com/wp-dyn/content/article/2005/10/18/AR2005101801663.html

http://news.com.com/2061-10789_3-5899905.html

http://www.afterdawn.com/news/archive/6960.cfm


[removed]640 cores ought to be enough for everybody...It's a [pretty common phrase](http://www.google.co.za/search?q=%22boil%20the%20ocean%22).downrated because the word "security" was left out of the title, making it ambiguous and misleading[removed]improving bash: zsh !Visual web developer isn't built at all for teams. It's built for single programmers building database driven websites with some AJAX features. For what he's looking for though, he won't need the team features for it so Visual Web Developer should be fine for him.Silliness.  Talks about the GPL/copy-left hegemony and the greater open source community as if they were the same thing, and adds nothing worthwhile to the [original article](http://programming.reddit.com/info/12wif/comments).[removed]its the name of the article about hashing algorithms, ok picky.Cryptographers get ready: the race is on to find the next gold-standard security algorithm...Summary: "I want good open-source code for a commercial, proprietary product." Open source and the www will improve developer productivity -- provided you're willing to give back and not live in your own castle.

Or: "Why large companies suck." If buying a simple piece of software takes a lawyer and a committee, it's time to get out.&gt; You have suggested that other languages have the same problem,

No, I haven't suggested that.

&gt; I also stand by my primary point:

That's all there is to say about that, then.

&gt; You have not responded to the criticisms.

Try searching for such a response around the area of my alleged suggestion.

&gt; I've clarified it a few times already but perhaps it's not entirely clear: I did try browsing as well

No, you haven't clarified that until now.

&gt; You're right, all this clarification is getting tiring.

I haven't suggested this, either.

&gt; That's what flames do, they add noise.

Please review this thread after a year or so -- it'll still be here, unless the reddit people manage somehow to destroy it with a clusmy markdown update.  My nagging impression has been that you, after stiffening in response to my initial comment, have resentfully refused to put much thought into anything I've said -- few other options seem to explain your repetition or, here, your sudden awareness that I might have taken your described sequence of events as how they came about for you.  Do you suppose that your own resentful behavior, your own self-contradictory parting shots, could constitute a far more dispruptive pattern of interaction than the one you continually accuse me of?Mit $foo (foo-magazin.de) erscheint ab sofort die erste deutschsprachige Fachzeitschrift rund um Perl.[removed]&gt; If I could choose Django's priorities, that branch would be #1. What they're actually doing now isn't bad or a waste of time, but I think it's less important.

Nothing stops you from working on it ;)

The point is that the main Django devs think there are other important things to fix in order to reach milestone 1.0, the current ORM works even if it's not the best thing since sliced bread and SQLAlchemy integration is a fairly big change (mainly to SQLA as it needs an adapter layer in order to have all the cool features of Django e.g. auto-admin when using SQLA) which is why it's a branch.

Just as magic_removal was a branch even though the django devs did consider it fundamentally important.I've given up on this entirely. With the slow adoption of IE7, I will still have to support IE6 until 2008, at a guess.

This along with the fact that IE7 is only barely more tolerable (they fixed png, and margins/padding behave a bit more reasonably? Big fucking deal) means that it's simply a dead end. We will not be able to put the features into our web app that we should be able to already, and we never will. 

Am I really supposed to try to re-write an important part of our site in VML once I've got the SVG working? Am I supposed to just say fuck it, and do it in java or flash?

Why am I spending all this bandwidth effort on images for rounded corners and drop shadows, when other browsers either do this in CSS?

No, but I can't just not support IE. I'd be shitcanned. We're better than most sites, 40% of our users use it rather than 80%. But still. However, I have a solution.

I've managed to (partially) package mozilla into an ActiveX cab file. The mozilla team did all the hard work, I've not had to recompile it. (Build it on windows, with VS7? Screw that!) But I have had a hell of a time figuring out how to get it into something that will install over the web. And it's beautiful. It needs some shim javascript, to integrate the titlebar, history and back/forward buttons with the moz object, but it works. 

Too bad I don't work for google. Imagine if they put this out... within a matter of weeks, the control would be there for all websites to use, and since it's cached, there'd be no penalty for downloading the hefty cab file.

Anyone that's interested, either in helping, or just in mooching off of me once I have it perfected, let me know.Got to support you there, and anyone having tried both Django and Rails will realize extremely fast that they don't copied each other in any significant way: they've got different "flows", different structures, different principles, different driving philosophies, ...

The only "common ground" is that both offer command-line tools (a shell, an appserver)... pretty weak.[deleted]Darwin is already on PowerPC and Intel. Those platforms are basically the opposite in design. If they can port Darwin to both of those, they can recompile it for ARM. 

Of course, it all depends on what you mean by "Darwin." If you mean the XNU kernel, then all of that will be in there. Half a kernel doesn't get you very far. Now, obviously, the iPhone won't be bundled with the .kext files for using a mouse or mounting a DVD drive or whatever, but it's still going to have "all" of the relevant parts of Darwin that apply to the operation of the phone, even if doesn't have a full Unix subsystem like the command line tools or whatever.Just to see why the article is biased - Symbian is criticized for having implementations where only signed applications are allowed - but somehow the author missed the fact that right now the official Apple line is that on the iPhone only Apple programs will run - so how is this better ???_"Please review this thread after a year or so.."_

Ditto. Again, thanks for being civil._"Do you suppose that your own resentful behavior.. could constitute a far more dispruptive pattern of interaction than the one you continually accuse me of?"_

Hmm, so me saying your original post wasn't helpful is comparable to you calling me all those names?

It's a genuine question.Thanks for this.  I definitely needed to read yet another self-congratulatory description of the stereotypical programmer personality type.The sad thing is that if you ask experienced Java developers (read the ones that spend all their time messing with frameworks) what is the result of :
&gt;  public void foo(Dog d) {

&gt;    d = new Dog("Fifi");

&gt;  }
&gt;
&gt;  Dog aDog = new Dog("Max");

&gt;  foo(aDog);

&gt;  System.out.println(aDog);

Half of them is going to reply "Fifi".

I'm not making this up. Someone asked that question when doing interviews for a senior java developer position and that was what he got. 
Unfortunately i can't find the link of the blog of that person. A cookie for the one who finds it.Ah, I think I finally understood this one:

_"You also called yourself a fool. When you did this, did your original complaint change to 'why am I a fool?'?"_

You're saying I can't go back and make retroactive clarifications or claims. That right?

Can you make a comment on the critique on browsing in isolation?[deleted]"experienced" "Java" "developer"
is the oxymoron function commutative?

I have a few Java questions at http://jqa.tmorris.net/triviaDoes filter return a new list containing only values for which the predicate returns *false or true*? Does the function you give reduce need to be *associative*? Are the definitions "standard"?You can also easily reason that it's 1 (via x/x), or infinity (via x/0), or pretty much anything else.  Mathematically speaking, it's an [indeterminate form](http://en.wikipedia.org/wiki/Indeterminate_form).May be that half only self-claimed to be experienced?

Or may be your "experienced" means number of year instead of real knowledge.

Some people manage to stay in programming jobs for ten years, generating thounsands WTF, but that doesn't mean they are actually experienced.&gt; Developing in Linux is great for a lot of reasons, but I didn't find this to be a very compelling list.

Me neither. Btw, [IEs4Linux](http://www.tatanka.com.br/ies4linux/page/Main_Page)  is a great IE-on-Wine-Installer that actually works. Saved me a lot of time playing with VMWare just for testing my web pages on IE...I would have called InstantCRUD "GreyGoo", but you're free to use the name if you like :)If only you could get paper sample that is printed with printer that is located inside the White House. Then you could replicate dead threat letter to the president with same signature. Secret Service would go ape trying to find out the  insider.Thanks. I can see that the expression doesn't really make sense.I've written for Symbian Series 60 (a while ago though), and can only agree with some of his criticisms. The strings (descriptors) and array stuff were ugly and confusing.

The Active Objects (cooperative multi-tasking) I didn't find so bad -- it's a task loop. Of course, using it for real multi-tasking (rather than just your main loop) might be a pain.

I really didn't find the Symbian version of exceptions so bad. I don't think they are any more dangerous than normal C++ exceptions -- Symbian style just makes you recognize the dangers of normal exceptions. For example, don't throw exceptions from constructors. Correct code with exceptions is tricky (as Herb Sutter's books Exceptional C++ illustrate), and on embedded systems with limited memory, important to get right.

All in all, I found Symbian/S60 had a steep learning curve, but definitely did some things well. It also had a pretty slick dev/distribution tool set, in my opinion. It is frustrating and sad that the new version will only run signed apps, as that pretty much kills the hobbyist niche.Never underestimate the bandwidth of a hard-drive sent via the postal system. But think of the latency![deleted]Damn, it's great to see that the Jython project is getting revived.You obviously haven't even looked at Django.

Want a real Rails clone in Python? That's Pylons. Though it only started as a clone, then diverged, it is more similar to Rails than Django is, and has ever been.So is C. And I'll bet you that a big chunk of C people don't know it, either.

It would have been interesting to come up with stats for the two ;-)And Windows for Smartphones is the same re. signing and privileges, so it's kind of standard practice.Why would you be writing code like that anyway?Thanks man!  xDDid anyone notice that the author took liberties in redefining what pass-by-value means in such a way to match Java?

OK, heres the question: author says that any parameters passed on a function is first copied.  These copies are what are sent to the actual function.  So are these copies passed by reference or by value?  A quick skim of the Java source code will show you that they are passed by reference.  And since these are, truthfully, the only variables ever "passed" we can easily say that Java is always pass by reference.  You can try to redefine these age old terms all you want but us old C dogs will never forget.Best way is to run Linux development in a virtual environment. You do not need the hassle of nuking your Windows box and application.

I run all my development inside vmware on windows, but have positive experience with cygwin as well.NSFW!!1!Does anyone know how this is implemented?  Could a third-party driver circumvent this, or did they put it on a fucking chip in the printer or something?This is close to one of my favorite computer science quotes:

Never underestimate the bandwidth of a station wagon full of tapes hurtling down the highway. - Andrew Tanenbaum.They can have only (restricted, modified) kernel and (an also restricted, chopped, modified, whatever) System.lib. It's still Darwin, but a twisted one.

EDIT: I don't know if it's System.lib or whatever the "libc" or Darwin right now, because I'm away from a Mac.Yeah, you're right. Old news. Nobody cares. Only 629 points and dozens of comments.[deleted]Glad to hear that Catalyst is doing well. Does it require mod_perl?Yes indeed, shame on me, I love that book and own both editions in hardcover. I've been using bitdiddle in newgroups and so forth since the internet was opened to the world at large.

I don't have it in front of me but thought perhaps Perlis or Knuth originally said it. I tend to attribute almost everything to Knuth, for obvious reasons. In fact I recently learned it was Hoare who said that "premature optimization is the root of all evil".

Regards,

BenYep, sorry ... is there a tag I should have used?&gt; (define (curry func . curry-args)
  (lambda args
    (apply func (append curry-args args))))
&gt; ((curry + 1) 2)
3
&gt; ((curry + 1 2) 3)
6
&gt; ((curry + 1 2) 3 4)
10I tried it, but the font was slightly harder to read than Adobe's. I didn't see any way to change it. But, yeah, much quicker.The Log framework provides an abstracted logging system.But then he wouldn't be able to use those nifty list comprehensions.Well,
I guess, unfortunately, this is life. Open source without indemnity is not useful. Thats the problem. It doesn't matter what the license is. Even if I write some stuff and give it to you with all rights, how do you know I didn't steal/copy from code from someone? You don't. Thats the problem, hence the commitees, lawyers etc. Yes, it does suck but what can you do?

The code bank that provides clean room algorithms with indemnity in return for a pay per use seems like a fair way to do it. The only way to give back is cash unfortunately to pay for the legal work. Contributing more source code of unverified source doesn't help.I don't have this opinion myself but I am interested in what Redditdom thinks.Well, yeah, I guess there are several methods to circumvent this. But you don't need to understand the pattern to apply them, just know that it's there."1. Being a great problem solver."

Oh no you don't. You get that problem of my Coding Plate right now. Take it back. Take your useless ACM contest problem back. You work on it if it's so interesting.

"2. Being driven and lazy at the same time."

Read: "Make it look effortless." This is not a spectator sport. I am not going to pretend that it doesn't take effort.

"3. Ability to understand other people’s code"

Nuh-uh. Ability to code so that other people understand. That's the Golden Rule of Programming, not the other way around.

"7. Having good communications skills"

Towards who? Other good programmers or to dumb-dumbs. Because seriously, that gumpher manager who got his job by being a foaf is so not getting a tutorial in cryptography from me. Forget that.

"8. Strong debating skills"

If I wanted to spend my time debating I would have become a politician.

"9. Extreme optimism 10. Extreme pessimism"

I suppose you want me to be a manager haircut handshake type who goes drinking every. single. night. until. 2. am. and then wake up and be a chipper programmer with no bloodshot eyes who'll delve into hardcore programming too. Choose what you want already.Or as a macro:

    (define-syntax curry
      (syntax-rules ()
        ((_ fun x x1 ...)
         (lambda args
           (apply fun (append (list x x1 ...) args))))))
what is meant by explicit variable?give exampleTry Erlang with Yaws. Since Yaws is one of the few webservers actually built with dynamic content in mind from the start it offers many of the things a webframework would do in other languages built in.[removed]Item 6 seems like a filler.  It might as well have said "understands at least algebra".  We could have put in there "eats food on a fairly consistent basis" and it would have meant as much.  Hey, you don't need 10 reasons if you don't have 10 good reasons.Has anyone tried Grails? It looks promising, the elegance of Groovy and Rails with the libraries of Java but I haven't seen much news about it yet. We're using Java + Spring MVC + Freemarker template engine at this moment.The issue is how much more server power you need to go with Rails over higher performance platforms and whether this matters.  The article claims less than 2x, and no, any increased server cost is dwarfed by the benefits.

A reasonable claim, but you can't blame people for keep pointing out the potential performance problem.
&gt;Select a section of code in your editor, hit the backspace key, and be done with it.

I often use the "Delete" key instead of the "Backspace" key.It's pretty amusing to see someone complain about sh syntax and then suggest a perl shell as alternative :-).

The problem with sh, bash, ... is not syntax.  And it's not missing control structures.  The problem are the data structures, but this is a fundamental Unix problem.  Bash and Zsh do have arrays, but AFAIK you can't
pass or return arrays arguments, only strings and integers.You bet.  I just donated, too.  Keep up the good work, EFF.&gt; 1)  The code is written, initially executed, and tested on the same platform that will ultimately power the application.

This gives me a bad feeling.  He seems to be saying that he's actually testing his code on his desktop machine.  That's a big no-no.  You test on a server that's as close to the production server's configuration as possible.
In C, it doesn't make much sense to talk about "by-value" and "by-ref", since you're regularly passing around pointers which are generally the mechanism for implementing "by-ref".

... Which is why you see args like char\*\*\*.  :-/

EDIT:  fixed asterisksThe difference is that in C, you explicitly say you're working with pointers. Java people are told everything is pass-by-reference and they simply believe it. They seem to think thinking about pointers is painful....*yet*.Like Gotebe said, C is also purely pass-by-value. You can use pointers to simulate pass-by-reference, but the function arguments are copies of memory addresses, not aliases of the actual variables.

In other words, you have to convert the reference to a value to pass it, with everything that this implies.[removed][deleted]True, and funny because in Java nearly every variable *is* a pointer.Best way to run Windows is in a virtual environment. Then when Windows decides to meltdown you can just nuke the virtual machine!
[deleted][deleted]I _am_ working on Django. I've got a ways to go before I can just start sending in the for switching to SQLAlchemy, as it is a big deal, and I've got some other things that I'll try to get by the dev list first. I've already got the getter/setter patch up there, and I'm probably going to try to fix up my syndication view that correctly handles E-Tags and expiration dates, but it unfortunately requires some extra methods on the syndication framework.It would be even cheaper to reconsider reasons why they transfer so much data from one place to another. It doesn't make much sense to me.I'd agree, except real-world performance kicks you square in the nuts if your ORM is too weak. I chose that metaphor carefully; it's easy to end up with query flurries (as I call them) that can get into the thousands or more queries to do relatively simple operations.

Even simple operations are hard with the Django ORM, like ordering one class by a value on a related class, which happens all the time. (Like, say, ordering a "story" on the date the "text" of the story was last edited.)This reminds me of the days when people would run around yelling that Common Lisp was pass-by-reference because you could mutate objects within function calls.

The distinction, of course, is between mutation of objects and mutation of *variable bindings*.  The latter can only happen within the variable scope.

I think C people get confused because they think that every value must be one or several machine word(s).  Those kinds of values don't have "identity."  Object-Oriented Programming is based around the notion of "object identity" (== in Java) which must be preserved through function calls and returns.  It is the central unifying notion of all object systems with mutable state.

It is important to separate the concept of "variable binding" from "value."  That's the root of the confusion.  Once you understand that, then you see that pass-by-reference is supplying the binding of some variable as the formal parameter, not the value.
It ships with IBM's WebSphere Application Server as the default administrative scripting language.Take off your spell-check-goggles and read the text, there are some good points there. Especially the bits about learning and problem solving.Agreed, useless for tracking counterfeit money unless you print out money that is 8.5x11 inches large."11. A tendency towards writing Top X lists."...&lt;irony&gt; blah blah blah &lt;/irony&gt;(Backslash-escape them.)And when they came for the Linux users,

I did not speak out;

I was using Windows.&gt; You're saying I can't go back and make retroactive clarifications or claims. That right?

No, I show that you do not in a discussion rewrite what other people
perceive as your 'my complaint' simply by making an additional point.

&gt; Can you make a comment on the critique on browsing in isolation?

You critique it on approachability -- and on those terms, with some
limitations (approachability depends on *where you are at the point of
approach*), you're probably quite correct.  I certainly think that you
can accurately transcribe approachability concerns that you have, with
almost anything.  However, *approachability* is not *usability*, and
the Erlang docs are very usable.  Except in the narrow sense if 'I
tried to locate the standard library of Erlang without any history
with that language or its documentation and I found the terminology
and layout a source of confusion', I don't think your critique is very
interesting -- an approachability issue only takes a little bit of
up-front effort; a usability issue is eternal.

I also regarded your approachability concerns as a side-issue, until
you said that you'd attempted to read these docs before searching on
google.

&gt; "You also called yourself a fool."   I can be sarcastic too.

But you *did* call yourself a fool, in the odd manner of pretending
that I had already suggested that you were one, and then admitting
that this suggestion was true.  That you object to the quoted sentence
makes me think that you were not very sincere in that admission.

You'd probably call such perceived insincerity in other people
'sarcasm' (a word that has more connotations for me -- in particular,
I connect it with puerile sneering, with teenagers who insult you to
your face and then pretend that you -- omigosh! -- simply cannot
understand their higher-echelon mastery of language), but I think you
were mainly using a common... I'll say 'rhetorical device'.

Of course, it is also a cliche, and as a hacker I feel entitled to
take things literally if it seems amusing to do so :-) I didn't mean any
of that as sarcasm.

&gt; but this was a genuine apology,

Aye, I took it that way.  I also took it as literally true, and I hope
you saw some change in my tone after I stopped classifying your
earlier message as I had described.

&gt; Me: "In what way are other mainstream languages deficient in documentation?"

Erlang isn't a mainstream language, but I regard its online
documentation as easily comparable with Perl's in terms of its broad
scope (it documents the interface of a great number of modules, and
then also provides user-guides for whole applications, and then also
provides a Reference Manual for the language itself, an Efficiency
Guide, high-level OTP architectural tutorials and guides -- and also
per-application changelogs) and in that I see it as generally well-
and usefully- written.

For non-mainstream languages, I describe Haskell's documentation [in
this comment](http://programming.reddit.com/info/l434/comments/cl4ie).

For less-mainstream languages, Ruby *used* to have very good core
documentation (about as good as Mercury's current documentation --
which is to say that it was small in scope, did not answer everything
and particularly not implications of very odd things, but that what it
covered, it covered very well), written by matz.  Post-pickaxe, it has
as its core documentation (in English)... the pickaxe book.  I've a
lot of contempt for this book, but at the least we can say that Ruby's
core documentation now contains small errors, and that it seems
dubious that now-Ruby's javadoc-style cheatsheet-reaching-upwards
stuff properly qualifies as documentation.

Mainstream languages far *very badly* in terms of *online
documentation*.  You can actually learn Erlang and Perl from their
respective documentation (I would suggest that you grab the PDF of
Joe's book), and still use either as a reference, years later.  For C,
every unix machine has a capable reference on many APIs, but no
language documentation whatsover -- which is probably why many people
exhibit confusion about C's arrays and function pointers and such,
even after they've written a number of C programs.  For most of these
languages, you must not only buy a book to begin to learn the language
but also buy books to continue to learn the language, and finally to
still buy books to program with the language.

You could also say that Erlang's 'OTP HTML docs' are a comprehensive
one-stop source for any kind of information about the language, its
architecture, its APIs, its history, and how you can *use* all of
this.

I haven't focused on specific deficiencies about other languages -- I care
more about specific efficiencies of the documentation of languages as I
deal with them, and with this experience I sometimes sort them, or compare
them, or observe that different languages have different characters when
it comes to their documentation.  For the last, see my comments on Haskell's
academicpaper-rich environment.

&gt; Hmm, so me saying your original post wasn't helpful is comparable to you calling me all those names?

No, the comparison I suggest refers to a pattern of interaction that I describe you as having, and to a pattern of interaction that you accuse me of -- by which I don't mean name-calling, but how you have characterized this discussion and my part in it.

Such as:

&gt; Thanks for toning it down a bit.

&gt; I'm not sure why I haven't given up on you as a troll yet.

&gt; Alright, this is pointless. There's only one of us being sarcastic and making ad hominems.

-----

&gt; Doing what? Is that sarcasm as well? It's getting hard for me to tell.

That comment follows these quotes of yours:

&gt; There's only one of us being sarcastic and making ad hominems.

&gt; You're just not man enough to say so.

and the 'this' that I asked you to stop doing refers to a similar group of quotes at the end of an earlier message of mine:

&gt; Now that we've hopefully got the flame war settled,

&gt; The dictionary defines a whine as

&gt; Will you at least grant that I merit civility?

-- and all of these I later refer to as "your own self-contradictory
parting shots".  Unless you *meant* to observe that only you make ad
hominen attacks, it seems strange to end your message with one.
Likewise, it seems strange to hope that 'the flame war [has gotten]
settled' and then quote a dictionary definition at me, or ask me such
a rude question -- a question that itself presupposes that a flame war *isn't*
settled!
Gliffy is cool, but when your documents get too big there are problems.That quote was the first thing that crossed my mind when I read the headline.Sorry that was unclear -- I would never advocate only testing on the desktop, but I like to run my automated tests on my development machine since they're so quick and quickly catch most bugs.  I run these again in my pre-production environment, for the reasons you've stated.[deleted]People have put currency on the platen of color copiers, made a two-sided copy, and passed the result off as currency, sometimes successfully. But the kind of person who does that is usually going to be a crackhead. A sophisticated counterfeiter wouldn't do something so crude.Bah, in my day, we chipped data onto stone tablets and shipped them by ox-cart.So what if what you print has exactly these same dots all over the paper? It could be a nice workaround having CUPS (or whatever you use) inserting the dots just to anonymise the printout. Anybody knows if that would work?this is actually an interesting idea for global networked websites like google or akamai. of course beeing cheap isn't everything; imagine the human hassle of managing such a complex hdd ping-pong networkAgree, and add: it is very hard to refactor and maintain Zope. Most Zope code ends up scattered in a slew of "files", in various little languages, which rapidly become mutually outdated. In real world use, the ownership system is either overpowered or underpowered and in either case overcomplicated. The ability to grant clients access to edit sections of the tree is a hack, and perceived as such. No non-paranoid project needs such fine grained controls on internal coders, and no paranoid project would touch Zope.

Conclusion: it's useless. Avoid.I'm still in suspense about this one:

  public void foo(Dog d) {
    d.setName("Fifi");
  }

  Dog aDog = new Dog("Max");
  foo(aDog);

Does this also leave (the object referenced by) aDog unchanged?  That's what would seem (IMHO) to be consistent with the other foo's behavior.Damn, score 132 on the first try. I miss the music though :(Well, I've  represented[1] the integers as lists. So I should explain, perhaps, how this is related to the kinds of things Google does.

Google's parallel computation system is based on 'map' and 'reduce' (the later is better known in Haskell as 'foldr').  So on some conceptual level, your programs look like this:

    analyze xs = (foldr (+) 0 . map f . map g) xs

This code, unfortunately, needs to make three passes over 'xs'--once for each 'map', and once for the 'foldr'. You could make the code run much faster by transforming it into a single pass:

    analyze xs = foldr (\x s -&gt; f (g x) + s) 0 xs

Of course, you need to know which optimizations are legal. And in a real application, you'd work with trees (not lists), so the foldr can run in parallel.

So we need a theoretical framework[2]. As it turns out, 'map' and 'foldr' are both just catamorphisms from a certain categorical algebra. This framework will tell you what kind of optimizations you can get away with, and easily allow you to generalize the techniques to more complicated data structures.

And what does all this have to do with representing the integers as lists? Well, that's what the integers look like in this framework, the same framework that allows you to optimize MapReduce-style code. So silly exercises with integers are basically push-ups for your brain.

[1] http://www.randomhacks.net/articles/2007/02/02/divide-infinity-by-2
[2] http://citeseer.ist.psu.edu/grust99comprehending.htmlWell you are making a mighty generalization from an anecdote, even if the behavior remains consistent with someone's real-life experiences.See, I told you C people don't get it either, just look at the comments here ;-)

They don't get the **concept**, only the **implementation** of it (i.e. use pointers to "implement" pass-by-ref). How lame!That's a long-standing bug which has been held up by a need to do much more serious refactoring in the query system; I _think_ we're getting slowly back to where it can be looked at, and then it'll work again.

Meanwhile, there's `select_related` if you want to fetch a bunch of stuff in one big query (which ultimately depends on which DB you're using -- Postgres likes this, MySQL hates it), and for stuff that would be intensive in the ORM but simple in SQL, well, I write a little SQL and pass the results to `in_bulk` or an `id__in` lookup to get objects back.[deleted]Java passes references-to-objects by value. End of story.Thanks, that did clarify a lot and give me more to munch on.

1. I didn't use the word useability, did I? (Not rhetorical; I'm not sure.) Even now I'm not saying the Erlang docs aren't useable or approachable or anything so broad.

2. Sorry I was rude. I didn't even realize it. I don't get flamed often.

3. _"you do not in a discussion rewrite what other people perceive as your 'my complaint' simply by making an additional point."_

  This is one last thing I still do not understand. I wasn't saying that was your criticism, but that this was part of what was in my mind, and asking if it made sense. At the point where I asked "Is my complaint well-founded?" I still hadn't the *slightest* backing-down or softening of tone from you.

  There's a difference between "You didn't say it right," and "You are a malicious sulking teenager." All I have been looking for is for you to go from calling me names to criticizing an action. I'm trying not to make this sound like a new insult :)So where do we send the check?Bad title: I need to know *how many reasons*!I know of one large newspaper that only uses macs for layout. All the remaining desktops are Windows 2000!Is there any conclusive proof that this makes any difference?  Hopefully the words in the URL are contained in the page, which both users and search engines will benefit from.I'm really impressed with this editor, particular the auto-completion for Javascript of all languages.  I'm doing some Javascript OO according to some principles laid out by Douglas Crockford, and the editor actually provides correct autocomplete suggestions for "private" variables defined within functions using "var" as well as pseudo-static properties of function constructors.While this article seems to only apply to color laser printers, I'd be surprised if some ink-jet printers didn't do something similar.  Anyone know?I can't find the source, but I remember hearing that Google decided to hire Vint Cerf after he gave a lecture and pointed out that the company that transmits the highest amount of data every day is actually Netflix.

Closest thing I can find is [SNAP](http://www.notes.co.il/benbasat/10991.asp).Most interesting. If there was some way to state when giving a codebase, "This is all written by me, and you are not liable for using it," would that help open source adoption? This is independent of the license; if the code is GPL'd you'd still have to make your modifications public. But you can be certain you won't be sued for using somebody else's IP.

I can see how this needs more refining to prevent, for example, companies from sandboxing their liability behind a whole bunch of shell corporations.The latest draft of R6RSLuxury!ty.Could not be said better then that!
The lingo used _unfortunately_ is the one thrown by book writers since java 0.9, and this is the lingo that people use to date: "Java passes objects by reference". The lingo also says with regards to pointers: "Java does not have pointers". But one should wonder why one gets "NullPointerException".
The reason Symbian C++ doesnt use the standard exception mechanism is that when Symbian OS was being developed (back when it was called EPOC), the standard for exceptions hadnt actually been defined. And back in the day, to be entirely fair, EPOC was doing miraculous things on limited hardware (check out the Psion 5MX for more information).  

Of course this doesnt excuse the fact that they havent gone back and modernised the system, or for providing lousy SDKs and tool sets (they used to be really bad back in the day). People have been bitching about these things for years (internally and as 3rd parties), and Symbian hasnt changed.  

The Symbian Gods know best. Operators dont want a thriving ISV market for smartphones. And Nokia, it seems is beholden to them both. The iPhone is an interesting product announcement, but the real story will be if/how Apple can disrupt the smartphone sector.Do you have any job openings?"Postgres likes this, MySQL hates it" - interesting. At work,  we're looking at switching from MySQL to Postgres, and we do a lot of that sort of thing. We're hoping Postgres speeds up our usage.Does this mean C# is also pass-by-value?My friends all said "Never underestimate the bandwidth of a 747 full of floppy disks."

But we all hated Tanenbaum, and were a little younger...

-DerekI've worked with Zope for many years. And there are indeed many reasons to loath it. However the linked blog post is inane. It's extremely hard to believe the writer's claims he has worked with the framework for 3 years and this post is the best he could come up with. He's upset because there's no web forum? Good grief.

The ultimate recommendation of "PHP or .Net", with the only explanation being that they have larger communities, is rather telling of the author's mentality. 

(By his description of the difficulties "salvaging source code from a crashed installation" makes it sound like he's developing applications with Zope's "through the web" management interface... which will only cause those who know what it means to roll their eyes. A serious Zope developer will code their "products" via the O/S file system... code is not existing only in the ZODB in these cases. As much as I too dislike ZODB, it certainly is not the problem this blogger seems to think it is. Personally, my applications all use Postgresql for data storage, so i don't really use the ZODB that much and mostly ignore it.)

[deleted]I don't care about a IDE like Komodo, but [scribes](http://scribes.sourceforge.net/) looks interestingI understand that, but I still think it is way too much for a single line.

Consider the five matrix rotations involved. I suspect that even an experienced APL programmer would have trouble following all of them.C# (and VB) offer both pass-by-value and pass-by-reference. In C#, you need to use the "ref" or "out" keyword.

By reference is generally discouraged except when you really need two return values like the TryParse pattern.&gt; this maps neatly into dropWhile: dropWhile p = foldr (\el ac -&gt; if p el then ac else el:ac) []

This definition is not quite correct.  What you've defined here is filter.  Check out issue six of [The Monad.Reader](http://www.haskell.org/haskellwiki/The_Monad.Reader) for plenty of dropWhile via foldr examples.[deleted]Even if it was only intended for tracking counterfeiting I wouldn't support it. I reject it not because I support the counterfeiting of currency, but because the potential for abuse is left virtually unmitigated and its utility is only obvious when some number of bills are circulated together, and like any such measure in this vein, its utility only persists so long as it is secret while its utility for abuse against 'normal people' remains intact indefinitely.This guy is clueless. Only idiots develop zope code through the web. You develop it on the file system, where you can use your IDE and other command line tools. The ZODB can be annoying, but you can store your data in an RDBMS, and backing up the ZODB does work just fine and is easy to do. And I have never had a problem finding zope consultants. Not as many as PHP, tho, that is too be sure, but no kiddies trying to sell their skillz in the zope world either, unlike PHP.
Zope definitely isn't php and is only needed for a certain type of project, but this article is basically a clueless guy who is venting. Zope has its place, particularly where you need fine grained security control for multiple levels of contributors. Plone in particular is pretty nifty, and it is built on Zope.

I love mod_perl, but the memory usage can be nuts with a larger app. I also second the Template Toolkit hating. I try to avoid interface code due to its presence, though this might have more to do with the ugly code that was written on top of it.Yes, but it would still be better than [this](http://en.wikipedia.org/wiki/IP_over_Avian_Carriers).
Less latency, but nowhere near the bandwidth.  And there is also the clean-up factor that has to be considered.Exactly!  I do tons of web development, all of it on Debian testing (workstation, server, testbox), and boot several versions of Windows using [qemu](http://fabrice.bellard.free.fr/qemu/index.html) to test things in different combinations of web browser and operating system.  When Windows (any version) tanks (and it does), I just restore the image from a backup.It is the same way used in Common Lisp and Scheme.

    (defun foo (dog)
       (setf dog (make-instance 'dog :name "fifi")))
     
    &gt;&gt; (defvar d (make-instance 'dog :name "max"))
    &gt;&gt; (foo d)
    &gt;&gt; (format t (dog-name d))
    max
Right,
The problem isn't the license, it's the legal indemnity, thats the biggest issue preventing reuse.I am going into my sixth year as a Zope newbie. I am not being self-deprecating. The other folks on the Zope mailing list mention in a nice way that my Zope knowledge is that of a newbie. I have the following observations:

(1) The comment about restricting coders in a paranoid way is right on. The Python script object (the only way to code in python through the Web/ZMI)  lacks the ability to import Python modules. 

(2) Whether only idiots use the ZMI or not, it would be convenient for us idiots to be able to do so if we wanted to. The inability to import modules in our script objects means that we cannot. 

(3) The Zope mailing list is excellent. People respond to newbies. They do to me. groups.yahoo.com archives the Zope mailing list. The distinction between a forum and a mailing list is artificial. You can use the groups.yahoo.com interface to interact with  the Zope mailing list as if it were a forum.

(4) The numbers of postings are down by more than half on the Zope mailing list from several years ago. But for some of us "idiots" Zope has a nice little community around it and you can learn about programming in an enjoyable way using Zope and even its ZMI/web interface. 

(5) Plone which runs on top of Zope seems to be huge. If you talk about the utility of Zope then you need to at least mention Plone. In some ways Plone is to Zope as Ruby on Rails is to Ruby.[deleted]Call me when they support lisp or haskell.Isn't the entire idea of a framework that you program to it, not the language? If it doesn't feel like that, you're using a library, not a framework.Look! It says "monads!" You MUST click! *laugh*

Hope you all enjoy the article. Let me know what you think.Nothing here about Canon codes ...On the same vein, there is [Scratchpad](http://dborg.wordpress.com/scratchpad/) too, but the problem is always the same: IDEs and text editors can't be compared. They are different beasts for different mindsets.Thinking about pointers is *not* as painful in Java (and other languages like Python and Javascript) exactly because it only supports pass-by-value. You only have to understand one indirection: the variable contains a reference to an object. 

When introducing pass-by-reference, you get another level of indirection, because suddenly two different variable names might refer to the same memory cell which in turn contain a pointer to an object. You can be confused on two levels now! 
From the comments:

*One interesting thing happens as hard drive sizes increase, without any comparable increase in bandwidth to get to the disk: you have to treat them like sequential, tape-style devices.*

We're actually using only disk-to-disk backups now at work.  Got some inexpensive 500GB USB external drives, and we're using rsnapshot for the backups themselves.  Backups are rotated off-site inside a well-padded Pelican case.

Been working pretty well so far.From another of the comments:

*Perhaps the best scenario is to rent the HD from "SneakerNet provider" (delivery companies such as FedEx are in a very good position). Every time you send a box, they replace it with an empty one, ready for file copying &amp; the next delivery. The cost should be even cheaper. That way no HD travels empty. Waiting for delivery of an empty HD is like waiting for a huge download full of zeros!*

Heh.  You could have a external USB hard drive specifically designed for shipping.  It's got a rubber plug on the side which opens up to reveal lights, USB / Firewire connector, and power.  Just build in the AC adapter, don't want to fuss around with DC bricks.

Otherwise, you just slap a label on it, and give it to the shipper, no other packing needed.

Prolly want to go with encryption for this, obviously.Hang on. The costs that he describes are based on the assumption that your Premium DSL or T1 or whatever isn't being used for anything else.  If your choice is between using bandwidth you are already paying for and paying UPS, the numbers would come out differently.

Or am I missing something?[deleted]No, aDog would have a name field of "Fifi." The address held in the aDog variable would however be the same.Of course this only works in languages that have a notion of reference equality. When I get my copy of The Seasoned Schemer back I'll have to see what name they used for this algorithm.An implementation of this idea is present in a comment of the ANSI Common Lisp specification for the LIST-LENGTH function:

http://www.lispworks.com/documentation/HyperSpec/Body/f_list_l.htm[removed]That's funny.  I always assumed that there was overlap between his generics work and ML but he forked paths.

I wonder what he *thinks/would think* about Haskell?[Collection of Alex Stepanov's papers](http://www.stepanovpapers.com/)No, the preferred deployment is FastCGI but mod_perl is reasonable, CGI is impractical, and for lightweight deployments, you can even use the built in test server.  There are also a couple of other deployment options out there (like POE).I agree with this sentiment, however, the current computer Go programs are barely at the intermediate amateur level.

When playing most of them I get the sense that I'm playing against someone who is blinded by anger. It's incapable of apologising for mistakes (often it doesn't see them at all until their consequences have already played out, and it's wasted many stones) and can't see the big picture of the board at all. Small scale technical things it's sometimes not so bad at, though it will often attempt things which are just impossible, and you'd think that it would see that, given that it's quite good at making technical things happen when they are possible. The effect is bizarre. It's a mismatch in levels of ability at different parts of the game that you don't usually see in humans. It's sort of an exaggerated version of playing against someone who is otherwise an okay player, but is having a terrible day.I don't do PHP, but I just tried Cakephp and it worked really well. It is inspired by Rails._"But you did call yourself a fool, in the odd manner of pretending that I had already suggested that you were one."_

[You](http://programming.reddit.com/info/12grg/comments/c12o3s?context=5&amp;style=nested#c12o3s): "I think you're a fool for not finding Erlang's documentation.. how about idiotic?"

[My response](http://programming.reddit.com/info/12grg/comments/c12p2d?context=5&amp;style=nested#c12p2d): "You're right. I'm a fool.. and you're God standing in heavenly judgement."

Hope this clarifies that you had indeed suggested I was one, and that I was indeed being sarcastic, not admitting anything of the kind.the company I work for ships 1tb hard drives around the world.  Yes, it is much cheaper than using the net, right now.  That may change in the future with economies of scale.Ok, now that was pretty damned funny if you have any IT background at all. :-)

10 years of experience once versus 1 year of experience 10 times, and all that.Anyone have a cheat sheet/overview of the recent developments?I use the Stratos Framework. It's pretty intuitive and allows for quick development.Hell, I wonder whehter he's ever revisited this post in general.  o_O  He works at Adobe now, right?  Someone there must read reddit...As one of the comments notes, this is basically just [Floyd's cycle-finding algorithm](http://en.wikipedia.org/wiki/Floyd's_cycle-finding_algorithm).CodeIgniter is really good for a lightweight, very fast and well documented framework.[deleted]You know, I work in IT, and I completely didn't catch the OS discrepencies in Jurassic Park.

Curse you, Spielberg, for creating CGI dinosaurs to distract me!The real hack\[1\] is that you can get the fixed-point combinator simply by folding `const f` over an arbitrary infinite list.  While this is not really news (Peter Freyd observed it back in 1990), Bernie Pope's article takes an interesting and scenic route to it.

\[1\] in the very best sense[removed]I ran that in Emacs and it worked!A problem here seems to me to be confusion between assignment and update:

public void foo(Dog d) { d = new Dog("Fifi"); }

Dog aDog = new Dog("Max"); foo(aDog);

doesn't appear as though it would work (to someone, like me, who hasn't really worked with Java) because the 'd' being set to Fifi isn't the same 'd' that was mentioned in the arglist for 'foo'.

But perhaps not giving it a type means it's an update?The Plone mailing lists are very active and full of people helpful to newbies. Maybe hard to find as they're on gmane. Nowadays its probably quite unusual to use Zope without Plone so thats why zope groups are quieter.
 Yes, Zope is weird and unituitive and documentation is patchy. Learning the Zope/Plone platform is at times bewildering and maddening, but at the hardest times, if you ask yourself what other technology could do the same job with so little, there isn't a clear alternative. Zope/Plone should be considered comparable to J2EE in what you can do with them, but you write a lot less code. If you want something less powerful, but simple to get working, Joomla is better. Many people who criticise Zope/Plone were trying to do something for which it was a mismatch. The extra complexity of Zope/Plone is only worth it if the problem you're trying to solve is complex. If you want a content management system for a large complex organisation such as government, NGO, university, Plone/Zope is a good choice. If you just want a simple CMS for some club or personal site, its probably a bad choice unless you already have the skills.
  Zope needs its own box to run on. This makes it hard to find hosting providers. Can run on OS X, Linux, Windows though.
  Multiple inheritance makes Zope hard to debug. python frameworks can be harder to debug than C++ or Java in general though.
  This guy sounds like he didn't read the books, didn't explore very much, tried to use Zope for an inappropriate project, basically didn't try very hard then got fed up.
  I do work with Plone for a non-profit. You need some discipline. Use filesystem-based products under source control, instead of working in ZMI. Migration of Plone 2.0-&gt;2.1 was painful. Zope migration never has been an issue. Customisation needs some discipline. If you customise version x then that customisation may need tweaking in x+1. So keep customisations in a filesystem-based product and document it. These are basic developer skills though.
  So I say to everyone struggling with Zope/Plone, keep an open mind, think about what problem you're trying to solve, and have patience.Doesn't every currently-usable laptop really have "real-time video phone capabilities"?  

Hm.  Wikipedia says:
&gt; CU-SeeMe is an internet video-conferencing client written by Tim Dorcey of the Information Technology department at Cornell University[1]. It was first developed for the Macintosh in 1992 and later for the Windows platform in 1994. Originally it was video-only with audio added in 1994 for the Macintosh and 1995 for Windows.

So it looks like this capability predated the movies it's used in. 

Okay, okay, I'll stop.  :)Yeah, because the Secret Service is so concerned by yet another death threat to the president. I bet he gets hundreds a day.Very good to get to know the true horror that lies behind most "smartphone" development today. Also interesting to note that the "glorious land of 3d party apps" is not what it's made out to be by analysts and the responsible companies (see earlier articles).Also known as [accumulate](http://cppreference.com/cppalgorithm/accumulate.html) in the STL.I don't think any one is particularly happy about the "lock-in" of mobile vendors. But if that's the price of getting accepted by network operators then Apple has to play along as long, at least until WiFi is more or less abundant (or WiMax is out and works well).It's old news that it is old news.
http://programming.reddit.com/info/12ujm/comments/c12uuc2 b. Inversely small files take incredibly long to save and regardless if you're copying to a floppy or transmitting it over to a remote storage location it will be labeled as "Downloading"See Anton van Straaten's summary at Lambda the Ultimate:

http://lambda-the-ultimate.org/node/1995[deleted]Nice. *Deductive Mathematics* covers about 3/4 of an Introduction to Higher-Order Math course and, while Wikipedia may be all the craze these days, it's well-written and well-ordered. Wohlgemuth spends more time covering propositional and predicate logic than I personally think is necessary, but the only glaring omission I can find is the lack of material about (equivalence) relations and partitions.yield.....yield...when will Jython *YIELD* ????Bah! In my day, we did it in a way that was slightly more difficult than that which you described, but not so difficult that it can't be topped by another comment!In general, Postgres would rather get one complex query than a bunch of simple ones. And MySQL would rather get a bunch of simple queries than one complex query.

So on Postgres it's often to your advantage to use `select_related` and grab all the data up-front in one query, while on MySQL it's usually advantageous _not_ to use it, and let related objects be fetched in a lot of small queries.[deleted]&gt; After working with Zope as an application server and development platform for almost 3 years I have come to the conclusion why Zope is the worst ever open source application server ever to be released.

&gt; Zope is not compatible with newer or older versions of itself. Thus upgrading can trash everything and should be avoided at all costs.

&gt; All source code is stored in a specially created Zope database and inaccessible from outside the Zope manager, thus making it impossible to salvage source from a crashed installation.

I worked with zope for about 6 months, about 5 years ago. Last week, I installed the most recent Zope with apt-get, and copied a 5-year-old "magic zope database" backup over top of the default one. Fired it up, 95% of it worked just fine, copied out my old content with no problems.

Zope has its annoyyances, but there's a lot of good design in there too. This guy is a clueless idiot. I don't believe he really has 3 years experience with Zope. I begain to find workarounds for most of the problems he mentions after 2 weeks of use, and that was a long time ago. It sounds to me like he's zealous for .NET or something but somebody forced him to use this other weird-ass framework, and he's been holding a grudge ever since.You should also donate to the JAGHow about a willingness to give up control of some of the code to the business people who really "own" it? The [different perspectives of programmers and the business](http://www.edmblog.com/weblog/2005/08/different_persp.html) can be a real challenge and even when technology can provide a platform for a solution, programmers must still give up a little control in order to get the right business results. Sometimes no matter how good a programmer is, there is still a [problem with them](http://www.edmblog.com/weblog/2006/11/the_problem_wit.html).In my company we started using Catalyst about one year ago and it proved to be a *great* choice: web development time decreased, code reuse (and fun!) increased.
The framework offers some interesting goodies such as the very handy chained actions. Even though there's no "imposed" ORM, most Catalyst users tend to use DBIx::Class, which is the best ORM I've seen to date.
Even though the web site could be better and the documentation can seem quite sparse (there is actually a lot, but you'll have to search through CPAN modules to view it), the community is great and much helpful: just enter #catalyst on irc.perl.org and try yourself!
Easier fix: Print a near-blank document (say, 1 slightly-off-white pixel).  Re-insert the same page facing the opposite direction, print the actual document.J instead of APL, because of its free implementation, and ascii readability.

Nothing can come close to it in terseness.I really like it when someone articulates what I feel but can't express.  This nails it.

TDD is awsome though I have a difficult time explaining why or convincing others.  The best I can offer is "just do it and you will see".   This article nails part of it.Q&amp;A with Jeff Barr -- Ask him your questions in the comment section and he'll respond in a later post.The file navigator is a real product by SGI.

http://www.sgi.com/fun/freeware/3d_navigator.html

I love the line "Hey, this is Unix, I know Unix"28). Type 'cookie' you idiot.D has built-in support for contracts:

    class Time
    {
      int hour, min;
      invariant {
        assert(hour&gt;=0 &amp;&amp; hour&lt;24); 
        assert(min&lt;=0 &amp;&amp; min&lt;60);
      }
      void Increment() {...}
    }
    
    Time t;
    t.Increment(); // the invariant will be checked when entering and exiting Increment()
    assert(t); // check if the invariant holds



The nice thing is that it is disabled in the release build.

*Note: sorry about the &amp;lt etc; I'm tired of fighing the reddit comment syntax.*
[deleted][removed]I always see this as the solution to the final permutation of a "how do you detect a loop in a linked list" interview question.  As I recall, the interviewer keeps adding restrictions until finally demanding an algorithm which uses no heap storage at all but can run as long as necessary, and this is the solution.

But generally speaking, don't feel embarrassed about "reinventing" stuff like this.  For example, I independently discovered [Duff's Device](http://en.wikipedia.org/wiki/Duff%27s_device) in college.  Now that is something to be embarrassed of.Reminds me of this old one:
http://www.somethingawful.com/index.php?a=112&gt; It's absolutely opinion. Pylons was, arguably, built from the ground up to fit Ben Bangert's style of web development. So it comes right back to opinion.

Except it doesn't.  Most of "Pylons" is stuff that was developed before Pylons (e.g., Paste or FormEncode), or separately from Pylons (e.g., Mako or SQLAlchemy), or even *for* Pylons but continues to be maintained as a decoupled package (Routes and WebHelpers).  When something in Pylons is identified as being sufficiently generic that it doesn't need to be tied to Pylons, it's often pushed out elsewhere (a number of things have gone into Paste like this).  Pylons also builds on a formal and stable standard (WSGI), and allows and encourages non-Pylons components that use this standard to work easily inside the stack.

These are why Pylons is **objectively** built from the ground up to be customizable in a way that Django was not, and with a development style in which does not seem to interest Django developers.

&gt; I've said it before and I'll say it again: before trying to paint us with the NIH brush, take the time to find out why certain choices were made -- they're not always right, but they are almost always informed choices, not simply "screw everybody else, we're doing our own thing" choices.

I think sometimes people say it too strongly, as though Django's NIH is like stealing from the community.  That sentiment is absurd on many levels.  But Django has a very different development methodology from Pylons or TurboGears (which though technically different on some levels, the two still share very similar development ideals).  

The use of PyDispatcher and simplejson are pretty weak counterexamples.  A willingness to inline a couple small libraries doesn't mean anything.  Django has no external dependencies.  Has anything been made for Django and also made usable elsewhere?  Has the template language been extracted?  The database layer?  Is there anything in the core of Django (or even the periphery) that uses another library, where substantive changes have been made to the library to support Django?

I certainly understand *why* a developer wouldn't do these things.  The communication and efforts at cooperation are boring and can stop momentum and can introduce unpredictable things into your stack as updates come from other directions, or patches aren't accepted, or whatever.  It's not always worth it, and Django may very well be in a better place by avoiding that work.  But there *is* a difference between doing that work and not doing that work, and Django developers don't do that work.  They just don't.We're about to adopt a bug tracking system.  Bugzilla was the default choice, but now we're looking at [Trac](http://trac.edgewall.org) or [Mantis](http://www.mantisbt.org).  I'm curious what tools other people are using and how they're working out.No, reasoning about space usage in Haskell is well-known to be hairy. For example, consider a simple example like: 

`id x = x`

`ones = 1 : map id ones`

This will use more and more space as you traverse the list, because Haskell will build up nested "map id"'s in the tail.A quick review of O'Reilly books proves this statement to be  wrong. There's O'Reilly books on .NET, ASP, Visual Basic, Java ServerFaces, Java Servlets and JSP's... etc.

a-p: Can you clarify who exactly made this statement?And with the new associated types Haskell scores fully on their chart.I was thinking it was term from history.  Perhaps it was what followed the War of the Roses.What you mean is that graphs are a pain in a language like Haskell?[deleted][deleted]I don't know who was supposed to have made that statement, but it doesn't make any sense to me.  I'll look into it.  

That being said, I can imagine that an editor might have said that he or she thought that Rails had the ruby framework market wrapped up for now, and that there wasn't room for a book on another framework till said framework had proved to have strong adoption.  That's a potentially legitimate market assessment -- the computer book market is pretty brutal these days, and topics that once would have made for a successful book now don't sell enough copies to recover their costs -- but even then, that would be a "for now."

A lot of publishers still throw stuff at the wall to see what sticks.  We tend to publish books that we believe will succeed.  And often, that means waiting till a new tool or framework has stood the test of time, and is at the right place on the adoption curve.  It doesn't do anyone -- the author, readers, bookstores, or the publisher -- to publish a book that doesn't sell.  Bookstores will give it a few months, and if it doesn't do well, it will be returned, and that's the end of that.  Waiting a bit longer may actually increase your chances of success.  It's a bit like surfing.  Paddling too early is as bad as paddling too late  -- you have to catch the wave.

O'Reilly has a history of publishing books before anyone else -- we published the first commercial book on the internet, published about Perl in 1991, Linux in 1993 -- but these technologies were actually not new when we published about them.  They had proven themselves.  They were just under the radar of other publishers.

As to publishing too early, Ruby itself is a good example.  We published our first Ruby books way too early, they flopped, and then we took our eye off the ball.
&gt;Like Gotebe said, C is also purely pass-by-value. 

Gotebe is wrong.  C can both pass by value or by reference.

Java passes a reference to a copy.  The copy is never copied back to the original.  People confuse this as being a pass by value because of this (and others confuse that the reference passed is a reference to the original and not a copy).

Languages like D and C# also pass a copy by reference.  But unlike Java they have the capability to copy back to the original (keyword "out").

The distinction between these four types of parameter passing is important to remember.  Sadly, most CS classes try to simplify this down to two types.[deleted]More quotes from the thread:

[1](http://groups.google.com/group/comp.lang.functional/tree/browse_frm/thread/c4b36dd81537e120/ccddd1b8c902029c#doc_27a5ca23e869fb3b): "The beauty of the STL is that you can mix and match algorithms and data structures. This is hard to do in a purely traditional message-based object-oriented framework."

[2](http://groups.google.com/group/comp.lang.functional/tree/browse_frm/thread/c4b36dd81537e120/03b4e60b948d16fd#doc_5f9d678079418551): "..iterators in STL are a much more elaborate things than iterators in CLU or
series in Common Lisp."

  "..inheritance is weaker than genericity."

[3](http://groups.google.com/group/comp.lang.functional/tree/browse_frm/thread/c4b36dd81537e120/82b04a41e728840c#doc_3ace2519c5f6488d): _Erik Naggum's response:_ "..if you can _only_ do it in C++, all of my experience tells me that that means you're doing something very seriously wrong.  if you wish to publish a paper on this work (which I would eagerly read), please don't tie it to C++, but show what it needs to be implemented, so others can
implement it for you in other languages.."

---

OT #1: Cool that [switching TLDs](http://groups.google.com/group/comp.lang.functional/msg/d7394ad977b3a396) does the right thing. I like the (intentional?) trick of using foreign-language versions to sidestep the upgrade to the dastardly new with-frame version of google groups. A large thread like this is especially good at showing up the limitations of the new layout.

OT #2: [Hilarious thread](http://groups.google.com/group/comp.lang.functional/tree/browse_frm/thread/c4b36dd81537e120/ccddd1b8c902029c#doc_27a5ca23e869fb3b)

OT #3: Man, what google groups really needs is a parent link in each message.
We use Atlassian JIRA at the moment and it is fantastic. However, for a piece of propietary software, it'd have to be nothing short of fantastic."It Worked! The Apache Web Server is Installed on this Web Site!"

Stupid piece of crap.  My name is Jason Kolb, not "localhost"!  Geez, this open source software can't even get my name right!
Scraps of paper.  I lost most of them, so they're closed.
How much does 500 GB HD cost? And is it easy to plug into your laptop so you can read and write to it? If you plug it in, is it any slower? (i.e., is USB the only option for a laptop and is it slower?)I don't think it's right to call this guy an idiot because he develops through the web in Zope.  Lots of people still do that, for legacy reasons or otherwise.  If you do, it **will** make you very very unhappy.  His criticisms are great, but whatever the details it will cause you to be passionately critical somehow.

Putting code in an object database was a bad idea.  Developing through the web is a bad idea.  There's better ways to use Zope, but if you are just messing with Zope here and there and/or have a legacy system developed like this you are just not going to be happy.Even for ints? (not Integers)Java passes non-object built-in types by value. End of afterword to story.[deleted]&gt; Gotebe is wrong. C can both pass by value or by reference.

Gotebe is right. C is purely pass by value. (This goes nowhere if you don't back up your statements.)

If you don't agree that C is purely pass by value, there's no chance you'll accept the Java version of it. This is because Java confuses the issue by calling pointers references (ref [Java Spec Section 4.3.1](http://java.sun.com/docs/books/jls/second_edition/html/typesValues.doc.html#12028)).

I guess you call this *call-by-value*:

    void cbv(int xFormal);
    ...
    {
      int xActual = 42;
      cbv(xActual);
    }

and this *call-by-reference*:

    void cbr(int *yFormal);
    ...
    {
      int xActual = 42;
      int *yActual = &amp;xActual;
      cbr(yActual);
    }

Correct?

In the first invocation, the value of xActual is copied into a different memory region (the stack) for use by the function. We both agree this is call-by-value.

In the second invocation, the value of yActual is copied into a different memory region (the stack) for use by the function. No difference here. That you can use the value of yFormal to access xActual does not make it a reference.
http://java.sun.com/docs/books/vmspec/2nd-edition/html/Concepts.doc.html#26454

"When the method is invoked, the values of the actual argument expressions initialize newly created parameter variables (§2.5), each of the declared type, before execution of the body of the method."

Care to explain this anomaly in the VMS? Not that I consider it authoritative like many Java-ninnies (I am ashamed to admit that it was once my job to implement it).

Of course, you could just point to "constant variables" or some such to discredit the source (which I strongly encourage after you understand the semantics of Java (and C it seems)), but I'm wondering if you're prepared to go that far.

Also: http://jqa.tmorris.net/GetQAndA.action?qids=37&amp;showAnswers=trueComponents _used by_ Pylons existed previously, yes, but Pylons itself seems to have come originally out of a desire to clone Rails (something that still shows through in some of its components), and then to become a "one WSGI to rule them all" framework. Again, it goes back to what someone subjectively wants from the framework.

&gt; Django has no external dependencies.

As of Python 2.5, this is arguably true because you can use the bundled SQLite module. However, step outside of that and you need at least an external database module, and if you step outside of mod_python we have a dependency on flup to get FastCGI -&gt; WSGI translation.

A better statement would be that we don't have a lot of external dependencies -- Django aims to follow Python's "batteries included" as much as possible, and providing a full stack out of the box is, I think, a large part of that.

&gt; Has anything been made for Django and also made usable elsewhere?

Can I count the endless clones of the Django admin app? Or, for a smaller example, web.py's wholesale appropriation of our debug pages?

&gt; Has the template language been extracted?

[Yup](http://wsgiarea.pocoo.org/jinja/).

&gt; Is there anything in the core of Django (or even the periphery) that uses another library, where substantive changes have been made to the library to support Django?

No, and I'm not entirely certain that'd be a good thing. Requiring other people to change their libraries to suit us doesn't strike me as polite.

&gt; But there is a difference between doing that work and not doing that work, and Django developers don't do that work. They just don't.

Again, I'm not entirely certain what point is trying to be made here -- lots of components of Django are rolled from scratch for historical reasons. As we've gone on, we've pulled in things where it made sense, but we're also carrying around a fair bit of legacy architecture which makes it tough to incorporate a major component without essentially a ground-up rewrite of either Django or the component. We've already done one ground-up rewrite of Django in the past year, so that would leave us in the position of dictating to someone else that they completely rewrite their code to suit what we want from it. And, personally, I don't want to be that guy.

I'm also not convinced that "glue" frameworks are going to be a viable niche in the Python world for much longer. With Django, for example, I was surprised for a while that there didn't seem to be any CMS or blogging projects really gaining momentum and attracting users and developers. And then I talked to a lot of people and found that they all thought it was so easy to do that with Django that they never bothered looking for someone to have done it already -- it was just as easy to sit down and produce something tailored to the exact needs of the moment.

I think that with the explosion of solid Python web components the same thing might happen to projects which are trying to serve as "glue" -- when you can [literally build a framework in an hour or so](http://bitworking.org/news/Why_so_many_Python_web_frameworks) by plugging your chosen components together yourself, it's harder and harder to justify working within someone else's vision instead of building your own to suit.

And I don't think the people who use that sort of framework -- whether they use one already existing or roll their own -- are ever going to like Django, just as I'm probably never going to like Pylons or TurboGears. Some folks like to plug and play, some folks like a full stack, and never the twain shall meet. Ultimately it comes right back to pure subjective personal opinion -- I'll use what I like, you'll use what you like, and the rest of the world will keep writing monstrosities in Java.Yeah, I wish we'd gotten semantics &amp; types in my undergrad classes.  I'd be having a much easier time with the language I'm writing.To point out the glaringly obvious, Pylons and Turbogears have an entirely different purpose/philosphiy behind them than Django. Django provides a tight, unified development framework with a certain flavour. It isn't meant to be all things to all folks. In practical, real world terms, there is a big advantage to this. Narrowing the scope makes it easier to document and easier to transfer knowledge to others.That whole story in an of itself encapsulates the real problem CS is having: people who learn a little about how computers work, and subsequently gain massive egoes and superiority complexes about what they do and know.

It's almost as if it's the first thing they've **ever** mastered in their lives. They jealousy guard it, snarkily commenting about anyone who moves in their intellectual sphere and appears beneath their skill level. (They rarely speak of people who actually have more knowledge, and when they do it's in hushed tones with people they trust.) And anyone who is simply baffled and does not know where to begin is treated to sermon after sermon like this, talking about what "real computer science is" and how the CS world is going to hell in a handbasket because you really don't *need* to know much about CS to build a functional java bean.

In studying math, chemistry, physics, literature, rhetoric and computer science in my postsecondary education, it's only been in the CS field where I've had to endure the eye-rolling and snooty, inference-laden remarks about my mental capacity when presented with something that, for the life of me, I just don't understand.

 I can't find the original article or subject matter, but from what's presented in this email, the author should more likely be focusing on the wonderful fact that someone is attempting, however haltingly and yes, without understanding, to program in a functional language. And what does this author do? Lecture that person about "the right thing to do" when the person was really just looking for help. Everyone has a set of parents, best leave them the job of telling you what to do with your life.

The importance of real CS is for its practitioners to stop treating it as a sacred art to be learned only by those deemed worthy by a chosen few.We recently (a few months ago) switched to OnTime.  It's okay.   A little slow though sometimes.Yes, it's a repost.  But there are still images to look at.&gt; Components used by Pylons existed previously, yes, but Pylons itself seems to have come originally out of a desire to clone Rails

Is this the Chewbacca defense?  Because I really don't see what your point is.  Several Pylons developers continue to actively start, contribute, and maintain libraries that are completely separate from Pylons.  *That* is my point about the developer communities.  Whatever Pylons is or why it came about isn't the point.

&gt; &gt; Has anything been made for Django and also made usable elsewhere?

&gt; Can I count the endless clones of the Django admin app? Or, for a smaller example, web.py's wholesale appropriation of our debug pages?

No, you can't.  

If Django borrows ideas from FormEncode, I would not count Django as an extraction of anything from FormEncode.

re: Jinja (the Django-like templating language you reference). 

That was written by someone for use in his WSGI-based framework, with syntax and some code from Django.  It wasn't and isn't developed by a Django developer.  *If* Django was serious about promoting its templating language it would provide an easily installable library that only included the templating language, and Django would itself consume that same library (ensuring no drift).

That someone forked the code so they could cleanly use the templating language elsewhere only further proves my point.

&gt; &gt; Is there anything in the core of Django (or even the periphery) that uses another library, where substantive changes have been made to the library to support Django?

&gt; No, and I'm not entirely certain that'd be a good thing. Requiring other people to change their libraries to suit us doesn't strike me as polite.

That's not impolite, it's called "participating in the community".

&gt; Again, I'm not entirely certain what point is trying to be made here -- lots of components of Django are rolled from scratch for historical reasons. As we've gone on, we've pulled in things where it made sense, but we're also carrying around a fair bit of legacy architecture which makes it tough to incorporate a major component without essentially a ground-up rewrite of either Django or the component.

That is *exactly* jesusphreak's original point.  Django is a world of its own.  If that's because of technical reasons that hardly makes it better -- if they were purely cultural then at least someone could choose to use other components if they wanted.  But they can't; it's either hard or it just doesn't work right (at least that's what people were saying further up in this thread).

&gt; I think that with the explosion of solid Python web components the same thing might happen to projects which are trying to serve as "glue" -- when you can literally build a framework in an hour or so by plugging your chosen components together yourself, it's harder and harder to justify working within someone else's vision instead of building your own to suit.

Absolutely.  This is part of why Pylons should (and I think will) remain small.  As long as all the best features are moved into WSGI-based components, you can implement a framework of your own design that also has feature parity with Pylons.  Or you can use a mini framework inside of Pylons (such as [OHM](http://pythonpaste.org/ohm/)) -- people will stay with Pylons for the community and the concepts, not because it's hard to move to something else or make it yourself.  

In this context "WSGI" itself could be considered the framework... but so be it.  It's complete, one-size-actually-fits-all, and totally stable, which is about everything you can hope for in a foundational framework.[Data.Graph](http://haskell.org/ghc/docs/latest/html/libraries/base/Data-Graph.html)

and

[Data.Graph.Inductive](http://haskell.org/ghc/docs/latest/html/libraries/fgl/Data-Graph-Inductive.html)

Do you have Asperger's? It's an "I don't trust that" expression.Nothing sucks like a VAX ;)Heh. Read some of Paul Graham's essays, at www.paulgraham.com (the ones farther down the list, about programming) and see if you get our drift. 

Basically, the position of many of the people on programming reddit is that yes, we are interested in practical applications, but no, we are not interested in Java. This is because Java is retarded in many ways I don't feel like having a flame war about right now. We think real-world programming is much better accomplished in Common Lisp or Scheme or Haskell or OCaml or Erlang, and we have examples and principles to show you why. 

Because these other languages have real, tangible advantages over Java, we don't care that Java is popular, because it's so horribly broken in so many ways that it makes it like pulling teeth to program in it when you know there are ways to do it that are SO much better but can't be expressed in Java. 

There are exceptions to this trend, of course, like grauenwolf, who programs in Visual Basic and is proud of his failure to comprehend the advantages of higher-level languages like Scheme or ML.[removed]I've heard stories that the Pragmatic Programmers have responded that way to inquiries.  Maybe you have your names mixed up?Have you learned Information Theory?
Only the different bit is significantSince Britney Spears is the most popular singer in the world, shouldn't you listen to her songs 24 hours a day? Or are you some kind of freak?Too bad it's still unrelated to programming as in "programming.reddit.com"...Not again...How about [this](http://www.tiobe.com/tpci.htm)[Trac](http://trac.edgewall.org/).Computer Science is not like other science, in that it is dealing with pure thought, or rather, information. There is only one other field which shares this property, and it shares completely CS's snootyness as well: *Philosophy*.No one is stopping anyone from submitting interesting articles involving java.[removed][removed]Not realizing the age of this, I figured he was leading up to haskell's data system, the way he was heading (except the pointer part).What needs to change is not Haskell and Lisp, but industry, and in particular the notion that unskilled programmers can be trusted to write mission-critical code.Maybe IBM aches, sorry AIX?

I'm ditching FreeBSD on all our servers as we speak, VMS clearly rocks!

I'll reformat my Powerbook G4 and install VMS there too!

VMS, here I come!
Mostly because there's no progress associated to java: no paradigm shift, no innovation in conception of logic, rules, flow, data. It doesn't help us see the world more clearly.

Take the IO Monad in Haskell: through types, it clearly separates the atemporal (non-IO) to the temporal (IO actions). How's that for a paradigm?Yes, the above title could be more representative of the gist of the article.  The article itself is more about the politics of getting a better language adopted than it is about technical merit.

Not that I disagree with implied point in this title, though.[deleted]HP does that, and more. HP is the most invasive afaik in terms of phoning home. It essentially reports all of your printing habits in as much detail as possible.What happened to math?[deleted]In Scheme filter returns a new list with those elements where the predicate evaluates to true. Haskell and Lisp too I think.
The function used in reduce (often called fold) is a binary function but isn't by definition associative. The ordering of the operands can be left to right (often called foldl) or right-to-left (foldr)
e.g. 
&gt;*(foldr expt 1 '(2 3 4)) gives 2^(3^(4^1))*

&gt;*(foldl expt 1 '(2 3 4)) gives ((1^2)^3)^4*
This also works great for finding cycles when rendering iterated function fractals (such as the Mandelbrot set).What sort of incentives would it take to make that happen?Two additional questions:
- If there's a loop, how do you find how many elements are in the loop?
- How do you "unhook" it, i.e. remove the loop without making any elements unreachable?Well known trick (see Common Lisp spec); I think the official name is tortoise and hare.&gt; There has to be an economic cost attached to buggy code, that comes back to the author somehow.

I would scratch the 'economic' and simply say 'cost', but... yes, that would have to happen to alter the landscape.

It would alter things in other ways, though.

How would open source software fit into that kind of scheme?

Would it lead to fewer programs created because of higher risks outweighing rewards?  In some cases that would be a good thing.  In others, though, people don't really seem to care about bugs, they'd rather have a cheap program that works 'ok', so it might lead to less overall value.

It's an interesting subject to think about.

In general, I think I'd have to say that the free market has served us pretty well.  People who truly do 'mission critical' sorts of software can and do pay a lot to go through it with a fine-tooth comb to make things work.  Plenty of others make due with stone knives and bearskins because it's apparently the best price/quality compromise.  Others try and get by with crappy code where they need good code, and go out of business.  Others risk spending too much effort to write really good code where "worse is better" would be enough, and go out of business.[deleted]&gt; What happened to math?

It looked at Philosophy and realized that being a snooty arrogant prick is a great way to have no one respect you.References are presumably implemented with pointers under the hood, but you  are isolated from that on the language level. 

I believe NullPointerException was just an naming mistake that confuses abstraction levels. It should really have been called 'MissingReferenceException' or something.

I think it is correct to say that Java passes objects by reference, since this is really what happens: references to objects are passed into the method parameters. 

However pass-by-reference is specific CS-lingo where we are talking about a reference to the variable memory cell rather than the reference to an objects that the memory cell might contain. It's a different abstraction level, talking about under-the-hood issues that the Java-language has no access to.

If you want to avoid confusion, say that variables are passed by content :-)Exercise 3.18.  Write a procedure that examines a list and determines whether it contains a cycle, that is, whether a program that tried to find the end of the list by taking successive cdrs would go into an infinite loop.

Exercise 3.19.  Redo exercise 3.18 using an algorithm that takes only a constant amount of space. (*This requires a very clever idea*.)If you want to be a programmer on a commercial product, then why study computer science? Doesn't the rest of the world have programming trade schools? Computer science is a science, it should train scientists, not trade programmers.And by scientists you mean.. professors. Now I've got it.i think i just threw up a little in my mouthThere are two (not fully formed) thoughts that spring to mind...

The high road -- [Stallman on Emacs lisp](http://www.gnu.org/gnu/rms-lisp.html):

&gt; Multics Emacs proved to be a great success -- programming new editing commands was so convenient that even the secretaries in his office started learning how to use it. They used a manual someone had written which showed how to extend Emacs, but didn't say it was a programming. So the secretaries, who believed they couldn't do programming, weren't scared off. They read the manual, discovered they could do useful things and they learned to program.

The low road -- [Java whitepaper](http://java.sun.com/docs/white/langenv/Intro.doc2.html#349):

&gt; Primary characteristics of the Java programming language include a simple language that can be programmed without extensive programmer training while being attuned to current software practices. The fundamental concepts of Java technology are grasped quickly; programmers can be productive from the very beginning.

Some claim, persuasively in my opinion, that programmers face similar deskilling pressures that others ([like factory workers](http://www.nooranch.com/synaesmedia/wiki/wiki.cgi?DavidNoble/ForcesOfProduction)) faced. 
There is already an economic and social cost attached to buggy code. In an open-source context you get a bad reputation, in a business context you lose customers etc. 

The fact is that even buggy code is valuable in *many* situations.[deleted][deleted][deleted][deleted]@home Trac, @work JIRA.

JIRA blows trac out of the water, it's nothing short of amazing, very well done, very clean, very clear, it's near perfect. The only drawbacks I've seen is that the JIRA-SVN hook is a script that gets run regularly instead of an SVN hook that calls JIRA (the hook that associates commits with JIRA bugs/issues, akin to what Trac does), so you have to wait a random interval between the commit and the time at which your commit appears in JIRA.

Other than that, it's just amazing. Well worth the (quite fucking steep) price for professional development.We use sugarCRM, it's not the most detailed bug management tool there is but it's great overall tool for managing everything like client requests and everything else.

http://www.sugarcrm.comIt's not big business but society itself that excepts poor drivers. It would be relatively easy to legislate a far more rigourous driving licence regime especially as there are already advanced driver training curricula available.

There are few votes in doing so though.That's not currying, but partial application/parameterization (as in Python's [partial](http://www.python.org/dev/peps/pep-0309/), or [SRFI 26](http://srfi.schemers.org/srfi-26/srfi-26.html)'s `cut`/`cute`).  Currying is when you transform an n-ary function into a chain of n closures, such that they keep on "collecting" parameters when called until the result of all n can be returned.

An easy way to tell the two apart is that you can't curry variadic functions like Scheme's `+` (you wouldn't know where to stop), and you can't partially apply `(foo a b c)` like `((((partial foo) a) b) c)`.&gt; (such as how it prefers to associate apps with mime types rather than file extensions, often guessing wrong about what sort of file something is.)

I think Konqueror is way better than MS there. I don't want every .cfg-file in my system to show up as a "Microsoft office configuration file", and who knows what .dat files will be presented as this month.Or instead of streets clogged with cars, many of them simply parked, we could have saner forms of transportation. Where we have reasonably skilled "drivers," which coordinate with each other. And not have so much of society built around cars.
You can omit the `append` and pass in the arguments directly:

    (define-syntax partial
      (syntax-rules ()
        ((partial f args ...)
         (lambda rest
           (apply f args ... rest)))))

(Note that this is partial application, not currying.)Downvote because of presentation media. I would like to read it in a format that is meant to be read, though.[removed][removed]This pretty much describes how I write code.  In fact, a few years ago I tried to describe my own personal process on the C2 Wiki: [PrototypeAllTheTime](http://c2.com/cgi/wiki?PrototypeAllTheTime).
Note that this is partial application, though, which is not the same as currying (see other comments).&gt;The reason Symbian C++ doesnt use the standard exception mechanism is that when Symbian OS was being developed (back when it was called EPOC), the standard for exceptions hadnt actually been defined.

Added to which, the LEAVE mechanism sidesteps the question: how do I instantiate a new OutOfMemoryException object to throw when I'm...out of memory! There are other solutions to this, preallocting an object to throw and so forth, but the Symbian C++ approach is bulletproof, even if it does impose a greater burden on the programmer. 

But it's telling that almost every explanation of the oddities of Symbian begin "back in the day". Symbian OS and its dialect of C++ are creatures of a time when only workstation class machines and up had memory usefully measured in megs. The shame is that they haven't, to date, adapted terribly much to a world where hand-held device can have a very similar spec to a SPARC 20.
That's terrible.  Never ever do that.&gt;The strings (descriptors) and array stuff were ugly and confusing.

Too true, but each of the descriptor classes exists to address a particular scenario of stack/heap/RAM/ROM storage choices and various usage models. Once these are grasped, at least the descriptor libraries don't seem to maliciously random any more, if still not particularly pleasant to work with. But this is just one more symptom of Symbian not moving with the times.

&gt;The Active Objects (cooperative multi-tasking) I didn't find so bad -- it's a task loop. Of course, using it for real multi-tasking (rather than just your main loop) might be a pain.

Horrendous. But the client/server strucuter of most Symbian apps and the single-document workflow at the UI means that you hardly ever have to worry about that.

The central problem I found with Symbian progrmming, S60 and UIQ, is that operating system services and APIs that a presented as if they are general-purpose were actually reversed out of applications. So, if your app happens to things a lot like the original application, you're fine, but if not, you're in trouble: all sorts of unstated assumptions about how the facilities expect to be used turn round and bite you.  
I certainly think language-neutral exceptions and restarts ought to be in the OS, and probably garbage collection too. I'm not so sure about unique error codes, though. That sounds rather centralized. Where's the code for "trans-warp error, you need to reverse the polarity of the flux capacitor"?

Plus, any modern OS needs to understand URLs as file identifiers. Not at the KDE level, at the fopen() level.This is pretty neat trick. Properties in python always felt a bit cludgy to me in the past, but this is rather elegant.

If you want to make it even fancier, you could use a decorator to get rid of the "name = property(**name())" part at the end like so:

    def make_property(f): return property(**f())

Then when you define the property you can just do:   

    @make_property
    def name():
        ...

Not a huge difference, but it saves you from a bit of redundancy in having to state the property name three times :)&gt; If there's a loop, how do you find how many elements are in the loop?

Once you have a node that's known to be in a loop, you can just follow pointers and count how many nodes have to be traversed to get back to the starting node.

&gt; How do you "unhook" it, i.e. remove the loop without making any elements unreachable?

So, you want to take something that topologically looks like

          #####
          C   #
    #####AB####

and transform it into

    #####AB##########C

where the list flows left to right. Three key nodes are labeled A, B and C.

This second problem is a little trickier than the first. We'd like to just set C's next pointer to null, but that's easier said than done, because B or C have no distinguishing features that set them apart from other loop nodes, at least when viewed from within the loop itself. From the outside, however, we can define A, B and C as follows:

- A is the first node whose successor is in the loop.
- B is A's successor.
- C is the node in the loop whose successor is B.

Once we have A, we can immediately determine B. Once we have B, we can find C by following pointers once around the loop. So the objective is to find A.

We can locate A if we are satisfied with O(n^2) time and O(1) space. First, run the hare and tortoise algorithm to determine a node in the loop, call it L. Then, for each node N in the whole list, traversed left to right, do as follows:
starting at L, go around the loop once; as each node is visited, compare it for equality to N.next; if there is a match, then N is the sought-for A node, so return it.

This could be sped up to O(m+n) time and O(m) space by putting the loop nodes in a hash set. But if we have to stick to the O(1) space requirement, then the above is the best idea that comes to mind, although I haven't given the problem much thought.I'm talking about buggy bloatware and the fanboy apologists who invariably defend it.For any given level of competence testing, there will be accidents. Aircraft pilots have accidents, and they're ridiculously well trained.

If the car experience teaches anything, it's respect for humanity. Day in and day out, billions of drivers make it home alive.Bah! Humbug!

The effect of the last 10 lines of the example code can be acheived simply by removing the underscore in "self._name" in the constructor.

So, he uses 10 lines to acheve something that can be done in zero lines ... and all because he believes in the popular mis-definition of encapsulation.
See [Don't say no! (Lisp isn't just for the best programmers)  ](http://reddit.com/info/17rd/comments) - Lisp is for the mediocre programmer, too.
Being a professor is good from an academic point but not the best occupation.

But don't bother to read my comment. How else will us guys on reddit bask in our superiority. We, after all, don't have to deal with the real world.[removed]The quality of software is going up. Not as quickly as I'd like to, but it's all getting better. People expect software to run without crashing nowadays. A decade before, that wasn't the case. Or look at another decade earlier. Quality? Hardly. You can go back as far as you want, software has never been as good as it is now. Surely this wasn't a serious question?

As for buggy bloatware, I think the term itself is unfair. People label software bloatware when they're unable to complain about something specific. Software doesn't magically get worse when the number of features increases. Some people actually need the kitchen sink.

As for software being buggy, that can be a real problem. But software hasn't gotten any buggier in the last decade either. Software is pretty damn hard to write, so bugs will invariably slip in. The policy towards fixing bugs is mostly an economical one. People who demand a 'zero outstanding bugs' policy are pretty naive. Some bugs just don't matter - fixing them ain't worth the time (and risk).

If somebody wants to defend a piece of software because it does what it's supposed to do (more or less), then good for him. Nothing to do with fanboyism.Indeed you're right. The point I was trying to make above was that Big Business wasn't the cause of the problem raised.The unique error codes are indeed centralized. It is likely that registering them in the system is part of the installation process of a program. (I have never administered a VMS server, so I don't know for sure.)

The advantage is that users only need look at the same usual place for help, whatever error they see. The documentation that ships with VMS is very thorough, every OS error is documented, with explanations for the end user, the programmer, and the system administrator.While I agree with your sentiments I don't hold out much hope until the self-driving eco-friendly car emerges.

I think people have got used to the flexibility and isolation of personal transportation. I personally use buses and don't drive but I live near a city with good public transport (Nottingham, UK) and I don't have 'Teh Fear' about other people.For example, a non-conforming implementation of a standard could be basis for litigation.His Notes on Programming, written for the courses he gave at Adobe, are very good reading. His algebraic, axiom-driven approach to designing classes with value-like semantics, as exemplified by the snippet

    T x = y;
    assert(x == y);

is one that should be second nature to most C++ programmers but sadly is not.No, it was definitely O’Reilly that was mentioned.Joel Spolsky once said that good software takes 10 years. I think unless the task is small, it will require an inherent bloat and awareness/dependency of other bloated tools.

The cozy realm of small problems is generally too good to be true. 

This quote from Douglas Crockford puts things in perspective a bit: "I used to say that the web browser was the most hostile programming environment ever devised, but that was before I found out about Mobile."

Check out his recent blog post on JSON/XML, which was a reply to an XML biased comparison one:
http://blog.360.yahoo.com/blog-TBPekxc1dLNy5DOloPfzVvFIVOWMB0li?p=632

Here's my own blog post as well: Enough Mistakes! - http://www.ralip.com.br/jp/blog/post?p=enough-mistakes

In my blog post I talk a little bit about my own mistakes, even though I am happy with the result so far.

Finally, check out this post on the new version of Tapestry, which was totally rewritten, and seems as easier and "type safe" as ever:  Tapestry 5.0.1 Preview Release Now Available - http://www.theserverside.com/news/thread.tss?thread_id=44119

I mean, we are duking it out, fighting our ways through the mess, and naysaying will not stop us. It's cold outside here, while it might be warm for you in your cozy realm.Thanks for the comments!It was [said by Sebastian Riedel (project lead at the time), on the Catalyst mailing list](http://lists.scsys.co.uk/pipermail/catalyst/2006-April/007097.html).  It has been mentioned a few times since, but apparently always in reference to Sebastian’s statement.See [Useless Use of Cat Award](http://partmaps.org/era/unix/award.html) :-)
&gt; I think laziness makes programming inherently easier

Laziness is a double edged sword. When you *need* to control execution order, laziness can be a pain in the ass.You're back!The fun part is, when you do fancier things inside the get, put and del methods.Good thing at least one of the main lazy languages provide [extensive facilities](http://haskell.org/ghc/docs/latest/html/libraries/base/Control-Parallel-Strategies.html) to control evaluation order (yeah, that could be parallel evaluation too ;-)

    f !x = x + 2

Isn't that hard to write :-)&gt; And what does this author do? Lecture that person about

It's regrettable that it toched all your wrong buttons, but the author is very thoughtful and guarded in his advice. Throughout.

I'd even say that he seems classically trained. It reads a little bit like Seneca's *Letters to Lucilius*, distinctly Latin: the recurring vocatives ("Oriana... Oriana..."), the use of the Ubi Est trope, etc. All in all, thoughtful, persuasive, gentle.Centralized errors are possible. I can think up some more thread control inspired by Erlang. "notify me if thread x crashes!", "what error is thread y in?" or even "try to continue thread y!". You would have to invent a whole new error handling system and completly break with UNIX and backwards-compatibility, but it's possible.

I don't know how you could put garbage collection into the OS. OSes do "some kind of garbage collection", namely they free the memory after a process exits. You can't write an OS that magically makes all your languages have garbage collection. If you don't realize: malloc and free/new and delete are not syscalls, but C/C++ standard library functions.

Putting URL handling into fopen is a bad idea. You would need to extend the semantics of file operations very much. For example what does write() do in this case? lseek? Where do you handle HTTP headers like Content-Type? File operations (in UNIX and Windows) are very different from URL operations. See Plan9 how you can do URLs through/on top of files.&gt;"When the method is invoked, the values of the actual argument

That is correct, which I already stated: Java makes a copy of the parameters and passes a reference to those copies (see my response to wicked below from yesterday).Thanks for expounding, Tim. I’ve quoted this to the list; if something comes of it, that would be great.No, really? Here I was thinking they were as easy as BASIC.Yes&gt;This is because Java confuses the issue by calling pointers references

Pointers are references.  That's their purpose.  And its not only Java making this "mistake" but also Wikipedia,  computer science, and lots of other people:

"Pointers are the most primitive and error-prone but also one of the most powerful and efficient types of references, storing only the address of an object in memory."

Sorry, Gotebe is still very wrong.

&gt;int *yActual = &amp;xActual;
&gt;In the second invocation, the value of yActual is copied into a different memory region

yActual is a pointer (which IS a referece).  The value, 42, was never copied.

&gt;That you can use the value of yFormal to access xActual does not make it a reference.

WTF do you think a reference is?  I can only guess you are digging around about C++ referencing, which would be pretty comical if true.

P.S. notice you are now trying to change your wording to use "call-by" instead of "pass-by".  You are getting warming.  Languages like C# are "call-by-copy-restore" when using "out" (notice that the whole language is not a single parameter passing type similar to C).  This is all refuted since, for example, call-by-copy-restore is often called call-by-value-result.  And, in truth, it should actually be called call-by-copy-reference-restore.  This is probably because some people can't get it in their head what a reference is.I would say that any file transfer takes "needed by plot" time to perform, independent of how many files there are, or how big they are.Yes, you missed that Ruby allows you to reopen and redefine parts of an existing class, little by little, as much as you need.

If this explanation is not enough for you (which I assume it is not), I think you'll have to learn Ruby to appreciate this feature. It's usually very hard to appreciate programming features that you haven't used yourself, because using programming features tend to affect the way you think about programming.Alas, those extensive facilities do not yet come with equally extensive documentation.I tried to use Plone once for a community web site, and gave up in disgust.  I think the key description would be "over-architected."
[Jim Gray](http://en.wikipedia.org/wiki/James_N._Gray) is more related to programming than half the posts here.
Yeah, because you are obviously doing so well in "The Real World" (TM). All projects are completed on time with minimal bugs, everything imaginable is doable,...

Who the hell needs scientists, lets just go back to hit each other over the head with branches (clubs with stone-heads probably weren't invented by one of you real world guys either you know).I agree that OS processes should learn some stuff from Erlang, both for error handling and IPC.

I think handling URLs on an fopen level is wrong but there should be a unified approach to it on the OS level, maybe replacing fopen with some kind of call that informs the caller of the capabilities of the underlying filesystem.For an emacs interface to maxima, there is [imaxima](http://members3.jcom.home.ne.jp/imaxima/Site/Welcome.html), and some [instructions](http://bc.tech.coop/blog/051029.html) on how to set it up.&gt; Yeah, because you are obviously doing so well in "The Real World" (TM). All projects are completed on time with minimal bugs, everything imaginable is doable,...

That has not been the opinion of actual professors according to their stories that have crossed reddit.

Fine, keep putting words in my mouth guys. Every question I raise is replied to with regurgitated garbage from a conversation that I apparently missed..We use Mantis to track 3 different projects. Somewhere close to 20,000 tickets at the moment.A ruby script and step by step tutorial how to convert perfomance statistics from a csv file into a chart - using gnuplot.Discussed here
http://www.voidspace.org.uk/python/weblog/arch_d7_2007_02_03.shtml#e622
Yes, yes, and yes.

The thing I found was that progress went in a real staircase manner -- you'd beat your head against a wall for a while, trying to figure out how you weren't plugged into/using the framework 'correctly' and then you'd get it, and have a big chunk of working functionality.

And some things just seemed badly designed, so that there was a lot more coupling than desired (with views and controllers and doc apps and such).Trac.Haskell has infinite lists.&gt; And its not only Java making this "mistake" but also Wikipedia, computer science, and lots of other people

At some point, when the rest of the world (including the designers of all the programming languages in question)  disagrees with you, you have to consider the possibility that you don't have your definitions and terminology quite right.There's not even one single functional language in the list. Why?Excellent exposition on the subjectGrep.&gt;&gt;yActual is a pointer (which IS a referece). The value, 42, was never copied.

&gt;This is where you are confused. The value of yActual is not 42, but the address of xActual.

I never said yActual was a value, but rather just a reference (did you even read what you just quoted from my comments?).  You are either one of the following:

1.  Confused about the differences between values and references.

2.  Trolling

3.  Dancing around because you made a big mistake with the article.

&gt;Please, find a source supporting your argument that C has pass-by-reference in addition to pass-by-value semantics.

Well, some of those articles aren't helping you much.  From the C++ Reference Guide link you gave:

"In Java, objects are passed by reference"

The rest is trivial issues over the belief of many C++ programmers that C is "pass-by-address".  Interestingly this is only a term C++ programmers use, since internally C++ uses pointers inside those references.  This allows conversion of C++ references to pointers and vice versa.  That conversion is not a good practice though since C++ references add type safety and several other limitations that help prevent bad issues when using plain old pointers.  [Here's](http://en.wikipedia.org/wiki/Reference_%28C%2B%2B%29) a Wikipedia entry on it.

Some people try to believe that C++ references are actually something totally different, internally and externally, and have nothing to do with pointers.  I can point to code on the internet that proves otherwise.  Simple fallacy is that most C++ implementations are originally coded in C (see g++ for an example).&gt; If you want to avoid confusion, say that variables are passed by content :-)

How about [call by object](http://effbot.org/zone/call-by-object.htm)?  Apparently, the term was coined by the pioneering language [CLU](http://en.wikipedia.org/wiki/CLU_%28programming_language%29) to describe just the kind of semantics possessed by Java, Python, Scheme, and so on.Bugzilla is awful. I like Trac (we have ~10 installations at my office, I run 2 at home), though I've heard good things about [roundup](http://roundup.sourceforge.net/), and the Python team [chose JIRA](http://wiki.python.org/moin/CallForTrackers) over both, but was overruled by the open-source bigots (50% snark).True. But  Symbian is owned by hardware manufacturers (such as Nokia) who are operating in a very competitive market.  Symbian was formed to protect these hardware guys from facing the kind of commodotization that has occurred in the PC world. I guess the being able to do superefficient things in terms of hardware resources continues to be the prime concern, over developer efficiency or ease of use.Hilarious:

&gt; 1.3: If I write the code int i, j; can I assume that (&amp;i + 1) == &amp;j?

&gt; Only sometimes. It's not portable, because in EBCDIC, i and j are not adjacent.


Or try 

&gt; 1.11: How do I declare an array of N pointers to functions returning pointers to functions returning pointers to characters? 

[...]

&gt; This works because, in C, declaration reflects use, but it's one of those weird distorted mirrors. [a]

What? You don't read words?True, however, they all seem to transfer at approximately the same speed, like the harddrive/network connection/whatever throttles it's transfer speed to accommodate the antagonist.

Try and transfer a 2gb file over the network and it transfers at 10 mbps, try and transfer a 2mb file and it transfers at 28800 bpsOk, so I posted this as a comment on the linked page. How come you can't do this:

Only have one pointer iterating the list. Keep track of how many nodes it has looked at, and when this count reaches some number (say, 10), store off a pointer to the current node. Every time the iterator moves on to the next node, compare it to that pointer. When you get to 10 again, store off a new pointer.Of course not;  most of the people I've talked to in math, cs, and philosophy are great people and fun to chat with.  Maybe I've been lucky.  In America, though, there is a popular culture "of stupidity" in which it is fashionable to not be smart, and to accuse smart people of being snotty because they are smart.  I don't know if it's the same way elsewhere.  It's a form of rationalization for mental laziness.
Well you can program javascript, Perl, Python and Ruby functionally.  So I guess there is:)I suppose people programming in functional languages don't put their projects on SourceForge. Similarly, there are almost no Ruby projects, probably because of RubyForge. Such a project doesn't exist for, say, PHP, so PHP programmers put their apps on SourceForge.&gt; You are either one of the following: [...]

I think it's safe to there is some confusion going on, only that we disagree on who's confused.

&gt;Well, some of those articles aren't helping you much. From the C++ Reference Guide link you gave:
&gt; "In Java, objects are passed by reference"

Seems like I picked a poor source! :-) Luckily they are right on the C issue.
[deleted]Bloatware is not software where people are "unable to complain about something specific".  Bloatware is unnecessarily bloated software.  Bloatware tends to have long startup times because it's initialising all sorts of crud I don't want or need.  Bloatware tends to be sluggish because every operation updates a million things I'm never going to touch.  Bloatware tends to accumulate bugs in obscure corners because there are billions of moving parts, all with their own edge cases.

For example, an office package should not require three days, two gigs of RAM and ten gigs of temporary drive space to compile.  It should not take upwards of thirty seconds to start on a reasonably modern machine.  (No, I'm not bashing OpenOffice just for the sake of bashing it.  These are real problems and are the reason I don't use it despite all the features I really like.  No, not everybody compiles it from source.  However, if I am going to contribute code to the project I'd better be able to.)

Not all big systems are bloatware.  Not all bloatware is necessarily big, although bloatware is always bigger than necessary.Erlang uses a structure called the "Judy Array" to implement its in-memory database ETS:

http://www.erlang.se/workshop/2003/paper/p43-fritchie.pdf

http://en.wikipedia.org/wiki/Judy_array

http://judy.sourceforge.net/downloads/10minutes.htm

So perhaps I don't have to spend the time building an Erlang-SkipDB module just yet.Also see:

http://en.wikipedia.org/wiki/Judy_array

http://lambda-the-ultimate.org/node/741

http://www.erlang.se/workshop/2003/paper/p43-fritchie.pdf

The Judy array is a basic data structure that is available for use in Erlang's in-memory ETS databases.1. If you want to learn to program, take computer engineering. Computer science is not supposed to be teaching you to program (much), but more *about* programming, languages and theories behind them. Besides, one's (occupational or not) development normally benefits a great deal from learning to program from another perspective, and it makes you a better coder. Of course you are expected to be able to code. Do you think you can get into architecture if you cannot even draw?
2. Scientists research and develop. They push programming further. In the same way, medical scientists don't do artery bypass surgeries.
3. Real-world programming is simply labor task. What is there to learn? Once you know the procedure, it's all the same. Write, compile, debug. I picked up that before I turned eight. Dealing with the real world is easy. Any competent programmer can learn PHP in an hour. Now, developing algorithms is not. Understanding complexities is not. Implementing an encryption scheme is not. In the real world, you'd be done with a few library calls. You don't need to study computer science at all.I didn't miss anything, in fact I say on my post that you can't reopen classes in PHP.

My point was that in both frameworks you need to create additional files/code (in specific places), and they achieve the same functionality, extending the AppModel or ActiveRecord.

Remember that the article was saying "why PHP will never have a (real) Rails-like framework". This might be true, but the example provided there proves nothing.It's initializing stuff _you_ don't want, and _you_ don't need. But others do. Everybody uses different features - so as products mature the number of features can only increase.

So why do you call it bloated, instead of sluggish or slow? Bloated implies that the product would improve if the bloat is surgically removed. But this so called 'bloat' serves a purpose - which is why software packages keep getting bigger and bigger in the first place.It reminds me of those linux [sic] newbies who try a new
window manager every week or two...

If you just want to get your work done and can't stand Emacs or Vi, try [THE Hessling Editor](http://hessling-editor.sourceforge.net/), the current incarnation of the venerable [xedit](http://en.wikipedia.org/wiki/Xedit).  It's mature, free software, portable and has a powerful extension language.

I myself use Emacs (and [ratpoison](http://www.nongnu.org/ratpoison/) btw), but have used xedit on an IBM mainframe over fifteen years ago.A very nice collection of Ajax programming tutorialsA CS program is equal parts Math, Psychology, Sociology, Architecture, Engineering Research, and Training Academy.

It's not really science, unless you think mathematics is science, or figuring out how to build a better mousetrap.  One hopes that scientific techniques are employed, and not just duct tape and trial-and-error.One pixel-width lines with similar colors?  What a horrible graph.Clearly it only takes 10 lines of code and about 5 minutes to write any conceivable application or library in Lisp, so what is the point of open sourcing it?None of this languages is functional in the sense of a functional programming language. They only have some functional features.I agree. And the colors make it impossible to differentiate some languages from others.What's the terminating condition?Using a union to avoid padding was priceless: "The double is for alignment"

I can just imagine giving that particular bit of advice to a newbie programmer and then watch him try to debug his program ;)I took EE. Those who actually do make a difference, whether through developing products *OR* in academics are those who understand both the ultra-pure theory and can actually do the uglier real-world engineering.

An example of this is the observation that lone or small groups of programmers/theorists can have a much larger impact than solve by-committee groups. The requirement is that these small groups will need broader skills than just theory for this to be possible.
 
&gt; Any competent programmer can learn PHP in an hour. 

I shouldn't bite, but there's far more to systems and language design than syntax. Sticking to only what is extremely pure, as with CS, will hide you from all of that. CS cannot claim a monopoly on this knowledge, in fact, I wouldn't even say that a CS degree is the best way to learn it. Think of it like chess. Theory is great but gaining a strong ability in actually playing it gives entirely new insight.

BTW, I was never talking about school or degrees.


So, give me the chance to clarify these from my original post:

* occupational -&gt; engineering ability

* real world -&gt; engineering considerations

* creates a product -&gt; impact on either the academic or commercial world

and you will better see the topic I was originally trying to discuss. Such a temperamental attitude over "bad phrases" by the voters here will likely keep me out of discussion for the rest of the day though - absence of sleep from working on a project since yesterday morning will prevent my wording from taking the most obvious meaning.

*edit:*

Oh and I should say that I both enjoyed the article and sympathized with the author's helplessness as his incompetent co-workers wrecked his work-project.

To all of you voters who try to categorize my posts into "for" or "against" the article without acknowledging that I am simply going deeper into the topics - go break off your right mouse button.Scarab (http://scarab.tigris.org/). I can't recommend it, but it is something. The URLs are terrible, exporting information is kludgey and generally it isn't very extensible. With that said it does get the job done, although we are considering switching to Trac at some point.&gt; Java makes a copy of the parameters and passes a reference to those copies

No, it just passes the copies directly.  The fact that they're copies is what makes it "call by value", as opposed to "call by reference" where the bindings are *shared* with the caller.

A simple [bright-line test](http://en.wikipedia.org/wiki/Bright-line_rule) for whether a language supports call by reference is whether you can implement the following kind function:

    int a = 2, b = 3;
    swap(a, b);
    assert a == 3 &amp;&amp; b == 2;

In languages like Java and C that only support call-by-value, you simply can't do this.  In languages like C++ and PHP, on the other hand, you can declare the parameters as references types (`swap(int&amp; a, int&amp; b)` and `swap(&amp;$a, &amp;$b)`, respectively), making `swap`'s reassignments affect its caller.I thought it was because Ruby programmers were too busy posting about how their language of choice was so amazingly better.For those of you, like me, without a clue what this is, this looks to be a good starting place for understanding this class of software library: http://en.wikipedia.org/wiki/Computer_algebra_systemThere are 62 Haskell projects on Sourceforge.  If Erlang or Lisp is anything like that, they wouldn't even register as blips.

Also, there's significant peer pressure in the Haskell community to host your project as a Darcs repository on a webpage, and then Cabalize it for distribution.  Haskell projects tend to avoid Sourceforge because it only supports CVS and SVN.(By virtue of passing everything by value in the first place, of course.)I am not discussing how cool Ruby is upon PHP, but why the author's example is ridiculous. Both RoR and Cake have solutions to the same problem, and they are very similar. You get more with Ruby, but the author has not managed, based on his example, to explain why a RoR-style framework isn't possible in PHP.

And I fail to see how if I master the Ruby language it will make RoR's necessity to create additional code disappear.I agree that the author's example isn't very good. But it's true that a lot of Rails' elegance comes from Ruby's dynamic nature, which PHP cannot replicate.&gt; I agree that the author's example isn't very good.

Gee... thanks. It only did take me saying 3 times :)

I don't even like PHP or Cake anyway, I am coding in Python/Django lately...I never claimed it was a fantastic example.The haddock for them is poor, and I'm not sure why nobody's fixed it. [The paper](http://research.microsoft.com/~simonpj/Papers/strategies.ps.gz) on the other hand is quite thorough and tutorial-esque.

On the other hand, you usually don't need so much facility. Haskell provides a primitive called seq with the semantics that evaluating (seq x y) will force the evaluation of x (up to knowing the top-level constructor in x, called weak head normal form), before giving a result of y. This is all you really need to get strictness wherever you need it, but you can build more complicated ways of modifying evaluation order in terms of it. One example is in the Prelude as ($!), strict application:

    f $! x = seq x (f x)

another is in Data.List, called foldl', a strict left fold.

    foldl' f z []     = z
    foldl' f z (x:xs) = let y = f z x
                        in seq y (foldl' f y xs)

In newer GHCs, as dons showed, there are also bang-patterns, which simply express the idea that when a pattern is matched, the value it matches should be evaluated (to WHNF) before proceeding.

As an example of what you can do with seq, suppose we want to get a list such that evaluating any of it will evaluate all the elements.

    strictList []     = []
    strictList (x:xs) = x `seq` ys `seq` (x : ys)
        where ys = strictList xs

Another thing I'd like to point out is that lazy evaluation *does* give you control over evaluation order. Lazy evaluation *is* an evaluation order. Specifically, it's outermost-first with sharing of duplicated parameters. (As opposed to strict evaluation, which is innermost-first.) If you want something to happen first, move it to the outside of the expression.Short version: it can do algebra for you. Also, it will do your integrals! And more.No; it's actually because we were too busy porting all that Java code to Ruby. ;)&gt;&gt;None of this languages is functional in the sense of a &lt;b&gt;pure&lt;/b&gt; functional programming language.

There, I corrected it for you:)[deleted]When the number/size of the featureset starts causing problems, that's bloat.  All too often, the features are peripheral and could be plugins or different apps entirely.  I don't need a photo editor in my email client or a database server in my web browser.

Sure, it's *nice* to be able to touch up that photo before I send it to grandma, but I can do that just as easily in photoshop/gimp/mspaint and probably much more effectively.

There comes a time when you have to decide whether these shiny new features are worth the bloat.  For some users, perhaps.  For others, maybe not.  I tend to have a somewhat lower tolerance for feature creep, especially when it leads to bugs and slowness.  I don't begrudge *you* using your favourite features, though -- I'll just go and find an app that works the way I want it to.This is so hard to decipher when you're colorblind.I am really not interested anymore.Not sure how accurate an indicator of language use this is.  Ruby has it's own Rubyforge site, for example, so most Ruby projects host there, not on SourceForge.Seeing Java up there at the top makes me feel all dead inside.Sorry, I thought it was obvious. When the pointer you stored is the same as the iterator (after "compare it to that pointer").Correction: none of those *encourage* a functional style, though Ruby depends heavily on blocks, which are perverted higher-order functions.An interesting combination would be Scala XML capabilities with [Taconite](http://taconite.sourceforge.net/).  Just a thought.Long live GnuPlot!C# folks might appreciate this related article:

http://blogs.msdn.com/wesdyer/archive/2007/02/02/anonymous-recursion-in-c.aspxIts almost as hard to decipher when you're not color blind!For a reddit meme, this beats the pants off fizz buzz. :-)I know, Reddit hates .NET -- but these tips are good for just C/C++ too.&gt; Several Pylons developers continue to actively start, contribute, and maintain libraries that are completely separate from Pylons. That is my point about the developer communities.

Unfortunately, I don't have a whole lot of time outside my day job, which has me putting in 12-14 hour days on Django and Django-based applications. About all I have time for is the occasional bug report + patch to a third-party component I'm using. Does that mean I'm a recluse who doesn't want to be part of the larger software community? Because that's what you seem to be getting at here -- apparently, if I'm not out doing complete rewrites of other people's software, I'm somehow at fault.

&gt; That was written by someone for use in his WSGI-based framework, with syntax and some code from Django. It wasn't and isn't developed by a Django developer.

You asked, "Has the template language been extracted?" It has. If you want to start getting nit-picky, do it up-front.

&gt; But they can't; it's either hard or it just doesn't work right (at least that's what people were saying further up in this thread).

And that's what I keep calling BS on. Want another template system? Use it. Want another ORM? Use it. The only "hard" thing I can see people complaining about is losing stuff like `render_to_response` -- so a shortcut we provide is designed to use the stuff we ship by default. Is that evil?

Again, people out there are using Django with Cheetah templates. People are using Django with SQLAlchemy. People are using the Django admin on top of freakin' PHP applications. If you don't like Django, don't use Django -- as I said above, I'll use what I like and you'll use what you like, and everybody else can make their own choices. No skin off my back. But don't try to sit there and feed me, or anybody else, this load of BS, mmkay?Sure, if you want to totally go against the idioms of those languages and if you don't mind blowing up your stack (no tail recursion), and rewriting all the libraries that use mutable state.  In other words, if you live in the Turing tar pit.The most unexpected factoid here (for me) is that, as late as 2001, C and C++ together still commanded more than 50% of projects, with Java far behind and Perl never in their league.

In the enterprise waters where I swim, Oracle's presence is felt heavily and most collaborative projects use Java with Perl and a GUI layer (Forms/Developer for the unlucky, VB or Delphi for the rest of us).I've used bugzilla, and am now using [RT](http://bestpractical.com/rt/).

RT is definitely a fuller product than bugzilla, but it requires more in the way of configuration and training to get started. Likewise, it's easy to not use it to its fullest potential.Are you keeping a list of stored pointers?

If not (the new stored pointer replaces the old stored pointer), consider a list where the loop is greater than ten elements long; the iterating pointer will never catch up to the stored pointer.

If so (you have a list of stored pointers), you need to traverse that list every time you iterate the main pointer.  You need another, inner loop for that, plus storage space.  Compared to Floyd's cycle-finding algorithm, you've just gone from O(1) space to O(n) space and O(n) complexity to O(n^2) complexity.Yeah, Sourceforge is somewhat of a dinosaur, before the time of many people, a lot of developers aren't as attached to it anymore (it does have some warts (ads, crappy mailing lists, painful download process, etc.)).  Project management also isn't as hard as it used to be and more alternatives are available, so a lot of newer projects are moving away from Sourceforge (to Google Code, Ruby projects on Rubyforge as you said, a lot of Python projects have moved to use Trac, etc.)This says much more about who uses SourceForge than about programming language usage in general.  Most projects using Perl, Python and Ruby just don't use SourceForge.  Perl projects are distributed through CPAN, Python projects are distributed through the Cheeseshop and Ruby projects are almost exclusively distributed through RubyForge.
I can't speak about other languages.Ah, exactly right. Crud.

Ok, so instead of 10, use 100. (But what if the loop is more than 100? Ok, use 200.)

Kidding, kidding. Ok, now the tortoise and hare approach makes sense.

EDIT: Hey, what if instead of a constant value (10) to wait until storing off a new pointer to compare, you used a value that doubled every time?

I'm actually not sure if this method is any better than the tortoise and hare approach, though. But it would work, wouldn't it?Also, it will drive you crazy learning how to use it!  While it's powerful, it's not easy to use for anything but trivial tasks.[Fogbugz](http://www.fogcreek.com/FogBugz/) at work, but i'm not a fan. It's not very user friendly, but does claim to provide a bunch of features we don't use.
[deleted]Most of its increased share came from C and C++.  At least Java has GC.There's a whole heckuva lot more software out there nowadays.

That means that even if software is improving in quality (which i believe to be the case), it can be difficult to notice. If one app crashed ten times a week back in the day, but now we have ten apps crashing five times a week, the overall number of crashes someone notices will be much higher, despite the improvement per-app.

Then you have the shift towards web-based apps, which are all-too-frequently developed with less stringent defect requirements than shrinkwrapped software. So TypePad went down yesterday. And LiveJournal drops off occasionally. And so on. But those web apps nowadays are still of higher quality than they were two or three years ago on a per-app basis. We're just using far more of them far more often than we used to.

So, no, i don't think the quality of software is going down. On the contrary, i believe it's improving. It's possible that people's expectations are going down, though i can't say i know that one way or the other.&gt;The Docucolor isn't a cheap ass laser printer, it's a digital press. 

Fine, that Docucolor should have tracking codes.  But why does my crappy HP All-Is-Broken need them when it can't even counterfeit Monolopy money that would fool people?Python:

     Y = ( lambda func:
          ( lambda f: func(lambda x: (f(f))(x)) ) ( lambda f: func(lambda x: (f(f))(x)) )
         )&gt; there's far more to systems and language design than syntax

Absolutely. That's what Computer Science is. It's different from day-to-day programming, because you don't need to study language design to do contract jobs.

Yes, there is a value in having real-world experience, but there is very little you need to know to actually get a job. Freelancing in my freshman year, I get to see ugly code all the time. From real-world programmers. With degrees. They write 20 lines of nonsensical PHP to get something done that could have been implemented in two statements. They copy and paste. They leave no less than a few dozen security holes in production-level applications. Does it matter? To me, a computer engineering student and a computer science enthusiast and someone who's been assigned to fix a bug in that code, a lot. To the employers/contractors/real world, ***no***, as long as it works. That is the engineering ability. That is as far from what Computer Science is about as possible.

Rather than chess, I'd think of it like math. 95% theory, 5% practice. Practicing multiplication plays little role in making you a good mathematician. Knowing theories (i.e. how numbers cancel one another out) will of course make you better at real-world calculations, but it's hardly relevant when you have a calculator at hand (analogy to big machines, heaps of memory and idle CPU cycles at your disposal).

To conclude:

1. Computer Science study is aimed to produce scientists. Sure, with little polishment, computer scientists will make *excellent* programmers, but that's not what they are meant to do.
2. I doubt real-world experience plays an important role, if any, in making you a better scientist. Maybe you can cite an example?
3. And as I stated above, even if it does, to people who have the intelligence to comprehend computer science, it should be ridiculously trivial.

Also, modded up for an interesting discussion. With my left mouse button.a clearer step by step explanation than any of the other sources.&gt;The advantage is that users only need look at the same usual place for help, whatever error they see.

Have you ever tried actually *using* the help system on VMS? Bleurgh! It's all but impossible to find what you're looking for.It was a pain, but at least the graph's legend is in order w.r.t. the order the lines are on the right-hand side of the graph.Add http://www.nothings.org/computer/judy/ to that.

I'm quite scared by the fact this library is 20kloc. Still, it looks like it does have some genuine advantages.Squeak and Seaside.  See www.squeak.org and www.seaside.st.[removed][deleted]since there are a lot of valid points that Sourceforge is not a broad example, look at the reference in the article to:
&gt;More than a Gigabuck: Estimating GNU/Linux's Size, see Section 3.3 (Total Counts by Languages) 




they're in order (highest line at the end/right) listed first.Not really. In business context the cost usually transfers to the succeeding programmer. As in, half of a programmer's problems originate from another programmer.It would be interesting to see an ETS implementation "skiplist" added to the charts. And 20kloc... whoa... that is indeed quite scary.According to the graph, popularity in Pascal is dropping rather rapidly, and Assembly and Visual Basic seem to be gaining momentum.&gt;A simple bright-line test for whether a language supports call by reference is whether you can implement the following kind function:

&gt;int a = 2, b = 3;
&gt;swap(a, b);
&gt;assert a == 3 &amp;&amp; b == 2;

&gt;In languages like Java and C that only support call-by-value, you simply can't do this.

Seems to work just fine in C:

    void swap(int *a, int *b) {
        int c;
        c = *a;
        *a = *b;
        *b = c;
    }
    
    int main() {
        int a = 2, b = 3;
        swap(&amp;a,&amp;b);
        assert(a == 3 &amp;&amp; b == 2);
        return 0;
    }RubyForge is the only one that compares: "distributing" is not the same as "providing a collaborative medium to do software development." RubyForge and Sourceforge host projects; the Cheeseshop and CPAN are just distribution mechanisms.
Amen! I looked up purple and though "*Assembly* is on the *rise*?!"JIRA seems to be really popular.  Unfortunately, it's out of our budget range.  I concur about Bugzilla being awful -- that's largely why we're looking at the other alternatives.Poor C. im hoping for a comeback!I think that was C that was dropping, but it's hard to tell with so many "blues"  I think projects in C would be dropping on sourceforge simply because generally you only program stuff in C once.  the linux kernal is C, everything else is built upon that, generally in higher level languages.I wish all my wife's higher-order functions were perverted.I've used the whole spectrum -- Trac, Mantis, RT, Scarab. They all suck in various ways. In the end, we settled on bugzilla. It's a pain to administer, but to the end user, it just works. That's why we went for a hosted solution. We pay someone else a pittance each month so we don't have to manage it. They take the pain, and we just get the end user benefits.

So for me, bugzilla takes the win.If you look at the trend lines, Perl is losing market share, Python is stagnant, and PHP is gaining.

From that I assume, Perl CGI apps are moving to PHP, but Perl is still keeping Python at bay as Unix's preferred scripting language.[removed]Half the problems and 100% of the preexisting functionality.&gt; There are those who consider that studies in harmony, counterpoint,
&gt; and fugue are the exclusive province of the intended composer. But if
&gt; we reflect that theory must follow practice, rarely preceding it
&gt; except by chance, we must realize that musical theory is not a set of
&gt; directions for composing music. It is rather the collected and
&gt; systematized deductions gathered by observing the practice of
&gt; composers over a long time, and it attempts to set forth what is or
&gt; has been their common practice. It tells not how music will be written
&gt; in the future, but how music has been written in the past.
&gt; 
&gt; The results of such a definition of the true nature of musical theory
&gt; are many and important. First of all, it is clear that this knowledge
&gt; is indispensable to musicians in all fields of the art, whether they
&gt; be composers, performers, conductors, critics, teachers, or
&gt; musicologists. Indeed, a secure grounding in theory is even more a
&gt; necessity to the musical scholar than to the composer, since it forms
&gt; the basis for any intelligent appraisal of the individual styles of
&gt; the past or present.
&gt; 
&gt; On the other hand, the person gifted for creative musical composition
&gt; is taking a serious risk in assuming that this genius is great enough
&gt; to get along without a deep knowledge of the common practice of
&gt; composers. Mastery of the technical or theoretical aspects of music
&gt; should be carried out by him as a life's work, running parallel to his
&gt; creative activity but quite separate from it. In the one he is
&gt; following common practice, while in the other he is responsible solely
&gt; to the dictates of his own personal tastes and urge for expression.

Walter Piston (Intro to the first edition of *Harmony*)

I think the same principles strongly apply to programmers and cs theory. I suspect it would be better to call it Computational Math or Theory; remember that math *is* philosophical in nature.http://daringfireball.net/2007/01/os_x

Also, OS X isn't a true micro-kernel. Mach was a microkernel experiment, but in the end became a sort of medium sized kernel.What, a whole 10 lines? Haven't you discovered the `cl-write-application` function yet?It's really into child classes?What an unusable graph.  I had to dig out color meter to differentiate the plots.

edit

Never mind.  They are ordered o_OI use, and _like_ Bugzilla. Quite keyboard friendly for a web app, and integrates well with web browsers using simple mechanisms like bookmarks (and keyword bookmarks). It's also pretty easy to manage since you can leverage existing Apache skills.

I've heard hundreds of "Bugzilla is teh sux0rz!" comments over the years, but I rarely see a corresponding list of faults. Things that are ugly (and Bugzilla is downright fugly) rarely win many hearts.Because this is SourceForge. It's not reliable at all, and they're not counting the other project-hosting websites and all the private projects or self-hosted all over the internet. I write in C++ and Lua for work, it's not counted at all but it's still a part of a big project which pays my food.Yeah, well, the newer libraries tend to be better documented, strangely enough, and things are improving -- there's a project to improve the Haddock documentation for the Hierarchical libraries.

Data.Map and Data.Set are two examples where documentation is already really good.

It's true that learning some of this stuff is easier if you're not afraid to look at the papers written about it though.Keep in mind that this is a survey of a *really* narrow segment of the programming population.  If you took a survey of OS support for these same projects you'd see what I mean.

I'm not sure that this has any useful meaning at all really.  We don't even know whether he was counting *all* projects or just projects with significant activity.  Personally I'd only count projects with releases, since there are so many 'projects' that get created on SourceForge and then go exactly nowhere.

There's three main kinds of codebases in the world; open source, commercial, and internal.  The vast majority of programmers work on internal stuff with between 2-200 users.

Most (90%+) commercial/shrinkwrap apps and games are written in C++.

Most (70%+) internal corporate apps are written in VB, it used to be 80% but the tide is shifting toward C#.
Yeah, but most python projects nowadays are pretty much expected to host their own trac, which does compare to rubyforge/sourceforge.At least they did something right for once and not tried to get rid of OpenIDWriting code *is* the process of designing.  Anyone who thinks coding is simply implementing doesn't really grok software development.

People often use an analogy of house construction when talking about software development.  This analogy is flawed because we are actually 'building' the house when we press F5, the rest of the time we are designing it.  We have this tremendous advantage over house builders and architects in that the actual construction process for us is effectively free and instant.

Mine's better, but I need to update it, as it's getting a bit out of date:

http://dedasys.com/articles/language_popularity.htmlshame, I was hoping for an actual interview between a parrot and a cardinal.I'm not quite sure if I'm following your argument. Certainly RAII is useless for an object whose lifetime could exceed the current scope, but much of your argument seems to imply that you believe RAII is unsuitable in general for any form of resource management. Could you clarify?

My general take is that RAII is a good technique when the lifetime of an object is clearly bounded, and otherwise other methods (GC, smart pointers, etc) must be used. At this point I'm not sure if we agree or disagree. :)

That C++ programmers tend to structure code to make maximum use of RAII is a very good observation. I'm not sure if that is necessarily a bad thing, exactly; but it does point out the serious deficiencies C++ has with regards to memory management.
It's the same.  Java, VB, and C# also use call-by-object.

I've also heard it called "call-by-value-reference" or your "call by value semantics for references", but you have to admit that call-by-object is a lot more succinct, and does a reasonably good job describing what's actually going on.Well, one can extend it using rexx, so it's as extensible as it gets.  But it surely doesn't have the display capabilities of Emacs.

As I said in another thread, I myself use Emacs extensively, but I used xedit on an IBM mainframe.  Most of the applications on that system run inside xedit.  I specifically remember "dir" (like Emacs' Dired),the E-mail client and the help system.  I also remember someone having implemented an environment for editing TeX for xedit (called "TeXshell" IIRC).

I suggested it as an alternative to Emacs or Vim because it's free, portable and extensible.  It's also lightweight compared to Emacs and doesnot suffer of this mode madness, like vi offsprings.  It's something in between, a powerful, respectable editor for programmers.Work: we have FogBugz but nobody uses it.  I end up using paper &amp; pencil most of the time.

Volunteer: we started with Mantis, didn't really like it, and last I heard were thinking of getting Trac.  Usually, the modus operandi was "If someone feels like fixing it when it's first reported, we do.  Otherwise, it never gets fixed."

Home: I program in Haskell.  I don't have bugs. ;-) I also don't have *users*, but oh well...

I've also reported bugs to some Apache projects that use JIRA, and really liked it.  It's about the only commonly-used bugtracker that I'd actually use, let alone pay for.I'm assuming the [contest](http://www.international-lisp-conference.org/2007/~111b2bf3b58f3f3bcc9f5e7~/contest) is just for people attending the conference, not the general public, right?

&gt;5) In order to qualify for a prize, the entrant must register for at least one day of the conference. Prize-giving might be on Monday April 2nd, but that's not a firm date yet.

&gt;9) Team entries will be allowed. Please indicate with your entry if this is a team effort, and name the particpants. At least one member of the team must register for at least one day of the conference.Discussions elsewhere on reddit: [1](http://programming.reddit.com/info/u719/comments) [2](http://www.cs.utexas.edu/~akkartik/feed.cgi?felleisen90expressive.pdf)I believe the semantics are the same in Java and Python, Java-people just use a more confusing terminology :-)

C# argument passing is much more complicated though, since you have both pass-by-ref and pass-by-value, and you have the possibility of mutable value types. This leads to 4 semantically different way of passing arguments:

 -    pass by value for reference types
 -    pass by reference for reference types
 -    pass by value for value types
 -    pass by reference for value types

Oh, and C# supports two different modes of pass by reference, "out" (where a new value is always assigned in the method) and "ref" (where the value is not necessarily modified).

I think Pythons 'pass by object' everywhere is a lot simpler to grasp. You just have to understand the difference between assigning a variable and invoking a mutating method on an object, thats it.I'm not so sure about that. Lets say we did use call-by-object in the general case. Then what would you then call C#/VB's by reference object parameters (ref/out/ByRef)?

That said, I do think using by-reference and by-value for both value and reference types is a bad idea. But it has been that way for so long that I think it will be hard to change.
This is actually pretty insightful.  I know of a lot of people that program in lisp that only do it because they think it makes them cool.  "Ohh, you're using that out-of-date-loser-language C to write Linux?  I'm writing a program to reverse a string in LISP.  Go away -- genius at work."

I'm actually not making this up :)SAGE provides a nice Python interface to Maxima and many other math packages. It might be a little more intuitive for programmers.

http://modular.math.washington.edu/sage/I disagree.  You're doing something seriously wrong if you have to look at decompiled C++ to find bugs.  (Bugs in the compiler maybe, but that shouldn't be your problem.)This is the sort of data set that a stacked percentage graph is perfect for.  There are a bunch of data sets that desperately need this kind of graph to make good sense of, and people don't use it nearly enough.

([Example](http://www.swiftchart.com/stackedlinepct_ex1.jpg))There are some patterns that really work well with by reference semantics, though I think closures may cover some of them.

In .NET, you have a TryParse pattern. For example...
    bool TryParse(string source, out int newValue)
    
    int value;
    if (Integer.TryParse("5", value) 
        { //do somthing }
    else
        { //do somthing else}


What would be the Python way to do this? (Ruby folks, same question.)
I was specifically talking about HELP/MESSAGE, which gives help about the last error the user saw, and HELP/MESSAGE/STATUS, which gives help about an error code.

The online help. Yes I have used it, quite a lot. It is a reference. It is indeed overwhelming when you first see it, almost the same as doing "ls /usr/man/man?" under Unix and using the resulting list as a starting point. If I remember correctly, it was not searchable. Usability from another age... You need to know your way, missteps have you start again from scratch.

However. That was only a small, ugly, sharp rock on the top of the moutain that is the VMS documentation. There were user manuals, programmer manuals, administrator manuals, manuals for each compiler, for the debugger, for each library. All fully indexed books, dozens of books. There was an overall index that was split over two or three books. The price was so high, the shelves were tossed in the package, err, on the pallet, for free. Nah, just joking. But that was where you learned VMS, not from the online help. That's also what defined good documentation for me."Comput_ing_ Science" is my preferred term. But "Theory of Computation" is fine.Where's Fortran? And Brainfuck? Whitespace? Intercal?

MY FAVORITE LANGUAGES ARE MISSING.&gt; Then what would you then call C#/VB's by reference object parameters (ref/out/ByRef)?

Call-by-reference.

Take a look at the [C2 classification](http://c2.com/cgi/wiki?ParameterPassing).  That's as precise as you're likely to see, and matches the terminology in most textbooks.Actually bugs in the compiler are our problem.  We found one last week -- exception in STL for VS 2003 is not in the std namespace.  We had no choice but to move it because it ended up creating a managed exception class in the root namespace (and then our assemblies couldn't be used in VB.NET code where System.Exception was used without the fully qualified name -- because VB.NET is case insensitive).Back when I used to help people learn VB, the terms I used were roughly:

1. pass by value semantics for values
2. pass by value semantics for references
3. pass by reference semantics for values
4. pass by reference semantics for references

You pretty much had to describe them that way because the syntax of VB reads that way.

    ByRef param As Object
&gt; perl supports tail recursion

[Evidence](http://www.google.com/search?q=perl+%22tail+recursion%22)?

Several links say otherwise, and there's a Perl6 RFC that seems dead, which combined with the fact that I haven't heard anyone talk about it with Perl6, leads me to think Perl6 won't have it either.Also, I have third party unmanaged code without source.  If you try to turn a pointer to function automatically into a delegate, you have to have STDCALL calling conventions.  I learned this by reading the assembly of the call through the delegate.  I don't have to read assembly often, but the view is there for a reason -- people find it useful.  I also need to do it to check that compiler optimizations I might be counting on were done -- like when I divide by 2 and expect the compiler to implement it with shifting -- we need all the speed we can get).Maxima was written for programmers - it's a programming language (a DSL), as much as it is anything.   The "problem", I believe, is that to use it effectively you need to learn to think about mathematical expressions the way it does.  What I particularly struggled with was learning to simplify expressions effectively.  I don't see how adding a Python interface could possibly help with that.  But, thanks for the tip - I'll check it out.Looks like boilerplate to me.I’d like to know whether the graph counts only stable software, or all the 90% "-1, pre-planning" sf.net projects…Never mind those.  Where's Logo?Gotta love this one: 
&gt; 17.3: What's the best style for code layout in C? - There are many systems of indentation advocated, but all of them have the same basic flaw; they will mislead the reader when the actual code logic does not follow the indentation. It is better to avoid indentation entirely, so the reader will not be misled.We try, we try. :) O'Caml is nice though, but "tiny" for me is &lt; 10 lines :)Python could return a tuple:

    success, value = tryParse("5")
    if success:
        # do somthing with value
    else:
        # do something elseFrom the article:

    function hasSupportForXUL() {
      if(navigator.userAgent &amp;&amp; navigator.userAgent.indexOf("Gecko") != -1) 
        return true;
      else
        return false;    
    }

Why not simply:

    function hasSupportForXUL() {
      return navigator.userAgent &amp;&amp; navigator.userAgent.indexOf("Gecko") != -1;
    }

Nice article, though.So we have three classifications for the four (or six if you count C#'s out) different semantics:

1. call by value [ByVal]
2. call by object [ByVal]
3. call by reference (for value types) [ref, out/ByRef]
4. call by reference (for reference types) [ref, out/ByRef]

It still doesn't seem like an ideal situation to me, and VB users lose the ByVal memonic aid.
Because it confounds allocation/deallocation of objects with allocation/deallocation of resources based on the semantics of 1 particular style of object allocation (on the stack).

These concerns should be separate and made explicit through orthogonal language features.

What you want is "dynamic extent" resource management.  RAII in C++ is a *hack* to achieve this.  In high-level languages, other ways are provided, generally.

Common Lisp calls it unwind-protect, Java calls it try...finally, and Haskell calls it finally (or bracket).
All of these operators allow the execution of some code when the dynamic extent of the form is exited.  Languages also typically *guarantee* execution of said code.

There is a paper somewhere by Peyton-Jones (I think) which details the operational semantics behind "finally" (Haskell) in the presence of asynchronous interrupts.
I tried to get something like that going, but the stupid birds kept fighting over the sunflower seeds and refused to answer my questions.Is there any way to move that into the if line? 

(Even if there isn't, I have to say I like how clean that syntax is.)&gt; There's nothing preventing customers from demanding accountability from the software publishers.

Which publisher?

The vast majority of software problems most people see are not the fault of any one publisher, but rather the complex interaction of multiple applications from multiple vendors.

Knowing which one to hold accountable is really hard.It's actually more like 5 classifications for about 16 different semantics - you forgot "where is the parameter evaluated?" (by-value vs. by-name) and "how many times?" (by-name vs. by-need).  And all their combinations in their cartesian-product glory.

Things aren't named so that they cover every possibility.  They're named so you have a way of referring to common cases and making useful distinctions.  For parameter-passing, each named style gives you an additional invariant to work with:

1. Going from call-by-name to call-by-need or call-by-reference ensures that each argument expression is evaluated only once per function call.
2. Going from call-by-reference to call-by-object ensures that individual bindings within the caller cannot be modified by the callee.
3. Going from call-by-object to call-by-value ensures that individual *values* within the caller will not be modified by the callee.

Going from call-by-reference of references to values doesn't give you any additional invariants: if the binding can be changed by the callee, you can't make any assumptions about its contents.  That's why I'd consider both cases to be call-by-reference.You've heard of "tying the knot"...I'm not so sure about tail recursion in Perl either, in fact `use strict` gives you warnings after 40 or 50 recursions.

On the other hand, *Higher Order Perl* is an awesome book. So much so, in fact, that I ended up learning Common Lisp...[removed]I can think of at least two other sites that host higher-quality projects overall that would be better indicators of language usage, but I am hesitant to mention them here.

*cough* tigris *cough* savannah *cough* 

(I'm sure RubyForge is yet another, but I am not 1337 enough yet to be perusing Ruby projects. Soon, hopefully.)
&gt; I've thought that if you were able to sue the authors over lost productivity/lost work due to bugs, then that might change things.

You can't make it the programmer's fault. It has to be the manager/employer/corporation's fault, so that it is worth it to them to pay for the best.Beware that there is not actually a tryParse-function in python, it was just an example of how to return more than one value. The pythonic way of parsing strings to numbers is just to use try-except:

    try:
        value = int("5")
        # do something with value
    except ValueError:
        # do something elseI don't know from Smalltalk but my former manager, an Oreilly author and former Smalltalker, likened my approach to MVC for the Web to Smalltalk.  What category does that put ME into?&gt;No; it's actually because we were too busy porting all that Java code to Ruby. ;)

Out of the frying pan and in to the fire.-1 amusing only to author. A waste of time.Oh I belive you.

Compare the stuff LISP programmers brag about vs the stuff Java/.NET programmers brag about.

With LISP you never hear things like, "I can create a web page with a data-bound grid complete with insert/edit/delete, sorting, and paging without any procedural code. Oh, and it supports internationalization via resource files and styles via both CSS and templates."

Its like that APL one-liner that supposedly contained the game of Life. For a Java or .NET programmer, that only solved the easiest part. The harder parts including the basic GUI, the installer package, saving/loading starting positions and other user settings, sharing starting positions with other people.

In short, a lot of these languages seem to focus too much on solving problems that are already solved. And while they clearly are better solutions, they don't solve things people really care about.
That is the 'old way' of doing it in .NET as well.

I say old way because throwing exceptions in .NET is very expensive. Whenever possible, they recommend avoiding using them for control flow.
Maybe you should be a Smalltalker.  There is nothing better to learn and understand objects.  Spend 10 mins just in the debugger and you will never want to program in anything else.I'm not sure why RTP, NC isn't in his "startup hubs" list but it probably should be...By the way: Last weekend there was a pre-release of 1.5: http://blog.lighttpd.net/articles/2007/02/03/pre-release-lighttpd-1-5-0-r1593-tar-gz
reason not to work for a startup outside a startup hub:
&gt;2. Salaries are Lower
&gt;Although closely related to the cost of living being lower, salaries for experienced and degreed computer scientists and engineers are much lower in Western Mass than in Boston.I wasn't critiquing the code for its merit (which you do a good job of) but for returning true or false based on the outcome of an if statement. I do really like your example of the "do this" syntax, though, very neat.I am running Firefox 1.0.4 (Debian stable).  The XUL tree control is *visible* in this article, but I can't close any of the folders on it.

So as advocacy for XUL in web apps, the article is kinda self-defeating....[removed][deleted]Yeah, bring in some lawyers to the game. This surely couldn't go wrong.
It is much easier to write almost any program of size in Lisp or Haskell than BASIC. In fact I would say that you almost have to be a much more "skilled" programmer to write a complex program in BASIC rather than in Lisp or Haskell.&gt; Seems to work just fine in C: [...]

No, you changed the test code to pass addresses instead, and the swap function to use dereferencing instead.  You're passing references by value, not values by reference.

I think you've made it clear that although you understand this distinction, you still personally consider them both to be "call by reference", despite the fact that this contradicts standard usage by the rest of the programming/language design community.

Until you agree to the same definitions of "by value" and "by reference" as everyone else, there's little point in trying to discuss them.

(Aside: [C# is not call-by-copy-restore](http://cs.nyu.edu/rgrimm/one.world/csharp/response2.html).  I can't confirm for D:  the manual doesn't seem to mention it.  I'm not sure what you mean with "call-by-copy":  the only references i can find use it as a synonym for call-by-copy-restore.)What I mean is, how are the two code snippets different other than one has 3 more lines of code than the other without arguably becoming any more clear? (Whether the correct implementation of the function is achieved is another matter.)Well yes, BASIC is a toy language, but it'd be easier to teach a non-programming person BASIC than Lisp or Haskell.The point is that cost of living is lower, so you don't need as high a salary.  A nice apartment will rent for less than $1000/mo.  A nice 3 bedroom house for less than $300,000.  This benefits the employer and employees.[deleted]What a poor article. Some Java Joe guy discovers Lisp, concludes that its all about linked lists and recursion. Doesn't use proper indentation, coding style, naming conventions or idioms.I think it would be every bit as hard to teach someone how to write something as simple as a "loop from 1 to 10, printing the numbers" in BASIC as it would be in Lisp. GOTO, GOSUB, explicit line numbers, etc, are complications.

And as far as a "toy" language, I'd assert that more computers have shipped with BASIC included than Lisp; that more games have been sold programmed in BASIC than in Lisp; etc. ;] (Heck, just counting Oregon Trail alone...)

I agree with you on Haskell, but Lisp is certainly not harder than BASIC.Who's to judge whether or not the author is at fault? Another incompetent programmer?

Back when I worked for IBM, the software was *full* of bugs - as in, one per LOC, but these were easily attributed to "user error", since, the user doesn't know any better. I distinctly recall one such incident where two clients were asking for directly contradicting requirements at the most fundamental level. The "art" of convincing both clients that they had what they wanted was a true eye-opener for me. I honestly couldn't believe "we" (I objected, but was told to get back in my box of course) succeeded in convincing one client that they had a black car and the other client a white car, when it was bleedingly obvious that they both had the same red car.

What needs to happen is a revolution in the industry, the downfall of places like IBM Software Group (wishful thinking?), who continually suppress the expression of programming skill to protect the corporation's profits (due to incompetent management, and yaddy ya (insert philosophy here)) and basically, keeping the entry-level bar, and client expectation as low as possible.

As you, and other Haskellers would say, "just write the damn code". Sooner or later, there will be no choice but to abandon this constraining dogma.Mainstream programmers are no strangers to solving non-problems either. Look at the mass of tools and frameworks which only exist to paint over Java's warts (Spring, AOP, 90% of the features in a Java IDE, etc). Or the various needless layers of indirection (meta-logging, useless wrappers around DOM/SAX which add nothing). Or the huge duplication of effort between all the hundreds of Java web framework projects, despite the fact that very few offer any real advantages over the others. Most mainstream programmers are not really doing anything relevant at all.Its not insightful at all. Winterman thinks that a language needs artificial limitations and intentional design flaws, otherwise its users and designers are just arrogrant pricks. How is that insightful?(loop for i from 1 to 10 do (print i))

    forM_ [1..10] print

Oregon Trail was awesome back in the day...
But he has an obnoxious analogy!  Vote it up!forM_ hides a lot of detail. Namely you can put a loop form anywhere in Lisp code, but `forM_ [1..10] print` actually yields an `IO ()`.Prepare to be assimilated.The author of the page is confused. The computers in the movie were Silicon Graphics. He thinks they were Crays, probably because there were Crays in the book. (Even if they were Crays, they would probably be running some kind of Unix.) I don't remember if the prompt shown was "C:" or something else, but yes, it seemed bogus. The 3D file manager shown is a real Irix application though.Here is what an intro to Java article would look like if it was written in the same style as these Lisp articles we're seeing:

- Start the article with the following punchline: "Fundamentally, Java is a language of arrays. Everything in Java, from data to the code making up your application, is an array"

- Anecdote about how hard the author found Java in undergrad, and how it took considerable effort to adjust their frat boy drunken mind to the extraordinarily heavy intellectual demands of first-year calculus and data structures

- Use static methods only

- Uses `this_naming_convention` for functions and camelCase for constants

- Never define any classes with fields or instances. Instead, just pass around arrays of Object[]

- Never, ever use any library features at all. This means strings are char[] not String. Want to concatenate to strings? The + operator is out of bounds. Write a loop.

- Refer to the "Java interpreter" and run everything in BeanShell

- Introduce interfaces as an advanced concept at the very end, and miss the point completely by giving an example which defines an interface only containing integer constantsWould you retards knock it off and just submit links normally? All you're doing is screwing up your own submission.The loop yields () so it's not that far off.

The Haskell is not a statement, but rather an expression denoting an action which can be used to print the numbers from 1 to 10.  Functions which can enact that will expect type IO () (ex. the main function).  It's also nice in that you can treat this action just like any other value, compose it with others or pass it around, etc.
Also - have you ever been to the Pioneer Valley?  I'd take a pay cut to work there just because the quality of life is much nicer.  It's a *gorgeous* area, you have free cultural events courtesy of the colleges in the area (and the towns too - there're often fairs and carnivals on the Amherst town common), and you don't have to deal with the stress and bustle of modern city life.

And as everyone's already mentioned, the cost of living is much lower.  In Cambridge, it's about $700/month to rent an apartment *with roommates*.  That price will get you a decent-sized 1BR without roommates in Amherst or Chicopee.  Studio apartments run from $900-1000/month in Somerville, that price will get a 3BR in Western MA.It's not like we're in the boonies (a common misconception).  One of the key points I made was that UMASS is literally next door and produces extremely good engineering talent with over 25,000 students.  That's a critical ingredient.  Many of these graduates would prefer to remain in the area, but can't due to lack of jobs with many ending up in Boston or elsewhere.  We are providing a handful of these jobs for those students who wish to remain here.  Regarding VC, you actually have to drive farther west to visit Bo Peabody's Village Ventures, with their headquarters in Williamstown, MA.Will Microsoft support OpenID the way they supported HTML when they discovered the web?I got that. Why is it there, and the only picture? And why does he have the text keep going on the right side of it?It makes me a bit sad that Python hasn't gone up much at all. And PL/SQL &gt; Ruby? Merh?

Oh and PS, the graph does sort of suck.The vast majority of software problems are caused by people proud of their ignorance, who strive to achieve the minimum necessary, like you.&gt;You're passing references by value, not values by reference.

Sounds like your dancing just like wicked now.  This is humorous to say the least.

&gt;despite the fact that this contradicts standard usage by the rest of the programming/language design community.

The same community that can't even get the terms straight?

&gt;Until you agree to the same definitions of "by value" and "by reference" as everyone else

But not values by reference or references by value :)  You're killing me.

&gt;there's little point in trying to discuss them.

Yeah, its been fun.  But enough is enough.

&gt;Aside: C# is not call-by-copy-restore.

Wow, I think you did find something correct.Quite an interesting article, though I do wonder how feasible using it would really be in a somewhat complex application. The idea is pretty exciting though! XUL is shiny and neat, but locking out 70% of users isn't.A colleague blogged about this compiler bug he figured out reading IL
http://atalasoft.com/cs/blogs/stevehawley/archive/2006/12/11/11184.aspx&gt; This is so hard to decipher when you're colorblind.

This was posted above but the order of the items in the key corresponds to where the lines end on the rightmost part of the graph.

So, for example, the language that starts out as #1, and then curves down to number #3 by the end of the graph is C, the #3 entry in the key.
&gt; You simply can’t imagine problems that your tools can’t 
&gt; solve well, much less can’t solve at all.

Interesting take on the [Sapir-Whorf hypothesis](http://en.wikipedia.org/wiki/Sapir-Whorf_hypothesis) there.It's boilerplate only because it's a silly example.  In a realistic Python class, you'd just assign to self.name and let it be public.

When you want to replace that self.name instance variable with a computed field, *then* you use this trick.  It'd be more like:

    def name():
        doc = “The name property.”
        def fget(self):
            # Some big long computation
            return big_long_computation_result
        def fset(self, value):
            # Some big long computation
            self._name = value
        def fdel(self):
            # Some big long computation
            del self._name
        return locals()
    name = property(**name())

This idiom's trying to answer the question "Where does the big long computation go?", and provides an elegant mechanism to avoid namespace pollution.  If it's *not* a big long computation, or if you want to expose it publicly anyway, then there's a really simple solution:

    name = property(fget=quickie_function_of_self,
                    fset=quickie_mutating_function,
                    fdel=quickie_deletion_function)

But if you want a long multiline computation, it has to be defined with `def` (because there are no multiline lambdas in Python), and it has to be defined *somewhere*.  This idiom makes it a local closure in a function that disappears as soon as the attributes have been assigned to a property.[deleted]&gt; The same community that can't even get the terms straight?

The same community that defined the terms to begin with, as well as the languages they apply to. :)Yeah, it's annoying, particularly since this (dictionary of functions) is one instance where multi-line lambdas are perfectly readable.  I can see the logic behind it though: the second example would need to be

    name = property(fget=lambda self:
                        # Big long computation
                        return ret_value
                    fset=lambda self, var:
                        # Big long computation
                        # over multi lines
                    fdel=lambda self:
                        # More big long function
                        # over multi lines
                    )

And that last paren would probably need to be exactly lined up to avoid throwing off the parser.  (I say "probably" because the present Python indentation algorithm won't work at all here; newlines and indentation are ignored within parentheses.)  Personally, I think the locals() idiom is more readable anyway.

Incidentally, the above example is exactly how it'd be written in Scheme, modulo syntax.  It's not as big an issue there because there're no indentation rules.  So it's a tradeoff: gain whitespace-sensitivity, lose the ability to represent certain constructs without a bunch of hassle.Will Microsoft support OpenID the way they supported HTML when they discovered the web?A normal 'higher-order' one can blend in with regular function calls seemlessly.  Ruby has .call(), which means there are two ways to do everything, and if you start with one way, and want to switch to a closure, you have to tack .call() onto everything. Eww.Will Microsoft support OpenID the way they supported HTML when they discovered the web?[deleted][deleted]Correct.  The writer notes the salaries are lower then notes there are fewer experienced programmers.  Hmmm...maybe a connection?

My impression is that the good people and good offices are similar price in different locations, at least within 20%.  Don't compare Palo Alto to a corn field.  Prime real estate and prime talent costs money--and every penny of it pays back.

What I am saying is:  Don't startup in a lesser location to save money.  That won't work.  Start up in a lesser location only if you have a rich connection to the good programmers in the area.
k, they decommissioned itA lot of philosophy students match the description perfectly, though. They get a bit of exposure to fancy philosophical thinking, don't understand it properly, and then spend hours making useless (and usually very circular) arguments about profound things. It looks very clever if you have no idea what they're talking about, but if you actually take the time to decode what they're saying, it's usually [worthless bullshit](http://www.stardestroyer.net/BoardPics/McDonalds.jpg).

Solipsism is very popular among pseudo-philosophers.&gt; forM_ [1..10] print

For this to work for the "novice" programmer (or the mentioned 'non-programming person') it would need to make some sense as to what `forM_` was and why anyone would ever try to remember "all these coding things". The Lisp example while more verbose is not more *arcane* -- which is important. The BASIC example would have line numbers and GOTO which would actually probably be "simpler" for some types of thinkers (e.g. non-programming engineers) than others.

Lisp with macros and generic programming, lambdas, etc, is certainly harder than BASIC, but you don't have to use any of those features to teach programming to non-programmers. Teaching BASIC to non-programmers would certainly have GOTO and GOSUB, keeping track of line numbers, etc.I would love people to give some examples of Bloatware.

I'm going to go ahead and get "Word" out of the way.

My impression, generally, is that things become "Bloatware" when they attempt to target both the general needs of newbies and the very specific needs of professionals and don't do it well. "Unfocused software".

However, we're not going to count "overkill", as almost every program with "too many bells and whistles" has a slim little brother who does less but faster. Least, they should.The fact that you are reduced to making pointless personal attacks makes me quite happy. It’s like you’re admitting that I’m smarter than you while at the same time shows your debating skills have been reduced to the level of juvenile name calling.

Now I know you are smarter than you are acting, and in some areas such as formal mathematics I will probably never be your equal. But so long as I can continue to debate topics I am interested in without degenerating into childish fits like you seem to have fallen into, I know that I will always be better than you.

In other words, grow up you big doo-doo head.

Agreed. One thing I struggle(d) with is the lack of identity in haskell data. Everything is a value, which is neat, but it means that just constructing a complexly interconnected data structure then working with it (as one would in most other languages) can be quite difficult.I would argue that bloatware is software which includes lots of _unnecessary_ code: code that doesn't contribute any value to the user.

Example 1: Mozilla, which has lots of infrastructure (GUI toolkit, etc.) that duplicates things that the rest of the system already has. Of course, the Mozilla code base goes back a long way, and when it was being built, they did need to build this infrastructure, since it didn't exist in a platform-independent way. But if Mozilla were built in 2007, they would build it on Qt or something like that.

Example 2: Windows, which has lots of cruft and duplicated APIs, to retain backwards compatibility with previous versions.

However, bloatware _is_ a relatively new phenomenon, for two reasons. Computer platforms have stopped changing every few years, so systems have more time to accumulate cruft. Also, systems have more memory, so they have places to put all of that cruft.the graph sucks and all the latest UP votes on reddit are ridiculous.I'm not trying to prove that I'm smarter than anybody, and I'm not trying to debate anything with you. I just want you to know that I think you're an idiot, and I will continue pointing this out until you stop posting here.&gt; Mainstream programmers are no strangers to solving non-problems either. 

I cannot argue with you there, but then again that is why Java is losing out to newer platforms like ASP.NET, Ruby on Rails, and Flash. 

Consider Lisp and Haskel. Do they have anything comparable to Ruby's GEMS? Or C#'s ClickOnce deployment? Or Flash's browser support?

Without this sort of support these languages are non-starters. They literally cannot be considered by many companies for no other reason.

&gt; Most mainstream programmers are not really doing anything relevant at all.

Tell that to the countless companies that rely on software built by your average mainstream programmer.
Sometimes those artifical limitations are necessary to support stuff like cross-platform development or backwards compatibility.So let me get this straight. So long as I continue to post on rediit, I can gain happiness from watching you act like a child and you can gain happiness from directing your childish rants at me.

Cool! I'm going to add you to my reddit "friends" list. Hey that's neat, it turns your name red on reddit!
Yeah, right. Because while VB.net is cross platform, Lisp and Haskell and every other non-mainstream language is Windows only. Yup, got it.&gt; I cannot argue with you there, but then again that is why Java is losing out to newer platforms like ASP.NET, Ruby on Rails, and Flash.

ASP.NET and Rails are not programming languages, and Flash targets a wholly different space than Java. Do you see Flash being used for backend server applications?

&gt; Consider Lisp and Haskel. Do they have anything comparable to Ruby's GEMS?

Yes. Does Ruby have a native compiler or a decent IDE yet?

&gt; Or C#'s ClickOnce deployment?

Do C# applications run on Mac OS X?

&gt; Or Flash's browser support?

Does VB.net have Flash's browser support? No? Must be useless then.

You're a laugh grauenwolf.I like your definition of bloatware. Your definition of bloatware affects only the developers of the software. After all, the end users cannot possibly know whether the software is 'bloated' or not, unless they look at the internals.

Still, we need a term for software that has accumulated cruft for historical reasons. And bloat seems like a suitable term for that purpose.

Thank you."Decreasing quality" and "increasing size" don't go together. Clearly, software is increasing in size, yet user experience has held constant or gotten better in most ways. Therefore, quality is necessarily going up in absolute terms.

The key effect in play is the threshold of pain. For $X, users on average will accept a certain amount of pain. This threshold doesn't change much over time. Commercial software will tend to hug this line for simple economic reasons. Free software hugs a slightly different line for wildly different reasons, but the effect is the same.

Until users are willing to pay more and start accepting less pain, the software experience isn't going to change much. We'll be able to do more and we'll be able to do it faster, but with roughly the same amount of pain. None of this is going to change anytime soon, because as software gets better, users on average prefer to pay less.

(Note throughout this that when I use economic terms like "pay more", that includes not just raw money but personal effort. I have a very high quality Linux installation that does exactly what I need, but the way I got there involves a lot of "paying" to acquire skill and the time to apply it, even if the software was "free".)No, as long as you keep rehashing the same inane points, which have been debunked by many people, in every single discussion, over and over again, without showing signs of intellectual growth or even staying on-topic, you cannot expect me or /anybody/ else to try having a real discussion with you.[removed]I think a much better measurement would be to look at new/updated packages in gentoo. I'll see if I can do this.&gt; In the enterprise waters where I swim, Oracle's presence is felt heavily and most collaborative projects use Java...

Cover your wounds -- you're swiming with sharks.
&gt; After all, the end users cannot possibly know whether the software is 'bloated' or not, unless they look at the internals.

Sure, the user can notice bloat: just not directly. I notice that Mozilla is bloated because it's big and slow. I notice that Windows is bloated because it's big and slow, and they can't seem to stop people from finding new security flaws in it all the time.As an alternative to generating the XUL DOM programmatically, it might be possible to use XSLT to translate the fallback HTML into XUL.  I've never tried this with XUL, but I have used XSLT with some ad hoc XML to generate dynamic HTML -- it's kind of like having macros for HTML.  This  seemed to work well in the limited cases I was dealing with.If I ever run a company, a thorough understanding of the implications of that idea in CS would be a requirement to get past any kind of probationary period.had to save it because I've worked with all of those peopleThere are fewer experienced developers because there are fewer people.  The real problem is it's harder to meet them because they're more spread out.  In NYC, you could start a user group for any technology on earth and get at least 10 people to show up.  At the IBM building on Madison, they have rooms full of user groups most nights of the week.

In a place like Western MA, you have to be more creative -- it's definitely possible.  Living in WMA is a lifestyle choice first and foremost -- I love it here and wouldn't live anywhere else--this is coming from someone who thought they'd never leave NYC.Given the design parameters of C++, Stroustrup did a solid job.

Heh, one CS class did a [mock trial](http://www.cs.virginia.edu/~evans/cs655-S00/mocktrial/) of him for all sorts of C++ behaviors; the final verdict was not guilty. :-DBy your own definition, those are only symptoms.[they're lucky](http://www.weather.com/outlook/travel/vacationplanner/tenday/05663?dp=ltempdp)Precisely. However, I can still meaningfully complain that "foo is bloatware" without having worked with the code. Therefore, Slava's original comment about "buggy bloatware" has some substance.[deleted]I’ve [derived a Y combinator in Perl](http://use.perl.org/~Aristotle/journal/30896 "Adventures in functional Perl programming: the Y combinator"), but [I still reject continuation-based web apps](http://plasmasturm.org/log/428/ "Discontinuous web"). That’s not because of what I don’t understand about continuations, it’s because of what I *do* [understand about the web](http://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm "Representational State Transfer (REST)"). Aside from that, I agree with the article and think his points are good.Several years ago, trying to develop end-user software in Java kind of sucked because (1) Swing looked bad and (2) relatively few people had the JRE installed.

However, now there's SWT, which isn't as ugly as Swing is, and also there are some very popular SWT programs: Azureus has like over 130 million downloads and it requires installation of the JRE to run.

So, has the situation changed? I know that Java on the desktop is essentially a failure, I'm just wondering about how common users' attitudes are towards the JRE.&gt; you don't have to deal with the stress and bustle of modern city life.

You want stress tolerant people in your startup.  Let's acknowledge the thus far unmentioned reality that the most ambitious among us tend to drain away towards the major cities.

The kind of people enamored with expensive city pleasures and status will work hard to afford them.  The kind of people who appreciate a simpler, slower life in slower towns don't need to work super hard for it.&gt;Northampton boasts a unique cultural downtown center with shopping, restaurants, arts, entertainment and nightlife rivaling those of major urban centers.

Is that a joke? I've been to Northampton and it rivals major urban centers like [Spinks rivaled Tyson](http://www.youtube.com/watch?v=p3Ypfh5xggk). Northampton is an ok town and a lot less boring than many of the towns around it; and certainly nicer than Springfield. But I disagree with him that it rivals major urban centers for culture/entertainment/shopping/food/nightlife.that's a disgusting graph.  seriously, only show the top 5-6 languages, or at least use better markers/colors.Haskell has [Cabal](http://www.haskell.org/cabal/), which makes it pretty simple to construct and deploy packages. 

There's also [Hackage](http://hackage.haskell.org/packages/archive/pkg-list.html) which is just coming online, in order to help you find packages for installation. CabalInstall is being written (and looks pretty close to completion) as a way to automatically fetch and install packages from the hackage database.You CAN program in many popular scripting languages in the same sense that you CAN [fit an eggplant in your anus](http://goatse.cz/) if you really try.  But I wouldn't call popular scripting languages functional languages anymore than I'd call an [asshole](http://goatse.cz/) an eggplant storage orifice.  

This is how it is in Javascript and Python, and I very much doubt that [Perl](http://goat.cx/) is any different, though I'm not as sure about that as I've haven't used it very much.  If Perl's libraries and standard functions expect you to do things like create an object and then modify its data, then it's not a functional language - it's a language with some functional features.No way. Support contracts for buggy code is HUGELY profitable.Purity has nothing to do with it.  Scheme and ML aren't pure, yet they are generally considered (by everyone except maybe some Haskell weenies) functional languages, because they were designed with functional programming in mind.Same here with Mozilla 1.7 (public computing lab).  Not too impressive._"..how to go about transcending limits you don’t even know you’ve imposed[?] There’s no formula for success, but two principles can stand you in good stead:  simplify, and keep on trying new things."_asdfasjfd;akjgs; why did they choose to put the latest and greatest on the C++ version? That seems so...backwards to me.
It's still less efficient.  In a circular list with a tail length of _t_ and loop length of _p_, Floyd's cycle-finding algorithm will take between _t + 1_ and _t + p/2_ steps to find the loop.  With your algorithm (let's say you keep the powers of two: the nodes at positions 1, 2, 4, 8, etc.), it takes at least _t + p_ steps (because you have to go all the way around the loop) and possibly several trips around the loop (quick example: 26 distinct elements, tail connects to the seventh element, your algorithm takes 52 steps, or _t + 2p + 6_).  The number of additional steps necessary increases quadratically with the length of the list.This is one of the biggest reasons why I switched to Ruby. Python certainly has metaprogramming facilities, but they all look like ugly hacks. Which is probably why they're used less than Ruby's are.That's the way to solve the problem! Legislation!I never said they weren't:)  I agree they are functional languages:)wow, great straw man!JIRA has approximately a gazillion features, it's true, but I'd happily trade most of them for a clean interface.

Mileage varies greatly.

We also use Atlassian's other big product, Confluence, and there I think the bag is a lot more mixed. It does stuff that strikes me as a _terrible_ idea in the wiki context (hierarchies of pages, impenetrable URLs, so on and so forth), but it's got some neat bells and whistles. I don't quite understand why anyone would pay for it given the range of quality, free alternatives, but it doesn't make my life harder on a daily basis - and sooner or later I'm going to have to steal a few of its ideas.

Edit: There's an interesting related question for someone to whore a little karma off of, if it hasn't been done - What wiki do you use?goo321:

&gt;they're in order (highest line at the end/right) listed first.Sounds like they are making some real progress.

I couldn't find any docs on CabalInstall. Is that mainly for developers (e.g. Ruby GEMS) or will it support end-user installations (e.g. Windows Installer) as well?

I have to say I wish MS had the kind of support you are talking about. Visual Studio has package discovery built-in, but it is so badly managed that it is useless even for Microsoft redistributables.
Apache XAP does something similar. Macros and what they call XModify to update the UI dom. Doing anything that was one time called "dhtml" is a breeze.Do you even know what a straw man is?I never said you said they weren't.  The point is that lack of purity is not why Python and Javascript can't be called functional languages.  Lack of tail recursion, convenient ways to do closures, and dependence on mutable state are the problems.Crappiest Graph Ever. 
ASP.NET is weird. It isn't Turing complete, but then again that isn't always expected of declarative programming languages. The fact that it is usually merged with other languages further compounds the issue.

Rails is another edge case. I have heard it argued that it is more of an extension of the language than a library. So I guess it depends on your definition of programming language and whether or not that includes so-called DSLs. 

I have pretty much given up on debating the definition of "programming language", it is just too fuzzy.

So far I haven't seen a decent IDE for Ruby, but then again I just started looking.

Actually C# does run on OS X. 
http://www.mono-project.com/Mono:OSX

VB doesn't have Flash's browser support, so yes, it is useless for a wide class of applications. Maybe Mono or WPF/e will change that, but it is too soon to say.

man, this brings back memories of college COBOL assignments on the university's VM/CMS system....[deleted]So? Does my capitalization pattern really have an impact on the conversation. Did anyone fail to understand what I was trying to say merely because I used capitals?

And for the record, the rUBYgEMS website uses both spellings:

&gt; The Gem::Specification object controls the data (and metadata) that goes into a GEM package. This reference defines the fields used in a Gem::Specification.

http://rubygems.org/
Lisp's macros let you do what you describe. I know Haskell recently got an implementation but, I bet it's easier to 'self-customize' Lisp code than Haskell.Huray! I got it!Aren't definitions fun?As per the comment on TFA's page, [Paul Graham's essay on the Hundred Year Language](http://www.paulgraham.com/hundred.html). Speaking of which, the last I heard about Arc was that it was on hold, he was busy doing other things, but that was ~6 months ago. Has anyone heard anything more recent?Isn't the point of OpenSource that you can extend it?Open classes are a bad idea. Open classes interacting with `method_missing` is a really bad idea. Open classes interacting with `method_missing` and implementation inheritance is a really, really, really bad idea.

Generic functions and multiple dispatch solve the same problems that open classes attempt to solve, without terrible gotchas like this.

Implementation inheritance has been shown to be flawed, time and time again. Just ask Sun: simply adding a new public method to a non-final class in the Java library can actually *break existing code*.

Instead of dismissing non-mainstream languages as "useless" and "not real world", these guys would do well to *learn from them*.So what are kids doing when they put together legos?  You can build and design.  Writing code *is* building.  It also *is* design.[deleted]totally off topic but isn't that from a meatloaf song? i have vague memories of sitting in my step-mom's car bobbing my head and singing "and into the fire, fire! .. and into the fire!"&gt; On the other hand, Higher Order Perl is an awesome book. So much so, in fact, that I ended up learning Common Lisp...

My sarcasm detector went off, but I'm not sure if this was a false positive or not. Did you learn Common Lisp to avoid the pitfals of the awfulness that is Perl? Or, did the concepts in the book lead you to realize that abstracting a report language to do these things was pointless and you should learn to use them in a language that was meant for them?[deleted]Just be reeeeaaaaallly careful with entry level programmers ;)I agree with the premise that you can do a startup outside the major hubs, but this post didn't do anything to convince me.

Atalasoft is not doing anything consumer facing like most startups are. Nor are they massively successful from the looks of it. They also seem to be buildling on Microsoft .NET and ActiveX in Visual Studio, so clearly location isn't the only part of Paul Graham's advice he isn't following.&gt; This benefits the employer and employees.

Besides being good for the company to have a lower burn-rate, how does this exactly help the employee directly? If the employee lived in an area with higher housing prices they'd also have a higher salary.[deleted]&gt; Many of these graduates would prefer to remain in the area, but can't due to lack of jobs with many ending up in Boston or elsewhere.

Are you just guessing here? What makes you think any young ambitious talented programmer wouldn't want to move to a big technology hub and shine?

My guess is that regardless of the merits of your company you're being left with the least ambitious of the bunch.

Some of those one-liners you can't really use without calling them as a script. And then why stop at one line?Interesting, since Joel's schtick is useability. Can you elaborate on what is not user-friendly?It's odd how Sapir-Whorf utterly fails in terms of "natural" languages, but works so well for programming languages.some guy lists his lib (for crypto) with 2 libs, Boost and GTKmm as all being the best, on his blog. Exceedingly useless. I think I'm going to modbomb linuxer in retaliation.Indeed. Hopefully the [monte carlo method](http://www.economist.com/science/displaystory.cfm?story_id=8585017) that [I read here recently](http://programming.reddit.com/info/12c9y/comments) is a sign of some long-overdue progress.BTW, XUL apps are fantastic for an office environment. Even if everyone is running IE and Winblows, you can build forms that open up with xulrunner (8.5MB runtime) and can have either a standalone ability, client/server, pull from a website partially, or pull from a website completely.

Recently, I built an app in my office for standalone capability to be used in zero-day issue where the entire server room might have crashed. It can allow the calltakers to continue doing their work in a cached workstation mode in the event of such an emergency. I was amazed how fast I could knock it out in XUL and Javascript -- it was faster for me to build this in XUL than any Java/Swing, VB, or .NET app I have built in the past.

XUL is also not a far stretch to learn from HTML, and most web developers know Javascript (or should).

By deploying with XUL and xulrunner, you can eliminate the whole IE/Firefox discussion in the office for your app, and forget about having cross-platform if/else exceptions between browsers. Of course, you can't do that on the WWW, but for an office environment, it's very suitable.

Another neat advantage of XUL through xulrunner is that you can go outside the sandbox with Javascript via XPCOM, making Javascript actually useful, such as reading/writing files, interacting with multiple kinds of databases (especially SQLite), utilizing AJAX, etc.
Heh, somebody at yahoo didn't bother to [even stitch the pages from informationweek in the right order](http://www.informationweek.com/showArticle.jhtml;jsessionid=TXRG1MFBDJDL4QSNDLRSKH0CJUNN2JVN?articleID=197003052). Amazing that people still made sense of it enough to upmod :)

[Cache](http://reddit-links.infogami.com/12yxe) for when yahoo's lame link to this lame copy goes dead.Yeah, Schemers hate that about Common Lisp.That was just incredibly funny, but I'd not give it to a C newbie.  They might learn something from it.
[removed]Quite a reasonable attitude.  It certainly would help us Catalyst-lovers to be able to recommend an O'Reilly book though -- there would be an adoption feedback loop. I do understand that your job is primarly to support trends -- not make them.I didn't say I was entirely innocent of this myself...

http://vintermann.paranoidkoala.org/archives/000012.html
People unfortunate enough to be exposed to someone else's Lisp, except perhaps their immediate mentor's. Lisp works great as a solo language, but it isn't always used as such.Not if the next step is to [extinguish](http://en.wikipedia.org/wiki/Embrace%2C_extend_and_extinguish)...XUL, the name makes me puke.In VB, you can bet backward compatibility is a part of the picture...[deleted][deleted]One man's feature is another man's flaw, to paraphrase Paul Simon.

I do think that a programming language needs heavy conventions (because "there's more than one way to do things" hurts reuse), and the only thing that hurts a language more than feature-mania are features that can be used for limitless obfuscation, or customization as some call it. That includes the preprocessor in C --- If you've ever worked on a C project where they try to implement untraditional language features through the preprocessor, you should know what I mean. Lisp macros are as bad in practice, because although they are safer, people have even less qualms about using them for everything.

I think culture is a part of a language which can not be dismissed. Yes, you can probably write nice and pretty safe C++ if you use the STL for everything, and you can certainly write a Lisp program that only uses well-established macro-implemented DSLs. But if you have to work on other people's code, then you will hardly ever see such good practices unless they are established as part of the language, formally or informally.I found the article interesting, but it really had more to do with Scrap Your Boilerplate than web-apps (just a heads-up).According to the [Online Etymology Dictionary](http://www.etymonline.com/index.php?term=fry), it's from Sir [Thomas Moore](http://en.wikipedia.org/wiki/Thomas_More), 1532. :)Maybe a lot of the developers are like me and know to use Ad Block Plus while people whose interests don't revolve around computers might not.

I don't see any ads on your linked-list blog post until I went to it and specifically turned ads on to see what it was you were talking about.[removed][removed]The flip side of this trait is NIH syndrome, where the need to understand everything in your system drives you to reinvent or reimplement existing libraries or infrastructure--because it's usually easier to understand your own code than that of others.[removed][removed]Of course, because web-apps are mostly about boiler plate. :)[removed][removed]The problem with this recipe is quite simple: you don't need properties here at all. The proposed "encapsulation" is just nonsense and it doesn't provide anything valuable besides usual attribute access. That's also why there is no proper syntax for properties yet. Either the usecase is nontrivial and you have to define separate descriptors anyway or it is trivial and you better omit it because you do create *boilerplate* ( just like creating a class for using a single method is a waste of keystrokes ).But there's no reason not to use freshmeat, though?Here on FreeBSD, a `portsdb -r` tally gives about:

1. Perl: 8182
2. Python: 1470
3. Ruby: 414

I'm not sure how much of Perl's lead is simply due to CPAN's very fine-grained distribution approach, though...  for example, there are *106* separate crypto/digest algorithm ports alone.Other than the 1.1 to 2.0 upgrade (which thankfully MS fixed by rolling back to the 1.1 model), my experience with ASP.NET has been overall quite pleasant. 

I really like being able to wrap my code in controls, on a good day it can makes assmebling pages really quick for me. How does Ruby on Rails handle packaging up reusable HTML/javaScript/Server code?

I admit the default databinding wasn't very good in 1.1, but the 2.0 controls seem to finally have that sorted out. And I can always hack the output using JavaScript or the data binding events.

One thing I really long for is Rails ORM mapping. I am seeing good things in LINQ, but that is still a long way off.

Do you know of any sites that do a full comparison between  Rails and ASP.NET? Peferable something like the Pet Shop comparisions between J2EE and ASP.NET.
I used to feel that way too, but now I don't care. 

I think the breaking point was the day I got corrected for using "Lisp" by one person while someone else was correcting me for using "LISP". At that point I realized we were spending more time on spelling than on the real issue.
I'd say the ability to return functions is fundamental for it to be usable as a functional language, and if the only ways of achieving this is are workarounds/much less efficient (such as wrapping functions in objects would be), then the language doesn't have proper support for functional programming.

Functions as in-arguments are avaliable in almost all languages I know of --- even Ada introduced them in the 95 version (I think they got by with generics before). It's polymorphism which is the barrier to using fold, map and filter.

&gt; Perl in 1999: The humorous [WWW] Python vs. Perl according to Yoda. http://www.python.org/doc/Humor.html#yodaIn a controlled Office environment even good old client-server apps are not a bad choice. :)) Why downgrade to XUL ?
&gt; I'd happily trade most of them for a clean interface.

Well comparing it with Trac's or Bugzilla's I found Jira's interface to be _much_ cleaner, clearer and more useable.

&gt; We also use Atlassian's other big product, Confluence

Well we also use Confluence (@work) and I agree with your take on it.I try not to on slides. Most often it doesn't add to the oral presentation, and on its own it leaves more questions than answers.

I got to slide 5:
&gt; What does this do for you?
&gt; * written minimal amount of application code
&gt;   total application + tests is probably more

What is the difference between "application code" and "total application"? I don't know. It doesn't say if we are going to get to know soon.

That's why a prefer a well written article that is meant to be read from top to bottom without "voiceover".[deleted]SBCL kick ass! I wish I have more time to help develop it.&gt; Implementation inheritance has been shown to be flawed, [...]

Eh, what are generic functions, but sane implementation inheritance?&gt;Pointers are references. 

Not, they are **not**!

C++: void f(T&amp; t) (when calling: T actual; f(actual);)

C#: void f(ref T t) (or "out T", no real difference for the matter at hand, check out the generated IL, I did)

Pascal: procedure p(var t: T)

In all three cases, inside the subroutine, "t" is a *reference*. You work with "t" as it were the actual argument. "t=blah" is equivalent to "actual=blah".

**No such thing** in C. You work explicitly with the pointer (*t = blah).

So, in C, references are implemented by the programmer, *not* the language. To do that, programmer uses *non-NULL pointers*. Pointers *only* are not references, as you claim. 

Pointers are only means of implementing references (just like your Wikipedia quote says; absolutely nothing wrong there). And, in C, less reliable than C++/C#/Pascal references, because nothing stops you from erroneously using dangling (or NULL) pointers as (conceptual) references.

All implementations of C++ and Pascal I saw, and C# of MS, use pointers to implement language concept of references: when formal subroutine argument is a reference, a pointer to actual argument is passed to the subroutine; code of the subroutine works with pointed-to object, i.e. actual argument. You can check out disassembly for that, too, it's easy (I did).

So, C language *does not have a concept* of references wrt subroutine calls. Just like Java. You may be confusing Java term "reference" (to non-primitive types) with "reference" (as argument to subroutine calls). These are orthogonal.Can I suggest combining cygwin and putty if you have to use a Windows box. Get it here http://web.gccaz.edu/~medgar/puttycyg. It is far more usable than cygwin in the standard Windows console - cut and paste is quick and easy for example.
This has come up before:

http://programming.reddit.com/info/xr7x/comments

Pretty useful comparison, I thought. 

I'm starting a new web project right now, and have been playing with Pylons, having used Turbogears for something last year. So far, I like the feel of it, but the author of the article is right in that you can barely shove a playing card between the two frameworks in some areas. 

One thing I did find with TG was that it was very easy to write my controller-code all in one place (especially if you start small and suddenly need to add features quickly), and
this became a bit of a maintenance headache. Pylons looks like it encourages better organisation in this respect.  

For completeness, I also looked at Django, seduced by its awesome admin package, but realised my problem was less 'CMS' and more 'web-app' than I initially thought.This guy is saying programmers want to work with open frameworks as opposed to closed ones.  Speaking for myself, maybe sometimes I'll use an open framework if the quality is really good.  Often times though the support / documentation of open frameworks is lacking.  If you have a problem, don't bother asking the devs who jumped ship 2 years ago.  Good thing it's open source though so you can trace through thousands of lines of code to figure out all the kinks (kidding) Closed source components have support, which is important if you build commercial quality software.

Foooor example, look all the samples on codeproject.com.  You can usually find a control that does what you need there and it is free and open source.  Unfortunately the price is that you end up re-coding the control in time because it never does exactly what you need and bugs are always cropping up here and there (because it was never tested by the provider).

Good programmers use the right tools for the job, to get the job done fast with highest degree of quality.[removed]All the detail behind forM_ also implicitly exists behind the IOing Lisp code, though.

One way of looking at it is that both languages only let you put that code in IO contexts;  the difference is that the IO/sequencing is implicit everywhere in the Lisp, and explicit and optional in Haskell.[removed]&gt; This guy is saying programmers want to work with open frameworks as opposed to closed ones.

Not quite - he says they want to use frameworks they can understand, rather than ones they can't - either due to access, or other reasons.  Note that he said Jetty, and not Tomcat.

&gt; Closed source components have support, which is important if you build commercial quality software.

It's important if it saves you time.  If you have to go through 3 or 4 discover-report-wait-patch cycles before it works right, versus a couple of days of examining the open source version, that can be a net loss of time (depending on circumstances), and thus a loss of money.

On the other hand, if the supported library rarely has problems, and fixes don't reveal further problems, then that's probably a net win.

The only absolute is that there are no absolutes.I think there's a balance required, depending on one's age. Younger programmers ought to be bloody passionate about the bits and bytes, have opinions that are strong and well thought out concerning typing, OOP, methodologies, O-R mapping, and so forth. They should care that code be readable, elegant, and run blindingly fast or slow if needed. They should be afraid to have senior programmers read their code and find bugs and they should strive to be their best. Sometimes that's confused with pride in your work :)

At the end of the day companies are collections of interests, they have owners, often folks with money and capital. Sometimes owners and programmers are the same people, in which case real happiness might be achieved. Other times they are different. Money is a tool just like computers are tools. Some folks/owners value their money, husband it well and try grow it, with specific purposes in mind. Others don't value it, perhaps they were born with too much and take it for granted, perhaps they manage it for others and don't care for it as well as they should. This latter type of money is not good for building companies, it becomes something to fight over, something that gets pissed away, though it still can be productive if seen for what it is and used with caution.

The point I'm making is that one needs passion about the work *and* passion about the company. I'd never want to build houses with someone who is solely a kickass girl with a circular saw or hammer.[removed]Additionally, spouses might have a harder time finding work in their field in less-populated areas.  Just a couple of years into my career i (telephone) interviewed with a company in Traverse City, Michigan, and they were actually willing to pay me  *more* than I was getting in Colorado Springs to offset this very consideration, but it wasn't necessarily a startup either.

Furthermore, what if you go out to this small town and decide you don't like the job.  It's not like you're going to find another job right around the corner.  I would argue that there is *no way* a programmer should consider working for a start-up in a small town.I disagree. You can measure a programmer's goodness very easily - the only measurement that matters is how big a multiplier he or she is to the group's total qualitative and quantitative output.Which is valid only if the future tasks are essentially similar to the current tasks.  

Unfortunately, with programming, the core idea is that you never have to repeat yourself.  Unlike a master joiner, who, in order to make 2 cabinets, must do it twice, the programmer does it once, and wraps it in a simple loop.

At some point, then, if you are a good programmer, you'll have to do something different.  And that's when you find out if you're broadly skilled, or if you just have a talent in one specific area.

I'm also almost certain that that metric is impossible to measure, except in some vague, hand-wavy fashion.  That would make it difficult to rely on for doing anything definite with.

Beside, the article isn't really about getting ways to assess a programmers worth, but, rather, to discuss the drive to understand - which the author asserts is the sign of a generally good, broadly skilled programmer.Even if we were to agree on qualitative and quantitative metrics for judging output, there are obvious problems with your proposed methodology for measuring "goodness". My greatest objection is that it depends very much on the group: the group's make-up, its size, the programmer's role in it, etc. I can imagine a mediocre programmer who can add epsilon to the output of almost any group; I can imagine a master programmer who is crucial to a project developed by a relatively small group, but who would function poorly, if at all, as a replaceable cog in a huge software-developing machine. It's easy for me to imagine, because I've worked with both kinds of programmers.No, that's why we don't see Lisp on the list.wtfThat isn't a macro (yeah yeah, work with me), that's a CLOS 'around' method that looks for (provides...), (uses...), and an apparent ass-load of similar things and takes a note of it.

The *real* reason it's cool is because it highlights the importance of being able to extend existing system via hooks (macroexpand-hook) or AOP-ish things (CLOS :around method).

Building dependency graphs is easy enough in any language, being able to do it by leveraging pre-existing tools is handy.

It's nice when the language is data.Using a pretty printer class, generics, and haxml combinators (I think) to generically render html, anyway. A nice twist on generic pretty printing of structures.Even more cool:

&gt;it turns out that not only can you use adg to merge two large systems into one, it’s possible to split one system into smaller components, as well!

[unidented](http://boinkor.net/archives/2007/02/unintended_consequences.html)

It it cool when language is so powerful that cool features can be way cooler than indented.Before I read the article, I was thinking:

"Don't ask employees to be passionate about the company!  Command them!"Python keeps inspectable metadata around for nearly everything, making this kind of thing a breeze:  [here](http://www.tarind.com/depgraph.html)'s an example of hooking up Python's included [modulefinder](http://docs.python.org/lib/module-modulefinder.html) helper to graphviz, to generate similar dependency graphs.

(`modulefinder` can also be used from the command line to get a quick listing:  `python -m modulefinder [file] ...`)These "signs of a good programmer" articles are starting to cause me angst.  Since I am a freelance developer I do not have colleagues or managers to tell me that I am a good programmer.  So although I *think* I am a good programmer, I don't know for certain, and so I've read a few of these "signs" articles lately.

They are on the whole unhelpful.  Sometimes they describe characteristics which should be common to any intelligent, competent person, such as this one's "need to understand".  Sometimes they are a [hodgepodge](http://eureka3d.com/blog/2007/the-top-10-attributes-of-a-great-programmer/) of characteristics that vary widely enough to be almost contradictory, such as being "driven and lazy", "optimistic and pessimistic", not all of which I have.  Or perhaps being a good programmer is the [ability](http://blogs.msdn.com/peterhal/archive/2007/02/04/what-makes-a-good-programmer.aspx) to work with other's code, while being a great programmer is the ability to create code others like working with.

This latter approach, which is to examine what a person produces as opposed to their personal characteristics, is the most grounded in reality, but it's again unhelpful to me since I almost never produce code that other people have to work with.

So for me, I think the mark of being a good programmer is making my customers happy.  Simple.Umm, wc(1)?
You don't even need to install the entire XULRunner run time, you can just install the Mozilla windows component and run XUL apps in IE.Seems like more trouble than it is worth?

I don't know about you guys, but dealing with HTML displays of my data takes up about .05% of my application development time.  I'd much rather just "get it done" than learn some new abstraction to make it "easier".

DRY is dangerous when taken to the extreme.&gt;Not, they are not!

Anyways, like most C++ compilers G++ is written using C.  I have to rush out the door so I don't have time to highlight all the parts for you.  Basically follow along this:

http://gcc.gnu.org/viewcvs/*checkout*/trunk/gcc/cp/call.c?revision=121361

Referencing is done through trees.  So where am I going on that thought?  Basically that C++ references are just candy coated pointers.  In other words, you could implement them in C if you wanted (if someone hasn't already done this I would be surprised).  Internally, pointers are still there but by overlaying them C++ adds type safety and other stuff.  The reason that pointers are still there is simply because they are the basic component of referencing.You are a good programmer.I find this CL example is different in the sense that it can dig out metadata that is __not__ around and collect it. I don't know if Python has ability to run code, when function is defined for example, and look at the abstract syntax tree. The idea of programmable programming language is that you can do things that language developers did not think about. Maybe in next version of Python ;)A condo is a quarter of the cost as NYC and bigger, salaries are much more than a quarter of NY or Boston.

The RHI salary survey lists these indexes to give you an idea across all industries (no link -- I have a hard copy)  NYC: 150, Boston: 123.9, Hartford: 108, Springfield, MA (which is in Western MA): 92.0 -- that ratio will give you an idea.

That said the question isn't which is better -- everyone would have a personal choice -- and talent of the founders is much more critical than location.  The question is how to make it work if you want to be outside of a hub -- what advantages would you have and how to overcome disadvantages.There are very good points.

I don't think people should move to a small town for a startup -- I think you move there because you want to live there.  That's what I did.  My wife found a job first, I telecommuted to my old job for a while, then I found something local.  

From talking to recruiters they have tons of jobs in the area and not enough people.  I don't think anyone talented stays jobless for long here.[deleted][deleted][deleted]No doubt--Lisp (and similar) are *very* organic and lets users discover that coolness. Unlike Java, which would rather its programmers were dead.He's referring to the GNU tool wc (word count), and its manual page section (1).

$ man 1 wc
Can someone post a summary please.Steve Yegge wrote an Emacs Lisp script to count how many words are in his blog posts, so he can make sure they don't get too long.  The world is happy.+1; it's one thing to generate metadata which can be inspected in hindsight, it's another to (trivially) hook in to the language (and, in this case, the asdf system) itself and do interesting things.

I'm not saying Python is less-capable or anything, but there is definitely a distinction here, which the OP is but one example of. Being able to define your own OOPLs on top of Lisp is another, IMO.I think the problem is even attempting to judge "programmer goodness" on a remotely universal scale. If the goal of such an attempt is to determine whether to hire a particular candidate for a particular project, then you're doomed to fail. Instead weigh the candidate's qualities against the needs of your project. 

&gt; To me this suggests that our understanding of "master" and "mediocre" is egocentric and flawed

Well, I don't claim to be a master chef, but I do think that a chef working at a three-star is, in some sense, "better" than a fry chef working at some random franchise. Different horses for different courses, as they say; and why can't the same be true for software development?This one concurs.&gt; It's odd how Sapir-Whorf utterly fails in terms of "natural" languages

The "strong" version of it (that language strictly limits/determines thought), maybe.  I don't think any linguists dispute the weak version (that language influences thought, in various ways).He wrote a function, dear.  You can think of this as 'he extended a really really long runtime-loadable configuration called ~/.emacs by one function'.  This function only intersects with 'scripts' in the implied (as you generally don't compile ~/.emacs) lack of precompilation.Django suffers badly from being seen as primarily a CMS tool, even in Ian's comparison. This simply isn't the case - Django ships with more tools out of the box that support CMS style applications (the Admin framework, RSS generation and so on) but the core of Django (the stuff that isn't in django.contrib) squarely targets general web application development. I have trouble thinking of any aspect of web application development that is better supported by Pylons/TurboGears than Django (not counting JavaScript libraries, which I personally see no need for in a server-side framework).Good luck measuring that. ;)&gt; A normal 'higher-order' one can blend in with regular function calls seemlessly. Ruby has .call(), which means there are two ways to do everything

Wrong. Ruby doesn't even have what you call "a regular function call". The only way to "activate" code in Ruby is by sending messages.

I can only imagine that this argument comes from people who haven't bothered to learn Ruby before criticizing it.

The basic philosophy in Ruby, ripped of from Smalltalk, is that all you can do is passing messages between objects. Functions are first class objects in Ruby, which means, all you can do with them is sending them messages, the most important one being `call`. Would you agree that Common Lisp is a functional language? Guess what, Common Lispers use `funcall` in places where Rubyists use `call`. So what are you asking for? Syntactic sugar for `call`ing function objects? In fact, Ruby has syntactic sugar for it, in the form of `yield`. And instead of the call message, you can use `[]`, e.g. `foo.call(bar)` is the same as `foo[bar]` (assuming `foo` responds to `call` messages the same way it responds to `[]` messages, which is the case for `Proc` and `Method` instances).

IMO, this is the cleanest possible way to combine message passing OOP and functional programming.

Contrast the situation with Python:

    foo(bar)

I've never looked at the CPython sources, but I can only imagine the ugly if statement for function calls, namely:

1) if `foo` is of class `&lt;type 'function'&gt;`, execute the function code, else goto 2)

2) Lookup the `__call__` attribute of `foo` and goto 1)

It would be possible to introduce a similar hack in Ruby, and in fact Matz has experimented with it in the 1.9 branch. IMO it's not worth it. There's little gain and it introduces ambiguities and special cases.

Now there may be arguments that Ruby is not a functional language, but the `call` issue is evidence that functions are first class in Ruby and thus an argument in favor of Ruby being a functional language.

What pisses me off are ignorant people spreading misinformation about and bashing languages they don't even know (as can be seen by their nonsensical statements about the languages).Ian's conclusion seems to be that the most important difference between the two is that TurboGears is built around CherryPy while Pylons is built on top of Paste and WSGI. The CherryPy wiki article http://cherrypy.org/wiki/CherryPyAndPaste offers a biased comparison - I'm not at all fond of CherryPy's global request/response objects, but I agree that they are slightly more developer friendly than a raw WSGI request. That's why Pylons offers a more friendly abstraction around HTTP than WSGI, which I seem to remember was partly inspired by Django's.There is a 64-bit version of Windows XP too, so this is not an advantage over XP ... Ofcourse the lack of drivers hit both Vista and XP ;-) I cana't use both XP64 or Vista64 on my AMD because there is not a driver for my tv tuner :-((It was probably more irony than sarcasm ;-)

I *do* think that HOP is a great book.  For me it was a real eye-opener about programming techniques. I would never have read about those concepts if it hadn't been oriented towards Perl hackers.

The irony is that HOP makes you want to learn different languages. I still think Perl is a good language and that it doesn't deserve a lot of the criticism that it gets. I believe that its implementation of closures and/or anonymous subs is more versatile than Python's.

My feeling though, was that at a certain point with higher-order stuff in Perl, you end up fighting the language somewhat, even though a lot of things are actually possible. You do get tired of writing things that look like `$a-&gt;{$b}` all the time.I disagree. If you put more effort into learning the system you are more likely to know about which additional facilities are available.

Basically I don't think the need to understand necessarily leads to a drive to reinvent or re-implement.Stacked percentages may have a use, but you lose the ability to compare values, as the base is different for each value. In this application, it is interesting to see where lines cross; some information that would get hard to obtain if you looked at stacked percentages. It would be hard to judge if Python was really stagnating when it was depicted as a strip zigzagging on camel humps. 

Contrary to most others here, I find the graph pretty good; maybe some of the lines might have been labelled directly, but that's about everything I would change.&gt; I have trouble thinking of any aspect of web application development that is better supported by Pylons/TurboGears than Django 

Better ORM (sqlalchemy), delegation (routes), faster and more powerfull templating (Mako) are a few to name.I don't think NIH syndrome is an inevitable consequence of the "absolute need to understand" described by the author. But I do believe that they are strongly correlated. I can only support this with anecdotal evidence, so take my comment for what it is: an observation, not immutable law.I'm using SQLAlchemy in a Django project right now. Django's URL configuration supports delegation. You can use Mako in Django just as easily as you can Django's default template system - in fact the Mako API is almost identical.

Just because Django ships with default components doesn't mean you have to use them.These articles are rationalist irritants, where there is actually no emphasis placed on establishing (through investigation and analysis) what traits of 'good X' result in 'better' code, but where the author assumes to define the qualities of programmers from words plucked from the ether. In doing so they do more to tell us about their ideal superheroes, or what sort of delusions of grandeur they possess when using their perceptions of themselves as templates.&gt; Instead weigh the candidate's qualities against the needs of your project.

Most people these days go further, and ensure that their projects have common qualities which make it easier to staff the projects, then try to form core teams out of people who have demonstrated that they can work well together and sprinkle some junior people on top of that.

&gt; Different horses for different courses, as they say; and why can't the same be true for software development?

I agree. http://jroller.com/page/trasukg?entry=mass_production_hand_cut_dovetails&gt; [I still reject continuation-based web apps](http://plasmasturm.org/log/428/) \[...\] because of what I *do* [understand about the web](http://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm).

Don't be hasty:  there's nothing incompatible about REST and continuation-based web resources.   While you can implement them unRESTfully by storing the continuations on the server side, you can also implement them RESTfully by serializing the continuations to the client, just like you would any other state.

(So far, fewer mainstream web programming platforms support the latter than the former, though;  but things can only improve.)An observation on testing and rails[deleted]Why choose a framework if you plan on not using any of its default choices?  And Mako is very different not only in performance but it also supports real python code not just a subset. 

Btw, doesn't using SQLAlchemy makes you unable to use Django's admin interface? 
The comments at the end (**What's Next?**) say it all - this is a babystep towards a haskell web framework, less capable than the most crippled equivalent in other languages and it would take a lot of work to get much closer. To compare it to ActiveRecord makes you look silly. 

The fact that haskell is weak for web development is not surprising IMO - it indicates that the haskell community has been busy with other things (shh - web development is boring). Now that haskell is thoroughly overground I suppose there is a wave of new users that want to know where their rails is but I don't think that's where it's strengths lie. It's strengths are many and broad but if you start using it for web development haskell-cafe will a lost cause.kiss ass.The important part is that he's been getting too many complaints that his blog posts are insightful but extremely long and you can fall asleep while reading them. As a solution to this, Yegge has decided he will only write for 1 hour a week as opposed to 4 hours a post, *and* in addition make sure his posts are "column" length, which is the reason for the word counter. His word counter function gives him the number of characters as well as the number of 5 letter words. In other words, *totalchars / 5*.But our mission statement says that we are "110% committed to impassioned excellence" what about our mission statement!?
Another issue with this whole practice is summed up by "Okay, we can't say 'Last Name' anymore; change that to 'Surname'."  That is, rendering the field name for display is the wrong way to do this: the field name should be usable directly by the web designer to place the data where ever, and the displayed name of the field will be whatever the web designer puts in there.  Quite possibly, this will be displayed differently on different pages, or in different locations on the page.

For example, it's common to want to say, "Hi, [name]" or "Welcome back, [name]." on each page in which you have to be logged in.  Reddit just has "[name] ([karma])" in the upper right, for example.  If you track a first and last name, and a nickname, and maybe a separate login name, which one is displayed there is a matter for debate, but the programmer doesn't have to care: all of those should be available in the page for the web designer to use where ever she prefers, or where ever the managers want to put it.

The problem with using the field names themselves in a visible way is that the managers or the designer *will* want to change how they display, or what they say, and this shouldn't require code changes, just HTML/CSS changes.  The obvious use for the field names, then, is as the handle by which the web designer can display these fields, rather than as a visible label.&gt; Basically that C++ references are just candy coated pointers. 

The implementation does not matter, the semantics is the point of the distinction. I bet all pass-by-reference implementations use pointers under the hood, but if you have to do it yourself, as in C, then it's not pass-by-reference.

Think about it: Even assembly language has pass-by-reference semantics according to your definition, since you may access a memory location through an address passed in a register or on the stack.Right; my response was more in light of the submission title's amazement (and implicit challenge).&gt; P.S. notice you are now trying to change your wording to use "call-by" instead of "pass-by".

I did that simply because pass-by-value and call-by-value are different names for the same concept. I should have been consistent though.eclipse is an elephantIt can also stem from the unavoidable desire to 'do it correctly' after the veil of abstraction has been pierced and the monkeys and clowns beneath have been exposed.Actually I was referring to the Unix tool wc, which predates the GNU tool by a good many years.
You should venture into rural communities and inform their inhabitants, many of whom are laborers and service sector employees, that you work harder than they do because of where you live. 

I assume that some of the 'stresses' that he refers to are seeing such a large portion of his earnings evaporate from a higher cost of living in conjunction with the effects living densely packed into an area has on his psyche, and not an endorsement of a desire to be lazier.&gt; Well comparing it with Trac's or Bugzilla's I found Jira's interface to be much cleaner, clearer and more useable.

I haven't used Trac, but as to the usability nightmare that is Bugzilla, I concur. I think what I mean isn't so much "clean" - JIRA feels almost clinical - as it is "simple".This is Rakes fault. It includes FileUtils at the toplevel, thus injecting over 30(!) methods with short names (like `cp`, `rm`, `ln`) into *every* freaking object in existence.

Rake is a dirty hack. Don't use it. Others have experienced the problem the author describes. Alternatives for Rake exist, which don't do this braindead stuff.

Rake is an example of the wrong kind of laziness and misusing powerful language features.Ah, but then you don't get the opportunity to use XPCOM without signing your xul package (a pain). With xulrunner, you can load your xul package and it can gleefully call XPCOM with just one mild line before some of them:

netscape.security.PrivilegeManager.enablePrivilege('UniversalXPConnect');

XPCOM gives your Javascript some "juice" at that point to go outside the sandbox, such as process I/O, sockets, file I/O,  databases, etc.
Unfortunately, in the engineering related firms I have worked with, these three attitudes get people promoted - even above people who are more competent and better suited for project/people supervisory work....  

* Is well-liked because they do whatever is asked, enthusiastically

* Accepts (and uses) phrases like, "this is what corporate needs us to do."

* Cares a lot about his career path in the company; focused on getting management recognition.

...Though not everyone is looking to move up, it sends a strong signal to all employees when they see the types of behaviours and personality types that get promoted/raises.

Many people still favor "yes" men.You should ask yourself, for every decision you make, "Is this good for the company?"Yeah, no kidding.  You're spot on.  I feel like all these articles that tell you to quit your job for one that barely pays anything, or that instruct on what sort of "passion" to look for, or tell you how answering your telephone at work is a waste of time and if you ignore it you'll gain respect, are kind of like saying "Don't like you job?  Travel to Valhalla and steal a unicorn!".Roger that; cool-but-not-wowful.Companies that promote yes men, create a poisonous atmosphere that eventually causes the company to slip into bureaucracy then eventually fail.As someone pointed out, this won't work on emacs buffers.  But it is a useful tool!
I would strongly question any assertion that most internal corporate apps are now written in VB.  Good measures of what corporate developers use are job site language requirements and general language resource indicators, like the TIOBE index.  

Today on the IT jobsite dice.com:

Java: 16,000 jobs
Basic/VB: 11,000 jobs
C#: 6,000 jobs.

These suggest that most internal apps are written in Java.  VB still has significant presence, but use of C# is surprisingly rare.

Also, commercial/shrinkwrap apps are a very small segment of the market.

As someone else pointed out, you can pipe marked regions to unix commands.

mark whole buffer, pipe region to shell command: wc

    C-x h M-| wc
That may be true, but in the mean time many people suffer.  I do not care about companies, per se (they come and go, and are really man made inventions)....but the harm to people involved is sad and frustrating.  Especially, when it seems to be the majority of companies these days promote and value "yes" (wo)men.  It is hard to find better positions for many people who just want to have a good job and take pride in their work.Oh, do you mean that you find JIRA too complex to use for what it does?Interesting thought.

There are lot of issues I can think of with this, though. Eg. the state is potentially a lot of data; how do you make sure it can always be serialised to a reasonably short URI? Also, if you do it right, the request stands alone, which more or less means all state must go in the URI in a way that you can decode it out of there – so then what about sensitive data in the state? That’s only one reason that it seems to me you would generally need a way to cherrypick what part of the state gets serialised; another is that if your state is so complex that every request ends up having a unique URI, you lose cachability and all the other goodies that go along with unique URIs.

Admittedly, these are issues I just thought of off the top of my head without even searching for any prior art on REST with continuations. Any thoughts on these? Do you have any pointers to writeups about experiments on this? Or even just deep thoughts about it?Couldn't have typed it better myself[deleted]This is shaping up to be an awesome editor for those of us forced to develop in Windows.  Its still early in development but in many ways it already trumps most mature Windows editors.  

Also, I've been using it since the first beta release and it is extremely stable.Oh, I didn't know that, Its a bit of a hack as you have to mark your buffer first, but... Karma for you!  Thanks!Thanks.All i read, before i fell asleep, was that we doesn't want to be called "Stevey".Take a look at the Django Admin, Zope's Archetypes or Rails Streamlined to see how auto-generated HTML can be very, very useful.Well, think of it not as a "hack" but as an opportunity to mark regions other than the whole buffer.

But if you want a single function to just do that one case...

    (defun count-words-in-buffer ()
       (interactive)
       (shell-command-on-region (point-min) (point-max) "wc"))

which I wrote after finding out what M-| did

    C-h k M-|

which also includes the documentation for the function

    C-h f shell-command-on-region

and perhaps you want to bind it to a key

    (global-set-key (kbd "\C-x w") 'count-words-in-buffer)

and of course, I tested all of this quickly in my *scratch* buffer.  Wow, who knew writing an emacs lisp tutorial was so easy?

Not available under Windows (except with cygwin, and perhaps SFU). Besides, the main point of the article is to show how extensible emacs is, not how to count words.[deleted]&gt;The reason that pointers are still there is simply because they are the basic component of referencing.

Or the reason may be C compatibility of C++? Your reason doesn't cut it for me. For example, C# and Pascal have references without pointers (yes, they have pointers, but *not* wrt subroutine argument types).

&gt;C++ references are just candy coated pointers.

True, but not the other way around. C pointers are *not* references wrt subroutine calls. Especially when they have value of NULL ;-)

I think, like wicked, that you are unable to separate the concept from implementation.Good programmers are arrogant. Arrogant to the extend that they feel comfortable writing about competency and what character traits they have that make them so exceptional. And why others are simply [incompetant ](http://www.google.com/search?hl=en&amp;q=site%3Ajoelonsoftware.com+incompetant&amp;btnG=Google+Search) buffoons.

Since you're angsty, you cannot possibly be a good programmer.

A good programmer would never bring his own skill into question. For he _knows_ he is right.[deleted]&gt; Not available under Windows (except with cygwin, and perhaps SFU).

[Oh really?](http://unxutils.sourceforge.net/)Except your boss (and most managers) are thinking "how can I get promoted/bigger bonus/avoid the blame for some f***-up/etc."Assignments are expressions in Ruby, so:

    if value = try_parse("5")
      ...
    else
      ...
    end

or you could use exceptions or a block:

    value = tryparse("5"){ ... do something else ... }

for example:

    value = intparse("5.3"){floatparse("5.3"){0}}

tries to parse as an int or else as a float or returns 0. These functions don't actually exist though...The Pylons were created by man.
They evolved.
They rebelled.
There are many copies.
And they have a plan.Io also has call by code. Methods get passed the code as a parameter and they can evaluate it in the caller's context. Like runtime macros.Private-public-key authentication is quite good, isn't it? Now put those key into an USB stick or Smartcard and give that thing the ability to do then en-/decryption. The private key *never* leaves the device and is not readable without destroying it.

Now you can make sure to speak to that device, which can still be stolen physically, though.

[This](https://www.fsf.org/associate/support_freedom/fsf-privacy-key.html) comes close i think.I make a lot less than I might in Silicon Valley doing the same work, probably a 40% cut.

In Silicon Valley I'd be living in an apartment. In Indianapolis my $1000--$1100 per month pays for a 2800 sq. ft. home.

Google this to feel insulted: "Hear him!  They are the double—ye have confessed it yourself."Why not just count words by counting word boundaries? Seems to make more sense than counting characters and dividing by 5....In 2007, most software development is web development. It's one thing to wish for Haskell to not be mainstream. It's another thing to wish for it to not be relevant. Also: it is precisely the role of Web frameworks to make Web development not boring. The Web is just a user interface. Saying that "Web development is boring" implies that command line tool or rich GUI development is somehow exciting. If a Haskell framework does a good job then it will make the Web development part so easy that the developer focuses on the interesting parts of their problem, whether it be scientific or business or whatever.&gt; Btw, doesn't using SQLAlchemy makes you unable to use Django's admin interface?

Sigh. No, no it doesn't. You can run the Django admin on top of SQLAlchemy. You can run it on top of SQLObject. You can run it on top of Rails. You can run it on top of a crufty old PHP/MySQL bulletin board (seriously, I know someone who does).

Here's how you do it:

1. Run `django-admin.py inspectdb` against your database. This will output a Django model file.
2. Tweak the model file in case it doesn't quite match your DB.
3. Fill in the minimum Django settings to tell it where the DB is and how to connect when running.
4. Drop the Django admin app on a URL somewhere.

So long as you're not using ultra-exotic SQLAlchemy features (like having a class be represented by two rows in one table, five rows in another, and six tables in an entirely different DB -- I dunno why people need to do that, but apparently SQLAlchemy supports that kind of thing), this will Just Work.

Yeah, you need to do a bit of configuration in there, but it takes all of about five minutes and, considering that the admin app really isn't intended to be a decoupled component (it's written as a Django application, not as a component of the framework), I'd say it's pretty darned impressive.

Can we _finally_ kill the "Django hates the world, they force me to use all their components" myth?If i understand, he hooks into the macro-expand call and tracks the dependencies. My favourite programming language would probably mean Python. Python has no macros, thus no macro-expand call,thus no "compile-time dependencies" as he calls it.Also.

&gt; Why choose a framework if you plan on not using any of its default choices?

So don't use it, then. No skin off my back :)

Seriously, there are full-stack frameworks which aim to give you everything out of the box, and then there are glue frameworks which give you a bit of duct tape and say, "go out and pick some components, use this to stick them together". Pick the type that goes with what you like.

&gt; And Mako is very different not only in performance but it also supports real python code not just a subset.

OK, so use it. The Django templating language deliberately forbids arbitrary Python in templates, and there are philosophical reasons behind that. If you don't agree with that, use another templating system ;)

Seriously, I like Django and I'm incredibly productive with it. Other people don't like it and aren't incredibly productive with it. Everybody should use the tool that suits their job and their style, which is why I hate people who complain that "Python should consolidate to only one framework".The weak version borders on a tautology -- assuming, as many folk seem to do, that we "think" in some form of language, weak Sapir-Whorf says "the way you think has an influence on the way you think".You can traverse, and even manipulate Python parse tree if you wish, if I remembered correctly, but they tend to differ a lot from the code you actually wrote (since Python is full of shorthands and infixes) in a way that is much less comprehensible than Lisp's. It's a tradeoff. Less machine-readability for more human-friendly code.Of course, the owners and executives will be thinking "is this good for the company" and "be passionate about the company".  But that attitude is not appropriate for the line workers.  Lineworkers need of craftsmanship and artisan pride, with plenty of teamwork and good customer service.

Would you rather buy horseshoes from a guy who is striving to make the best fitting, longest-lasting horseshoes (researching and learning all the latest techniques) or the guy who has big corporate mission statement posters and a glossy company logo on the horseshoes?  If you choose the latter, you probably are already in a corner office.

In software, and probably most other professions, the article is correct.  Corporate yes-men are good at pushing the company story but not necessarily good at the actual line work of the company.  Don't elevate hollow sentiments and reward based on blind obedience to them.  Reward quality and timely work.Given the format he was describing to display the data, i don't think he'll run into that problem -- he seems to be generating an administrative screen, not an end-user screen.&gt; For completeness, I also looked at Django, seduced by its awesome admin package, but realised my problem was less 'CMS' and more 'web-app' than I initially thought.

Give me a little time, I've been busy at the day job, but I've got something just about done which will hopefully start the backlash against that perception*.

* Though, to be fair, 99% of all "web apps" are really just a CMS in fancy clothes, since ultimately it's about some type of _content_, be it photos, documents, task lists, what have you.I took that one right out of the article...

All successful (all truely successful, not just promoted) people learn in life that you should say "no" sometimes.  I have to do it with my kids, I have to to do it to the Sales guys that ask me to sign up for unsupportable project creep.

The point being that the "yes" guy will say yes and do what is asked...even if it's not in the interest of the company as a whole.  The realistic guy will say yes or no, depending on what is doable within reasources and viable for the company.[deleted]Pretty much. Or at least for what I do with it. It just seems like there's a lot of overhead for a given task.The author's soft tone of voice doesn't hide his elitist worldview, as he deftly illustrates with this:

&gt;being able to solve problems like the one you presented here is part of the difference between someone assembling kludges for a living, and a real computer scientist. Make the right choice.

Thus people who can solve problems like these are "real computer scientists"; those who cannot are "assembling kludges for a living". Nevermind aptitude, nevermind the desire to learn or the quality of education - if you can't do something like this, you are not a "real computer scientist", whatever that means.

In all my travels I have never seen professionals in any other field so eager to wall off people trying to learn, even if they are trying to learn the wrong thing. I see the modding down of my initial comment as little more than unfortunate confirmation of that fact. Many people in the computer field place a premium on appearing smarter than others, and take opportunities like this to lecture to people they see as lesser mortals because of it.To me, the most amazing thing about that link you provide is just how many people have misspelled incompetent while criticizing others' incompetence.  ;)  Kinda ironic.I think the author's point on "understanding" a little too broad AND narrow.

Too broad in that it's hard to discern when one understands a block of code.  Is it when one knows all the possible outcomes?  Is it when one knows exactly what happens at the machine level?  Is it when one knows how best to optimize the block?

Too narrow in that it doesn't encompass what programming IS.  Programming is about solving problems.  In order to solve a problem, you have to understand THE PROBLEM, not just the code.  

I could know a bunch of code inside out, but unless I know why I wrote that code in the first place, I might not be composing it in the best way possible.[deleted]Out of interest, are you [this](http://simonwillison.net/) simonw? 

To fill out my above aside on Django, I was drawn to my new project having spent half a day at work being bored witless at a [moodle](http://moodle.org/) training session, and realised I could do a better job. Since Moodle is basically a glorified CMS for teaching materials, my thoughts immediately turned to Django. I wanted to use it for the CMS tools. Once I explored the problem a bit more, I realised the CMS stuff was really a sideshow, and the teachers I work with had problems that wouldn't be solved that way. There were a couple of things about Django which didn't quite click with me, as a programmer (though that's not the frameworks fault, and given time I could have got used to it), so I had a look elsewhere. 

Regardless of the relative merits/warts of Django/TG/Pylons/Rails etc, I spent most of this afternoon debugging that infernal postback loop in ASP.NET. Now *that* is a web framework with not just warts, but great seeping buboes.&gt; What pisses me off are ignorant people spreading misinformation about and bashing languages they don't even know (as can be seen by their nonsensical statements about the languages).

This is unfortunately what you're doing yourself with your bashing of Python:

&gt; I've never looked at the CPython sources, but I can only imagine the ugly if statement for function calls, namely:
1) if foo is of class &amp;lt;type 'function'&gt;, execute the function code, else goto 2)
2) Lookup the __call__ attribute of foo and goto 1)

At least in practice, `foo.__call__()` and `foo()` are the same. How that's implemented is irrelevant. It's not a hack, it's a part of the language design.

I *have* bothered to learn Ruby, and one thing I wish for is a clean way to do higher order functions. As you correctly point out, there are no real functions in Ruby, however, at last it should be possible to do higher-order *messages*.

Also, I don't like the fact that a passed-in block is a pseudo-object unless you specifically put it in the argument list. Look at these two:

    def foo(&amp;block)
      block.call
    end
    
    def bar()
    end

Why is it OK to call 'foo' without a block when it clearly wants one, and why is it OK to call bar with a block when it clearly doesn't? In fact, if I call foo without a block, I get a MethodError. If I alter foo slightly, though, to use what you call syntactic sugar:

    def foo
      yield
    end

And then call it without a block, I get a LocalJumpError. As you can see, two methods I'd expect to behave exactly the same if "yield" was syntactic sugar, don't.

It's messy. Most of the time, it stays out of your way, but sometimes, it's a pain in the ass. I wish this pseudo-object stuff could go away in favor of blocks that behave sensibly, such as in Smalltalk (at least what I've seen of blocks in Smalltalk).I'd be very interested to see that, I just want to find the best technology to deliver my ideas. As I mentioned in reply to simonw in another thread, I wanted to use Django specifically for the CMS, then realised I didn't need it so much. I moved from needing to serve lots of content to lots of people, to serving targeted content in various convoluted contexts to a relatively small group of people.Yep. That's the question to answer. Everything else here reflects people's excuses for putting other [usually selfish] interests ahead of their own quality of service. It is especially lame to cite the bad behaviors around them as reason for compromise; one should smartly conclude that people succeed _despite_ bad behaviors, not _because_ of them. And where it would appear otherwise, that only reflects a broken environment that promotes badness. How to address that? By being good anyway.

It's never time to give up (although sometimes it's time to leave).

Unrelenting attention to service, the act of being a servant to the company, is a lofty, virtuous, challenging and MOST REWARDING strategy. (Just never leave your ethics hat at home.)The author actually made that point -- discussed the way Rake broke the expected behavior.Er . . . are you suggesting that the author of the linked weblog post is dismissing Ruby as "useless" and "not real world", or that he doesn't learn from Ruby?  If so, I think you may need to read the post again, because you missed most of it.

If not, I don't understand how your point relates to the post at all.  Perhaps you could clear that up.[removed]This makes me wonder if Google has any special insights into sparse matrices.Testing with Rails is way too coolDisagree. Mabye curiosity is a sign of a good programmers, but not "absolute need to understand". There're times when you just have to treat something like a black box.My point was that it's not available by default. Thanks for the pointer, though.Well if you complain a lot you will not get promoted.  However in most companies I've worked for, if you complain but then create solutions, then you are promoted.I do small business web apps, mostly.  My clients (and their clients) want backend adminstrative screens for managing what their websites and webapps look like, within the design they have.  It's incredibly useful that I can deploy the same or very similar code to multiple sites and have the look and feel of the app very different depending on the client's needs.  

If you're writing a backend site for a single app for a single company, I can see where such customization isn't such a big deal, but there are a lot of sites I deploy with widely varying functionality, and I don't want to write more than one user management module, ever, if possible.

Perhaps it's just me, though. :)[deleted]I don't think this is only Rake's fault. The fault is also on the Ruby community, which often advocates the use of meta-programming features even when they are not needed and actually increase the risk of your code breaking in weird and unexpected ways.

This is what makes me uncomfortable when using Ruby: every module I import might change the behaviour of any other code without me noticing.I'm sorry you work at a place like that.  I'm not sure it's the norm, at least at smaller companies, where the survival of the operation isn't something you can take for granted.No, Ruby is pretty mainstream by now. Languages that get OOP right, such as Common Lisp and Dylan, are not mainstream.Still, it makes a good point.

As programmers, sometimes we may seem to be overly literal--but sometimes it's more obvious to us when things are ambiguous and subject to interpretation.If he were being sensible, he'd just use 'forward-word' (which may not be perfect, but works a lot better than dividing characters / 5), which further emphasizes his point about editor macros, makes the code cleaner since he can strip out the division, and just looks nicer.I take your point.  

&gt;In 2007, most software development is web development.

Is that so? I'm not saying you're wrong but it sounds unlikely to me.To make my point clearer: It's easy to duplicate Rake's features and syntax _without_ resorting to dirty hacks and interfering with other people's code.

&gt; This is what makes me uncomfortable when using Ruby: every module I import might change the behaviour of any other code without me noticing.

The solution is easy: When the API does something "magic", find out how it achieves this magic. If it's implemented in a clean (e.g. without injecting methods into Object, builtin classes) use it, otherwise don't use it and contact the author.

For Rake the following three lines in the source were enough:

    module RakeFileUtils
      include FileUtils
    ...

then, at the loplevel:

    include RakeFileUtilsI think this article misses the point of being passionate about a company, which is really just a collection of people with a dedicated purpose.  It is an ideal starting point to create passion.

Leaders need to have a vision for the company and direct that vision effectively.  A company should create a culture that values positive relationships over negative ones.  It should nurture employees.  And it should provide a product or service that is valuable to customers.

When servant leaders develop this foundation, employees must take the next steps of - believing in the company's cause, foster a positive work environment, treat coworkers and leaders with respect, and not trying to sabotage company initiatives.

In short and to steal a famous quote, each of us from the CEO to the janitor needs to be the change we want to see in our company.But but... Paul Graham doesn't talk about RTP!![deleted]In the meantime, keep in mind that [Tabblo](http://www.tabblo.com/studio/) is Django-powered, as is [one of the largest social-networking sites in Poland](http://grono.net/).

See the [list of Django sites](http://code.djangoproject.com/wiki/DjangoPoweredSites) for more examples.Me too. I used to be a frequent user; unfortunately, its inability to produce .so's forced me to switch to Chicken Scheme, which also kicks ass, but not as heavy-dutily.Is Haskell's web weakness because of inherent limitations of the language, or is it just because no one has created a good web framework for it yet? When I first started learning Ruby, it was not as good as some other languages for web applications, but that is because Rails had not yet been created..ram?  Where's a .wav? or a .mp3?I misread that as "[Why the Hate for Loop?](http://common-lisp.net/project/iterate/)".&gt;Note how the google page ranking algorithm works. It models a kind of random walk through the web and the solution to Ax=x is the limiting likelihood of picking any particular page.

Either A=I or you mean Ax=b&gt; This is unfortunately what you're doing yourself with your bashing of Python:

What was wrong in what I've said about Python?

&gt; it's a part of the language design.

So the designs for function call are:

Python: If object is of the built in function type, execute function code, otherwise lookup `__call__` attribute and repeat.

Ruby: Send `call` message to function object and treat it like any other "send message".

I prefer Ruby's (IMO simpler) design.

&gt; Why is it OK to call 'foo' without a block when it clearly wants one, and why is it OK to call bar with a block when it clearly doesn't?

The rationale behind this is to allow easy delegation, e.g.:

    def foo(*args, &amp;block)
      # do some work
      # we don't care about arguments and block here
      @other_object.bar(*args, &amp;block)
    end

&gt; If I alter foo slightly, though, to use what you call syntactic sugar... And then call it without a block, I get a LocalJumpError

Agreed, this is not ideal. I'd say both versions should raise the same exception. Though it's not so bad, since in both cases it's a bug in the code and thus explicitely catching the exceptions is not necessary.I lost interest and stopped reading after a page or two.Wonder what Knuth would think of this example of [literate programming](http://en.wikipedia.org/wiki/Literate_programming). I like it. The medium is the message.&gt; Python: If object is of the built in function type, execute function code, otherwise lookup __call__ attribute and repeat.

As I pointed out, this is wrong. It's simply: fetch the `__call__` attribute, and execute its code. That's not more complicated than Ruby.

&gt; The rationale behind this is to allow easy delegation

The block could easily go in the splat argument. It would look like this:

    def foo(*args)
      @other_object.bar *args
    end
    foo(1) { 2 } # args is now [1, lambda{ 2 }]

&gt; Agreed, this is not ideal. I'd say both versions should raise the same exception. Though it's not so bad, since in both cases it's a bug in the code and thus explicitely catching the exceptions is not necessary.

That "yield" is not the same as simply calling a block complicates matters. You now have two mechanisms that do the same thing. That's more complicated than the case with Python.Before taking anything away from this blog entry, make sure you read the first comment.yeah, i can spot one major tactical error. More than one parrot. tsk tsk. [from mokipedia] "its well known, in certain circles, that parrots are notoriously bad mannered upon meeting members of the clergy (citation needed)", and also. the cardinal might get nervous and clam up. sorry I ruined your post, but I'm already signed up with Acronyms AnonymousSummary: wah wah this is different to what I'm used to I don't like it.* Employment is for those who want to survive in life.
* Entrepreneurship is for those who want to succeed in life.[deleted]Of course it's because the majority of users are less interested in web development.&gt; One sign of a good programmer is their absolute need to understand.

And tomorrow we'll see an article that says "One sign of a good programmer is the trust he has in other programmers, therefore not needing to absolutely understanding everything."* I hear, I forget.
* I see, I believe.
* I do, I *understand*."I'd never want to build houses with someone who is solely a kickass girl with a circular saw or hammer."

I don't think that metaphor really works, though. A more correct metaphor is building a house with a kickass girl with a circular saw, who cares about how the house turns out, but doesn't give a shit about the property management company that's going to sell it.&gt;&gt; Python: If object is of the built in function type, execute function code, otherwise lookup `__call__` attribute and repeat.

&gt; As I pointed out, this is wrong. It's simply: fetch the __call__ attribute, and execute its code. That's not more complicated than Ruby.

No, try this:

    &gt;&gt;&gt; class Foo:
    ...   def g(self): return Foo()
    ...   __call__ = property(g)
    ...
    &gt;&gt;&gt; f = Foo()
    &gt;&gt;&gt; f()
    Traceback (most recent call last):
      File "&lt;stdin&gt;", line 1, in &lt;module&gt;
    RuntimeError: maximum recursion depth exceeded
True, but emacs isn't available on windows by default either.&gt; This mess seems to be a symptom of its heavy insistence on static typing — this is what happens when you try to enforce typing too strictly. Eventually if you try to incorporate high level features and abstractions, you bump up against the fact that it’s not just about what type one piece of data is.

That's what I resent most about mainstream languages. They give static typing a bad name.That's because you're using properties. Properties seem like a hack to get something like Ruby does with accessor methods, but it's not a fault of the function call mechanism. What it's doing, as far as I understand (I'm not a Python expert) is this:

1. Fetch the `__call__` attribute of `f`. Since `__call__` is a property, call its getter method, which returns a new `Foo`. 

2. Call *its* `__call__` method. If this had been a function object, this would have executed its code. Instead, it repeats #1.

It seems perfectly consistent, it just does something you don't expect.It's exactly how I've said in the first place. Note that it has nothing to do with properties. Try this:

    &gt;&gt;&gt; class Bar: pass
    ...
    &gt;&gt;&gt; b = Bar()
    &gt;&gt;&gt; b.__call__ = b
    &gt;&gt;&gt; b()
    Traceback (most recent call last):
      File "&lt;stdin&gt;", line 1, in &lt;module&gt;
    RuntimeError: maximum recursion depth exceeded in __call__
&gt;The last time I looked Bruce was hiding out on the IBM forums writing articles for people who where interested in "crossing borders" like he had.

Bruce Tate is all about selling books.  Java doesn't sell very well, so his current favorite is Ruby.  As soon as the next thing comes along and Ruby book sales lag you can expect him to become a quick convert.

The rest was spot on.  Java is a creaky boat thats slowly filling with water.  Sun seems to have noticed this too, but instead of fixing it they seem determined to make the JVM a platform for various languages.  I thought this was a horrible strategy, but after looking at Scala I'm not too sure.No, A is not a variable.No one's made a Matrix joke yet? I'm simply shocked.There are ways to vary look and feel through the method he's beginning to use.

As Smallpaul says elsewhere in this thread, many frameworks offer some degree of html generation in their administrative interfaces.

Django is the example i'm most familiar with, so i'll talk about that. The administrative interface is entirely generated from your data model code. In the simple, common case, it shows all fields with the names given in your code. However, in your code you can override the name that's displayed for the field very easily (it's a matter of adding a single string argument to your field definition).

You can also change what fields are visible, what order they display in, how dependent objects are displayed, etc. These require slightly more work to change, but we're still talking about only a few lines of code. The stylesheet is also overridable, though that requires an actual css file rather than simple code changes.

So, a few extra lines of code with no html changes, or largely-identical html page templates cargo-copied from application to application?

In my mind, the answer to that question is clear. Given that the vast majority of changes requested are covered by this system, and that other changes can simply fall back to the template model (all the administrative interface templates are completely overridable), this approach makes a great deal of sense to me.
this article consists of useless ramblings only.
..neither does it make a clear point, nor gives it any particular examples of what's exactly wrong with Suns ui approach.The only real solution is to quit your job and start your own company where this isn't the case.&gt; It models a kind of random walk through the web and the solution to Ax=x is the limiting likelihood of picking any particular page.

Interesting - despite doing statistical mechanics for many years, I'd never thought of it that way.  MAke sense, of course, now that I think about it.

However, this makes me think about something: (from the article)

&gt; A page has high rank if it has links to and from other pages with high rank.

This _feels_ wrong to me.  People don't do random walks.  Prehaps rephrasing slightly (and I'm well aware that slight differences in English turn into large differences in maths), to give

&gt; A page has high rank if it has links to and from other pages with high rank _and get lots of traffic_.

Or maybe the 'lots of traffic' clause should be is a subtly different place.  The point is to acknowledge that humans are not photons, and whilst the algorithm clearly is good, it's not perfect.  A link farm is not the same value as Wikipedia, for example.

If people were doing a random walk, then the places with high page rank would get the high traffic - so in effect it's a correction factor for the non-idealness of the model.

Or, given I've been struck down with some sort of 'flu for the past week, I might be blathering nonsense...  Wouldn't be the first time.

This is an interesting perspective. I was in management for a few years, but I got out because I was concerned about losing touch with the technology. I might have been able to stay in management if I was also doing a little code.I always though it was all the people blogging who have little idea of what they are talking about that gave different typing systems different bad names...WTF? Of course they do!Read up on [eigenvectors](http://en.wikipedia.org/wiki/Eigenvalue#Eigenvalues_of_a_graph), especially of graphs.I mean, I wonder whether they know things about sparse matrices that the wider mathematical community does not. Maybe they've got some beautiful theorem on sparse matrices that is one of their trade secrets.

Or maybe they just use their massive computing power to crank it out in parallel. Who knows.[Peter, Michael, and Samir are chatting as they hang around the printer] 
Peter Gibbons: Our high school guidance counselor used to ask us what you'd do if you had a million dollars and you didn't have to work. And invariably what you'd say was supposed to be your career. So, if you wanted to fix old cars then you're supposed to be an auto mechanic. 
Samir: So what did you say? 
Peter Gibbons: I never had an answer. I guess that's why I'm working at Initech. 
Michael Bolton: No, you're working at Initech because that question is bullshit to begin with. If everyone listened to her, there'd be no janitors, because no one would clean shit up if they had a million dollars. 
Samir: You know what I would do if I had a million dollars? I would invest half of it in low risk mutual funds and then take the other half over to my friend Asadulah who works in securities... 
Michael Bolton: Samir, you're missing the point. The point of the exercise is that you're supposed to figure out what you would want to do if... 
[printer starts beeping] 
Michael Bolton: "PC Load Letter"? What the fuck does that mean? 
On the contrary, static-typing having a bad name means that those blinded by the myth will never learn what a type system really is. This gives those who do know a competitive advantage and so, according to thought experiment, should reap the rewards.

More cookies for us! Or is that being too greedy? :)Unholy gods, I had better take it off of my resume, and I wrangle Java code for a living.Uhh... a matrix joke, hmm.

What do you call a young eigensheep? A lamb, duh!You can take the last cookie or the last carton of milk, but not both.[deleted]this is awesome.No one can be told what the matrix joke isActually this tracks load-time dependencies between modules. Ie, module A needs module B and C to run. Python certainly has such dependencies, but its not clear if one could implement an automated way to track them.*Making simple things simple and hard things possible* (A.Kay)

I don't see how Haskells elegance in doing some simple things simple ( writing simple parsers is not a big deal. Writing them efficiently definitely is one ) helps the language in any way. This is also true for the ubiquitous quicksort examples whereas research has [proceeded](http://citeseer.ist.psu.edu/cache/papers/cs/9438/ftp:zSzzSzftp.cs.ust.hkzSzpubzSzdwoodzSzvladzSzsurvey.pdf/estivill-castro92survey.pdf) towards adaptive sorting schemes for quite a while.

Besides this I would like to see that at least Haskellians do not argue with being mentally handicapped ( "regexes are too hard for me" ) which is the rhethorics of scripting language apologetes these days ( "please don't confront me with anything I dislike. I will just react as a willfull imbecil in that case" ).It's truthy.

Would you buy the more modest claim that there are more Web interfaces to new applications than any other single kind of interface?

But anyhow, the more persuasive point is that techniques like the one demonstrated are designed to allow you to AVOID writing web applications. Like if you have a big dataset that you're doing all sorts of interesting Haskell-y computations on it, but you also want to be able to hire people who will browse and update the information to clean it up. That's the sort of thing you do with auto-generated admin interfaces in the "mainstream" web frameworks.I saw an ad in the article but it was for PID(?) Loops and suction. Not very relevant.&gt; "Don't like you job? Travel to Valhalla and steal a unicorn!"

I did that. It got old after the second trip.Yes, it does.

&gt; Ever since I've become serious at testing, my usage of debuggers has increased, if only because I write more code now than before (I didn't used to write any tests), and this extra code needs to be debugged, like any other.  Or do these people assume that just because the code that you write is called "tests", it's suddenly become bug-free?

If your tests are complex enough to require debugging, *you're doing it wrong!*. You should never be afraid of small tests, or tests that seem to obviously work. I have written many tests that consist of nothing more than:

    TEST("FooClass.set_name() and FooClass.get_name() work") {
      FooClass foo;
      foo.set_name("bar");
      assert(equal(foo.get_name(), "bar"));
    }

&gt; But here is my real secret:  most of the time, I don't use a debugger to debug.  I use it to verify that my code works as I think it does.

Every minute you use your debugger as a test library, is one minute you're wasting time not writing a real test.

&gt; I launch it to inspect all the variables, verify my assumptions, stare at my code for what it really is, not for what my biased view tells me it is.

&gt; ...

&gt; I also use the debugger to modify variables and try to trip my code, cause errors that could or shouldn't happen and make sure it reacts accordingly.

That sounds a lot like testing to me, except you're doing it in an ad-hoc unstructured manner.

&gt; Of course, eventually, I capture all of this in tests, but these approaches are complementary.

You should have recorded your tests in code, instead of debugger commands.&gt; and of course, I tested all of this quickly in my scratch buffer. Wow, who knew writing an emacs lisp tutorial was so easy?

The great thing about elisp programming is that everything you do in elisp corresponds very neatly to editing commands that you invoke while doing regular text editing, so there's no friction between your mental model of the API and the actual API. And everything is excellently self-documenting, so you can do all this stuff *very quickly*. Add in a bit of Lisp knowledge, and off you go.Java Joe alert!You might as well argue that a shovel is better than a hammer.  Programming languages are tools.  If the tool works, use it.Many philosophical differences keep me in the Python camp over Ruby. Most are six of this, half-a-dozen of the other sorts of things. But there are a few cultural things that would make me want to scream.

The casual acceptance of "monkeypatching" is the biggest of these by far. As your system grows in size, and the number of libraries you want to use in one program grows, the probability of monkeypatching screwing something up in an incredibly-difficult-to-debug-way approaches 1. Quickly.

Ruby is not the first language to allow that sort of thing. Ruby's community will not be the first to discover it is an incredibly bad idea. It's a medium short-term gain, it's a gigantic huge long-term loss. It can be a good trade for a quick script that you want to "just work", it's an incredibly bad trade for any bit of code that might leave your personal posession, like, say, a major library.

Expect a lot more of these stories.

Also note it's purely a community thing, and the resulting libraries. Python, as far as I know, can do everything Ruby can do with modifying classes on the fly, but since the community rejects monkeypatching, it's never a problem in practice. The solution isn't changing the language, it's refusing to write libraries that do that, and refusing to use them.

See also Javascript libraries lobbing things into the default prototypes. It's a bad idea.

If you want a theoretical explanation: The Liskov substitution principle may not be absolute, but it's definitely something you need to keep in mind. When you monkeypatch a class, you are replacing that class with a new class, which has a new interface. If it's not Liskov-substitutable, then you're asking for trouble. Given the types of things that tend to be monkeypatched, the people doing the monkeypatching, and the richness of introspection (which is part of the interface of a class), the odds of a given monkeypatch being substitutable are about 0.

(Note: I'm using Liskov substitution more as a convenient noun than an absolute truth. I could say more about it, but that would be out-of-scope.)

Blindly adding methods to the _root object_ in a language with a "method\_missing"-like functionality is just suicidal; it's not possible to create Liskov-substitutable replacements of every class simultaneously, in all likelihood.It's all a matter of who you're working with. Small companies are pulling in people from the same pool as everyone else, and a startling amount of those people are lying, opportunistic greasebags.But regexes ARE hard. Hard to read, hard to remember :))
I agree: small fonts are hard to read.  But what about white text on a black background?  The majority of web pages have light colored backgrounds with dark text (just like paper).  Why give our retinas a jolt by doing the opposite?&gt; XUL is shiny and neat, but locking out 70% of users isn't.

You did see the part where it produced normal DHTML equivalents for those 70% of users, right?Java Joe alert!And what's that reason?No form of advertising really works on me.  I typically decide I want to buy a particular type of product (i.e. MP3 player) and then research it by reading comparative specifications and professional and personal reviews.  I have never seen an ad for something and purchased it based on the ad.

Perhaps developers tend to do this.Because it's a pain to write

    for (int ii = 0; ii &lt; my_list.length; ii++) {
      System.out.println(mylist[ii]);
    }

vs

    for (String item : my_list) {
      System.out.println(item);
    }

Not only is the code clearer, it abstracts the loop process. What if you're operating on a container that doesn't support random access, such as a linked list? Or one with no concept of item order, such as a hash table?

When working with Java or C# code, I can never remember which lists use .length or .size() or .count or any other of a dozen other names for the same thing. With index-less loops, that doesn't matter.via
http://www.zefhemel.com/archives/2007/02/07/fighting-injection-attacks-with-stringborg
Fighting Injection Attacks with StringBorg [ ZefHemel.com ]
Ah!  Okay . . . you probably should have mentioned which languages you thought the author was denigrating, to make the focus of your statements clear.  Your comment makes a little more sense now.

Anyway . . . I didn't read his comment about Lisp as dismissive.  He was just giving us (the readers) a brief caveat about his lack of experience with it.  I'm pretty sure he in no way suggests that one cannot learn from Lisp -- just that he hasn't thus far.

I don't blame him for having chosen a different path through learning languages than you.  I, too, have probably taken a different path.  I intend to eventually get around to gaining some competence in Common Lisp and/or Scheme, but so far I haven't.  Would the fact that I haven't really learned either of those languages make me a Lisp-slandering boor as well?

edit: I can, however, see where you got what appears to be your interpretation of his words, though.  I just don't think that's what he meant.Honest question here, and I figure this is as good as any to ask it: why doesn't Django support easy_install?

I love that with TG or Pylons I can say 'easy_install Pylons'. I can't do that with Django why? Is that getting 'fixed' anytime soon?&gt;In order to solve a problem, you have to understand THE PROBLEM, not just the code.

You are not embracing the Zen.  With this article, the author does not speak of problems, nor solutions, nor.....hell anything really.  ;)Admittedly I don't understand why *Ruby is ( or is not ) an adequate Lisp implementation* is a problem for either Ruby or Lisp besides some programmers psychology and juvenile rhetorics ( "Lisp is for great hackers..." ) that seems to be important for some male egos only.

Recently the author of the blog made the case for the *return of algorithmic skills* in the professional IT world. Honestly I would like to read more about evidence for this than about flogging dead horses and pseudo-problem discussions.[deleted]Oops, another comment. :)

&gt; Seriously, there are full-stack frameworks which aim to give you everything out of the box, and then there are glue frameworks which give you a bit of duct tape and say, "go out and pick some components, use this to stick them together". Pick the type that goes with what you like.

But there are also some frameworks which give you everything out of the box AND make it easy for you to customize the framework. Django makes it rather difficult. Something like TG gives you a base set of components to use out of the box, but because it supports things like Buffet you can plug in any templating language with 1 line.

Heck even Pylons using its idea of Paster templates makes it easy for anyone to have a basic set of compoments to use.

I would love to see such cross-platform components put into Django because it would make it that much better and that much more flexible.

I know I said this just the other night on Reddit, but the fact of the matter is, Django doesn't provide any more functionality out of the box than any other framework (with the exception of Pylons' main template not implementing an ORM, it leaves you to chose that), but Django is much less flexible than TG or Pylons. Or maybe I should rephrase that: perhaps it isn't technically less flexible, but it isn't nearly as easy to customize.This act was copied by the Homebrew Computer Club on their Altair 8800see also:
http://www.lshift.net/blog/2007/02/01/rabbits-rabbits-rabbits
LShift Ltd. » Rabbits, rabbits, rabbits

http://patricklogan.blogspot.com/2007/02/rabbit-run-rabbitmq.html
Making it stick.: Rabbit, Run: RabbitMQ
home: Mantis.
work: Mantis. 
Every/any bug tracking system needs some discipline to actually work. Develop this and you 'll find which one suits you. I 'd suggest to test run before you buy. I did that before we decided in a tracker (actually our first installation was bugzilla; trying to show to Q&amp;A how the whole thing worked was hilarious, Mantis came in more natural). I guess I could live with any tracker as long as there is one ;-)&gt; Why not just...

He's saving that for next week's one hour 800, er, 1387 word post.
&gt; the issue is not whether there’s some corner case “gotcha,” but whether using Ruby is, on the whole, more beneficial than not using Ruby.

Okay, but ugly corners can be perfectly good arguments against languages on the whole. As a model, you could evaluate a given language on (A) its huge wins, like macros in Lisp, pointers in C, and regular expressions in Perl; (B) its libraries and other more or less easily replicable stuff; and (C) its losses. If you consider two languages tied for your purposes in A and B, it’s not rude or cheating to look at how they fall over.

Gotchas are a kind of bug, and bugs are a perfectly good reason not to use something.

I’m not saying anything in particular about Lisp or Ruby. My only point is that it’s not reasonable to discount ugly special cases just because they’re special.CherryPy 3.0 is much more WSGI-based, and from the initial announcments TG 2.0 itself will be leaning much more heavily on WSGI (there was even talk of going outside of CherryPy for 2.0).&gt;   if value = try_parse("5")

Under what circumstances would that run the if code and when would it run the else code?[deleted]We differ. :)  

I don't actually write the HTML or CSS, except for minimal examples to show intended usage.  So, since my web designer or the client's web designer may want to make changes of kinds I can't predict, I prefer to write the code in such a way that no code changes are required except for actual functionality changes.  The alternative is to regularly be called back to make minor code changes for web design changes, which is personally annoying and seems unfair to my clients.  These things should as orthogonal as I can make them.

I understand that there is a lot of HTML generation out there, and it seems to me that that's the best option in exactly one situation:  you're going to be working on this application for a long time (rather than delivering it as a product in a few weeks), *and* you don't have any separation of programming and HTML writing on your team, so that the programmer is going to be intimately involved with the HTML anyway.

If, on the other hand, you code up and deliver fully finished products in days to weeks, and the client may well hire someone else later to do further HTML or CSS work, they ought to be able to hire someone whose specialty is actually design, regardless of that person's programming background or lack thereof.  My wife does web design (and other art) for a living, and she charges a **lot** more if she has to wade through screenfulls of PHP or other "should have been in the code" templating junk to implement her design, so I've heard the ranting from the artsy side, too.

Another webapp possibility is the kind of work where there are multiple people involved in web design or image production, and multiple people involved in programming.  It doesn't seem to me that it's cost-effective to expect each person doing this to be able to do all three kinds of work.  Sure, there are people out there that can do all three of those well, but the vast majority of people who *claim* to be able to, in fact only really do one of those well, and sort of punt on the other two with just enough to get by, mostly.

Anyway, I understand that I'm in a niche, here (small business webapps outsourced to other small businesses), but it does seem to me that the vast majority of webapp frameworks today, with their assorted HTML generation systems and complex template sub-languages, just serve one market: 1-5 person teams on which every person does some of every kind of work on a single site or small stable of sites, indefinitely.  And sure, that's what many startups look like.  It just isn't what I'm doing.  :)He took my comment off his blog too, pointing out his mistake (nicely of course).I think it's probably true.  All those in-house client-server applications that companies used to commission?  Most of them are just building webapps, now, I think.&gt; It doesn't. It uses a much different paradigm so it doesn't need to.

Sorry If I'm sounding dense, but are you really saying there are no clean ways to reuse blocks of HTML/JavaScript?

Another thing that concerns me is how much the code is mixed into the HTML. Look at this page:

http://www.onlamp.com/pub/a/onlamp/2005/01/20/rails.html?page=5

That looks a lot like the code burned us over and over again in classic ASP.

&gt; There are lots of testimonials on the web from ASP.NET programmers who say they are much more productive with rails and rails is easy to learn so you might want to fire it up and see if you like it.

I don't put much stock in testimonials alone, as I have no way to know if the programmer was an experienced ASP.NET developer or some hack that was frustrated because he .NET wouldn't allow him to continue in the classic ASP style.

If I cannot see actually code that looks cleaner than what I'm writing, I get a bit nervious. And just trying it out myself won't help much if I'm still writing in the ASP style instead of the Ruby style.Ah, the venerable "The Code is Ugly and I Hate It" pattern.[deleted]That's all true, but I'd rather see more posts on web development in Haskell, like this one, over posts about parsers, closures, and 'functional programming is good for composition'. 
When will the Happs hackers get a blog!?&gt; So long as you're not using ultra-exotic SQLAlchemy features (like having a class be represented by two rows in one table, five rows in another, and six tables in an entirely different DB -- I dunno why people need to do that, but apparently SQLAlchemy supports that kind of thing), this will Just Work.

nah we cant do that ;)Agreed. Thanks for your clarification.There's an [88k powerpoint presentation](http://home.comcast.net/~tom_forsyth/papers/pixomatic_gdc2004.ppt) about Pixomatic for those curious about Abrash's more recent Hawt Software Rendering Craziness than Quake 1...

It's not very rambly, either. Beware of what you wish for! :P
good luck finding HTML scripters on craigslist who know haskell....This is more of a bug in Ruby's scoping than a fundamental difference between Ruby and Lisp. Do I recall correctly that Matz talked about fixing this in the next version?Were you trying to express an idea? You didn't. Whitewashing the entire problem space, normalizing it into a 1-dimensional projection, then proclaiming it to be a simple and homogeneous entity is not a valid argument.[removed]Python primarily makes three products of the source code available for inspection: the bytecode (I wrote [an article posted to Reddit](http://programming.reddit.com/info/75wj/comments) some time before that explains how to use this, for example, to find out what globals a piece of code accesses), and two kinds of parse tree. One is hideous to read due to the shorthands A_B mentions; the other is actually pretty obvious. Here's a comparison:

    &gt;&gt;&gt; import parser
    &gt;&gt;&gt; st = parser.expr("2+(6*7)")
    &gt;&gt;&gt; st.totuple()
    (258, (326, (303, (304, (305, (306, (307, (309, (310, (311, (312, (313,
    (314, (315, (316, (317, (2, '2'))))), (14, '+'), (314, (315, (316, (317, (7, '('), 
    (319, (303, (304, (305, (306, (307, (309, (310, (311, (312, (313, (314, 
    (315, (316, (317, (2, '6')))), (16, '*'), (315, (316, (317, (2, '7')))))))))))))))), (8, 
    ')')))))))))))))))), (4, ''), (0, ''))

gak! On the other hand:

    &gt;&gt;&gt; import compiler
    &gt;&gt;&gt; compiler.parse("2+(6*7)")
    Module(None, Stmt([Discard(Add((Const(2), Mul((Const(6), Const(7))))))]))

Maybe you miss the context. In Lisp context people talk about language X having lisp nature or being acceptable Lisp. This does not mean that language is bad if it does not have Lisp quality. It means that you can't program in Lisp way. 

Here is what Matz himself has said:

&gt;Ruby is a language designed in the following steps:

&gt;* take a simple lisp language (like one prior to CL).
* remove macros, s-expression.
* add simple object system (much simpler than CLOS).
* add blocks, inspired by higher order functions.
* add methods found in Smalltalk.
* add functionality found in Perl (in OO way).

&gt;So, Ruby was a Lisp originally, in theory.
Let's call it MatzLisp from now on. ;-) 

Because Ruby is not ready language (big, redesing is coming), you can fairly say that it has many defects in its core language that Matz and Co. try to correct. Lisps like Common Lisp and Scheme have been ready for decades. There will be more additions to language, but old code still runs. 
You have misinterpreted me entirely.Veni, vidi, gnocchiHow? You didn't leave me anything to interpret.Been playin' this tune for 25 years, and the rewards have been outstanding.

Dare to be that good, my friend.oh, it came backHardly. A good programmer assumes that 2+2 usually equals 4.&gt; Because Ruby is not ready language (big, redesing is coming), you can fairly say that it has many defects in its core language that Matz and Co. try to correct.

Actually, Ruby is 13+ years old, and quite "ready".  That a language evolves over time is a *good* thing, not a marker of immaturity.  

Are there defects?  Yes.  Many? I'm doubtful.
It's not like there's anything available by default under windows as far as programming goes, so that point is moot (unless you want to dev with Notepad and hand-tailor your binaries that is)[removed][removed][removed][removed][deleted]I, for example, like the modularity of Pylons, but the larger community of Django is often a bigger advantage.how in the fuck has this drivel gotten to number 1?  oh yeah, dipshit redditorsyeah, cuz like you can totally just divide by x on both sides!!I haven't actually investigated the issue, but it could be possible with [import hooks](http://www.python.org/dev/peps/pep-0302/). Maybe even with just the old method of overriding the built-in `__import__` function.Yes, see "Block arguments are always local" at http://eigenclass.org/hiki.rb?Changes+in+Ruby+1.9#l8You got me. And you, apparently, have been busy watching Office Space?

If you should ever have the fortune of working for an on-the-ball, kick-ass boss who knocks himself out for *you* **and** *the company*, you'll get where I'm coming from. My people know what I mean. They would get a good laugh from you guys. They too once had low aspirations.I like this line. "Our job is basically telling a computer what we want it to do. The challenge comes in how really stupid the computer is..."Note:  the 2.5 release merged in the long-awaited AST branch, so the human-hostile parse tree is a thing of the past.That is one confusing comment, mr. According to you, is the program good, or is it whack? ; )&gt; Hear hear

Upmodded only because I can't remember the last time I read a post where someone spelled this correctly.[As of 2.5](http://www.python.org/doc/2.5/whatsnew/ports.html), there's a fourth one:  direct access to the newly-introduced internal AST.  Other than [PEP 229](http://www.python.org/dev/peps/pep-0339/) and [this overview](http://www.python.org/doc/2.5/lib/ast.html), official documentation and supporting tools for this are sparse, but hopefully this will improve.[deleted]I just dropped a class because the professor couldn't explain this for his life. This page just saved me 3 credits of boredom and agony.I doubt [efficiency is going to be a concern](http://shootout.alioth.debian.org/gp4/benchmark.php?test=all&amp;lang=ghc&amp;lang2=python).Yawn.  A steaming pile of HR horse-shit.  Well, I guess somebody's got to live in a world where this kind of crap matters, otherwise the economy would not function, right?  So thanks to all of you for stepping up and taking the plunge.  Way to go!  You make it possible for the rest of us.  (I already know that I'm a condescending bastard.  No need to tell me about it.)[removed][removed]Do they know about the Netflix prize?Well, 13+ years means that we are talking about young language that is under development. 

When I'm looking  at the discussion of the new features of next Ruby, I would not call it evolving. It's more like redesign. And for good reasons. Compare that to next Scheme version. Scheme will add lots of stuff but that is just augmenting the current language. The feel of the language is not going to change much. But Scheme started at 70's. 

Alan Kay and Co. got Smalltalk right  quick. From Smalltalk-71 to Smalltalk-80 just in 9 years. Python and Ruby have reimplemented some Smalltalk ideas with different syntax, but they have still long way to go before they surpass it. 

That is a very good read, thanks.Hmm. There don’t seem to be any arguments there that I’ve not seen before, any that would challenge my opinion that continuations and REST are not really compatible. The discussion seems to only reinforce that, in fact. The blog post author seems to give up arguing at some point, falling back on this:

&gt; I don’t see why anybody would rather read a W3 spec than create something useful.

Well, maybe that is because the “W3 spec” (since when is REST a W3C spec?) describes the only system that has ever successfully scaled in the large, while all the message passing style architectures have consistently failed for decades. Maybe continuation-based webservers will finally break this trend, but based on experience I’ll be damned to hold my breath.

As a commenter writes:

&gt; Doesn’t Seaside stores state in the URL itself, making it impossible to bookmark your progress through the webapp? I’m sure you can get a RESTful Seaside webapp, it’s just you’d be working against the whole continuation-based foundation of it.

Now, in the reddit comments [you basically say](http://programming.reddit.com/info/104kd/comments/c10c5c) that Seaside is good for building intranet applications where productivity is more important than scalability. That is a position I can totally agree with; the right tool for the job and all that. But it’s basically tacit concession that Seaside is not meant to scale to the open web. Certainly what I read about its architecture seems to preclude avoiding any per-client state in order to stick lots of cheap caches in front of the app servers, at least if you want to benefit from the things Seaside does well, rather than fighting it.Care to explain why IBM still exists? Of all places I have worked for, the OP's original description is most definitely accurate in this context.

I fondly recall the day that the "Internet Security Specialist" sent a broadcast on the internal IM system asking, "what is the S in HTTPS?". That moment summed it all up so very well.Is LL(1) efficient enough for you? That's the default with Parsec, you have to use try to get anything else.Yes, the world would be a wonderful place if we could completely eliminate shared state.

But there are some cases where you just &lt;i&gt;can't&lt;/i&gt;--Tim Sweeny at Unreal Games uses the example of 10,000 in-game entities interacting on a 4-to-12-core system.

And when you must have shared state (ugh), then Software Transactional Memory is a powerful tool for staying sane. It sharply reduces the problems caused by deadlocks, race conditions, and other traditional concurrency nightmares.Valid issues, but they mostly have to do with the amount and nature of the state in question, not the server-side representation of it (in continuations or in plain data structures).

&gt; Admittedly, these are issues I just thought of off the top of my head without even searching for any prior art on REST with continuations.

Well, ideally, the two are orthogonal:

* Continuation-based resources (versus not) should only be about how you represent and manipulate state in your server code.
* REST (versus not) should only be about how and whether you transfer said state to the client, or not.  A sufficiently advanced environment would allow you to toggle both independently.

&gt; Any thoughts on these?

More work needed!  The widespread lack of support for serializable continuations doesn't exactly help, though.Yup, that's me.It looks like an absolutely optimal algorithm (what they ask for) would be NP-complete.[deleted]Because I prefer Django's core HttpRequest/Response API and URL handling to the mechanisms used in TurboGears and CherryPy. I also like Django's templating, which combines execution speed with safety (no executable Python in your templates), extensibility (you can do lots of smart things with custom template tags) and really neat tricks around template loading and inheritance. Of course it helps that I helped design those components, making them a really good fit for my tastes.

And like ubernostrum says, if I need the Django admin interface I'll use inspectdb to introspect the models for it.I thought they were only asking for an approximation that maximized social utility as given in the problem specification.&gt; Valid issues, but they mostly have to do with the amount and nature of the state in question, not the server-side representation of it (in continuations or in plain data structures).

True, but it seems to me that the central issue is whether it is possible to still retain (nearly) full control over the amount and nature of the state even as its server-side representation is made transparent. It seems to me that you can’t get the former without giving up at least a little of the latter.

&gt; More work needed! The widespread lack of support for serializable continuations doesn’t exactly help, though.

Oh, I can agree with that.That sounds like a non-version, not a "weak" version. :)  (For starters, i doubt any researchers in the field still entertain the idea that we "think" in anything approaching natural language.)

What i'm really trying to say is that while the "strong" version applies to neither natural nor programming languages, the "weak" version applies to both.Those accursed monkeys, bedeviled clowns; they are the reason I can never find rest.I don't get it.[removed]Wait, if it's an approximation, then it *doesn't* maximize social utility in every case. Otherwise it is optimal.[gforge](http://gforge.org/)&gt; This feels wrong to me. People don't do random walks.

You can also interpret it in terms of a kind of trust network. Think of a link as a referral and the rank as a measure of trust; if you trust someone, you are likely to trust their opinions, hence the quality of their referrals. Or perhaps 'trust' is a loaded word, and something like 'interest' or 'usefulness' is better. Whatever the exact term, the interpretation comes down to this quality being, in part, transitively conferred.Welcome to engineering.&gt; it seems to me that the central issue is whether it is possible to still retain (nearly) full control over the amount and nature of the state even as its server-side representation is made transparent.

It's probably a lot easier than you think.  A reasonable implementation won't serialize data that isn't needed by your continuation, so in most cases the state stored at any given point should automatically be minimal (and probably an improvement on what you would achieve with manual management, unless you go to great lengths).

The worst optimizations you're likely to have to do in such an environment would be things like occasionally reducing the scope of some state variable via a let-expression, or similar (which is generally a good practice anyway).

&gt; It seems to me that you can’t get the former without giving up at least a little of the latter.

I think the more pertinent question is:  can you approach the efficiency of automatic scoping using manual state management? :)[deleted]Hey, I know what [PC Load Letter](http://en.wikipedia.org/wiki/PC_Load_Letter) means!

I learned it from a trade journal (bought with my own money) which I read last night -- I follow it because it's my hobby and it was recommended by one of the leading people in my industry.
Thanks for that link.WTF! they don't accept CL, Scheme, Smalltalk or Ruby?
`n=16`. A doubly exponential (or whatever) complexity isn't too bad when its multiplicative constant and `n` are small.They don't ask for an algorithm for the problem in general, just for a solution to this one specific case.  You can pre-calculate the solution, and have your program just print out your answer.Oh no, give me back class Symbol &lt; String
[removed]&gt; First of all you don't even have to use javascript, you can do all your ajax stuff in ruby. 

Yea, we used to think that about ASP.NET too.

&gt; Secondly you can use builders to make your XHTML and they are like any other objects you can reuse or extend.

Ok, now we are getting somewhere. Can you give me an overview or link on this.

&gt; Finally the views are kept separate both as pages and partial pages and you can render them any time you like.

I don't feel like talking about views, the advantage of this is pretty much a no brainer. But can you tell me about these partial pages?

&gt; No it's not. Obviously you need to have access to the items you need to display, you need to be able to refer to them, you need to be able to iterate over them. That's the minimal amount of code required to display the objects you want to display.

Iterate over the objects? No, that isn't generally done in ASP.NET. Normally you pass the collection to a grid or repeater. All you need is the property or column name and any formatting info.


&gt; If you don't want to try it then that's fine but I think you will have a lot of fun both with ruby and ror at the least it will give you a new perpective.

Actually I am working on learning Ruby, but I find these kinds of conversations are far more effective in learning than bumbling along on my own.
[deleted]This is about publicity.The answer is out there, OMouse, and it's looking for you, and it will find you if you want it to.Worse the that, check the sidebar, headers and every other comment: grey on black, white on grey.

The white on black is kinda a switch, but at least it's high contrast.
OCaml for the save.
&gt; Later on someone reports a bug in your program, and you have no idea what's going on, as the bug report is vague, it only happens sporadically, and your clues are few and far between. Pressure mounts from management to find a solution, customers are angry, and you begin to wish for a new job.

This is *precisely* why I left my .NET programming job.

My favorite example:

I had written some .NET code that used a web service to connect to our master server, see if any updates were available, download them, and install them.

It didn't take much code (it was the most elegant thing I have ever coded in .NET) and it ran very well.

For two weeks.

Suddenly, the code just stopped working. The web service connection code appeared to be functioning normally (no errors during the connection process), but nothing was ever transferred across and the program would crash if data was sent or requested.

I'd like to stress that neither the web service updater code I had written *nor* the web server had been changed when the problem arose.

I was not able to find a solution, but another coder in our company *did* (after a late night).

It was this very solution that solidified my hatred for .NET and Microsoft APIs in general.

The solution:

1. Open up a second web service connection.
2. Do not ever actually use that second connection.
3. The first connection will work fine after the second one has been created.

.NET works great until it doesn't, then you hate it forever. :)&gt; yes smartie-pants, you can describe skip lists but you can't implement them in bug-free c code, so who the fuck cares?

What if you want decent algorithms **and** correct code?
This is great. I've been looking for something like this for a while - I knew there had to be a good way to use xpath for screen scraping, and a couple of Firefox extensions already get you 90% of the way there...Read it again, they mention that you can send in solutions in languages that aren't listed[deleted]Hpricot looks awesome in parsing. I used to use mechanize,urllib... for getting content, but when it comes to parsing pages, it becomes clumsy with clientform,tableparser.... I want something which understands xpath, can  do dom... hpricot might make me learn ruby.wow you're just a golly-gee swell kinda guy arncha? that's a real hoot.If the matrix is very sparse (as it is in the page ranking algorithm), the computation is not so bad.These are all just tools.

Trade-offs make sense in context.  One cannot speak about them nebulously with any applicability.
Who cares? Anyone who can solve this will obviously have very few friends anyway.&gt; wonder if twentysomethings should be doing interviewing at all

ideally, *no one* would.[deleted]How about the language of *tim*. It just so happens to be customized for this exact problem. Simply feeding it a link to this page will produce the answer.

Unfortunately, in order to run it they will need a license to the compiler.. which runs for 6 figures/I couldn't say, but it does seem (from my admittedly very limited knowledge of both C++0x and Haskell) that the new 'concepts' feature that is being added to C++ is quite a bit like Haskell type classes (according to the WG paper, they are not identical, but I still haven't figured out in what way they differ). The C++ idea of concept is basically implementing in the language something the Stepanov had developed while he was working on the STL.

So - I don't know if he is aware of Haskell (I'd guess he is to at least some extent), but it certainly seems that he has been applying concepts from that work to C++ (perhaps independently discovered, perhaps not).
Dammit... I'm quite happy with my current job, but I think I want to solve this problem anyway, just for the fun of it.

Unfortunately, since we're in a deadline crunch at my current job, I haven't really got the time to be spending on writing a socially optimal seating algorithm implementation right now... 

And yet, this little, evil voice in my head is going "But it'd be a fun language comparison to do it in Perl, *and* in PHP *and* in JavaScript!" (My three main languages of choice).
&gt; for loops: ... conflate three separate kinds of operations -- filtering, reduction and transformation.

Exactly! While in functional languages you would write:

    sum . filter (&gt;100) . map read 

And then rely on list or array fusion to have the compiler turn that into a single loop, in a first order language with only a couple of hard coded loop forms you must manually fuse the operations!

    foldr (\x acc -&gt; 
        let y = read x
        in if y &gt; 100 then acc+y
                      else acc) 0

That's a lot harder on the brain and its not as obvious what's going on. (though probably more intuitive to an imperative programmer if we change 'foldr' to 'for').
[deleted]The brute-force solution here is actually O(n!), and 16! is a very large number.

But I don't think you need to brute-force this. If you look at the patterns for assigning scores, you'll notice that most of them are very localized, with only one exception. So I wouldn't be surprised if some simple optimization algorithm would do the trick most of the time, taking a seating arrangement as the state and seat transpositions (maybe even neighbor transpositions) as the state-transforming moves.Wow, 3 parts and an appendix. 

For me, I'm tired of logos made out of the initials of the company. People should think harder than that, and think of an interesting symbol, like Apple's apple.Hmmm. Not all websites are yet another MVC crud app. Not all interesting problems are solved in anyone's library.

Do you realize your position sounds about as stridently misguided as the 24yo dr.strangelove strawman you've erected?
I'd prefer to see some actual argument rather than just "it's bad", followed by a bunch of comments agreeing.

And conversely, does anyone have a non-banking account example where STM solves a problem that would exist otherwise?Yeah, whoever uses a pear will totally dominate the corporate arena. (edit: oh shit; what about a pineapple?  beat that, Apple)[deleted]nice: [1](http://www.ee.ryerson.ca/~elf/emacs/logo/images/gnu-wallpaper3s.png) [2](http://www.ee.ryerson.ca/~elf/emacs/logo/images/gnu-wallpaper-stones.png)[deleted][deleted]&gt; I thought state is stored on the server and the URL encodes a reference to the state…

Which is basically what we call sessions in other contexts and is exactly the opposite of REST, wherein there should be no implicit per-client state on the server…

&gt; But I think this is a REST vs. Rich debate, not a REST vs. Seaside debate.

It’s a continuations vs. REST debate; that’s where we started anyway. I don’t know anything about Seaside beyond that it’s based on continuations; it may well be a great framework if you buy into those for web apps.IMO, Parsec is better suited for more complex parsing tasks. For instance, I used it in my [toy regex engine](http://jcreigh.blogspot.com/2006/12/simple-regex-engine-in-haskell.html) and am very happy with the results. (Don't follow the link hoping for in-depth coverage of the parser...I almost completely glossed over it, because I was writing the engine as an exercise to learn about backtracking. But there's not much to it, and it should give you a little more of a feel for what you can do with it.)

EDIT: Forgot reddit commenting link syntax...again.Yeah! Your company is way more successful than Facebook! Smart people who understand math will never produce anything of practical value. I'm going to take another hit from the bong now.Some people care. It's a fun puzzle. What's the problem with that?&gt; What You Have, What You Know, What You Are

Dick... Dick... and Dick.  Yup.  Does it matter if the answer to all three questions is the same?list comprehensions[removed]Is he hacking Ruby on an Apple ][?

Syntax highlighting please.
via
http://japple.blogspot.com/2007/02/c-and-gadts.html
Everyone Else is Crazy: C++ and GADTs&gt; It's easy to duplicate Rake's features and syntax without resorting to dirty hacks and interfering with other people's code.

Exactly my point. The problem is most Rubysts love using magic tricks as much as they can.Ouch.  I've just spent a couple of days on the [Conjugate Gradient method](http://en.wikipedia.org/wiki/Conjugate_gradient), so my head was all wrapped up in Ax=b problems. However I should recognize an eigensystem when I see one.I wonder how it compares to the HTML parser of HTML parsers, [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/)

BeautifulSoup is seriously the best thing since sliced bread.&gt; I don't know how you could put garbage collection into the OS.

Disallow languages that don't already have it.  Would you really miss C that much? ;-)
http://eigenclass.org has a running log of what's coming in Ruby 1.9 (and eventually 2.0).From time to time I read Ruby mentioned as "implementation of Lisp". What kind of stupid stuff is this? It shares important features like 90% of good modern dynamic languages of today. If you really want to play at this "implementation of" game maybe it's acceptable to say that Ruby is an evolution (not in terms of *better* but *next, modern*) of SmallTalk.
Anyway how can you call a language with a non trivial syntax, no macros, not Homomorphic, an implementation of Lisp?If you're ready for it you can turn reception of such a question into a huge strength.

1. Answer 'overconfidence'. People will respect that that's a weakness, but then think - what does he have to be confident that we don't know about? When they ask you to give an example, give an example of some heroic struggle where you did amazing things but got ruined by a act of the gods which you can frame as overconfidence. This is a cool-kid answer.

2. Answer 'just one?' and then proceed to disect three aspects of your personality that are merely imperfect. Make the last on 'There are times when it would help if I were less of of a perfectionist' - self-fulfilling. This is a strategic smart-ass answer.

3. Or you can just answer it honestly and be brutal and forthright about an aspect of your personality. If you appear to be dealing with it and getting an understanding of it in my experience people will respect that. Actually I think that's the point of asking the question.
Discussed here
http://blogs.ittoolbox.com/database/soup/archives/on-postgresql-support-14280
On PostgreSQL Support
this looks quite nice, but what about not-well-formed html files?

i mean when the html is not valid xml, then firefox will "fix" it to be able to build a DOM tree from it.

probably hpricot does the same, but the question is:

if you have a not-valid-xml html file, does hpricot builds EXACTLY the same dom tree from it as does firefox?

because if not, then the firefox-xpath will not find you the same thing in hpricot...You can also "+blink+, I didn't realize people still asked that question.  Let's just skip it for now, OK?"I just quote Renton from Trainspotting: "The truth -- well, the truth is that I've had a long-standing problem with heroin addiction. I've been know to sniff it, smoke it, swallow it, stick it up my arse and inject it into my veins. I've been trying to combat this addiction, but unless you count social security scams and shoplifting, I haven't had a regular job in years. I feel it's important to mention this."

I overanalyse everything with game theory and have a penchant for self reference."Are you the devil/gatekeeper/keymaster?"
"I tend to undercook children."

I got asked once "how do you feel about lying to customers?". I should have bailed then and there, but nooooooo...

"Being unable to recognise criminals quickly enough."[deleted]They are.

(Sorry.  Should try harder to resist.  Bad mammal; no biscuit.)Um, what? [This page](http://www.postgresql.org/support/professional_support_northamerica) seems to show over a dozen companies offering 24x7 support, and that's just in North America alone, including one that's been around for 10 years.The only puzzle is how you missed the joke.People who do not have the capacity to understand basic computer "problems" like this do not deserve to get updates or have their "virus popups" go away.The thing to do is think of a weakness that has little or no relevance to the job for which you're applying.I can only agree with 3. I think when responding it's important to try and figure out why they're asking. If they are really trying to get to know your character, I would try to answer (more or less) honestly -- something that isn't too damning, or with the caveat that 'Since I know I don't do &lt;that&gt; as much as I should, I try to keep an eye on it'

Anyone who tried the flaws-that-are-really-bragging (perfectionist, overconfident, work too hard) would get the 'bullshit artist' label from me.

Of course, that's what some jobs are looking for...I think you're right with number 3. They're just asking you to demonstrate self-awareness. Usually the employers don't care too much what the weaknesses are, per se, provided you come across as someone who is capable of identifying your weaknesses and dealing with them/working around them. This is a much bigger asset than any particular strength/weakness itself.-500 for a strange and suspicious answer.I think my greatest weakness is outrunning cheetahs.  If I'm ever backpacking through Namibia and there's a hungry cheetah over the hilltop, I'm totally screwed.

...but I'm working on that.I think this is one of the reason google can be 'gamed'/manipulated. (Of course, with bots etc it would be possible even in your more accurate case)Yes, solving the problem for n=16 is a probably a little bit optimistic ;-)&gt; From what I remember nobody mentioned the obvious: equality is hard.

Equality _can_ be hard. This is usually due to the same object having multiple distinct surface representations. A simple example is this: the usual way of representing rationals is as pairs of integers (m,n), n != 0, where (m,n) and (m',n') are considered equal if and only if m * n' = m' * n.  So (1,2) and (2,4), despite their superficial distinctness, in fact represent the same number, something every school child is taught.

I still don't think this excuses the Java fuck up. :)

&gt; !(a &lt; b) &amp;&amp; !(b &lt; a) &amp;&amp; a != b

The falsity of this for all choices of a and b is a defining property of a total order; it doesn't really have anything to do with whether equality is hard or not.

Pick any non-total partial order, and you get examples where your predicate can be true.  For instance, take sets as objects and let &lt; be the (proper) subset relation. If a = {1} and b = {2}, then a is not a subset of b, nor is b a subset of a, and a and b are not equal. Therefore !(a &lt; b) &amp;&amp; !(b &lt; a) &amp;&amp; a != b is true.Say "My greatest weakness is my lack of tolerance for people who waste my time asking cliched questions with no good answers."You obviously don't know what Matz said about Ruby origins.

Scroll up to the [comment above](http://programming.reddit.com/info/13dy6/comments/c13ep4)
 and shut up, please.Do companies actually ask this question these days? I don't know anyone who's gotten it.1. I have a habit of coming up with bad jokes.
2. Uh...ah...I'm not always able to answer questions without thinking for a while.
3. I don't have perfect memory, so...uh...what was the question again?Well, if those are your only weapons then, good luck! (:Actually, the one that will always work is:

"I'm a perfectionist".

I just say I'm impatient, I hate when things are late, you can't beat that, they'll think ahh this guy's going to do all his work in a timely manner.What is your greatest weakness?
    &gt; Cryptonite


There's no such thing as the 'worst' interview question. You're talking with somebody you hardly know and trying to discover whether he's worth an investment of a few 100k. So by asking him a broad range of questions you get more information.

If the programmer gets agitated or aggressive when you ask him the silly weakness question, that's a red flag. He can dismiss the question with a joke, answers like a polititian or answer honestly. But all these options give some information about his personality.

Trivia questions definitely rank lower on the list. Because with trivia you can only distinguish between right/wrong.

But if there is a stupidest question, it is probably: "Microsoft started an investigation regarding software piracy in our company. How can we make the software look legitimate before MS gets here?""My greatest weakness is giving snippy replies to interviewers when they're being rtards, thereby making myself unemployable"You could answer that you don't suffer fools easily&gt; They're just asking you to demonstrate self-awareness.

Yes, but also they and you are playing a game whereby they're attempting to do something difficult (choose a good employee) in a short, artificial situation.

So I think the point the article tries to make is that everyone has to play the game, in which you win by *appearing* to demonstrate self-awareness rather than being ultimately honest.  i.e. whatever your weaknesses are, you should only reveal them if they're definitely not going to be deal-breakers.  If they are, you should somehow prevaricate.

Whether this is good, healthy or morally right I can't say.  But when you accept that every other candidate is playing the same game, and all but the most naive know they're playing it, it gets easier to deal with.Very insightful piece.The technique of region inference, originally invented by Tofte, et al. for SML, is also a kind of compile-time garbage collection. In general you still need to supplement such a system with a run-time GC, to take care of objects with life times that cannot be inferred.This reads more like a marketing fluff piece than a research paper. You can find more meaty, technical papers describing the system here:

http://domino.research.ibm.com/comm/research_projects.nsf/pages/metronome.pubs.htmlYou don't know enough people then ;)  I get this one on one out of five interviews or so.  I've got my own answer ready, it's true, but can be turned into a positive: "I've got unrealistically high expectations"If I see X and X's creator stated it's Y I still see X.
I understand the idea but to say "an implementation of Lisp" is just wrong. With the same reasoning you can say that Foo is an implementation of Bar given that you start with Bar and add/remove 5/6 main features to morph one into another. Proof: ruby resembles much more SmallTalk.And if they've asked that first question they will also ask "Where do you see yourself in 5 years?"

"Not here"

I was asked that in an interview for a non-extendable 12 month contract...I think the self-aware people (or would aware or knowledgeable be a better word?) will know which answers are the deal breakers and will know they should avoid them.

But it is a bullshit question anyway, and apart from giving the candidate an oppurtunity to disqualify themselves, it does not tell you a lot about the candidate."So, do you have a boyfriend?"Too old, too obvious. Remember the interviewer is a human being as well, not an automaton. They know that answer. They know exactly what you're trying to pull by answering with it.

I've never had this question in an interview but I have come across it on an application form. I went for three which counted kind of ambiguously, which could have been strengths in another way: 1) I am a perfectionist *in the sense that* I prefer to tinker with things until they're PERFECT rather than move onto more important new stuff, 2) I am reluctant to ask for help and prefer to struggle through problems on my own, 3) I can be overconfident in dispensing advice.Heh. I've done that once. It caught the interviewers by surprise, but they seemed okay with it.

Mind you, I didn't get a job offer out of that interview - but I don't *think* that had anything to do with me refusing to answer that question... :)[removed]When I was in third year applying for internship positions I would answer that I am relatively inexperienced outside of academia.

I think this was a great answer because I was no more or less experienced than anyone else who interviewed with me.One could argue (with at *least* as much justification) that the question itself is too old and too damn stupid.

I've been on a fair few interviews in my career but I've only been asked that question a handful of times. The last time I decided to refuse to answer (politely, of course - something like "Look, I know that's an classic interviewing question, but I'm not a big fan of it and I prefer not to answer.").

I think I'll stick with that approach in future, I felt much happier "answering" in that way :). If nothing else, it's a good signal to the interviewer(s) that you're confident enough in your abilities that you don't need to put up with silliness.Interviewers often ask people the same questions they've been asked in the past. It's a meaningless cycle.&gt; Equality can be hard.

Defining equivalence relations on a set can be non-trivial, beyond the two trivial equivalence relations (where everything is equal to everything else, and where nothing is equal to anything else). Herein lies the rub: equivalence relations are defined on a *particular* set. If you have a strongly-typed language, every type is extensionally defined as the set of its values; trying to come up with an equivalence relation between what effectively are disjoint sets (32-bit integers and 64-bit integers) is not really sound, from a semantical perspective.

So what you normally try to do is to invent a new, bigger set and come up with embeddings (AKA casts, or conversions, or...) that (1) preserve the "meaning" of the original elements in the new set, and (2) give rise to useful equivalence relations. This means that writing equality tests is actually two, not one, hard and not quite related problems.

I'd contend that failures in equality comparisons actually stem from unfaithful (from a semantic point of view) embeddings more than failing to come up with a sensible equivalence relation.I always answer this with something they are offering me as part of the job - it provides an opportunity to show their enthusiasm for the role.

e.g. my biggest weakness is not having worked for a company that let me double my sales targets.

Worst question ever asked: Tell me a joke!

Best reply: Can I go now - I'm really busy.Yes, but that's not the point. The point is:

Do you expect all Windows emacs users to have unxutils installed? No.

Would all Windows emacs users download unxutils? No (I wouldn't)

So, it's not a bad idea to develop such a solution as Stevey did--especially that it's simple and short enough.[Who does suffer fools gladly?](http://video.google.co.uk/videoplay?docid=2426875888982457266&amp;q#18m22s)Good read.

From [page 2](http://www.theregister.co.uk/2007/02/07/stefan_esser_interview/page2.html);

&gt; The only problem with Apache 2 is that some of its MPM modules actually are multithreaded. The problem with PHP is that **it links against a lot of third party libraries that are either thread safe or not. Normally the PHP core should be thread safe.** But the moment a function is called that is provided by a third-party library, one can never be sure. [...] If you use a threaded MPM it is like russian roulette depending on the extensions you are using.

That point needs ramming home in a few places and it's worth being aware that the same 3rd party [libraries](http://www.libgd.org/) may also turn up in [Perl](http://search.cpan.org/dist/GD/), [Python](http://newcenturycomputers.net/projects/gdmodule.html) or [Ruby](http://raa.ruby-lang.org/project/ruby-gd/)

That's counter to stuff like [5. PHP isn’t thread-safe](http://maurus.net/work/php-sucks/);

&gt; PHP 4 and 5 can only be used with Apache2’s mpm_prefork model, not with mpm_worker. That means that PHP limits your performance choices with the Apache HTTP-server.

The issue is thread safety and it's not PHP-specific.I know it isn't exactly a high caliber job but I got asked this at a Hollywood Video interview. Needless to say, I stared at her stupidly knowing that one of my biggest weaknesses is procrastination and an addiction to World of Warcraft. How do you tell them that you procrastinate without making them leery of hiring you. I know that if I were hired I'd do just as good if not a better job than anybody else, so why should I have to lie?&gt; trying to come up with an equivalence relation between what effectively are disjoint sets (32-bit integers and 64-bit integers) is not really sound, from a semantical perspective.

Let's ignore signed integers and just focus on naturals.

There is an embedding i : N32 -&gt; N64 of the 32-bit naturals N32 into the 64-bit naturals N64 (please excuse the notation); there is also projection p : N64 -&gt; N32, going in the other direction, which is defined by p(n) = n mod 2^32. These are complementary in the sense that p(i(n)) = n for all n in N32.

Unfortunately i is not a full homomorphism, for i(2^31 + 2^31) = i(0) = 0, while i(2^31) + i(2^31) = 2^31 + 2^31 = 2^32 != 0, and so on. (I'm being a bit sloppy by not labeling which ring the elements are in, but the gist should be clear.) But there is still a sense in which i is a "good", if not faithful, embedding. In particular, if equality on N32 and N64 is defined in the usual way, then the (i,p) pair respects this equality, in that the equality relations are isomorphic when the equality on N64 is restricted to the subset i(Z32). The Java debacle about Long.equals() comes down to this not being the case there; it's not related to what happens when we start worrying about those numbers in N64 which have no correspondents in N32, which indeed is a far less clear-cut matter.

I think the best defense in favor of Long.equals()'s behavior is to say that Object.equals() is not intended to be an equality operator in any standard sense of the word.Q: What's your greatest weakness?

A: Kryptonite.The behavior-based question turns out to actually be useful, ie "At times, we all make mistakes or find ourselves insufficient for a task.  Tell me about a time when you failed at something, and it was your fault.  ... How did you deal with the situation?"

You get to learn an actual weakness, you get to learn how they actually dealt with the weakness, and you get to see how they handle an uncomfortable question, with a much lower chance of the candidate lying or making stuff up.

Of the four people I interviewed last year, at least one was a shoe-in as soon as he answered this question.&gt; "I've got unrealistically high expectations"

With that answer they might think you'll be constantly disappointed and complaining, even when it's not justified: -2000 points

I prefer to joke.

"Kryptonite" is a good answer.

Or, "Well I don't think it's really a weakness, but some people don't like how I have eight pints of beer every lunchtime and sleep all afternoon."My greatest weakness is not being able to fully satisfy more than a thousand women a night. Were my harem ever to grow beyond that number, they're not all going to get screwed!
Brute force = 16! ~= 2e13 ~= not feasible unless it's *really* fast. (1000/sec ~= 2e10 sec ~= 231,000 days)

So - what's your algorithm? I ask because I'm really not sure how I'd approach it.Maybe 20 years ago.

Nowadays you'll get minus several million for resorting to such an unoriginal clich&amp;eacute;d lie.&gt; So - what's your algorithm? I ask because I'm really not sure how I'd approach it.

I would use some kind of heuristic search. There's a natural definition of state change or move here, namely the act of exchanging two seats. Straight-up hill-climbing probably wouldn't work, but I don't think you need to get much fancier than that, due to how the scoring works.Oh, what a pleasant website!  It doesn't even try to present a page to links -- it "isn't cool enough to support [my] browser" and would prefer that I "keep it real" with Firefox or Opera.  So uncool that it can't even have a disclaimer -- it'd rather not show any kind of page at all![deleted]I'd never call insightful any piece that spoke seriously of 'pecking orders' outside of behavioral studies on animals -- nor of any piece that explained that men 'think of sex 24/7 and size up every female they see (and women do -- but are more picky, or something)'.  If I try to think well of this article, I take it as a parody.  I more easily think ill of it -- and offer it this alternate title:

DISINFORMATION FOR GIRLSAgreed, a really dumb article.  "Red Hat decided not to sell it standalone anymore but now some other companies provide technical support on it".  That's the gist I got, and it doesn't even make any sense.Why wouldn't it be relevant? Nothing has changed.[deleted]This isn't a bad interview question, if you're listening for the right information, and this blog posting shows exactly why.

In an interview, one can ask questions that require thinking, which is important. E.g., *"how do you quickly detect a loop in a linked list?"* Or, *"how much space and time does that algorithm require?"*

But, a good interviewer will also ask meta-thinking questions. *"What's your greatest weakness"* is not the best meta-thinking question, but as far as it goes it doesn't completely suck.

The objective is to find out how well you think about what you're doing as you are doing it and how effectively you post-process your experiences into behavior changes. 

For a software job, this is actually *really, really* important. If you're writing code and all the while able to meta-think about what techniques might make the code shorter, simpler, easier to read, easier to maintain, then you're going to end up with better software. Programmers lacking the meta-thinking skill may be fine coders, as far as that goes, but they need to work on meta-thinking to become great developers.

Answering honestly is the way the win. Know what your weaknesses are and have something intelligent to say about them. After all, if you don't know your own weaknesses, aren't you going to have a hard time managing expectations? Doesn't that put your project at greater risk of failure?

Something like this isn't going to cost you an offer:

*Honestly, right now, I'm spending way too much time on World of Warcraft. Ridiculous amounts of time. It's just way to much fun and way too interesting. I get a kick out of the game, which is part of it, but I really enjoy the relationships that I've established there. In the real world, I'm just who I am, but in WOW, I get to create my own personhood from whole cloth. That's one of the most powerful things about the Internet is the way it changes the rules of society.*

When I interview candidates, I personally prefer to ask meta questions that are more specific to work habits. For example, "on a side project you start to find yourself slipping behind your committed deadlines, what do you do?" Or, "you find yourself frequently arguing with your product manager in design meetings, what might you do?" These aren't questions that have a right answer, at all. But, they have lots of wrong answers and those wrong answers say alot about how effectively someone will fit into an organization.sure you COULD answer that way. It certainly would be satisfying.

But believe me, you are sealing your fate if you do.  The message you send is "smartass who will defy you at every turn".  There's no way that a supervisor will want the job of correcting you, if this is how you make your first impression.Questions that do not address "Can the candidate do the job?" are useless.

I've been asked "Are you married?" and "What's your favorite color?"  Experienced supervisors usually know what questions to ask--at least that's been my experience.

Always direct the conversation away from stupid questions and direct it towards useful ones.Alice, Eve and Mallory *all* have a crush on me! Get in line, girls! I'm waiting to see what happens with Zoe - I don't know her well yet.the key to answering this question, is to think about it from the employer's perspective.  If anything, this might be the single and only honest question in the entire lot of what they ask you.  After all, what is it that they are thinking during the whole time they are interviewing you?  They are wondering whether they will regret hiring you or not, they are wondering if there is anything about you that they can't tell from an interview, which will come back to haunt them later.

The interview is a game, a competition in a way.  The employer is probing for as much information as they can get, the applicant is trying to hide everything except for his/her very best side.  

So, even though total honesty is unlikely in a setting like that, asking what exactly it is that they are going to be dismayed to learn about you once they have already hired you, is ONE way to find out, right?  At least it's an honest question.

The key is to think it through in terms of what the employer REALLY wants to know, and then answer the question that the employer is REALLY asking.  The employer doesn't want to see you as a flawed person.  The employer is hoping that you are the perfect candidate.  What the employer is looking for, is reassurance that they won't be disappointed with you once they hire you.

So, be reassuring.  Say "certainly I am human and I struggle to improve myself every day, but I have always been successful and happy in the places where I have been employed, and I feel that all my employers valued me and were pleased to have me as an employee, weaknesses and all."

Did you name a weakness?  No.  But you answered the question they REALLY wanted to ask, namely, "Is there something about you that is going to make me regret hiring you?"&gt; Perhaps you should adapt your workflow to fit better with the abilities of your SVN client? (Or modify the SVN client to support interactive commits.)

LOLIn my opinion, the best way to answer this question is to mention a past weakness of yours that you have overcome.

Example:

"In college I read a lot of Ayn Rand and as a result I had the mentality that I could go at it alone that I didn't need to work with a team. However, I came to the realization that the world doesn't work that way and since then have focused on developing myself as a team player. I have coordinated project groups, reported my findings to supervisors and volunteered my time to help a teammate finish an assignment on schedule."
The best answer, IMO, is one that describes something that *would* be a weakness at some of their competitor companies, but isn't so much at the place where you're having the interview.

My current job asked me this question and i replied, "punctuality", knowing full well that this company uses a flexi-time system.Can feel the heat from this thread and it's only 9am.

The author made an excellent attempt at offering usable advice on this topic. What would your advice be?Wow, this is really good at explaining something very nice in functional languages. It doesn't necessarily only explain closures. 

It also really explains very well why I love some of Ruby idioms instead of "for" loops. I knew I liked it, I just couldn't articulate why exactly.How about answering that your greatest weakness is answering precisely that
question?
"Whether this is good, healthy or morally right I can't say." 
 
It's not good, not healthy and not morally right. 
There, I feel better already.Actually nobody wants to work for "rtards" anyway... so he saves himself having to figure that out a few months in.blah, I thought this was going to be about social equality.  Bad title!  you should make it clearer that we are discussing mathematic, save some time for the rest of us.Please don't use emotes when writing articles! While it is okay in conversations, such as the comment threads on Reddit, because it emulates the otherwise missing body language, it is not necessary when writing an article - people managed (and sitll do) to get by without them.Stefan certainly knows what he's talking about.  Unfortunately, it seems like there's constant friction between him and the rest of the PHP developers.  Some of the PHP developers and other contributors have a shameful attitude to security and release engineering, and Stefan doesn't want to compromise and condone poor security practices.

I do wonder why he hasn't given up and moved onto a more deserving project by now though.  Watching the interaction between them is like what would happen if Theo de Raadt was asked to work on the Windows 95 team.  It's amazing he's stuck with it as long as he has.
[removed]Yeah, you could generate the k highest scoring configurations, disregarding the 3 nonlocal patterns, with some dynamic programming. For the second phase (optimising for all the scoring patterns), I'd tend to try and exhaustively enumerate them instead of going with an heuristic, if possible, though. Since the first phase gave you upper bounds on the scores, it should be possible to stop the search early enough.Depending how you define "fools": teachers, child minders, social workers, people who work with the mentally impaired? These people all work with other people who may not have any desire or capability to listen to what they have to say, for whatever reason. Many of these people are volunteers, which, I believe, covers "gladly" as well.I'm not sure who voted me down, but believe me this is the correct approach. This is how they teach MBA to answer the what is your weakness question. It does still get asked, thus if you don't have an answer prepared you will look foolish, either hemming and hawing your way around or trying to be cute. Stock answers don't cut it - know what your weakness is and know how to flip it into a strength.do your job and leave the monkey politics to the losers.Shoot!  And I came here just to say that...Do the rest of us deserve to receive all the spam that their compromised machines spew out?I didn't much like the explanation of fold in this article;
please accept this one:

    :- func my_foldl(B, (func(B,A) = B), list(A)) = B.
    my_foldl(Accum, Func, []) = Accum.
    my_foldl(Accum, Func, [Head | Tail]) =
            my_foldl(Func(Accum, Head), Func, Tail).

There's a lot that is arbitrary about this function -- its
order of arguments, that it doesn't thread additional state
through the given function, that it offers the accmulator to
the given function as its first argument and not its
second -- even that it offers the accumulator as an *argument*,
and not as a dynamically-scoped variable.  Even that it
operates on ordered lists and not on unordered sets is not
of unimaginably set-in-stone importance.

But that's pretty clear, isn't it?  When my_foldl runs out
of list to process, it returns the accumulator.  When
my_foldl can take an element off the list, it recurs with
a new accumulator, the same function, and the remainder of
the list.

You can use this to sum the elements of a list, to build
a vector with the same elements as a list, to print some of the elements of a list to the terminal, to collect
the unique elements of a list, to randomly double the
occurrence of individual elements of a list, to destructively
modify a list, to get the nth element of a list, and so on.
Folds are 'silly basic' operations.

disclaimer: that example happens to be in Mercury, and
so can't actually itself (although close analogues of it
can) do i/o and destructive modification.  This will do
fine for all of them, though:

    (define (foldl accum func list)
      (when (empty-list? list)
        accum)
      (when (not (empty-list? list))
        (foldl (func accum (first list))
               func (rest list))))

... in an imaginary version of Scheme where it would
be acceptable to write something as unstylish as that :-)
But it is also clear, I hope.
this sends the message of either "I am too uncreative to think of something else to say" or "I have something to hide so I'm going to avoid your question""My greatest weakness is a hatred of loaded questions."At least in the United States, "Are you married?" is illegal.

Which hasn't stopped nearly every (small company) interviewer from asking it. But it is, technically, illegal.

(My personal policy is to work it in anyhow, as I don't really _want_ to work for anyone who would discriminate based on that. But you're not forced to answer that question.)I think that instead of having a dialog box pop up when the desktop is loaded, maintenance tasks like this should be full-screen and appear before the desktop loads.  Pump up the font size, use simple language, and have two big buttons "install updates" and "skip for now" that appear *after a minute of showing the message*.  Put a setting in the control panel to change to the more conventional method; the idiots won't find it and will be forced to actually read the message.&gt; Some of the PHP developers and other contributors have a shameful attitude to security and release engineering, and Stefan doesn't want to compromise and condone poor security practices.

Perhaps I'm wrong though but think the key issue is ownership - unlike, say, Python, I don't think there's anyone in PHP who feels ultimately responsible any more - it's much more a committee determined by who was available at any given time. It's probably less a question of "malpractice" and more a lack of time / resources - passive rather than active failure.

Think there's a more general story here of the downside of the Bazaar (as in Cathedral &amp; ) like "just because it's Open Source, doesn't mean anyone's looking" and the impact of learning curves but it's not really in anyone's interest to tell it yet.

&gt; I do wonder why he hasn't given up and moved onto a more deserving project by now though. Watching the interaction between them is like what would happen if Theo de Raadt was asked to work on the Windows 95 team. It's amazing he's stuck with it as long as he has.

I believe (but could be wrong) his original motivation was self-interest - he was/is involved somehow in running a typical LAMP shared hosting company.Did you get the job?How about: I'm brutally honest, and i think this question is outdated and stupid.

? :)I agree to the blog post. The reason is primarily that sharing memory in concurrency is simply a transitional thing: It works fine with 2 cores and works somehow with 4 cores. But with 8 cores and above it's starting to become impossible because the communication overhead would be much to big to improve performance by adding more cores.

So the only sensible way to go is separation of computation into small independent chunks with at least communication as possible. To do this we need models for computations which play well with this. Erlangs model for example is clearly superior to any shared memory model if you consider massive real parallelism. No matter how clever a system for locking may be, it can't solve the problem of sharing huge amounts of data over a big network of distributed nodes.

So why waste time finding a solution which will only works well in the small range of 2-4 core systems? It's pointless, so just forget it and go on with something which has a future.

I've been increasingly leaning towards the idea that there really isn't any such thing as equality in the global sense. For one thing, as soon as you say this, it all starts to make sense. (In fact it almost sounds tautological, but clearly many people and even language designers aren't thinking this way.)

Is (32-bin-int 32) == (64-bit-long 32)? Numerically, sure, but add eight billion to them both and they get radically different answers, so clearly they aren't behaviorally equal.

There's logical equality, hash equality (an important one that can end up having no relation to anything else because of performance considerations), string equality, reference equality, and definitions of equality made up on the spot ("two binary tree nodes are equal if they carry the same number and the left node is recursively equal, but ignore the right node") that solve some need. (Yeah, I've got no idea exactly what that would be, but I've done similar things.)

The idea that we can come up with a global "equality" concept is rapidly revealed to be hopelessly idealistic if you actually think about it.A possibly simpler syntax:

    foldr f z []     = z
    foldr f z (x:xs) = f x (foldr f z xs)

    foldl f z []     = z
    foldl f z (x:xs) = foldl f (f z x) xsOver a decade ago, an interviewer asked me, "Is there anything you're not good at?"
I thought this was merely a complimentary rhetorical question. He had been appearing very impressed with my credentials.
Finally I said, "Sports."
He didn't crack a smile but rephrased the question somehow. 
I was honestly stumped; he seemed to be referring to ways I might not be up to the demands of the job, which was low-level secretarial, and I couldn't think of any. 
Can't recall my eventual reply, but it was probably along the lines of "too much of a perfectionist" simply because I'd actually been told that.
Didn't get that job; no regrets. By the way, I've never been asked that question since. 
The most frequent question lately has been "what didn't you like about your last job?" (from which I was laid off). And I say, "Nothing. I'd still be there today." 
And no one has seemed happy with that answer, though it is honest.My suggested answers:

 * "This fucking Tourrett's Syndrome."

 * "Inability to cope with clichéd and pointless questions."

 * "Well, sometimes my coke habit gets the best of me. There was this one time..."

 * "I never answer truthfully to interview questions."
Answer the question honestly, picking a minor *correctable* weakness.  But **DO NOT** stop there.  Follow up immediately with steps you are taking or can take in the future to correct the problem.  A weakness is not a weakness if you have a solution to address the problem.

My example response: "Sometimes I come across as arrogant, though this is unintentional.  It is because when I am asked a technical question, I am often unable to think of an simple answer off the top of my head and explain it quickly.  I usually solve problems by doing - and this is often too fast for the client to keep up with.  I have been addressing this problem by telling the client that I will get back to them later, then analyzing the problem slowly, breaking it down, then later presenting it to the client in terms they can understand and at a rate they can assimilate.  I've also recently been more interactive and have followed up with the client afterwards by having them repeat the steps back to me so that I can be sure that they've understood correctly.  I've found that spending a little more time with the client has led to better understanding."Much better than "I'm a perfectionist".  Follow up by saying "I realize that workflow and deadlines do not allow me to intimately examine every facet of the problem, no matter how interested I am.  I've taken steps to address this habit by writing more notes to myself.  I now try to address the problem by solving it immediately and quickly, then going back to my notes to analyze the problem more deeply when I have spare time, or on my own time."[removed][removed]&gt; I'd never call insightful any piece that spoke seriously of 'pecking orders' outside of behavioral studies on animals

Is that because you think they don't exist?I would guess reddit might be a place you could find an Ayn Rand fan or two.The interviewer don't care what the answer is, so long as it doesn't start like 'well, after stacking the four dead prostitutes in my trunk like cordwood...'. They just want to see your thinking process about having to reveal something you'd rather not.

I came up with a really good answer a few years ago that's generic enough that it's not terrible while describing a lot of people in my professions, so it's an answer that they would be expecting anyway.  Then I can use it as an excuse later (Look, I told you my weakness was stacking dead prostitutes...).Considering that you can do message-passing very easily using STM, I don't see what the big fuss is.  If you believe that message-passing is the way to go, but you must use STM, then restrict yourself to a message-passing library built on STM.

P.S.  The nice bit about STM is that it allows libraries using concurrency to be constructed easily and safely.
It is a stock answer in that it is YOUR stock answer.  Always be prepared ahead of time with many anecdotes and examples of prior experiences.  Always talk a lot and explain your points.  Don't just end with listing your problem, follow up with the solution immediately without them prompting you.STM and the Actor model aren't mutually exclusive and not all problems are embarrassingly parallel.

Also STM does scale if used properly.The author offered 1) Show confidence, and 2) Don't show offense.

Do I only need to offer advice as good as this to pass?  Fine:

1. Be cool.

2. Be uncool when #1 fails, but quickly return to #1

3. If you discover that you have fallen out of cool without a purpose (e.g., *not* to tell Mr. Bram that you are not amused by the calendar he made you a gift of), just return to cool at the soonest possible moment.

4. Remember that nobody *really* cares about you as a person until you accomplish one of 1) stabbing someone in the back, 2) letting someone fall madly in love with you, or 3) becoming familiar.

5. Remember that option three of #4 only takes time, and that #1 is only for *your* sanity.  Even if every third word you say is 'fuck', given sufficient familiarity, nobody will mind.

6. Ignore people who waste your time with paragraphs of sex-and-dominance-politics just to massively frame what turns out to be somehow both simplistic and stupid, depending on which eye you close as you look at it.Saved mostly for my own reference, but in case anyone else finds it useful...

I found the Python one particularly helpful.You're arguing semantics with me. When I said stock answer I meant saying something like "my weakness is I'm a perfectionist." HR directors have heard it before and they aren't impressed by it.

I also didn't advocate ending your answer with listing your problem, so I'm not sure where you pulled that from. We seem to be in agreement. You said "follow up with the solution immediately" I said "mention a past weakness of yours that you have overcome"
Interesting piece, but made me want to put down a few very subjective thoughts in response.

1). IMO the 'pecking order' is a real thing, but not as important or simplistic as the author makes it.  It seems to me that men subconsciously alter their behaviour to show different amounts of respect and familiarity to others, and we do this whether we're amongst friends or colleagues.  Part of joining a new group is figuring out how well you get along with people, whether you share a sense of humour with each of them, whether one of you can learn stuff from the other, etc.  This translates to where you fit in a pecking order but **doesn't** mean that on entering a new place you need to aggressively try to put yourself high in some hierarchy as if you wanted to be leader of a pack of wolves.

2). A geek pecking order is different from a 'normal' one (and they can co-exist).  In technology, there can be many ways of doing things, with one significantly better, and the person with the most knowledge specific to *that domain* can out-argue anyone.  Hence 'alpha status' is strictly meritocratic and can be split across different domains.

As an example, amongst my friends, there's a guy who's the butt of jokes, easiest to play pranks on, etc, who's probably lowest in the normal pecking order.  (Note this doesn't mean he's bullied or anything, it's just a small part of our subconscious behaviour). However he knows everything about Perl and gets respect in that and other topics he's knowledgeable on.  Whereas I'm probably the least technical in many ways, so while I feel I'm fairly high in a normal pecking order (feel my opinions are respected etc) I'm happy to defer to others on geek topics.

To summarise, I don't feel it's productive for, say, a woman joining a technical team to think about the men around her as a bunch of sex-obsessives tied to a fixed pecking order.  It's more complicated than that, and I think, as the author says, "it's all about respect."That would actually be a *beautiful* response.So we should use two different abstraction for the same problem? Why not find an abstraction which makes concurrency easy instead of trying to save old abstractions by using hacks like STM? 

It's like saying "Why should I need to use a functional programming language, I can also do functional programming in C++ using boost".

While Erlang has proved that it works very well in situations with massive concurrency, STM hasn't done that yet.

(Programming sub-reddit?  Non-programmers interview too, ya know.)

This can be a good question or a bad question, depending on the interviewer.  A bad interviewer asks the question because it's a standard question, notes the response and moves on.  A good interviewer asks the interviewee to elaborate on his or her answer, and asks followup questions - most notably, what the interviewee does to mitigate this weakness.Wether or not Apple stabbed a Licensee in the back doesn't take away any validity from Steve's claim that FairPlay can not be licensed due to their commitment to the 4 big labels. This commitment basically says that Apple should be able to fix a breach of FairPlay within a given timeframe (weeks, according to his post).OO languages have more difficulties with equality because they also have to contend with the notion of object identity (sometimes confused with pointers).

Common Lisp has far more than 4 equality operations; every type for which it is sensible has its own equality operation.  The eq,eql,equal,equalp set are generalized operations which have some fairly arbitrary specifications based on the language.

eq compares object identity, but the CL language provides for unrestricted copying of numeric and character objects (efficiency purposes).  eql is an operator which covers over the implementational detail about characters and numbers by pretending that all numeric and character objects which are = or char= are also eql.  equal and equalp provide two levels of structural comparisons.

However in a functional language like Haskell, this distinction between object identity and value equality disappears.  That still doesn't lead to a universal "equality" operation because values can be interpreted in many different ways: for example, is a given number just a number, or is it a reference to a record in a database?  Which do you compare?  Even though you should know whether it is a reference or not (via typing) that doesn't mean you know which equality you want to check.

However, Haskell basically leaves that decision up to the programmer, with type-classes; which I think is a very nice way to generalize the problem.

If you desperately want to compare pointers in Haskell, well, there's always http://haskell.org/ghc/docs/latest/html/libraries/base/GHC-Prim.html
I'm not sure about the scaling. How do you limit the distribution of changes? In other models this is limited by the paradigm itself. But STM is created to give the illusion of a big unified memory where everything nearly works like on a single-threaded application. If you use such a system there seem to be no need to look at things like communication overhead and this would lead so rather slow programs in the end.

If the paradigm forces the programmer to think about it OTOH, then it's much more likely that the resulting programs really use resources in a sensible way. In the end all this concurrency is about performance - isn't it?

If a problem isn't embarrassingly parallel then we have a problem because we can't expect to solve it noteworthy faster than today, because single processing won't probably get much faster anymore.
Here is the review portion of the article.  I'd say he did his homework.

&gt; The thing that sold me on Scribes in the end was the fact that it takes a different approach to the general text editor layout - it uses large but not obtrusive icons, it keeps you notified of what it is doing and doesn’t get tangled up in complex menus and options. It also does away with the somewhat popular tabs and replaces them with a document switcher which can be activated with the press of a button. I agree with the creator’s choice and believe at the moment, it makes it a nicer application to use.What do you mean?  Library provides the abstraction.  Drop in "pure message-passing library" whenever you feel like it.

STM is far from a "hack."  Have you even read the Peyton-Jones papers? 

http://research.microsoft.com/Users/simonpj/papers/stm/index.htm

They're very well written.

Note that he doesn't say the PHP core **is** thread safe, even he doesn't know whether the PHP core is thread-safe or not, he can only express the facts that, from his studies, it seems to be...

And the fact that libs such as GD are bundled in the PHP package you usually get, and is distributed by the PHP guys, make thread-safety issues of GD part of PHP's thread safety issues.Those that pretend to know what they are doing when they do not, regardless of their sex, are annoying. There may be many professions where bluster is a useful tool for social promotion, but when it comes to something as prone to disastrous wastes of time from flaws like software, acting like you know what you are doing when you do not is never going to translate to respect. Yes, many people in the industry will act like they know what they are doing when they do not, but this does not translate into respect in a group. It's never worse to ask questions if you are uncertain than to make an assertion that is obviously wrong to everyone else. If you are always making incorrect assertions, no one will trust your judgment. If you want to be respected in a group, do good work.

If you are right, and know it, then be assertive. You also should expect to deal with people that are assertive but have no idea what they are talking about, but that does not mean that you should emulate them.Another good strategy for handing this question is turn it into a relative rather than an absolute.  ("I'm very good at research and at drafting documents, but I'm not as good at staying organized.")Programming languages that sacrifice freedom for equality will end up with neither.Of course we are in agreement.. why did you think I was disagreeing with you?  I was just appending my opinion to yours.  Not all responses are antagonistic.And here I thought this was going to be about affirmative action.I ask a slightly different question; "Tell me about a time when you weren't pleased with your own performance." The goal is similar...to see how self-aware they are. But, this usually leads to a specific example of how their weakness manifested itself. I'm looking for some growth, what lesson did they learn from it. Something like, "Once I totally failed on how to implement a solution. Since that time, I took a refresher course and have standardized my implementation strategy."

I've actually had some say "nothing really jumps out at me." this is the worst answer ever.The tone of CAPITALIZING a word in your opening sentence threw me off. I'm glad we're in agreement though, all of this discussion is making me think I should start a career advice blog.No. Whole episode was mental - 1 post to fill and about 20 of us are invited to turn up - given a written test, then a practical test, then a 20 minute interview with 3-man panel. Complete waste of time and money on their part and for say 15 of the interviewees.

Best interview answers - whatever reflects who you are. If they ask moronic questions there's no mileage in "playing the game" and giving the "correct" answer. Answer honestly (not brutally honestly) and then if you get the job its because they want you for being you, not who you pretended to be. If you don't get the job...there's others.&gt; It's probably less a question of "malpractice" and more a lack of time / resources - passive rather than active failure.

That's probably *partly* true, but at least some of the complaints &amp;mdash; e.g. bundling security fixes and unrelated updates into minor releases &amp;mdash; are deliberate choices.  And even if it's "passive" failure, it's still failure, and a lot of it could be easily addressed *if they cared to*.
Bullets! My only weakness. How did you know?&gt; The brute-force solution here is actually O(n!), and 16! is a very large number.

O(n*n!).  There are 16! permutations of people, but the time required to compute the point value of a given row is O(n).STM is a hack in the sense that it tries to make an otherwise obsolete paradigm useful again. But it don't addresses the real problem: Communication overhead in massively distributed systems. 

Sure, you can solve this 'by hand', but but as I wrote: Thats like staying with C++ for functional programming because you can emulate it somehow with a lib.

Libraries are rather limited in creating new abstractions (kind of an exception are languages with rich meta-programming facilities like macros in Lisp, but this has it's disadvantages too). The only solid way is to put 'big' new abstractions directly into the language.
[deleted]When I was interviewing for an IS Coordinator position I told them my greatest weaknesses were procrastination and untidiness. I followed up with examples on how I attempt to mitigate those weaknesses. They were extremely impressed that I didn't use the typical cliches "I'm a workaholic" or "I'm a perfectionist" or "I have unrealistically high expectations." Not that it was the sole factor in landing the job, I believe the honesty was a refreshing change to their process.Or just pass the valid HTML to your browser, and say "we haven't tested this".  If the browser doesn't render valid HTML, then the browser has a bug which needs to be fixed.Don't forget Henry Baker's classic paper on this topic: [Equal Rights For Functional Objects](http://home.pipeline.com/~hbaker1/ObjectIdentity.html)For the first question, perhaps they are seeking to determine if you are overqualified.  Everyone has things that they are not good at, if it's low level secretarial, tell them that you are not very good at complicated macros in Office that requires programming knowledge (Visual Basic).

"What didn't you like about your last job" is a perfect opportunity for you to shine.  Do your research ahead of time and find out what differentiates this new company from its competition.  Then list the unique qualities and benefits of this company from the lack thereof in companies that you have worked for in the past, or missing in their competition.What is the joke? It's easy and anyone could solve it? If that's what you're trying to say, what's the solution?

edit: I get it now. How did I miss that?&gt; Too old, too obvious.

I disagree -- it's a weakness of mine.  I often spend time trying to make sure my algorithms work in all (even unlikely) corner cases.  In reality, this is a waste of time.  Repairing the damaged record the one time out of 1000000 would probably be more efficient, so I guess I'm technically wasting my employer's time.

OTOH, it's nice to never need to fix any bugs once the system goes into production.Readability guidelines from a site which chooses white on black for its text? Nice.Heh, in my experience people don't like honesty (especially if you include the word "stupid" in your assessment of them).Wrote [Gadsby](http://en.wikipedia.org/wiki/Gadsby_%28novel%29)?It will run the else if `try_parse` returns false or nil. The same technique doesn't work for `parse_bool` obviously (but who wants to do that?).SVN is awesome.  Conflicts, at least in my team, can be resolved using a graphical merge utility.  This way I can pick and choose what goes in as the next version interactively.  Its quick, easy, and works great.  If I had to do this from the command line it would be a pain in the ass.  But I don't plan on using SVN from the command line anytime soon.IIRC we used to support it, but `setuptools` proved to be such a maintenance nightmare (has to be changed every time there's a new minor minor minor release, can't even print its own help message if you don't have an Internet connection, etc.) that we dropped it.&gt; Django makes it rather difficult.

To date, the only support I've seen for this statement is "if I use another templating system I can't use `render_to_response`. Is there something else I'm missing? Django's shortcuts and convenience functions are designed to work with, well, _Django_. Is that wrong?

&gt; I know I said this just the other night on Reddit, but the fact of the matter is, Django doesn't provide any more functionality out of the box than any other framework

Gonna call BS on that. Show me the equivalent of `django.contrib` in the _out-of-the-box_ versions of Pylons and TG. Show me the equivalent of generic views in the _out-of-the-box_ versions of Pylons and TG. Show me the equivalent of Django's admin app. Show me the equivalent of Django's flexible auth backends. Show me another framework that ships all this stuff _out of the box_.

I'm all for constructive criticism (and I'm known to do a fair amount of it myself), but I'm really, really, _really_ against inaccurate FUD.We "think" in _something_ (assuming that we "think" at all). It's ultimately impossible to quantify in "natural language" terms because of the extensional/intensional gulf, but there's _something_ in there that either resembles language or comes to resemble it as we learn language.

Which gets back to "the way you think has an influence on the way you think". Weak Sapir-Whorf is basically a cop-out -- nobody ever proved that not having a word for a particular shade of blue means that you can't distinguish the actual color, so the hypothesis has to pull back onto vaguer and less dangerous ground. As Popper would say, it's begun immunizing itself against falsification (which is a shame, because its strong version, by making wild, falsifiable claims, did arguably teach us something, if only "no, that's not how it is").The author of the post is being extremely inflammatory, and hiding the lack of content among the flames.  If you read past the flaming, you'll distil his point down to "shared state is bad".

It's evident that he hasn't read or thought enough about STM to be able to cite any of the non-introductory papers that discuss implementing STM.  Instead, he's tilting at windmills.  If he had done any deeper reading, he'd presumably leap at the chance to cite some of the current severe problems that STM really has, and thus bolster his argument with some, you know, facts.

To wit: the need for a transaction manager to avoid deadlock and livelock; the necessity for pluggable transaction managers, because different transaction management algorithms have different progress properties when breaking livelocks; and the fact that this is very much an open area of research, not something to be rushed into by fanboy implementors.  Oh, and lest we forget: the impossibility, in most languages (except Haskell, the one he points to so snidely as a backwater), of preventing participants in a transaction from causing side effects.

Instead, he comes off as ill-read, missing the point, and boorish besides.My weakness is kryptonite.How many of you guys would pirate vista or not?


http://www.pollburner.com/takePoll.php?id=5ff95084e881[removed]If you say it so that the interviewer knows you're joking, it might work.  Especially if the interviewer is a bit cultured and you manage to say it using the accent of a high Scot.&gt; ...
&gt; - They have had bad experiences in the past and insert process as a way of preventing surprises.
&gt; - They don’t hire well and know it, and do everything they can to prevent other people from finding out.
&gt; Fortunately all of these things are fixable in fairly short order.

Those are easy to fix? Please tell me how!First off, I don't see how any of this applies to open source. All of the author's comments seem to assume that you are dealing with an office environment. Even though a lot of OSS development comes from corporations, an individual project's team will still be spread across the world.

Secondly, as a man I find this woman's assertions about male behavior completely unfounded. Aside from an explicit corporate hierarchy there is no "pecking order". Everyone is treated with respect unless they prove themselves to be incompetent.

I think what the author is describing as "the unwritten rules of a male dominated group" are actually just office politics. I also think you are significantly less likely to find this in an IT environment, as opposed to other workplaces.Very interesting...I don't think that anything as simplistic as a "pecking order" exists among humans. There are chains of command, levels of authority, and webs of social connections. However if you try to overlay a status hierarchy on top of this, especially an implicit one, then you are seeing patterns that aren't there.not a bad article, though i was amused to notice that just after reading it (approx. 10:00 a.m. PDT), i was unable to load the homepage for the co-authors' company (red-cliff.com), which was linked to several places in the article.

"No web site is configured at this address."

Perhaps the cracker being tracked down in the article has had their revenge, eh?Darcs is amazing--I really love it--but it doesn't work for every project.

Problem #1: Darcs stores three copies of your source tree, per branch (the patches, the working copy, and the current head revision). This doesn't matter for small projects, but when you start looking at many gigabytes of data and hundreds of branches, Darcs will get pretty space-hungry.

Problem #2: No mature Windows GUI. This matters when convincing non-technical users to store their work in version control (and also, to a certain extent, when teaching them to dig through huge repositories).

Problem #3: Will it be possible to export history from Darcs to the latest and greatest system in 10 years time? By storing data in a widely popular system, like Subversion, you pretty much guarantee that you'll be able to import your data into whatever we'll all be using in 10 years.

So there are very valid reasons, at least on large code bases in a commercial setting, to keep using Subversion. That's not to say that the newer systems aren't much more interesting, just that there are reasons to go with the crowd in some situations.Why is this on the programming subreddit?Y'know, some of us still haven't managed to convince our employers to upgrade from CVS to SVN (or, heaven forbid, from VSS to CVS).  Give us a little more time to become frustrated with SVN before asking us to drop it.

(That said, I do like Darcs.  I'd just rather fight battles over unit testing, code robustness, and language/framework choice than convince them to ditch a generally-okay version control system.)Warning: PhD thesis.It does seem to be somewhat of an unproductive question, as everyone's suggested answers seem to focus on surviving/rejecting the question, instead of strengthening your image.

Perhaps a better question would be something like:
"What professional/skill set self-improvements do you plan to make in the next year or so?"

As an employer, I would much rather evaluate a set of answers to the previous question when screening applicants than the greatest weakness question.

As an applicant, I would rather acknowledge where I think I need to, and plan to improve.Full disclosure: I don't actually program. I *do*, however, hire extremely smart programmers for hedge funds and investment banks. Unfortunately, I still don't know where to find most of them. Where do the UNIX mavens and Perl monks post their resumes when they're looking for work?That anything like escape analysis? They sound similar, if not the same.[removed]In the bull shit world of HR you better come up with a good answer that shows that

a) you are self critical enough to know areas of your performance need improvement and

b) that those areas of weakness are in no way related to your fitness for the job

Personally I don't think it's a useful question for a prospective employee. It's a good question for annual performance reviews. If somebody knows that they suck in a particular area, and are working to improve it, that gives me more confidence than when somebody is perceived as having the weakness by everyone but themselves.http://jobs.joelonsoftware.com/ is relatively new and one of the best, as it is recruiter-free
That term can mean many things, but it is related.

For example, some of the newer versions of Java were supposedly to gain some form of escape analysis where objects allocated in a given stack context could be determined to not "escape" that context. Thus, to avoid a heap allocation altogether (and thus not generate any garbage to collect, except in the trivial way of moving the stack pointer) the object is allocated on the stack. (At least the semantics are similar to this.)

But it is done at compile time (or JIT time, I don't know the details). Less general, though.Craigslist has treated me well. I've hired six people in the last two years, and I've found five of them through Craigslist.I'm all for something better than svn, but the comments in the post nail this: it's just a missing feature, not a major underlying issue in svn. I can't speak specifically to Darcs, but other distributed systems beat svn in terms of the fact that they are distributed, which opens up a new world of possibilities.I might also get where you're coming from if I won the lottery,
became an astronaut, or had my own talk show.
Reddit. ;-)

njbartlett's right.  Smart, passionate developers are almost never *looking* for jobs - because they always have a friend that's just waiting for them to grow dissatisfied with their current job and snap them up.  If you want to get top developers, your job is to be that friend.

I got my current job through a friend I met on the [C2 Wiki](http://c2.com/cgi/wiki).  All my previous jobs were also through connections: one was through a coworker of my mom's, another was through a former math teacher, and a third was through the parent of a friend.  I've been approached for jobs by people on #haskell, some friends at college, folks I meet at parties, etc.  

Right now, the only thing that could draw me away from my current employer is a co-founder position with an equity stake.  I've actually got two of those lined up: one's a nights-and-weekends thing until we launch and see if it takes off, another is slated to start this summer and I've told the other co-founders to get back to me then and see if I'm available.  Both are through college friends or older alumni.

So basically, your best bet is to learn enough about technology that you can avoid appearing troll-ish, and then hang out at programming discussion forums.  Here's one of them.  There's also [C2 Wiki](http://c2.com/cgi/wiki?RecentChanges) (going downhill lately, I'm afraid), [Lambda: the Ultimate](http://lambda-the-ultimate.org/), and various open-source projects.  Open source stuff may be the best, particularly in fields related to what you're hiring for.  There, you're likely to get folks who "get things done" in addition to being "smart".The Stellenmarkt on http://www.linux.de/
But it has become a bit quiet there.I wish more people would present quaternions and octonions in the context they belong in (or make most sense in. Perhaps it's a matter of taste but if elegance is the metric I'll say "belong"), meaning Geometric or Clifford Algebra: http://www3.telus.net/j_suter/paper/ga_primer.pdfMore background on the John Amaechi storyHow about "Sometimes I come across as arrogant, though it is not really my fault that I have to spend so much time working with idiots.  I intend to remedy this some day by starting my own company."

:)Yeah, I ran across that exact bug ticket [a few days ago](http://programming.reddit.com/info/12j02/comments/c12ljo), hence my questionIt's posted on programming.reddit.com!http://www.nhplace.com/kent/PS/EQUAL.htmlMy motto is: you hardly ever need equals on a class.  What you need are context-specific comparison functions.  There is no one-size-fits-all equality definition for any class.True. Recruiters basically exist because personal connections don't always match the jobs that need to be done.

But there are some pretty smart people (or some pretty brazen liars) posting resumes to some of the mainstream sites and replying to job ads.He agrees with you; his point is that STM isn't necessary 99% of the time.

His point is that, if you can at all get away with it, you should use message passing instead of STM.I decided to avoid trolling reddit for leads because I like reddit too much to irritate all the redditors. And I did assume that most of the best candidates are already working at  jobs they enjoy. But I figured it was worth a shot -- I can definitely speak the language when I'm talking to quants, but most of the positions open are actually for developers or IT folks, and all I have in common with *them* is vague contempt for anyone who needs a GUI to get around.

We've actually had some success recruiting through open source projects -- unfortunately, that often means competing with Google, and while hedge funds can be almost as nice, they just can't win on cachet.

My personal problem with recruiting from open-source projects is that I actually *use* them, and so do lots of other people. And there's a fine line between supporting a programmer who does OSS independently and killing an OSS project by forcing their main people to crank out .NET frontends for a midtown investment bank. I'd rather not making a living exclusively by screwing over other people.Good link.

While I'll leave my original comment for "posterity", having typed it out, now I _know_ there is no such thing as equality; I just never took the time to think about it clearly. 

Therefore, Equality isn't hard to get right, as it doesn't exist. If you nevertheless insist that it does exist, then it's not "hard" to get right, it's _impossible_.

The connection to the "copy" idea is a good one, too.Warning: direct link to a PDF.nice write ups about this can be found at http://jeremy.zawodny.com/blog/archives/008513.html and at http://dabbledb.com/blog/?p=71 ."Problem #3: Will it be possible to export history from Darcs to the latest and greatest system in 10 years time? By storing data in a widely popular system, like Subversion, you pretty much guarantee that you'll be able to import your data into whatever we'll all be using in 10 years."

Wouldn't you be able to, at worst, simply copy out all the gzipped text files (the diffs), gunzip them, and then use patch to reconstitute selected stages and check them into whatever you'd be using then? It's all stored pretty transparently, as I understand it. I'm not too worried about its longevity myself; the Haskell community has essentially standardized on using darcs, and I think they'll be around for another 10 years.Saying "the square root of -1 exists" in an unqualified fashion like that makes me cringe a bit. Especially when you follow it by "it is real", which I realise is not meant to be interpreted mathematically, but if you do, the statement is false. In R, the field of real numbers, the square root of -1 doesn't exist, and in C, the field of complex numbers, it does. The real numbers, complex numbers, integers, natural numbers are all on equal footing with regard to "reality" though. They're all abstract mathematical concepts. If you want more, they've all come in handy in modelling physical situations. (Note that the number "3" is not the same as "three oranges" or "three apples", and doesn't really have a proper physical instantiation without some interpretation.)computerjobs.com and dice.comI get 2-4 job offers monthly from professional colleagues, and I know this is common for the developers I look up to.Pipes is amazing. Unfortunately you'll just have to take my word for it since the site is overwhelmed. ;)I'd take off my white glove and slap the interviewer with it.You missed a good point:  Good programmers may not enjoy .Net frontends for a midtown investment bank.

.Net is your first problem.  Who made the .net decision and why?  .Net is a technical solution to technial problems, and so it should be driven (or not) from the technial level.  Too many managers are driving changes such as .net, while standing in the way of others.   (Note that I am not saying anything against .net, only that there are other solutions that might be better)

Midtown investment bank is the next problem.  I will work at an investment bank if that is the only job I can find, but the entire banking industry has earned a reputation as a place to work as last resort.   I don't mind wearing a suit too much, but I prefer jeans/t-shirt.   
That's two questions. One in the title item title, and quite a different one in the "disclosure". To people who ask me the latter I usually suggest the scout out local user groups for leads. It doesn't seem my advice is ever taken however. 

Now for my "full disclosure". I work for a company who runs a fledgling search engine for jobs in Canada. The search engine is new, but the company has been in the business of publishing Employment related publications for a dozen years, and running Canada's "Employer Summit" conference. So there are some deep HR roots here.

We aggregate job postings directly from employers sites. We do not scrape job boards as we find the content tends to be low quality and often spammed by headhunters using various strategies (and it would be just too easy &lt;-;). The major job boards in Canada seem to be looked on with general distrust by people due to this. Other aggregating sites are less discerning. We also attempt to index only full time career level positions. 

Your original question makes it sound like you're a job seeker looking for good quality places to look for a job. For that, if you're Canadian, I'd suggest taking a look at what we're trying to do: connect seekers to high quality employment potential (and raise the bar for employers seeking quality applicants via competitions of various kinds). But it seems you're a headhunter looking for heads yourself... so I don't really know what to suggest (aside from my suggestion at the top of this post). 
&gt;You missed a good point: Good programmers may not enjoy .Net frontends for a midtown investment bank.

That *was* my point, actually. I know that many of the jobs out there are pretty uninspiring (that's why they have a high base salary and a guaranteed bonus), but I also know that those jobs need to get done. At smaller firms (which I prefer, anyway), there's much more flexibility in terms of where ideas come from and how they're implemented -- working for one of the better quantitative hedge funds is basically grad school with a seven-figure prize for a good dissertation.I actually am going to start going to LUG meetings (I run OS X and Ubuntu at home; I'm agitating to convert the office from XP to something sanely Unix-based). It's the same question, with different emphases -- in both cases, I'm curious about where people go to advertise jobs. Whether I'm asking so I can compete with respondents or so I can get them jobs shouldn't be especially important.

I'm actually asking so I can *avoid* being one of the obnoxious recruiting types: I'd much rather contact people if and only if they want a job and I have a great job for them, rather than cold-calling random people and hoping to hire them for more or less anything. I'm more into K-selection than r-selection -- hence my efforts to find people who know they want jobs, rather than pestering the ambivalent.Yeah!  Throw feces at them!  That'll show 'em!Several links to audio, video, articles. Really current (Nov 2006 - Jan 2007), relevant stuff.I am struggling to reconcile your statement with the fact that I know you know Haskell.

Higher order functions in a lazy-functional programming language provide more than suitable foundation for a highly expressive abstraction in "just a library."  Monads allow you to structure and enforce the abstraction in the type system.  

It is not too difficult to conceive of a library in Haskell which presents the following interface: forkIO as newProcess which also returns a TChan as a message-passing conduit, plus writeTChan as sendMessage.  Combined with a Message class, you can have channels of (Message m) =&gt; TChan m, which is easily extensible to the application's needs.

If you ever decide you need a massively distributed system then you can drop in a replacement library which implements the above over networks or something.  Perhaps you want to add some kind of Serializable class restriction in that case.
Buy BC. Kill a lvl 62 mob. Congrats, it dropped a purple and you're uber.Get a blog.Could be worse. Could be Yahoo! Big Truck.I think an evolutionary-algorithm approach might work well. 

It saves having to think up any heuristics :-)I was once asked that question when being hired for a student position for a job.  What an awful question!  How many people can honestly project where they'll be in a few days time?  A better question is "Where do you want to be in 5 years?"Your first example reminds me an awful lot of Prolog.  Now there's a language I hated when first learning, but grew to appreciate some of it's facets towards the end of the semester.Enterprise?  Oddly, since this comes from O'Reilly the majority of the Rails community has no problem with them saying that word in reference to Rails.  If anyone else uses it watch out!

&gt;IMO, it might be a lttle easier to setup Apache + mod_proxy + mongrel + Capistrano + memcached, than it is to go through the motions of installing JBoss or Tomcat and then dealing with the lack of OS integration that most Java servers suffer from.

OS Integration?!  Rails is no better than Java app servers in that category (which is good).

There are better ways to get Rails in to the "Enterprise".  The best way is to integrate with the existing system.  For example, if you already have a bunch of Java app servers sitting around just reuse them and start implementing JRuby.  As time progresses, and you have more JRuby code, moving over to Rails becomes much easier.  Developers also get a chance to learn a new language as they go.

Take baby steps.  Don't try to push rewriting huge amounts of existing production code.[deleted]What colour?They apparently use some ML in production.&gt; "In college I read a lot of Ayn Rand...

That's where I would end the interviewA perfectly shaped A**.[deleted]It was in response to the parent's concern that the problem "would be NP-complete." This is not a concern, because since anyone solving the problem has few friends, the algorithm does not have to scale very well anyway.I've noticed playing go games against go programs like GnuGo is they're often very aggressive in their play (at least at my level which when i was really into it was only about an amateur 15 kyu[Go rankings order in amateur &amp; professional. amateur went from 30 kyu down to 1 kyu. then from 1 dan up to 9 dan. a 9 dan amateur was equivalent to a 1 dan pro, so clearly i was never very good :) ]). So the extremely easy way to beat the AI (even w/ a 5 handicap!) was to simply give it obviously bad openings, which it would aggressively try to take, then you spank it's ass hard. 

Go AI's don't seem to get subtlety or misdirection. And you have to play against the AI *VERY* differently than against a person. People are never as aggressive as the GO ai, nor as stupid. 


--vat
This is better than the average JavaScript usually does, but sadly still not good enough.  He forgets to allow for people who open links in new windows/tabs by holding down shift/ctrl while left-clicking.  Click handlers should always abort when the event indicates that a modifier key is held down.  Something like this does the trick:

    if (event.ctrlKey || event.shiftKey || event.altKey || event.metaKey) return true;
The scaling depends on the characteristics of the algorithm in question. If the likelihood of a contention is low it'll run very quickly. If contention is high and you perform a lot of work in the atomic block you're not in a nice place.

The way I see it STM (as an idiom) is embedded within the Actor model anyway and transactions are a pretty common abstraction that may benefit from having STM support. It's at least worth researching.

I have hope for the future re. unembarrassingly parallel algorithms. I think we'll be able to speed the vast majority of algorithms up but probably not proportional to the number of cores. As the number of cores increases some brute force and exhaustive search algorithms start looking more attractive as well.

I'd totally agree with your point about message passing if it didn't totally deny STM a place at the table. If I had to choose out of Actors and STM it'd be Actors in a heartbeat, but I have a feeling that STM will find its niche.See I disagree. I think this is a great way of determining how how good at candidate is at presenting bullshit, which lets me honest, is a task found in most jobs. 

So I see this question as getting valuable information.[deleted]&gt; You build a part of a page and then you can include that part in any page. This is like having custom controls in ASP.NET. Partials also have their own caching scheme so if you want your page can be dynamic mostly except for certain sections.

Why didn't you say so in the first place? That sounds like it is exactly what I was looking for when I asked for ways to bundle HTML/JavaScript/server code.

&gt; I myself have a love hate relationship with it, it's good for what it does but if you push it then it quickly becomes a nightmare. 

I agree with you there (though the 2.0 version is far less painful). However, there are more generic data-bound controls like the repeater that don't force you into the data grid's straight jacket. 

&gt; It's nuts. For example the rails frameworks extends the integer class you can type this 20.minutes.ago 20.hours.fromnow Time.now.atbeginningofyear

I'm actually really looking forward to getting some of those capabilities in the next version of .NET. I really like the idea of type inference and extension methods, though the .NET version of the latter isn't a nice as the Ruby version.

One thing I don't like is the ability to frag a pre-existing class. That seems like a bit too much power without any clear purpose.

&gt; Anyway to me the things that nobody talks about in rails is the stuff I like most. Migrations, testing (unit testing, functional testing and stress testing all built in), mocks, fixtures, and capistrano. 

You know, I'm actually quite leary of that kind of stuff. I have yet to find a site or book that even explains how to write decent unit tests, let alone the other types. Most of the ones I do see are pretty much just testing the property getters and setters, as if that's really where the bugs hide.

If someone does come up with decent guidelines on how to test, I will probably change my opinion on the issue. And right now that is the one area where I see a real advantage with Ruby over .NET in the next 2 years.

You start where this all started: choose your next move based on however you can provide the greatest possible value to your employer (customer, etc.). Life unfolds one step at a time, and all you can do is make the next step a good one.I can see a situation where you would want to try to parse a boolean and display an error message if the user enters something other than true/false. For example, a tool that validates config files.I'd guess you're either at the top or bottom of some "pecking order".[deleted][deleted]Even in Haskell the abilities to create new abstractions are quite limited. Why isn't 'normal functional programming' enough and why do we need monads (which are only really useful after they got syntactic support). And monads are still to limited for lots of things so later they invented Arrows - again with syntactic support. I don't believe that this was it, there are certainly more things to come which can't simply be implemented in a lib alone. Think about deriving(Typeable) for example. Can you implement something like this in a lib alone? Surly not with todays Haskell.

And how do you implement forkIO in the first place? You can't, such a thing have to be part of a lib. Maybe you can create simple cooperative multitasking based on continuations, but that's it. The limits of creating new abstractions simply via libraries are clearly limited. And especially Haskell with it's lots of GHC extensions is the living proof for it.

Now you can of course create some simply Message passing system on top of the current abilities of Haskell. But I think that we would see the limits of such an approach very fast. 

But the main problem is that I think that the whole paradigm of 'one big chunk of memory' has to be questioned if we really want to be able to take advantage of the coming massive parallel architectures. And STM is simply a method of managing this 'big chunk' (In functional programming it should even be unnecessary, because it's only needed if you have mutation which shouldn't be possible in f.p. anyway). But if you try to use STM in normal imperative programming it won't work well because in principle it requires referential transparency. 

To come to an end: I think that both locking and STM are just too low-level to be useful in the future. Even if STM is a bit higher level than simple manual locking, its still far from being an abstraction which can solve the problem of creating massive parallel systems.
Why not use emacs? I don't understand the fascination with textmate or scribes. Can anyone explain?I don't say that STM shouldn't be researched. But I'm quite sceptically. It would far better to find mechanisms to split up big monolithic systems in small independent chunks which can than do their computing with a minimum of necessary communication.

Replacing locking with STM is like replacing raw memory addresses with C++ pointers. It's an advantage, but it doesn't really solves the problems. But with such a small step forward the risk is big that people prefer it because going one step further could become to unfamiliar. And the result would be lots of badly written software which wouldn't be able to really utilize the possibilities of the upcoming hardware. STM is just a too small step.
If it interests you, you can find some candid discussion in [this mercury-users thread](http://news.gmane.org/find-root.php?message_id=%3cf6aec790610090530j53151aeap80f9b96384127e25%40mail.gmail.com%3e)Well said.I don't think it really is hard to get right. Just look at the objects and ask yourself, "In this domain, what would most people think equality is for this object?"

For somehting like Long, the domain is numbers. Therefore the obvious question is, "Are the objects numerically equal?".

I realize that for some objects there is no obvious answer. That's fine, in taht case the obect shouldn't have an equality operator.

Of course it also helps when the language doesn't overload the = or == operator to mean both reference and value equality.So, how do you figure out that there are better tools for certain tasks if you don't actively explore the tool space, try different tools in assorted contexts, and discuss tool strengths, weaknesses, and techniques with your peers?
Oh, no, this syntax is far simpler:

    : foldl ( acc f list -- acc' )
      dup null? if 2drop exit then
      dup tail &gt;r head swap dup &gt;r execute
      r&gt; r&gt; recurse ;I'm always a little stunned when reading a "sociological analysis" about poor girls who have to fight for respect in male groups. Anyone of those hobby anthropologists ever looked in the opposite direction and asked for the kind of respect a woman earns from other woman when contributing to nerdy stuff? Woman get checked by other woman get checked by other woman. This is far more serious than some boys out there being not chevaliers or bullying each other occasionally.Thanks -- I'd missed that in the what's-new documents. The new AST stuff is even easier to work with; although it doesn't have as friendly a repr() as the compiler module, for programmatic inspection, the _ast module is nicer. You get the AT like this:

    &gt;&gt;&gt; import _ast
    &gt;&gt;&gt; ast = compile("2+(6*7)","&lt;string&gt;","eval",_ast.PyCF_ONLY_AST)

you now have an object with various attributes -- the structure's kinda irregular, but easy enough to inspect with dir(). 

It also has line and column indexes back to the source code, and there's an article I just found [discussing future directions for it](http://jeremyhylton.blogspot.com/2006/02/python-compiler-sessions.html) that suggest they'll add ranges, which would probably help with syntax hilighting and source-code transformations (refactoring etc), as well as providing a way to access the tree more regularly.

So much for math being useless to the modern programmer.The other thing Ruby has which doesn't fit with the Smalltalk model is top-level forms in source files. Where would you put these in your browser?It does get asked.

For a contracting role I got, the owner of the contracting company insisted that he attend the interview with the organisation that I'd contract for. He actually asked this question during the interview with 
I've never understood how throwing the prototype away is different from [rewriting from scratch](http://www.joelonsoftware.com/articles/fog0000000069.html).

The prototype may not be very close to what you want, and there's nothing wrong in [constantly rewriting big chunks of it to try out different things](http://www.bluesnews.com/abrash/chap64.shtml). But it's invaluable to fix a lot of the framework around you when you work on one aspect. If you switch to another aspect hold this one constant as part of the scaffolding.

The real reason to sell something as a prototype seems to be political: managers who can't appreciate component rewrites. It is paradoxically easier to pitch wholesale rewrites to them. Does this make sense? I have no direct experience with large corporate hierarchies or managers.&gt; Think there's a more general story here of the downside of the Bazaar (as in Cathedral &amp; ) like "just because it's Open Source, doesn't mean anyone's looking" and the impact of learning curves but it's not really in anyone's interest to tell it yet.

Maybe the problem is related to PHP's status as a "just for web" language.

With more generalized languages, the demand on overall quality is higher since there are far more tricky edge cases. So people try harder and find it more rewarding to work on those projects.

That would mean that there might often be advantages to using general programming languages, even when PHP would actually be enough. When you use Python, Perl, Ruby, etc. you are benefiting from all those non-web experience/demands that end up making the language stronger.Athas: Thanks for the tip; I'm not quote used to writing articles etc yet but now that you have mentioned it I will be sure to make sure I remember NOT to write emoticons :).

Bearclaw: I've tried emacs a few times but honestly never really liked it - I'm sure if I spent more time customizing it and learning it I would but the thing is...(continued) I dont want to - I want to be able to setup a text editor the first time, load up my code and work away, All with minimal effort.

I guess I'm lazy but I prefer it that way.This is written by a very smart woman. All women should read this !
Is it unreasonable to assist the IDE by having the developer add metadata? 

I'm specifically thinking about providing types for method parameters. Why not offer type possibilities in a comment that can be understood by the editor? I understand a lot of code would have to be modified, but it is a solution. The information has to come from _somewhere_ afterall. 

Or maybe you could interpret the operations that are performed on the arguments within the method to determine possibilities. Just sounds a bit messy.It seems to me that simple dynamic programming will bring it down to 2^n which is definitely feasible for n=16.
&gt; I've never understood how throwing the prototype away is different from rewriting from scratch.

There are a couple major differences:

1. Usually when you rewrite from scratch, it's because nobody can understand the existing software.  This, by definition, means that you're throwing away all the knowledge that was gained writing that software.  The point of prototyping is to acquire knowledge without cluttering up your production representation with all the accumulated dead-ends that go with knowledge acquisition.  It's rather pointless to acquire knowledge and then throw it away.
2. Prototypes don't have users.  When you rewrite from scratch, you hang your users out to dry until you can match the existing functionality.  Meanwhile, your product is out there in public for competitors to copy.  It's also extremely demoralizing to slave away at a rewrite and know that what you've currently got is *inferior* to the piece of shit that you just threw away.

&gt; The real reason to sell something as a prototype seems to be political: managers who can't appreciate component rewrites. It is paradoxically easier to pitch wholesale rewrites to them.

It's more visible and more heroic to rewrite the whole product from scratch.  Managers aren't paid to silently improve the quality of existing software.  They're paid to successfully pull off the latest and greatest in flashy innovations.Does your solution take nonlocal patterns into account? If not for those, I would have a O(n^2*2^n) solution:

bestWay( prev, setSeated ) = 
  max( Score(prev,next) + bestWay( next, next &amp; setSeated )

bestWay( prev, 0xFFFF ) = 0Thanks.

_"Managers aren't paid to silently improve the quality of existing software. They're paid to successfully pull off the latest and greatest in flashy innovations."_

That is gradually changing, I hope.[removed]Anton van Straaten gave a [presentation](http://ll4.csail.mit.edu/slides/rest-slides.pdf) at LL4 about implementing continuations RESTfully.

His approach was different than yours though.  REST does not prohibit server-side state: if it did, there could be no documents, and no WWW!  Rather, it prohibits state that is not somehow encoded into the URL.  Every state must be named by a unique URL, and each URL must encode only one state.  That way, you don't do silly things like fuck up the back button or break bookmarks.

So the solution is simply to hash the continuation, store it server-side, and include the hash in the URL.  It's cache-friendly, because the contents of the URL never changes.  It's back-button friendly, because you're just moving to a new dynamically-generated page, and the old one still exists.  It's bookmark-friendly for the same reason.

Garbage collection is left as an exercise for the reader, however.&gt; I think the best defense in favor of Long.equals()'s behavior is to say that Object.equals() is not intended to be an equality operator in any standard sense of the word.

Exactly! The problem is precisely that `equals` implicitely embeds *forgetfully* its argument (which should normally be of the same type as the class type) into `Object`. And in `Object` (as a set), the only meaningful equivalence relations are the trivial ones arising from equality of reference (denotational equality).

The alternative would be for the `equals` method on `Number` and each of its descendants to dispatch in runtime on the type of the argument; but that's what the `xxxValue` methods do (polymorphically, instead of by hand).

In other words, what's unsound is expecting `((Long)0L).equals(0)` to have the semantics of (numerical) equality. What *does* work is `((Long)0L).intValue() == 0`, or `((Long)0L).longValue() == (long)0`, or somesuch.I [responded](http://programming.reddit.com/info/133qx/comments/c13l4g) to pjdelport with some pointers to prior research.Or "car" and "cons"...also, kind of look at the java example.

A much better approach is to state your "weakness" as a strength but honestly, not just say you work too hard or something transparently silly.  For example:

I tend to be a bit of a generalist and I like to gain expertise in many different areas.  This helps me with creative solutions to problems but sometimes I lack the immediate depth required to solve some types of problems as fast as some others might.

Or alternately: I tend to focus very highly on my particular discipline so that my skills are top notch, however, I do not do enough reading about other areas of my profession and I would like to be more aware of developments in the broader industry.

You may get extra points for tying some aspect of the position to your ambition for self improvement if the interviewer cares about the development of their people.  For example, you hope that a collegial atmosphere will expose you to stuff outside your position or alternately you look forward to digging deep into some aspect of the job to improve the depth of your skill set.

Either way, you come out sounding like someone who has done some self assessment and knows what they need to advance professionally.
The process can be [automated](http://www.linkedin.com/), though&gt; Those that pretend to know what they are doing when they do not, regardless of their sex, are annoying.

That's very true, but I don't believe that was what she was advocating. I've read quite a few of her BSD sysadmin articles over the last few years, and she certainly knows her chops. Maybe she could have more explicitly said that it is necessary to know your stuff in order to command respect, but I think that point was assumed to be understood. Her advice to other women was to act confidently even if they don't feel confident. For all I know, that is excellent advice (I wouldn't know). I find it amusing that so many (male) people here are reading the article about pecking orders and are getting their feathers ruffled.
This was submitted to Slashdot (they paid for the book), but they've rejected the review, so I thought I'd link it here in case anyone was interested.

Cheers, Andrew[deleted]No longer relevant.

I haven't typed an url in a _long_ time. Feeling lucky search in the firefox address bar automatically takes me to the most popular website for the search terms in question. Works 99.9% of the time. If it doesn't work immediately, I pick different keywords.

Sites I visit often show up in my address bar history, so I only have to type the first two letters and the arrow keys take care of the rest.

The urls themselves don't have to be meaningful either, because firefox shows the title of the page in the address bar also.

Is the advice in the article still sound? Absolutely. But really, typing URLs is _so_ 1999.MS Internet Explorer left 80% of the Internet's users vulnerable to 0-day attacks for 284 days in 2006 compared to Firefox's total of 9 days of vulnerability.time to switch to firefox!:D

It's true; I've lost most of my IQ points because of Java.What the fuck is the internet?The only work I've done so far in my brief life has been in high-end retail, so this may not apply to more "professional" jobs, but when I'm asked this question I always say:

"I really hate cleaning, and I'm not very good at it, but frankly I'm so incredible at sales that I don't see this as a liability."

Always been offered the job.[removed]nothing new here....unless you haven't read Fred Brooks and/or "The Pragmattic Programmer"I don't see any reason why C++ (or Java or C#) should be taught as the intro language classes. Actually I feel all three are pretty inappropriate for most beginners. At that level, the primary thing you want to do is get the student accustomed to the basic ideas (variables, loops, functions, objects, some basic data structures, and how to think about and debug code).

If I was going to advocate for a beginning language, I'd say something like Python. Error messages are straightforward (I've saw many people get frightened by large chains of errors from C/C++ compilers), and there isn't a whole lot of syntax that you have to remember right off the bat.

There will be plenty of time to learn C and assembler when you take your OS and architecture classes (and ML/Lisp/Haskell when you take AI and compilers).
Is there a reason he's not recommending target="_blank" if all he wants is a simple link opened in a new window?  Yes, it's deprecated.  So is forcefully opening new windows: http://www.trilithium.com/johan/2005/03/target-blank/[deleted]I am strictly talking about Computer Science/Engineering students in my article. I come from an engineering school where even the Civil Engineers needs to take an introductory course in programming. Yes you are right, these are the people that get frightened by the error messages.

However, if you first learn Java/C# and think programming is always just use of libraries. You will have some problems when you go to your data structures class and actually have to use all kinds of fancy syntax, pointers, unions etc.

I think the idea of pointers and low level (compared to Java and C#) makes more careful programmers.

A prediction: five years from now, CS professors at upper echelon engineering schools will bare their teeth and snarl with naked contempt at the mere mention of any programming language but Haskell.Anyone do better than 36?For the college level I agree with C++ as the language of choice.  Or possibly even C.This is a pretty decent resource.A lot of the time there is no real framework, and trying to hack one in is harder than writing it from scratch.

Lets say your company is really into TDD and the MVC pattern. Well, those things really slow you down when you don't care about quality, so chances are you are not going to use them in the prototype.

When it comes to write the real one, the effort to seperate the code into MVC is often less that just rewriting that page/screen from scratch. When that applies to every page/screen, why not do it from scratch?

There is also a lot of short cuts you can take, like using globals to store context information that, in the real version, isn't really global. Back-threading that info into all the right places is non-trival.

Finally, there is the lack of consistancy. I often write prototype screens in two or three different styles to see which is more flexable/maintainable in the long run. When I do the real app, I only want to one true style. Otherwise it will confuse the developers that come after me.
&gt;However, if you first learn Java/C# and think programming is always just use of libraries.

C++ can give a similar impression.  If you're not making use of the standard C++ libraries like vector, string, etc., you're not really teaching C++; you're teaching C.

Personally, I think all of the languages you mentioned are pretty bad choices for a first language.  "Hello world" in Python is one line; it's about seven in C++.  And once you get the beginner to memorize that program, try explaining it to them.

How deep do you go?  Do you tell them that cout is an object of a class that overloads the &lt;&lt; operator?  Or do you just say "don't worry, we'll talk about this later"?  Neither method is very appealing.  The Python program, on the other hand, is rather easy to explain:  print() prints text on the screen.  You're done.

It's more important for a beginning programmer to grasp basic concepts, like variables, loops, etc., rather than learning how to define them in C++.  Python and other simpler languages let you express these concepts succinctly; C++ turns them into a mass of confusing symbols and words.

I learned C as my first language, so it can be done.  There are just other options available.List comprehensions are an excellent bit of syntax, amazing in their clarity for list-&gt;list transformations, but there *are* looping control structures that you can't do with list comprehensions, or at least not easily and clearly. Many of those control structures can be done just fine with higher-order functions.&gt; That's they ruby way of doing things though. Yes it feels very uncomfortable coming from a world where classes are set in stone.

I have serious doubts about how well that will scale in terms of developers. Everytime you do that you have to make sure EVERYONE knows exactly what you did, why you did it, and that it will hurt no one.

Seems to me just adding a new method or subclassing an existing type is much more maintainable.

&gt; I don't know of a good book so I just take the stub tests rails creates for me and extend them. 

A lot of people do, but that doesn't give you real systemantic testing. I'm not saying it is a bad place to start, but it seems to me that we should be doing much more.

&gt; They are really pushing the envelope when it comes to development and some are insane geniuses.

If the Ruby guys can pick up the LINQ stuff, I would be really impressed. So far LINQ is the best 'ORM' tech I have ever seen. 
Its the age old issue of if the UI looks finished, then the pointy haired types will think the app is done.
I recommend using pencil on graph paper and a scanner for the GUI. Drop in real form fields as needed. It keeps everybody thinking prototype.A prediction: C will still be around, everywhere you turn. And they'll still snarl.[removed]THAT WAS SO METADude, this rocks.  I've been looking for something like this for a while.I would actually consider using this answer. It is funny, but not too disrespectful.Yes, all these are good reasons, but these seem to be *really* early prototypes. None of Joel's reasons to avoid rewriting from scratch apply to them.

I think the key metric is testing. As the amount of testing that goes into a codebase grows it becomes more and more uneconomical to consider rewriting from scratch, and running into long-fixed bugs all over again.

There's a difference between 'hacking something in' and refactoring to a more scalable architecture. You can do the latter without actually having to start from scratch.I don't think C++ should be taught first. I'm not sure how new programmers would take to the idea of learning about pointers, memory management etc... along with object-oriented design. That being said, I believe that C should be taught first to get used to the internals of a computer, and then move onto OOD with C++.

Although, I would be interested to see how a student would turn out in the long run after learning a functional programming language as their introductory language. They don't seem to do that at any universities near me, so maybe someone here can shed some light.If that was going to happen, it would already have happened. We've had languages "purer" than C++ for a while now.Hmmm. A commercial venture claims to be beyond the state of the art, and hopes to reach 1,000 qubits by 2008. But they have no peer-reviewed publications to back up these claims.

Can anyone point me to credible sources (from outside the company) who think this will work?

"1000 qubits": http://dwave.wordpress.com/2007/01/19/quantum-computing-demo-announcement/#comment-931

"Almost zero technical stuff" in the demo: http://dwave.wordpress.com/2007/01/19/quantum-computing-demo-announcement/#comment-1007ya, that's a pretty serious claim they have there. I'm a bit worried.Unfortunately all five of them thought I was paying them for gay leathersex.  That darn Craigslist!Sometimes smart (or well-educated) people realize that they can exploit their knowledge to fool people into paying for science toys.

"ahh, errmm... i spend your money on a thermal epsilon error reducer but it turns out that the harmonic oscillation of the zed-particles caused a large fluxuation and, well, can i have more money?"Oh my, *any language*?  Any language at all?  You happily predict that [every language on this list but one](http://en.wikipedia.org/wiki/Alphabetical_list_of_programming_languages) will warrant 'upper echelon' professorial sneers?  I can't even guess at what apocalyptic tide could sweep away so many languages, but I am certain that even five years of rising water will not turn Haskell into a suitable replacement for languages contrary to it -- no, not even for 'upper echelon engineering school CS professor' use.Anyone find the mention of NP-complete problems misleading?
  
Quantum computers can't solve NP-complete problems in polynomial time.  They can go faster, but can't reduce an exponential problem to a polynomial problem.  Nothing can!

(Factoring is not NP-complete.)
My point is that Computer Scientists are in the process of choosing Haskell, almost unanimously, as their favored programming language.  If they are forced to teach students C++, Java, or C# as a first programming language, they will do so with condescension and cynicism.  They will feel that the superiority of a programming language based solidly on Category Theory to one that barely parses is not strictly a matter of subjective preference.yes.

as an armchair quantum-computationalist - i initially believed that QC would be able to solve NP-complete problems in poly time (given the parallel nature of the beasts.)

i've since been assured this is NOT the case - but the proof in question is beyond my abilities.

does anyone have a *simple* explanation as to why QCs cannot solve NP-complete problems? something that can counter the intuitive argument "hey - just split off into n^2 parallel machines and check to see if the input is accepted by the turing machine."Are there type systems which can make guarantees about space usage?&gt; To date, the only support I've seen for this statement is "if I use another templating system I can't use render_to_response. Is there something else I'm missing? Django's shortcuts and convenience functions are designed to work with, well, Django. Is that wrong?

As I stated earlier, it isn't wrong, it is just inefficient. TG and Pylons both support at least a dozen templating languages because they support a standard, Buffet. Its not just a matter of writing convenience functions and shortcuts for one engine; you can have your cake and eat it, too.

&gt; Gonna call BS on that. Show me the equivalent of django.contrib in the out-of-the-box versions of Pylons and TG. Show me the equivalent of generic views in the out-of-the-box versions of Pylons and TG. Show me the equivalent of Django's admin app. Show me the equivalent of Django's flexible auth backends. Show me another framework that ships all this stuff out of the box.

TG provides Catwalk, widget browser, Identity, etc. It probably supports just as much as Django out of the box. Widgets are very similar to generic views.

Pylons isn't quite there yet, but I think its a matter of time before it is.I agree.  It's great, actually.  It's funny enough to avoid sounding incredibly sarcastic.Haven't been asked, but how does this line generally go over?
"I plead the fifth."josh please don't post this crap, thanks.The link to the spiraling quine in Perl is broken. You can see it [here](http://www.perlmonks.com/index.pl?node_id=176043).With big oh, is it still meaningful to say O(n*n!), as opposed to just saying O(n!)

Also, how does one google for O(n*n!) ?  Would you write it as TeX and google for that?Exactly, list comprehensions give you map and filter, not reductions. With parallel list comprehensions you can get zip in there too. 

But still the example:

    (sum . filter (&gt; 100) . map read) ["10","1000","2000"]

Becomes as a list comprehension:

    sum [ y | x &lt;- ["10","1000","2000"]
            , let y = read x
            , y &gt; 100 ]

Since you can't reduce a value in comprehension syntax easily.Wikipedia is your friend.

http://en.wikipedia.org/wiki/Big_O_notationYou can't specify width/height, and the new window will look like a normal browser window (i.e. with tabs, address bar, etc.) instead of a popup window. It's a fairly lame excuse in most cases, but it does have its use.&gt; It’s a continuations vs. REST debate

This is a false dichotomy, though:  you can implement continuations RESTfully just like you can implement non-continuations non-RESTfully.via
http://alexfletcher.typepad.com/all_bets_off/2007/02/rpm_the_story.html
Open Source Unleashed: RPM: The story
As far as i understand it (which isn't very far :), region inference is usually implemented as a type system extension.

There's also linear typing, which is quite closely related to the stack discipline in stack-based languages.Nice job management system, probably a necessity for any runtime on highly parallel machines. Someone should make a Boost version...Yes, the simple explanation is that an exponential amount of information needs to be stored.  No matter how small your "bits" are, if they are of a constant size, it is impossible to encode an exponential amount of information in polynomial space.

If you could get your bits to shrink as the problem scales up, maybe you can do it, but nothing I know about in physics suggests that's possible.Whenever someone talks about a having a fixed number of quantum bits, that means they are building a nondeterministic *Finite State Machine*. In order to solve NP-complete problems in polynomial time you have to either show P=NP, or you have to build a nondeterministic *Turing Machine*.

A Turing Machine is a Finite State Machine plus a read/write tape. I've never heard of any Quantum Computing project that aims to build a nondeterministic read/write tape. That is the missing piece that keeps you from solving NP-Complete problems.Downmodded for not linking directly to [the article](http://dwave.wordpress.com/2007/01/19/quantum-computing-demo-announcement/).

[Edit: Fixed link syntax. Too much time using Trac…]Sorry to reply to my own comment, but I realized there is a much easier way to explain this.

Quantum computers can make nondeterministic decisions, but they can't perform nondeterministic IO. So, once the size of a problem exceeds their hardware, they loose their advantage.Don't worry: the fourth one should stay up.Pappy BQT says
http://arxiv.org/PS_cache/quant-ph/pdf/9701/9701001.pdf

"The computational power of QTMs lies in their ability to maintain and compute with exponentially
large superpositions."

Which sounds like exponential computing.. but superposition is not computing. The results say

"The proof of Theorem 3.5 shows that this approach is essentially optimal: no quantum algorithm can gain more than this quadratic factor in success probability compared to classical algorithms, when attempting to answer NP-type questions formulated relative to a random oracle."

So how do they prove this? 

First, they define "oracle". It is a quantum computer that takes exactly 1 unit of time to answer a question relative to the digital computer running the deterministic part of the algorithm. We call these oracles A(x)...

Ok. Let us make 2 oracles. 
A(x) = 0 for all x.
A(y) = 1 for some y and A(x) = 0 for all x != y.

How can we tell these two apart?

With a classical computer, we take some n bit string at random at test it. Thus we know the answer with probabilty = k/2^n after k queries.

What we want to do for the quantum case is to separate the empty oracle's superposition from the 'y' oracles one. We'll call the state positions |Wk&gt; and |Wk(y)&gt; where k is the number of interactions with the oracle we are trying to "figure out". 

Let us check out a uniform superposition that we will start with.

|W0&gt; = 1/sqrt(2^n) * Sum over x |x&gt;

Now what is the maximum separation after one evolution?

| W1(y) &gt; = |W0&gt; - (2/sqrt(2^n)) * |y&gt;

"This results in a quadratic growth of the success probability, approximately as 4k^2/2^n for small k."

The paper proves this bound for classical quantum computers. You don't get EXPONENTIAL separation at all, you only get quadratic separation.

The simple way is to say that exponential superposition does not mean exponential separation. That should work in dinner conversations.

HOWEVER, http://arxiv.org/PS_cache/quant-ph/pdf/9912/9912100.pdf
shows that if you were to build a CHAOTIC quantum computer, then you would not be bounded by the results of the previous paper. 

No one has made chaotic (non-linear) quantum gates yet, but it may be possible. The above paper provides some ideas.

Hope this helps.Sure, I agree with the post I guess you used the title in order to target exactly who think Ruby is *an acceptable implementation of Lisp*. My idea is that while *of course* semantically Ruby and Lisp are different like you said and Ruby isn't just an alternative way to express more or less the same stuff thinking in the same way of Lisp while programming,  this should be clear from the fact that Ruby and Lisp are different languages from the start.

I like Ruby as a viable alternative to non mainstream languages but in the field of metaprogramming it shows its limits.I came, i saw, i had a tasty dumpling?Great, big… tracts of land.Damn-hell-ass yes.[deleted]Back in the day Factor ran on the JVM, I found a number of issues which lead me to abandon this approach and write my own VM/compiler:

- No tagged immediate fixnums so everything is boxed

- Slow dynamic dispatch

- No multiple return values

- No tail call optimization

- No continuations

- Slow startup time because sources have to be parsed, no way to just dump an image

- A lot of code had to be written in Java, not Factor, because the lack of an image precludes any form of metacircularity

I think this fellow will run into similar issues as he continues working in Cat, although since his explicit goal is building a language which targets .NET (I just wanted to build a language) I doubt he'll abandon it. He probably has more patience than me, so he might develop interesting workarounds.Ruby always struck me as overly complex. It seems the implementors are having a hard time achieving stability and elegance; Ruby 1.8 has major problems and 2.0 promises major redesigns, despite the language being 13 years old.&gt; Ruby is such a dynamic language that it hard for me to understand how they can even do this

Why do we have this persistent myth that dynamically typed languages are by their very nature interpreted? Is it just the multitudes of naive implementations out there (Ruby, P*)?We are building skynet.  Eventually AI will be able to learn knowledge from our tools like wiki, blog trends etc.  Yes we are building the machine.It's not just misleading, it's downright suspicious.via
http://lambda-the-ultimate.org/node/2050
SMP Erlang vs. Haskell vs. ML | Lambda the Ultimate
amazing what happens when knowledge is sharedAnyone who just wants some number to use is probably not very interested or motivated. They just want something that sounds legit. They want a number in case someone asks them or they need to report on it.

The motivated owner or manager is going to want to know the entire situation. What has been done, what needs to be done, what are likely problems, etc. They'll use that to judge in their own head how far along the project is.A good programmer won't assume that without knowing what kind of algebraic structure those are supposed to be in.There seems to be a tendency amongst some who dislike Java to ignore the plain fact that it is widespread, and still increasing, and to simply declare that it is 'dying', or 'outdated' or 'past it' or 'a creaky boat'.  This is almost like attempting some kind of magic spell - as if by declaring something often enough, it will become reality.  The fact is that Java is still growing in use, and is likely to continue to do so for many years to come as it takes over much of the still substantial use of C and C++.[removed]How about this:

Once you find a node in the loop (L), find the length of the loop (by running around it once).  Then, from the start of the list, start two runners, offset by the length of the loop plus one.  Advance them in lockstep:  as soon as the trailing runner's next node is equal to the leading runner, you've found A, the last node not in the loop.What incredible timing.  I just started working on a parallel distributed Genetic Algorithm for evolving Neural Networks in Python (with thanks to Pyrex for providing compilation of processor intensive operations), and this is EXACTLY what I needed.  Man I love Python  ;)You're too clever, pjd. Yes, that works!MySQL is not a relational database, so it can scarcely qualify as the worlds 'nicest' database. Sorry.I submitted it some days ago, but I'm upmodding it because I think very few people have seen mine, and this really deserves to be seen :)&gt;chunk of code could be written in any style, as long as its side effects are encapsulated. Here we break away from the pure functional model. If local destructive updates, statements, for loops, or other imperative constructs make coding easier, we can allow them. The key is not allowing these local decisions escape; our firewall against complexity must hold.

I think this is good idea that should be experimented upon. I specially like the approach of Clean and Linear Lisp. Clean has unique typing. If object has only one reference to it, you can destructively modify it without breaking functional abstraction. Linear Lisp had related idea: you cant have multiple references to same object. You must use explicit copy (might be copy on write). If part of program gets only unique objects, it could have internal state as complex as it likes. 
Have you looked at [LLVM](http://llvm.org/)? Might be too low level though.

EDIT - fixed link.i won't comment on quantum computing, but from computational theory in general -- 
to solve an NP problem in polynomial time, you need a parallel computer where the number of parallel computational nodes grows exponentially. 

quite simple, really -- the overall measure of 'difficulty' is more or less constant for an NP problem; what you can do, however, is split it into an exponentially growing number of subproblems that are computed in parallel for a polynomial physical time.
(the overall sum of times on all nodes is still the same or even a bit larger.)
Personally, I learned scheme in the first year of college, and this was a class that ALL engineering students had to take (including chemistry, physics... etc).  Since then, they have moved onto Java and I personally really regret this.

Some people might claim that Haskell is a good course to get started, and indeed it does have many interesting concepts.  However, the reason that I think scheme is ideal lies two important factors:
  1. Scheme has a simple syntax (you might not like it but the fact remains that it is simple)
  2. Scheme easily allows one to study different semantical models (we studied both FP and OOP through message passing by using scheme)

The first ensures that students can quickly come to grips with it and move onto actually learning about computation.  The second means that students will be able to quickly tackle many different concepts in one language so they have a wide array of knowledge about programming in general (what is recursion, what is FP, what is OOP, what is message passing, etc..)

&gt; Anton van Straaten gave a [presentation](http://ll4.csail.mit.edu/slides/rest-slides.pdf) at LL4 about implementing continuations RESTfully.

Very intersting. I don’t follow everything yet, mainly because I can’t read the code slides very well, but it addresses several of the first questions that came to mind about combining continuations and REST, so it’s exactly the kind of stuff I was looking for. Thanks!

&gt; REST does not prohibit server-side state

That’s why I’ve been careful to say “no per-client state.” Even that is strictly incorrect, but it’s close enough for the purposes of discussion.

My big unaddressed concern is that I believe URIs should if at all possible be human-readable; I spend a whole lot of time thinking about the design of my URI spaces. (In practice I’ve ended up punting on several constraints in all of the cases I’ve worked on so far; but as my practice accrues I’m getting better at doing it well.)&gt; If you are right, and know it, then be assertive. You also should expect to deal with people that are assertive but have no idea what they are talking about

Which is exactly what the article is about, and it is something women are not terribly good at, for whatever reason. Which is why this is a good article, not only to women (men stand to loose potentially good quality employees or co-workers by not recognizing some of these mechanisms).Seems great, but somehow it bothers me that he did not use something like an asynchronous callback.

Say I have a calculation like this:

f(a) + f(b) + f(c)

I'd then do:

    f1 = submit(f, a, ...)
    f2 = submit(f, b, ...)
    f3 = submit(f, c, ...)
    result = f1() + f2() + f3()

The addition of all three results would have wait until all three calculations have finished. Although, it could already start once two results are ready. Far better would be something like:

    class addToBigNumber(object):
        """Coroutine class to add up numbers and stores the result somewhere where you can get it from, e.G. global variable."""
       ....

    submit(f, a, ..., addToBigNumber)
    submit(f, b, ..., addToBigNumber)
    submit(f, c, ..., addToBigNumber)
    result = addToBigNumber.result  # blocks until numbers are added

By this is would not be important which job finishes first, but calculation can go on on other computers.

Or am I totally wrong?

//edit: only 3 tabs indented =/PyPy seems so promising. I'm really looking forward to an production-ready release.Interesting links in this vein:

http://www.andrewsmcmeel.com/godsdebris/

http://reddit.com/info/11xjo/commentsIt's called **estimating**.  Look it up.Personally, I would find this too weaselly to be convincing. I would respond "We'd like to hear about a weakness you actually have, not one you used to have."Excitation growing here, too.That, to me, is a perfect answer. It is a flaw, in its way serious flaw, but not really a relevant one, as it's expected that's the case for you. But it's still good that you recognize as a candidate that it is a weakness. Of course, it won't be relevant in all cases.You left off arrogant smartass.&gt; Tasks have only three states: not started, started but not
&gt; finished and finished. Period.

Except for the fact that a task is never *completely* done. Given more time, code can be refactored, documentation can be improved, and glitches can be fixed.

So there are really only two states. Not started, and started but not finished. But where do you draw the line between 'Not started' and 'Started but not finished'? Clearly thinking is a crucial element of software engineering, therefore by considering whether you've started you've set the answer in stone. By measuring the data changes, so Heisenberg would be proud of you.

So there's only one state, called 'Started but not finished'. And if the question 'When will it be done?' always yields the same reply 'Started but not finished', you're clearly just saying 'Fuck off, I'm busy'.

Not that insightful for a frontpage article.I need to write better titles. They always look like whack on reddit.Exactly. And while three signficant figures is stupid, there's also a big difference between, "I just started coding" and "all the standard use cases work, but we have one bug where..." Figuring out where you lie between these endpoints may be an art and not a science, but it's not meaningless.Skynet is nothing compared to what we are building.Erm, no. The analogy is broken. Even though it centers the validity of Boyd's law on the total energy expenditure needed to iterate (less energy required to iterate = greater number of iterations  ==&gt;  more iterations in a given time = with a given energy budget), it fails to carry the energy expenditure analysis to code engineering. Yes, you can write *faster* unit tests, but you have to write *more* unit tests (which can translate to *more* stringent contracts satisfied). Yes, you can iterate UI designs fasters, but the cost of coming up with better designs is not considered at all.

My problem with Agile methodologies is that practitioners never even consider "thinking time": the presupposition is that the use case analysis, scenario narratives and design decisions that arise (necessarily!) prior to coding even the simplest of unit tests or spikes is, for all purposes, free. There is no such thing as coding without thinking.&gt; TG and Pylons both support at least a dozen templating languages because they support a standard, Buffet.

The Cheese Shop page for Buffet links to a spec on the TG site which 404s. After some Googling I find out that it was developed for CherryPy. I should have suspected them -- those evil SOBs are obviously out to destroy the Python community by developing things on their own which work well with their own software. How dare they!

Again. In all seriousness. Using Django with another templating system just isn't that hard. People do it all the time.

Still calling BS on other folks doing out-of-the-box stuff as well as Django does (and if they did, wouldn't they have to become just as evil as us? Shipping useful add-ons to your own stuff is a horrendous sin, or so I'm hearing).&gt; My big unaddressed concern is that I believe URIs should if at all possible be human-readable

I agree - but in many of the use-cases for continuations, how would you name them?  Usually, continuations are used when you have a multi-step, transient process that the user must go through.   Like filling out a form.  

The best I can think of is to name them with the user, then the form name, then a timestamp, and then a unique int that identifies a place within the form.  So if I was ordering something at 5:30 yesterday and on the second page of the form, it might be:

    /forms/order/nostrademons/20070208173027/2/

That's not too bad, though it's a little verbose.  It'd require "categorizing" continuations by user, date, original function, etc, and also having some notion of a top-level that continuations originate from and escape to.  Then continuations could be numbered consecutively by small ints.

My beef with many of the continuation &amp; state-saving web frameworks is they *force* you to use their crazy session hashes and hidden input fields, even if you're doing simple GET-based, view-only resources.  I know JSF is like that, and from the examples I've seen, SeaSide is too.  There's plenty of room for better web frameworks; if I weren't already working on a programming language, a web startup, and a day job, I'd try my hand at one.[removed]And to really get to the heart of the issue: Django is a full-stack framework. Pylons and TG are mostly "here's some glue, go stick some stuff together" frameworks. There's nothing fundamentally wrong with either approach, but they are different approaches.

The Pylons and TG guys don't have to maintain a templating system, an ORM, a dispatcher, etc., so they have the time to hunt down, integrate and improve generic code that's designed to take anything as a plug-in (though the result ends up feeling strangely Java-ish, because it really comes down to spec'ing and implementing a bunch of interfaces). And that's really cool.

Over on the Django side, we have a lot more code to maintain to stay full-stack, and so we don't have the time to guarantee that Django works with anything else. We don't go around deliberately shoving obstacles in people's way (no time for that, even if we wanted to), but we also don't provide shortcuts designed to work with any Python component ever written -- we have precisely enough time available to make sure that our components work and work together.

There's a market for both types of frameworks, one type is not inherently "better" or "worse" than another, and both types appeal to different sorts of programmers. Ian Bicking, who I was arguing with just the other day about this, is a god among Python programmers and has such a deep understanding of the language and the available software that, for him, anything which isn't an absolutely free-form "pick some components and glue them together" framework is unacceptable. He has the knowledge and understanding of all the necessary interfaces and systems to do that, and so a glue framework is his ideal.

But people in that situation need to realize that not everyone is like them. Some folks just want a full stack that works out of the box, and then want to get on with writing their code instead of worrying about all the interface bits that make it work or which of the eleventy billion templating systems they want to use. I'm one of those folks, as are (I think) most of Django's other users.

So, again. use what you like. If you don't like Django, don't use it. Not gonna bug me in the least if you don't. But don't go bringing the FUD, mmkay?Are you an HR Director?Yeah, I'm pretty excited too, although I wonder how fast it's really going to be when all is said and done. Also, given the  apparent complexity of the code (going by the summaries of the progress and not having looked at any of the actual code), I wonder how easily extensible it will be. That was supposed to be a key motivator for writing a python interpreter/etc. in python.&gt; "Where do you see yourself in 5 years?"

"Building a time machine so I can come back here and answer that question."

&lt;looks at watch, then the door...&gt;

"Any minute now..."Nope. I have been involved with interviews and hiring though, but in a technical setting.

It is important to note in some jobs they may be looking for introspection and self-awareness, and in others the ability to smoothly bullshit. I think your answer would be good for the latter, but not for the former (which fits with you saying they teach it that way in an MBA program :D).

When I hear your answer, I hear "I'm great. I didn't used to be as great as I am now, but now I'm really really great," when what I'm trying to learn is where you're *not* great, which doesn't have to be damning.The last time I took a serious look at it, about a year ago, the code base was already insanely complex. Certainly it was much more complex than CPython ever was. Of course, it does a lot more than CPython in a number of ways, but the argument about it being inherently easier to modify for its user community, because it's coded in Python rather than C--well, I don't think that holds any water. If PyPy was closer to being a re-implementation of CPython in Python that would be likely be the case; the reality is that CPython is built on technology that has been well understood for the last 30-40 years, whereas some parts of PyPy are more like rocket science in comparison.Anyone knows a Emacs browser that works well on Windows?Yeah, but in general you could turn it around... "Well I'm not really experienced with your company..." or "I'm not really experienced with your specific field..."

But you can always finish it up with "... but I've shown in my past a solid track record of learning quickly and adapting to new situations"  ... or some other bs.Uh, if P=NP then quantum computers can indeed solve NP problems in polynomial time.This trick is so ancient it's an M20 exercise in Knuth. :)Beat me to it!So, I'm curious about these people who are against STM and for message passing, like the guy who this blogger is responding to: how do they propose to handle coordination between processes? Take the ATM example, with each bank account presumably represented by a process--how would you coordinate a transfer of funds, while satisfying the necessary invariants? The only robust technique I know is a two-phase commit, which probably also implies the existence of a third mediating process, the transaction monitor, if you want to deal gracefully with failure. It's pretty tricky to pull off. I know Erlang kinda sorta deals with this by sticking all the transaction management in Mnesia, but that only solves a certain class of coordination problems.

(By the way, I'm a fan of message passing when it makes sense, I just think there's plenty of room for STM as well.)[Crush](http://citeseer.ist.psu.edu/247990.html) is the new fold.Knowing C# doesn't mean you know the syntax, it means you're familiar with the .NET framework. Learning a framework well takes a year, maybe longer.

So the employer has to take a calculated risk.

* Will he be able to pick up the language quickly enough?
* Will he start writing real C# code, or C++ style code in C#?
* Will he learn .NET, or undeliberately waste time duplicating parts of it?

I think these are valid concerns for the employer. Most likely the 10 year C++ veteran will be an amazing C# programmer in a year, but why take the chance? After all, there's another 10 year C++ veteran waiting in the lobby who has written an SQL database engine, 3 distributed online games and a Compiler generator, all in C#, just in his spare time.Ah, let him rot. I've never come across someone simultaneously so arrogant and so ignorant (he has succeeded in "writ\[ing\] a web framework that ‘fixes’ the convoluted Java Servlet Specification", he has "a well thought-out, very usable web application framework"). He wants everything to be Haskell. He even links to a [rant he wrote in January](http://blog.tmorris.net/strong-type-systems/) with the words "we know how important this is". That rant is about how (paraphrasing) "Java can't implement the Maybe monad in a type-safe way, hence Java has a weak type system". The problem is not with Java, but with him, as what he wants to do can be done.So how come OCaml isn't mainstraim? It addresses the concerns of the author. And it's fast and portable to boot.Rockets have been well understood for the last 30-40 years as well ;)Perhaps it's the fact that the obvious static decisions that a compiler for a statically typed language does can't be done for a dynamic language in which the compiler's assumptions may be violated at runtime. Also, AFAIK the really fast dynamic language implementations (e.g. Strongtalk) do most of their magic at runtime.wait - is that it?  so a QC with n qbits can solve NP complete problems with (input size + tape size) &lt; n in poly time?

but that's still HUGE.  theoretically, yes, i see where that's limit.  but in practice...  ?

consider that a classical computer with m classical bits can't solve P problems with (input+tape) &gt; m.  in practice, we don't make a huge deal over this limitation.[removed]I got my first Smalltalk experience 1986 with digitalk PC implementation. I was impressed, and quite optimistic when Bill Gates praised the OS/2 implementation. (insert a lament here)[removed]You'd use a simple regex, not enormous tools ;-). Besides, config files in Ruby are code or YAML. Or you could use the block form (you could even combine them into one method: parse, and if it fails and a block is given, call the block, otherwise return false). Or you could return nil an check like this:

    if (value = try_parse(xyz)) == nil

TIMTOWTDI :)An intelligent, thoughtful response is outside the capabilities of the current administration.  You can bet they will drop real bombs on Iran in the situation of a cyberattack (even if cyberattack did not come from Iran).

I'm a believer that the best protection from cyberattack is to constantly defend against a cyberattack.  When you do this, nonstop 24-7, you constantly become a harder target to hit.  Its not like our government is poor and unable to put a little armor around these things.hm... still too much for me.

the other arguments in this thread say basically "once your computation exceeds the n qbits, you're back in classical land."  but i suspect (without being able to understand it) that your argument is more than that.  true?

let me ask you this: as the other poster suggested, is a QC with n qbits equivalent to a non-deterministic finite state machine of n bits?Mastering Rails itself is easy, the real challenge and reward lies in mastering Ruby.STM is from one POV is just a pattern in concurrent systems. Abstract the memory part of it and your left with a particular take on Transactions.

I think a lot of the hostility comes from the erroneous idea that you have to put all your eggs into one basket; message passing xor STM.

Perhaps there's a fear that all the noise about STM will turn people off the Actor model and CSP, which would be a crying shame but unlikely IMO.I've got a scoop for you: in teh real world ATM transactions aren't atomic.

Databases transactions are too strong to hold on a large scale (where systems and networks may be up or down, 3rd parties and business partners applications are interfaced etc.).

For ATM withdrawal there's a system (including insurances) for compensating abnormal transactions.

Why is there a maximum that you can withdraw in a week?This is about the dumbest idea ever.  A real DoS attack against the Internet is not going to come from anywhere - it's going to come from *everywhere*.  No sane hacker is going to sit their at his PC pinging away at the root DNS hosts.  He's going to write a virus or trojan that infects hundreds of millions of hosts worldwide, and then trigger *them* to attack to root DNS hosts at a specified time.  At that point, he'll be long gone, probably off the Internet entirely.  

Your only recourse then is to nuke all of America into submission.

Really, if the government is serious about preventing this sort of cyberwarfare, they need to ban the use of Windows worldwide.  And have every other program that consumers install subject to a security audit.  I doubt that'd be politically palatable, but nothing short of that eliminates the threat.I have never had a customer come and describe a problem in terms of a for loop, ever! They usually say something like, all customers who satisfy criterion X are eligible for upgrade Y.

However, because I am paid to program in Java, I then go and translate that into a load of for loops and iterators. In a functional language it would probably be a single expressive line of code, maybe a list comprehension in Haskell. In Smalltalk or Ruby I could achieve it with some blocks and collect. All of these solutions would be closer to the original problem statement than my nest of for loops in Java.

Yeah, I admit it's pretty trivial, but it's a fun trick. :-)

There's a lot more good stuff in Okasaki's book, including how to get a true, non-amortized O(1) queue, and all sorts of exotic data structures.  The real challenge is analyzing the running times--Okasaki actually develops some interesting theory to handle laziness.

I recommend either the PDF or the book highly.Anyone interested in functional programming should read Okasaki's thesis. It's good.This article touches on what I am coming to think is the real key to the multithreading debate, and almost the only thing worth talking about: How it affects the programmer.

Even if you make threading ten times easier than it is now, you still top out way too early. Threading needs to be _at worst_ log harder, and if we ever want "mere mortals" to be able to write highly-threaded systems, we need something that is a constant difficulty. (That is, once you internalize how to do it, you can write threads that can be easily understood on its own, with no non-message interactions with other threads.)

Thus, we need at least one threading solution that, even if not perfect, scales as a constant factor. Erlang-style message passing is the only thing I see as even close to that, even STM is still too complicated.

Given that some things may still need to be coupled even more tightly, a mix of STM and message passing (where, when in doubt, you use message passing) is plausible, as long as most systems have clean message passing boundaries separating the various STM domains. Heck, if you partition the system well enough via messages, you can have old-school imperative domains synchronized with modern semaphores if you know what you're doing and you need raw, steaming performance.

But I don't think we're going to be able to get by without message passing being the main organizational principle, because in the 64+ core world we're shortly going to be living in, anything harder than O(1) programming effort just isn't going to work very well on any problems that aren't trivially parallelizable.I just use the default. Its always worked well for me.

What isn't working for you?I would like to have seen some comments on the upcoming OpenMoko in this article. I think it addresses some of the problems very nicely by providing an open (as possible) hardware platform.

Time will tell if this is a step in the right direction.I tend to agree with dons, His syntax is far simpler IMHO.Me [too](http://reddit.com/info/134l7/comments).  Third time's the charm I guess.  Upmod.&gt; The Cheese Shop page for Buffet links to a spec on the TG site which 404s. After some Googling I find out that it was developed for CherryPy. I should have suspected them -- those evil SOBs are obviously out to destroy the Python community by developing things on their own which work well with their own software. How dare they!

Yes, they developed something on their own which is design to be flexible. Nobody said there was anything wrong with developing something on your own, I do think that it is inefficient, unproductive, ignorant, arrogant maybe even misguided in developing something on your own when something already exists and works pretty damn well.

&gt; Again. In all seriousness. Using Django with another templating system just isn't that hard. People do it all the time.

And again, in all seriousness, why not using something like Buffet which would make it that much easier for the user?

&gt; Still calling BS on other folks doing out-of-the-box stuff as well as Django does (and if they did, wouldn't they have to become just as evil as us? Shipping useful add-ons to your own stuff is a horrendous sin, or so I'm hearing).

You've totally missed the point of the discussion if that's what you are hearing. Nobody said a framework shipping with components was wrong, TG does it after all.You basically have the choice of w3 and w3m if you really want to stay within emacs. W3 is a bit old, although someone's supposedly working on a new one. W3M is quite fast, but that's often overshadowed by the fact that it only shows the page after everything is loaded.

Still, it works quite nicely and has several extensions that make it fit better into Emacs. I'm using the version provided by my Cygwin install, can't say how hard it'd be to compile it without intermediate libraries. Cygwin has some other neat tools for Emacs users, so it's certainly not a waste of space and time.&gt; And to really get to the heart of the issue: Django is a full-stack framework. Pylons and TG are mostly "here's some glue, go stick some stuff together" frameworks. There's nothing fundamentally wrong with either approach, but they are different approaches.

You are right about Pylons. It isn't a full-stack framework that provides everything you needs. But TG is definitely a full-stack framework (infact it provides pieces that Django doesn't at all, like Javascript), so if were are talking about FUD...

&gt; The Pylons and TG guys don't have to maintain a templating system, an ORM, a dispatcher, etc., so they have the time to hunt down, integrate and improve generic code that's designed to take anything as a plug-in (though the result ends up feeling strangely Java-ish, because it really comes down to spec'ing and implementing a bunch of interfaces). And that's really cool.

You know what...the Django guys wouldn't have to be doing that if they were using community standards and libraries that everyone else did. Then they could focus on the framework rather than the little bugs in the details.
Sigh.

OK, after this I'm done with the discussion:

Django has its own components because the ecosystem of stuff that's out there now in 2007 didn't exist way back then. At this point, rewriting to be a Pylons clone would require throwing out the whole codebase and starting over, and at that point there would be no reason to do Yet Another WSGI Glue Framework anyway.

Being a full-stack framework and shipping a bunch of our own components with shortcuts which work with our components is not arrogance. It isn't ignorant. It isn't misguided. It isn't evil. Get over it.

If you can't handle "you do your thing and we'll do ours", I'm honestly sorry. But this entire debate comes down to you like a framework which does X and I like a framework which does Y. I really, really like that you can get a framework which does X and I can get a framework that does Y. I really, really hate that you seem to think all frameworks should do X because that's what you prefer. If you like X, use the framework that does X and celebrate the fact that Python web development is such a mature field that people can choose based on their preferred style of framework.

Can we call this done, finally? Please?true, but i think the point is that most of the heavy computation is done on the "cluster" by in the f1,f2,f3 functions as you call them. so adding the results of f1(),f2(),f3() should be trivial -- time-wise. 
also, i havent looked at the module in detail, but it promises load balancing--that could also alleviate the problems you mention."I get turned down for UNIX dev roles because the version of VB I've used isn't the latest"
- huh?*The implementation that's in Firefox is slightly different from what's presented in the specification; however, you can expect that the specification will probably be updated to reflect that changes that've been made.*

Great. Now the monkeys are retrofitting the specs to accommodate the deficiencies in the world's favorite toy-browser.

Good job, asshats. You truly follow the Netscape-way of browser development.&gt; The best I can think of is to name them with the user, then the form name, then a timestamp, and then a unique int that identifies a place within the form.

That’s a decent first take. More broadly, it wouldn’t necessarily be the username, any token which uniquely identifies a particular… “client entity” (for lack of a better term) would do.

&gt; It’d require “categorizing” continuations by user, date, original function, etc, and also having some notion of a top-level that continuations originate from and escape to.

That’s the sort of thing I meant when I said elsewhere in the thread that to retain (nearly) full control over the amount and nature of the state in the URI you have to give up some transparency in the server-side representation. Something like Perl’s variable attributes might suffice as annotation. You’d also need to describe the URI layout somehow. Then the framework would simply take care of magically making the bits from the request URI show up in the variables.

&gt; Then continuations could be numbered consecutively by small ints.

Aieee. I’m not too sure about that. I get an inevitable sinking feeling when guessable state tokens are involved… hijacking and XSS is just around the corner, it always seems. Of course, there’s the question of how you keep URIs at least somewhat human-readable when they’re required to contain a random string… hmm.

&gt; My beef with many of the continuation &amp; state-saving web frameworks is they *force* you to use their crazy session hashes and hidden input fields, even if you’re doing simple GET-based, view-only resources.

Yeah, agreed.&gt; Django has its own components because the ecosystem of stuff that's out there now in 2007 didn't exist way back then. 

Again, the key example against that is newforms. That is 2006; FormEncode existed well before it.

&gt; At this point, rewriting to be a Pylons clone would require throwing out the whole codebase and starting over, and at that point there would be no reason to do Yet Another WSGI Glue Framework anyway.

I imagine it would be pretty easy to make a Django-clone using simply 'off-the-shelf' components and Paster templates. I was interested in doing that at one point, but I stopped because there aren't certain design decisions I'm not a fan of (but that is besides the point; it is possible).

&gt; If you can't handle "you do your thing and we'll do ours", I'm honestly sorry. But this entire debate comes down to you like a framework which does X and I like a framework which does Y. I really, really like that you can get a framework which does X and I can get a framework that does Y. I really, really hate that you seem to think all frameworks should do X because that's what you prefer. If you like X, use the framework that does X and celebrate the fact that Python web development is such a mature field that people can choose based on their preferred style of framework.

You seem to really misunderstand my point. I don't want all frameworks to be Rails-ish clones. I do want them all to use standards and be as flexible as possible The framework "X" you are mentioning to me isn't some specific way of doing frameworks, it is a way of doing frameworks that is flexible, and that's exactly what WSGI and Pylons are all about, and that's where Django isn't at.

As I said before, you can have your cake and eat it too. Django can be Django and still be very flexible, currently it doesn't have the infrastructure to really support that flexibility, though.One question:  Is PyPy destined to replace CPython?  Not in a philosophical way of which will be better, but rather will Guido and the powers that manage the language move to this, or is it a separate project like Jython?  I haven't been able to find good clarification on this in the past.Yeah no kidding.

We've gone from Common Lisp to Python, from Smalltalk to Ruby. As much as I like Ruby and Python, I'd have to say that is a step backwards in power and possibilities.

Imagine if Smalltalk or Common Lisp had achieved the relative popularity that Python has had from 1990 until now in an open-source project. What would've happened?It is easy to pick on the big guys, Sun and Microsoft.  This essay could have read a lot better if he would have said, look; "I have been using Scala for the past few weeks to write a web framework that competes with Walmarts and Ebay's J2EE based web-application".

It is worth drawing a distinction between "closures as functions with environments" and "function pointers", because C and C++ have one, but not the other. And, correspondingly, you can actually do some things called "higher order programming" in C(++), and there are other "higher order programming" things you can't do.

Given the heritage many people have coming from C(++) as their primary language, I think if you're going to talk about closures it's important to make sure you're actually doing something with the enclosing environment, lest you trick C(++) programmers into thinking their language has "closures", ending up with a diluted term.This is a good example of how some people are obsessed with high principles and theories but ignore simple, practical approaches.

Or to put it in another way: they assign too little value to simple pragmatics.I think your experience strongly depends on the language. It's pretty rare for me to have Perl or Python code that can't be refactored. I think that's all the design solutions are already so much "closer" to each other (as measured by source code "diff" distance) that you're much more likely to be able to smoothly go from where you are to where you want to be, with a working program the entire time.

With Java or C, there aren't likely to be enough waystations to make it worthwhile, so you're more likely to want to just chuck the prototype and start over.

This is all relative, not absolute.This strikes me as nothing more than a glorified RPC library. I was hoping it was a working implementation of something like [POSH](http://poshmodule.sourceforge.net/).He completely misses all the jobs that require things like "Minimum of 15 years Java, 10 years C#".   (For those living in a cave, Java has been out for about 10 years, and C# for about 5 - so while you can hire people with that much experience they are all liars)   Unfortunately this is not isolated incidences, every time I go looking for a programing job I see many advertisements requiring more years in today's hot fad than the fad has been around.

Someday I'm going to ask a lawyer if I have a truth in advertising case against these people.

(edit: spelling)Sure, but then so could any normal Turing-machine-equivalent computer.  After 30+ years of research, no ones' ever been able to demonstrate a polynomial-time algorithm for an NP-complete problem.  There's no proof either way, that P=NP or P&lt;NP, but  I don't think you'll find anyone who doubts P&lt;NP.Let me be the first to say this:  Someone set them up the bomb...We need to be more professional and stop talking bit and bytes and version numbers to non-techies.

So, instead of saying "C++.Net 2.0 with ClickOnce," say "2006 -- Developed new auto insurance claim managment application, using Microsoft's latest products, installed at 12 customer sites with a total of 500 users."

For search purposes, you can put the nerdy keywords in a list--but without version numbers--just for automated searching to find your resume.

I understand that .Net and J2EE are huge libraries, but we need to give up that advertisement of specific skills in order to short-circuit the goofy game described in the article.

If we stop advertising details, maybe we will be treated more like business experts able to contribute great value to the company.
The majority of Haskell extensions are to the type-system.
Monads and Arrows are bad examples to pick out because, in
particular, Monads were easily defined in terms of type-classes.
do-syntax and whatnot is not particularly important as you 
seem to make it out.  It's an argument in favor of things like
Template Haskell, perhaps.  People using Arrows get by without
any syntactical support for the most part.  Derivations
of instances can be done with some libraries, though they
do tend to use something like TH to achieve it.  This is all
a digression though.

Haskell does have the necessary concurrency primitives, so tell
me why would a message-passing system implemented on those "see
the limits" very shortly?  And I mean especially those
limitations which are not already endemic to message-passing as a
paradigm?
What about wxWidgets?  I didn't see that one in the discussion, but it seems to be very mature.  The Python bindings also seem to be very mature and complete."Agile methodologies" == "coding without thinking"?

Enjoy your waterfall...Some enterprising advertisers have used ASCII code to highlight their Adwords Titles. And it works for now. Until Google figures it out and stops it that is.Its a separate project for now. I don't know if Guido has ever directly commented on PyPy, but it isn't something that he has slated to replace CPython. Again, maybe that'll change in the future, but as of right now its just a separate implementation of Python.&gt; A new language could allow only special annotated functions to use STM, clearly indicating which functions are logically pure. This is analogous to implicitly passing a Haskell Monad to every pertinent function.

Steele's new language "Fortress" does this.  Also, it restricts IO to such specially marked functions (and you can't invoke an IO function from a non-IO function).
&gt; "Agile methodologies" == "coding without thinking"?
&gt;
&gt; Enjoy your waterfall...

Taking the contrapositive of your unwarranted reading of something that I didn't write:

&gt; "waterfall" == "thinking"?
&gt;
&gt; Enjoy your scrums...
[deleted]He has both unix and vb experience on his resume, but HR rejects him for their unix opening because his vb experience is not with the latest version of vb.

Personally, I wouldn't even want to work for a company that would blow me off for one of the reasons mentioned in the article.Thanks for clearing that up for me.  Much appreciated.QFT:

&gt; notfancy
&gt;
&gt; &gt; "Agile methodologies" == "coding without thinking"?
&gt; &gt;
&gt; &gt; Enjoy your waterfall...
&gt; 
&gt; Taking the contrapositive of your unwarranted reading
&gt; of something that I didn't write:
&gt; 
&gt; &gt; "waterfall" == "thinking"?
&gt; &gt;
&gt; &gt; Enjoy your scrums...

Learn what "contrapositve" means...via
http://www.scala-lang.org/docu/papers.html
The Scala Programming Language: PapersWell, they're claiming that a future version of their design will overcome that limitation:

http://dwave.wordpress.com/2007/01/19/quantum-computing-demo-announcement/#comment-939

I understand that quantum simulation of quantum systems would have remarkable consequences. But is there any reason to believe that it will be practical within our lifetimes?

What's the best overview on this subject?The paper (pdf) is here: http://programming.reddit.com/info/13qdf/comments[removed]Nice learning object editor, if you are into the e-learning .I guess as long as the bus runs open source software and GNU/Linux, RMS would be happy.[removed]I believe the HiPE Erlang compiler uses a hybrid heap architecture which tries to keep message data in a shared heap area to avoid copying.

Paul.Well, if you limit your problem instances to a fixed size, then you are actually defining a regular language. These can be solved by a Finite State Machine.

Now the important thing to realize is that with an FSM nondeterminism doesn't by you anything. Any nondeterministic FSM can be converted to an equivalent deterministic one. The size of the deterministic one can be exponential over the size of the nondeterministic one, but the speed should be the same. So all that quantum computing gives you is a (potentially, but not yet) cheaper way of manufacturing such a FSM."a classical computer with m classical bits can't solve P problems with (input+tape) &gt; m"

No, this isn't correct. Computers solve these types of problems by writing to harddisk/tape drive/network/etc. The read-write tape is equivalent to stateful IO.The limitations of the Java Servlet Specification have been known since before it was even published. *Finally*, this knowledge has made it to mainstream - though perhaps not always for the right reasons. This allows the liberty of writers to make claims such as this without ridicule from the masses. It seems not to be the case here - can you explain why? (Great how the onus is on the minority opinion to prove itself now ain't it? :)) Take this question lightly :)

He (I) doesn't want everything to be Haskell - why would he (I) use Scala in that case? The importance of higher-order kinds is not obvious for many readers, but perhaps more obvious is the importance of provable referential transparency and laziness - even to a Haskell/Clean newcomer, therefore, this doesn't need qualification.

The primary reason for using Scala (it may not be obvious) is to meet social, as opposed to technical, demands. It seems these demands can still be achieved with CAL - which on the surface, appears to be a superior language.

To qualify 'fixing' the Java Servlet Specification, I'd need to show you code - which unfortunately, is not mine to show - it is my employer's. But I'm sure I can work something out there - perhaps another blog post with references to why it is 'fixed', why it can still run under Tomcat (another social demand) and why it is at least (relatively) well-thought out (the reference to HAppS is not coincidental).

His (my) ignorance is not to be refuted - heck, look at the title of the post, but I can't understand your comments that seem to express over-zealousness.

Please show me how you're going to write the signature of bind in Java without higher-order kinds. I hope you're not going to refer to the 'solution' referred by Ricky Clarkson in the same post - for no other reason than it is incredibly wrong (though, I went gentle in my responses, so it may not be clear why).

I find it a little ironic that just a few hours ago, you were arguing in favour of higher-order kinds in a language that doesn't support them, and now undermining my statements of importance of higher-order kinds and claiming that indeed, this *can* be done in Java. I find this to be a problematic position that I am having problems understanding.

&gt; The alternative would be for the equals method on Number and each of its descendants to dispatch in runtime on the type of the argument;

http://programming.reddit.com/info/13ggn/comments/c13l4lThere are people that think they are the same. The results from a poll that I saw had roughly 10% in favor of P=NP. By now mathematicians should have seen enough surprising results to not assume anything until it is proven.&gt; The alternative would be for the equals method on Number and each of its descendants to dispatch in runtime on the type of the argument;

This is called a higher-order type, which you have suggested is possible in Java[1]. I'm keen to see the solution that you have come up with. However, you have (perhaps inadvertently) supported the argument that higher-order types in a language are extremely important. I maintain that higher-order types do not exist in Java and what you claim can be done, in fact, cannot be done (both in my example and in the example that *you* want).

A rethink? I don't want to get into one of those silly internet arguments, but I thought I'd highlight this point.

[1] "...what he wants to do can be done" http://programming.reddit.com/info/13mzl/comments/c13op5Blame big marketing money behind poor languages, and non-technical personnel picking the tools.you can only go searching for the one true language for so long. sooner or later you have to stop and call what you've currently landed on nice, as i think Steve Yegge said more or less in a blog entry.and you can still search google using ancient copies of its homepage because like good net citizens they didn't break their URLs.Think a big question is how does is handle failure - if f2 fails for some reason, does it take care of re-running the "job" for you?

Also wonder about the DIY networking. Twisted, asyncore or spread would seem more likely. Would even be interesting to try HTTP so you can put nodes "anywhere".Sounds like this guy had a bad job and got fired for bitching about it.  Note to him -- don't complain; quit.  If you're as valuable as you think you are, you'll get another job.  (And it will probably be better than the on you had before.)Personally I feel like strangling puppies every time I hear about buzzwords like web 2.0, but this was a nice pragmatic explanation of the technology and concepts behind the buzzword. Too bad half the internet sucks now because of abuse of the concepts, see digg, delicious, etc. To me only reddit survives, but I doubt it will stay free of such asshattery for too long. Already I see so many posts displaying the fanboy/hyped up/etc that plagues so many great places on the internet. That's what happened to Usenet, and if the "people" are not careful, reddit is next.As opposed to the big marketing money behind Python, Ruby, PHP, and PERL?The languages you just mentioned aren't even known of in many commercial environments.  These are the languages I wish were used more in commercial software, obviously, but Java and .NET are by far the most used languages/platforms.Please read the full post, and comments, before making statements like that.

&gt; Robert Sayre: differences from the spec have to do with accepting an array argument (and the examples contradict the spec text at this time). The final implementation will conform with whatever the spec says.

Additionally, you have to remember that this situation is very different from a typical specification-implementation. Firefox is, literally, the first browser developer that has attempted to implement this method (as is the case with a number of other aspects of the upcoming HTML 5 specification).

For example, look at [the report concerning the implementation of DOM Storage](https://bugzilla.mozilla.org/show_bug.cgi?id=335540#c11):

&gt; Ian Hixie: "As you're the first implementors, you get to tell me what you want."

The first step of building a good specification is writing down what you think might work, the second step is actually implementing and testing it.Some even older goodies in this 1987 [Alan Kay talk](http://www.archive.org/details/AlanKeyD1987) ([Reddit](http://reddit.com/info/25951/comments)), including [Sketchpad](http://en.wikipedia.org/wiki/Sketchpad) and a chunk of [the Engelbart demo](http://video.google.com/videoplay?docid=-8734787622017763097).&gt; Really, they're the only alternative if you're the sort of person who can't assume you'll bet on the "right" technology every two years for the rest of your life.

Well, I assume I can bet on the "right" technology because I bet on all of them.  It only takes a weekend or so to get a basic grasp of a technology (enough to assess its suitability), and then if you do a project with it over the next 2-3 months, you'll have fairly deep knowledge of the ins-and-outs.  The key word being "project" - many people think they can learn all about technology X through a "Teach yourself X in 21 days" book.

But yeah, if you don't want to spend your free time checking out cool new software toys, the industry does kinda suck.  "Cascade of ADHD teenagers" describes it pretty well.  And I think it's shameful how little attention people pay to fundamentals.  It's much easier to pick up technology X if you actually understand how it works inside instead of just "Type this to get that".  Plus, you may even realize that you don't need technology X at all.

BTW, the author's a woman.  Her name is given as "Katie Lauren Lucas" in one of her other essays, and while I could believe Katie might be a man's name, Katie Lauren is stretching it.  Most comments here get the pronouns wrong.It links to a [FAQ](http://backrub.tjtech.org/1997/FAQ.htm) where the search engine is referred to as "Backrub". I didn't know Google used to be called that. Yay for learning something new.Can this really be considered a scheme interpreter since it doesn't meet the requirement that tail calls be optimized?

otherwise, this is really quite neat!Like I said, I'm done with this. I don't think we're going to hit any new ground, and I end up having to repeat stuff over and over and over again (like why we didn't use FormEncode).

You do your thing, I'll do mine. Be happy that we have the choice of frameworks with different styles, and don't try to force everybody to conform to the style you'd like.her.
And a fast enough collision would obviously result in a more open Linus too.And why would anyone think otherwise?[deleted][removed][deleted]After stumbling on God's Debris, I was able to believe Scott Adams wrote the book. But I had a sneaking suspicion that he has never read any books.And [fold](http://haskell.org/ghc/docs/latest/html/libraries/base/Data-Foldable.html#v%3Afold) is the new crush.

(See [Applicative Programming with Effects](http://www.soi.city.ac.uk/~ross/papers/Applicative.html) and note that _reduce_ and _accumulate_ in the paper is linked-to Data.Foldable's _fold_ and _foldMap_.)
I don't see why people think this is such a bad question.

"Do you have any questions for me?"

"Yes. What would you say this company's largest weakness is?"&gt; F/{0}

Pfft. [Set difference is old-school backslash, man.](http://en.wikipedia.org/wiki/Complement_%28set_theory%29)Hmm, I'll have to think about this one. Perhaps it's not so much that there aren't enough waystations, but that it seems like drudgery rather than interesting coding. Like it or not, after refactoring include files for the umpteenth time the enthusiasm wanes.

Languages that make certain things hard also make it harder to constantly keep in my head the global map, where I am and where I want to be. Perhaps difficulty shouldn't be measured in development time but in context switches. The harder each step is, the less often I know the big picture rather than being focussed tightly on the next step. If you can't see all the steps laid out in front of you the journey will seem more difficult.

---

Thinking about 'diff distance' is interesting. Doing a complex code merge I often find myself wondering why diff can't be smarter. A microsecond later I remind myself that the better question is why my changes in a coherent patch must be scattered all over the codebase. Some languages are better than others at [separation of concerns](http://en.wikipedia.org/wiki/Separation_of_concerns), and there's room for yet more improvement.As the author of the linked-to article, I'm curious: why was the article submitted to the main reddit, where it is likely to be viewed as noise, instead of merely the programming subreddit?And what's the "programming" part of it? Google engineers have computers and are programming? Yes, we know.I submitted it to the programming reddit. I didn't realize it was showing up on the main reddit, too.

I don't recall checking the "submit to main" checkbox, but i suppose it's possible i did. If so, it was accidental.Well, as true Turing machines require infinite memory, ALL computers are just really big FSM's. So while their argument has technical merit, I don't think it answers the WHY that you asked.

To understand the paper, you need to understand a bit more about quantum computing. They just don't "give you an answer."  They give you ever more probable superpositions of the answer. So basically you run them a bunch of times to lower the error bound.

So back to the paper, we want to find a difference between these 2 oracles A and A'. If we use a "regular" computer, all we can do is pass random bit strings into the function and see if we ever get a 1 out of it. If we do, we know that we have A'.  If I pass in a 5 bit string once, I'll have a 3% probability of knowing the answer. (1/32). If I run it again, I'll have a 6% chance of knowing the answer... etc etc.

Ok. Hopefully you are with me there. That was a classical computer testing oracle A and A' to determine which was which. Now lets take a QUANTUM computer and see how well we can do.

We set up an initial uniform superposition that I defined above. Then we set up some sort of "separation algorithm" that separates A from A'. The law of how quantum mechanical systems evolve (that's the W1(y) above) states that for each run, we only get a (2/sqrt(2^n)) improvement on the error bound. This is because quantum mechanical systems are probabilistic, and not deterministic. So while we can create exponential superpositions, we cannot create exponential separation of those superpositions. (separations means "being able to determine between the 2 cases" here)

So for a 5 bit string on a quantum computer, after the first run we have
4*1^2/2^5 = 12.5% chance of knowing the right answer

for the second run we get
4*2^2/2^5 = 50% chance... etc etc

So as you can see, it is MUCH better at finding the answer... but it is not EXPONENTIALLY better. See it makes more sense if you compare the "random check" case to both the classical and quantum computers. This helps separate what quantum computers can and cannot do. And hopefully it will show why designing good quantum algorithms isn't just a matter of taking a 'normal' solution and applying it to the quantum world.

Does this help more?Java and Python suck, but this is a misguided rant.

When Haskell programmers figure out interactive development environments, macros, and homoiconic syntax, will their heads explode, too? :-)Imagine that you want to multiply 6 square matrixes of a very high rank. You have 3 workers.

    result = m1 * m2 * m3 * m4 * m5 * m6
           = (m1 * m2) * (m3 * m4) * (m5 * m6)

The parts in brackets can be parallelized. As soon as two results are calculated, they can again be calculated on a worker machine. 

But if-I-read-the-docs-correctly-and-I-think-so you cannot leave it to parallel python to chose, in which order the operations take place. So if m' = (m3 * m4) and m'' = (m5 * m6) is finished since 10 minutes, but (m1 * m2) is not, the calculation (m' * m'') will not happen, though it already could be calculated on a worker. Instead it will wait until m = (m1 * m2) is finished, then calculate m * m', and then (m * m') * m''.

This is nothing "exotic"; this happens as soon as you have some kind of reduction of values with an associative operator.
And check out
http://www.scottaaronson.com/democritus/

It was an invaluable resource for helping me to better understand quantum computing... And I still have a LOOOONG way to go. *laugh*

Edit: and check out Scott's thesis on the topic as well.
http://www.scottaaronson.com/thesis.pdf
&gt; I would doubt if they picked it up. It's probably patented technology anyway. They have activerecord and they like it. They have a couple of others too but they don't seem to have that much acceptance.

MS cannot enforce a patent on it becuase C# falls under ECMA. And even without that, too much of it is based on SQL and Functional programming.

From what I have seen so far, ActiveRecord isn't incompatible with LINQ concepts. LINQ is about writing queries against various domains where the domain chooses the query method (object, SQL, XQuery, etc.). From what I have seen, this should fit nicely into ActiveRecord.

In fact, I would go so far as to say that LINQ would work much better with Ruby than C# or VB. The lack of support for duck typing makes it hard to work with annoymous classes.

&gt; Error 800423f4 appears in the backup log file when you back up a volume by using the Volume Shadow Copy service in Windows Server 2003
http://support.microsoft.com/kb/828481

Thanks for your response.  And it's no problem.

The only reason I asked is because I suspect that the article will get blown off the main reddit and, because votes are shared across all the reddits, it will get hammered artificially on the programming subreddit, where there might have been the chance of interesting discussion.

I've always thought that articles should float independently on each reddit/subreddit.  Oh well.  Maybe in Reddit 2.0.

Cheers. &amp;#8212;TomNevertheless, they've been fantastically successful compared to LISP and Smalltalk.  And interestingly enough, the best LISP and Smalltalk implementations were/are commercial (and quite expensive as I understand it).

I'd argue that Python, Ruby, and PERL (I'm less sure about PHP) were as successful as they were because they had high quality open source implementations and rich standard libraries.  Apparently this is more important than good development environments.

It's probably also not coincidental that until recently, there weren't multiple competing incompatible implementations of Python, Ruby, PHP, and PERL.

Since they were using more than one Linus for this study... I am interested in how they cloned him more than about the "gets hit by a bus" part.  And what about the ethics of creating all these clones only to kill them up with buses?  Why aren't they put to work on creating improved file systems, Firefox or something?  And who's paying to bury the clones once they have been killed by the bus?  Linus himself, or are they maybe just rotting a field someplace?  

These are questions that need answers![removed]Python, Ruby and Perl succeeded because they were small evolutions of the status quo. Note how all these languages are mostly used to write glue code, with the real logic locked away in C libraries. Back in the day, Perl was just a beefed up AWK/SED; Python was a Perl with simpler syntax; Ruby was Python with a few concepts from Scheme thrown in.

On the other hand, Lisp and Smalltalk were always a big paradigm shift away from the C-centric Unix worldview. People can accept writing 100 lines in a scripting language, but writing your entire application in a high level language, which is the Lisp mindset, is an idea which will still take a number of years for the Joe programmer to digest.

Now you might ask why Lisp and Smalltalk haven't taken off for small scripting, and I think this is because traditionally (but not today) Lisp implementations lacked good (or any) FFI, and Smalltalk is image-centric and doesn't fit well with scripting.

If you ask me, the whole concept of a "scripting language" is just a bad workaround for the OS being written in C, anyway. If you have an OS written in a high level language you don't need "scripting".You misunderstand me. I meant *virtual* dispatching, like this: in each of the descendant of `Number`, implement `equals` as:

    public bool equals(Object o) {
        if (o instanceof Number)
            return this.value == ((Number)o).xxxxValue()
        else return false;
    }

where `xxxxValue` is the method corresponding to the class implementing `equals` (that is, in `Long` it is `longValue`, and so on.)&gt; LINQ can certainly be patented and it probably is.

I'm sure they are going to try; MS is too big to risk not collecting defensive patents. But honestly, when was the last time you heard of MS actually starting a lawsuit over patents? Dispite all the fear mongering, that just isn't how they operate.

Good luck on the reboot.I don't really want to publicly embark on a debate on a matter of opinion. I was harsh where I could have been kind, or at least abstained, but I got carried away, and I apologize. I shouldn't have said "let him rot". I'm sorry.

&gt; Please show me how you're going to write the signature of bind in Java without higher-order kinds. 

Here's [my solution](http://cut.and.paste.org/index.php?id=318). Unfortunately, as there's no interface over static members in Java, I couldn't write a `return` that satisfied me.
&gt; and I end up having to repeat stuff over and over and over again (like why we didn't use FormEncode).

Where? I just glanced over all your comments from the last week and I never saw where you even mentioned FormEncode. I may have just missed it; do you have a link?

&gt; You do your thing, I'll do mine. Be happy that we have the choice of frameworks with different styles, and don't try to force everybody to conform to the style you'd like.

How does that make any sense? The style I'd like is flexibility. How does that force anyone to conform?Spring Web ServicesMagic, infinite Santa-sackesque harddrives?How I wish I could believe it... For those that remember it, I used to believe in the Extropian future that Louis Rossetto-era Wired promised relentlessly from each issue... What happened to the hopes of the mid-90's? The 2000's have been for me, so far, a long disappointment.

The technology, the "Machine" with capital M has grown by leaps and bounds. Who have the reins of the bandwidth? The "commercializers". They even feel entitled, they say "Who do you think pays for all this innovation?"

I don't believe in Web 2.0. We're not really listening to each other, and when we try, some advert pops-up and co-opts our precious little attention span. How can we communicate, link to each other in the middle of a roaring storm of market bustle? The parks are fenced, the plazas are cordoned off and littered with oh-so-unobstrusive sales gimmicks. "Who is paying for this?" they demand. "It helps us recoup costs and make the space available to everyone, *free-of-charge*!", they intone.

I feel I have to expend more and more brain-cycles filtering sales pitches. I have almost no lateral vision anymore, lost at pretending that the ad sidebars aren't in each and every single page. "Google is good! They have text-only advertising!" Yes, but where's the space for *our* content? How many horizontal pixels are we permitted to use?

What can we do to ensure that the dystopian cyberpunk fantasies of direct-advertising-to-the-brain-stem don't come true?Are there any docs about the architecture of the system, sounds more interesting than I'd imagine from your description?

I'd of thought it was only Byzantine failure they'd deal with and that the withdrawl limit was to stop a run on banks and limit fraud.I think I largely agree with you -- the point I was trying to make to the previous poster is that "marketing" seemed to be a poor reason for language success or failure.

I think one point where we may disagree, is that I think that the successful scripting languages' nearness to the status quo wasn't just a win in terms of programmers' comfort level, it was also a win in terms of actual usefulness.  Most real programmers don't live in a vacuum -- the problems they need to solve tend to be deeply embedded in the status quo, so programming languages that diverge to far from it are going to have difficulty gaining popularity.

&gt; I just glanced over all your comments from the last week and I never saw where you even mentioned FormEncode. I may have just missed it; do you have a link?

[Four days ago](http://programming.reddit.com/info/12u8e/comments/c12wbx), in reply to you:

&gt; Perhaps Django's form system needs things that third-party systems can't do -- like, say, generating a form from a Django model class or instance, and providing automatic hooks to save a new or edited instance. Perhaps having our own library means less work in the end than adapting somebody else's. Perhaps there are a lot of things that go into these choices that get glossed over when you ask things the way you're asking them.*Imagine if Smalltalk or Common Lisp had achieved the relative popularity that Python has had from 1990 until now in an open-source project. What would've happened?*

Well, what would have happened? Elaborate.Most of my job experience has been working for companies who do software as their primary business, some large, some small.    I've rarely ever seen anything like the problem the author describes although I can readily believe that it's common in the non-software corporate world.Well said.Well duh!My favorite part is still the recommendation that his muffin intake be monitored.  I picture very serious looking doctors watching closely as Linus, in a hospital gown, contemplatively chews a poppy-seed muffin.Is resembling, of course, but where the implementation comes from is another thing. 

Don't get me wrong, I'm not a Lisp advocate here, but speaking of origins of Ruby: this Matz's confession was cited really dozens of times and it's not stupid to call Ruby an implementation of Lisp because it was technically done that way, even if with different "ultimate" goal in mind.I also said tape drive/network/etc. It's not like the internet is limited to a fixed number of bits.&gt;Personally, I wouldn't even want to work for a company that would blow me off for one of the reasons mentioned in the article.

This is an HR issue.  I've seen this happen tons of times.  The IT department sends in a job hire request for someone with skills X and Y and the HR adds Z.  HR reps usually have no idea about computers but will regularly combine an old job hire request with a new request to "fill it out".  This filling process is usually done by opening a Word document from an older request and doing slight editing.  This results in stupid job reqs for "Unix admin with VB skills".  The IT department may or may not complain.  But this leads to the second problem: many companies today are consolidating their HR departments to a single, remote location.  Contacting these people is near impossible so that most IT managers just throw their hands up and surrender.Hmm, that sentence in TFA stood out to me too. What is this map fusion of which he speaks, anybody have a pointer? Google/wikipedia was of no help.Okay thanks.

I think a lot of times (and maybe this isn't the case here), we as programmers get lazy and we'd rather do things from the ground up rather than take the time to learn an existing system and build upon it. I guess we are both speaking from a theoretical point of view, we don't know exactly why Adrian built newforms from scratch rather than adopting an existing system.

Have you ever looked at Paste, SQLAlchemy, or any other these other tools? These guys know how to write good code; I have a feeling that FormEncode could've been adopted very easily (check out FormEncode, it is very very similar to Django's validation system).

Oh well, not a big deal, still would like to see Django using more existing code. Its always more productive when communities work together and use existing projects, and I see that more and more within the other framework communities (Pylons, TG, web.py, etc), yet Django seems to be the fifth wheel here.*tears* I miss segfault.&gt; I am interested in how they cloned him more than about the "gets hit by a bus" part.

fork()[deleted]In my opinion, the majority of the problem is all of the inept developers out there with stellar resumes.  They're sure-hires and then they come on board and don't know even the most basic stuff that any self-respecting redditer would know.  The industry lazily and erroneously blames the failures on platform versions and irrelevant details (to include specific languages) rather than revisiting their own interview process and HR dept. members/policies.

On a resume level, I probably couldn't compete with a lot of developers out there, but I guarantee that in reality I would code circles around a lot of them.  The industry is just a little gun-shy with regards to taking chances in that position when the "fully qualified" individuals that they already have aren't even impressing them...I don't believe it. AI at that level still seems as science fiction as flying cars. It's just a tool. Great at math and sorting data, but horrible at understanding ideas, concepts, or strategies.

&gt;A better question is "Where do you want to be in 5 years?"

"Celebrating the five-year anniversary of you asking me this question." - Mitch Hedberg R.I.P.Did you learn that in Tool School.  I detest interviews.  They're basically contest to see who can be the fakest person to come talk to them.[removed]&gt; I've got a scoop for you: in teh real world ATM transactions aren't atomic.

I can believe that, but it doesn't change my question, unless you're implying that there's never a need for truly transactional coordination between processes. I used the ATM example because it's the one a lot of the recent STM papers choose to discuss.Proof that those rails evangelists didn't invent the screencast.More people would be using better languages, perhaps. Or who knows, maybe not. Java could still have come along to drag the 90% back to the dark ages of procedural programming.87 is older than 83?Holy shmoly, someone rape this guy with a thick cluestick.And here is a [simple bzr branch browser](http://nearfar.org/stream/2007/02/10#1010) written using web.py.&gt; They sued over the FAT patent.

Reference please. 

Everything I have seen so far shows Microsoft just defending their patents. Consider this article from last year:

&gt; Patent watchdogs expressed dismay over the USTPO ruling, arguing that Microsoft could use the decision to cause major headaches to Linux vendors and users by demanding payment for use of the technology in existing products. Florian Mueller, founder of the NoSoftwarePatents.com Web site, which scrutinizes patents in the European Union, said the ruling sets a dangerous precedent even if Microsoft doesn't pursue such a strategy, and that it illustrates a desire on the part of the software company to become more disruptive to open-source efforts. 

http://www.eweek.com/article2/0,1895,1909857,00.asp

Note that it doesn't say Microsoft actually tried to collect royalities, just that everyone has a fear that they might.

All this fear about Microsoft using its patents for evil is irrational and misguided. Microsoft knows they would get hammered in a patent war with the likes of Apple, IBM, and Sun if they actually tried anything like that and only the lawyers would win in the end.

And don't give me none of that FUD nonesense. All of this fear about Microsoft and patents hurts them more than it helps them. OpenXML just might be taken down by this pointless hysteria, and MS Office along with it. Do you really think that helps Microsoft?

&gt; There is also the case of MS funding SCO to sue IBM.

Oh give me a break. 

Do you really think they follow the marching orders of Microsoft? Think like you are on SCO's board of directors. Would you even consider letting Microsoft tell you what to do? No, SCO did this on their own.
Of course, Lisp and Smalltalk were considered too slow and too academic to be taken up in any way by the mainstream of their time. Most programmers in the 70's and 80's still programmed in C. That the mainstream has shifted even to Java -- an ugly and bloated language that nonetheless is much more abstract and powerful than C -- if not to Ruby and Python, shows how far we have come.I would personally love to see it. If this is real, it's going to change a whole bunch of things in this world.Now that's big news. Finally Microsoft is coming around?&gt; At one point, I thought about posting a pedantic rebuttal, but really, that doesn't help anyone. So let's call this a fair cop and move on.

uh, magnamimous of you.[removed][0x3a28213a](http://xkcd.com/c138.html)
the first ninety percent of the task takes the ninety percent of the time, and the remaining ten percent take the ninety percent of the time.With this:
          
    if token == "(":
        yield (OPENBRACKET, token)
    elif token == ")":
        yield (CLOSEBRACKET, token)
    elif token == ".":
        yield (DOT, token)
    elif token == "'":
        yield (SINGLE_QUOTE, token)
    elif token.isdigit():
        yield (ATOM, int(token))
    elif token[0]=='"':
        yield (ATOM, token[1:-1].replace(r'\\', "\\").replace(r'\"', r'"'))
    elif token[0]=="#":
        if token[1]=="\\":
            char = token[2:]
            if char == "space":
                char = " " 
            if char == "newline":
                char = "\n"
            yield (ATOM, char)
        elif token[1]=="t":
            yield (ATOM, True)
        elif token[1]=="f":
            yield (ATOM, False)
        else:
            raise Exception("Invalid token "+ token)
    else:
        yield (SYMBOL, symbol(token))

and this:

    predefineds = {"+":predefined_function(lambda *args:sum(args)),
                   "*":predefined_function(lambda *args:reduce(int.__mul__, args)),
                   "-":predefined_function(lambda a, b:a - b),
                   "&lt;":predefined_function(lambda a, b:a &lt; b),
                   "&gt;":predefined_function(lambda a, b:a &gt; b),
                   "=":predefined_function(lambda a, b:a == b),
                   "cons":predefined_function(lambda a, b:[a, b]),
                   "car":predefined_function(lambda(a, b):a),
                   "cdr":predefined_function(lambda(a, b):b),
                   "display":predefined_function(display),
                   # things that apparenly are not 'predefined functions'
                   }   

and this:

    if code[0] == LAMBDA:   
        return eval_lambda(continuation, context,  code)
    elif code[0] == IF:    
        return eval_if(continuation, context, code)
    elif code[0]== BEGIN:  
        return eval_begin(continuation, context, code)
    elif code[0] == DEFINE:
        return eval_define(continuation, context, code)
    elif code[0] == SET:
        return eval_set(continuation, context, code)
    elif code[0] == QUOTE:
        return eval_quote(continuation, context, code)
    elif code[0] == LOAD:
        return eval_load(continuation, context, code)
    else:
        return eval_apply(continuation, context, code)

-- I wonder why people take this language's culture seriously.  Surely this code
is perfectly Pythonic and oh just so beautiful and elegant to Python eyes, but
I think it both ugly for being comb code and also *less* maintainable for the
extra pass required (before and after an edit) to assure a maintainer that, yes,
all of these sequential tests are in fact disjunctive -- except for the 'else'.
Perl is called 'Perl', unless you speak of the implementation: 'perl'.  'PERL' are cute backronyms.  Likewise, people call Lisp 'Lisp'(, and Fortran 'Fortran') -- and don't seem to appreciate having the pre-internet-culture LISP hung around their necks for all eternity.  Please adjust your terminology accordingly, the way you'll do anyway with national and racial terms.[removed]The SOA community is in tight competition to post the most ignorant knee-jerk reaction to a concept they'll never understand! Excellent.They would, of course, use clone().&gt; but perhaps more obvious is the importance of provable 
referential transparency and laziness - even to a Haskell/Clean newcomer,

The importance of referential transparency is surely 'more obvious'
to everyone everywhere -- even C programmers understand that the 
pain they just felt from came from poking themselves in the eye with libc's strtok.
 
I don't know why you say 'provably referential transparency' instead
of 'referential transparency'.  I'd say 'promised referential 
transparency' so that you always assume that you deal with that, 
and so a programmer violating that has some responsibility to 
alert readers to this violation.  For languages that very 
strongly promise referential transparency and then let you 
violate it anyway, see:
 
1. O'Caml, which AFAIK does not try to control programmer use of its features here.
 
2. Haskell, which protects it with a really scary name.
 
3. Mercury, with a finer grain of pure, semipure, and impure 
functions -- with a requirement that top-level code be pure, 
a default assumption of purity everywhere, with requirements 
that programmers document-with-code every semipurity/impurity, 
with compiler inference to check that things come out pure, and 
finally a requirement that the programmer at some point promise 
that a predicate/function using impurity is, itself, pure.  See 
the 
[Mercury Reference Manual on 'impurity'](http://www.cs.mu.oz.au/research/mercury/information/doc-release/mercury_ref/Impurity.html#Impurity) 
for the precise meaning of these terms.

Lazy semantics don't excite me at all, however.  You can do 
neat things concisely with it, you can have automatic memoization 
and such with it, but ultimately I think that it is not very 
interesting.  I'm tempted to think that most people excited about
it haven't learned enough about iterators|coroutines|partial-computation,
or about other variously strongly functional languages, to consider
laziness separately from their enthusiam for Haskell.
&gt; I don't really want to publicly embark on a debate on a matter of opinion.

Hah!  This recant is really amusing.  You perfectly happily -- would not have later recanted -- engaged in *public derision*, but when it is revealed that the person you deride has a reddit presence, may be familiar to you, suddenly your derision was you getting carried away and you'd rather apologize than have a debate 'on a matter of opinion'.

This seems like evidence for [the monkeysphere](http://www.pointlesswasteoftime.com/monkeysphere.html).&gt; The importance of referential transparency is surely 'more obvious' to everyone everywhere -- even C prog rammers understand that the pain they just felt from came from poking themselves in the eye with libc's s trtok.

I'm not so sure. I keep hearing about this 'inherent mutable state' thing and I keep having to point out that time/change is a convenient illusion - at least, from the perspective of software itself. Maybe I just hang around different people.

&gt; I don't why you say 'provably referential transparency' instead of 'referential transparency'.

The type of referential transparency that is notated by its type. In Java, int sum(int a, int b) might format your hard disk. Of course, there is always a way out as you pointed out. So perhaps, "provable on top of a given, relatively sound axiom".

&gt; Lazy semantics doesn't excite me at all, however. You can do neat things concisely with it, you can have automatic memoization and such with it, but ultimately I think that it is not very interesting. I'm tem pted to think that most people excited about it haven't learned enough about iterators|coroutines|partial -computation, or about other variously strongly functional languages, to consider laziness separately fro m their enthusiam for Haskell.

Who's to be excited about it? It's a fundamental computing concept - I was referring to its importance. On iterators, ever asked even the most unskilled programmer in one of the most unskilled languages to return you the first 5 elements off an iterator. They won't keep iterating to the end (if there indeed, is an end) 'just because they can'. Even monkeys use laziness, but then, monkeys are easily excited :)

Having said that, I have seen code that reads a Java InputStream to the end unnecessarily, or uses an (strictly evaluated) array when only a part of it is required to complete the computation, so perhaps not *all* monkeys. I have worked for a corporation after all - I've seen code that is difficult to imagine.I think you miss a crucial point though: wall street does run on VBA and excel spreadsheets. It makes no difference if you believe it's ugly or not. Until last I heard, VISA was running on supremely old technology - possibly flat files (although, I'm waaaay out of memory here, and am low on sources), which is the reason why you have a minimum 1 business day lag on your account statement.

 And javascript is actually quite elegant given that a) it works everywhere, b) it has some very nifty things such as closures. Programming with Javascript is a joy and is a domain that's budding.WTF? For what it's worth, I think it is a respectable position to take. A debate on a point of disagreement is only useful if one or both parties learn something and a judgement was made that this probably won't occur. Why carry on about it? Is this one of those internet forum things?[removed][deleted]&gt; which is the reason why you have a minimum 1 business day lag on your account statement.

I doubt it. More likely due to nightly batch processing, and to give themselves a small margin to correct the most obvious errors.Maybe so, and to some of us, it is most obvious. But, what are you/we/anyone going to do about it? Complaining about it will just give him fuel for his next look-how-stupid-i-am post. Pointing and laughing is not very constructive - not for anyone.I really like the idea of this post.  At the least, it might save you from [advocacy-thick](http://www.perl.com/pub/a/2000/12/advocacy.html) discussions :-)

Of course, [you missed quite a few languages](http://en.wikipedia.org/wiki/Alphabetical_list_of_programming_languages) in your 'every language out there', even with your terminal disclaimer.  Also, a language that has always been more portable than Java is [Inform](http://www.inform-fiction.org/I7/Welcome.html) (disclaimer: version 6 of this platform feels much more like a programming langauge.  Version 7 tries to do something wildly different.)  Lua also conflates arrays and hashes.  Tcl also (with the same sort of efficient caching of representation) conflates numerical types with strings.  And the reason for PHP's success can't possibly be due to its 'good integration with standard databases', as its mysql integration (in stark contrast to its postgresql integration) lacks prepared statements.  (disclaimer: I get that complaint second-hand, and had to look through PHP docs to remind myself of it.)&gt; wall street does run on VBA and excel spreadsheets.

Does it?  APLs seem to have a strong presence in the financial world, and [K](http://www.kx.com) markets itself exclusively to financial use (to the point of slamming the door on its freely offered limited-memory-implementation and docs).One has to wonder how these guys remember to keep breathing.
This amuses me, though:

&gt; to achieve what Lisp set out to achieve so long ago.

What does this person think Lisp 'set out to achieve'? :-)You may find it worthwhile to check Wikipedia again.[Mercury.](http://www.cs.mu.oz.au/research/mercury/information/doc-release/mercury_library/queue.html#queue)

[Erlang.](http://www.erlang.org/doc/doc-5.5.3/lib/stdlib-1.14.3/doc/html/queue.html) -- Erlang is interesting here because it necessarily deals with only purely functional data structures.

Ah, but the [O'Caml queues](http://caml.inria.fr/pub/docs/manual-ocaml/libref/Queue.html) use in-place modification.  FOR SHAME :-)
&gt; Surely this code is perfectly Pythonic and oh just so beautiful and elegant to Python eyes

...no.  Believe it or not, writing messy code in Python as easy as in any other language.&gt; I think it is a respectable position to take.

I think it is not respectable to sneer and then say --oh, geez, you heard that?  Well, I don't want to talk about this, so *sorry about that.*

Also, did you follow my monkeysphere link?I like the tone of his post a lot.  There is something interesting and well done in a lot of languages.  Sometimes it's good to think about that rather than just "this sucks that sucks", which isn't all that hard and gets tiring.
Due to copy-on-write semantics, he wasn't actually cloned until the moment of impact in each case.
Or, you can use Lisp today and get both the power and speed.
It does eliminate tail calls;  the interpreter is written in continuation-passing style.From what I've seen, a lot of market analysts build software for the market world - not programmers. These things start as simple formulas in excel (which is quite good at doing just that), and then eventually get GUIs added onto them using VBA.

 It's sure as hell ugly, but it's the essence of rapid prototyping without realizing it.

 Does this mean the Stock market itself runs on Excel? most likely not. But a lot of software does get created in a rapid prototyping method and eventually solidifies into something people use.[Ah](http://en.wikipedia.org/wiki/Map_%28higher-order_function%29#Optimizations)[deleted]I think he got it right:

*agile &lt;--&gt; not thinking*

*(agile --&gt; not thinking) and (not thinking --&gt; agile)*

take the contrapositive of both the implications:

*(thinking --&gt; not agile) and (not agile --&gt; thinking)*

assuming that not agile &lt;--&gt; waterfall:

*(thinking --&gt; waterfall) and (waterfall --&gt; thinking)*

*thinking &lt;--&gt; waterfall*
Although as it comes from 2000, it omits screencasts and posting 'FOO in MyLanguage' to reddit.As a business consultant, he has [such wonderful written communication skills](http://www.noahcampbell.info/2007/02/04/soa-as-a-technical-strategy/):

&gt; Nothing gets a enterprise architects [sic] debating then [sic] the notion of a reference architecture.

What the fuck?

This is a gem of clarity:

&gt; Enterprise Architecture is seeing SOA as a particular implmenetation [sic] approach. The trenches are debating SOA verses [sic] REST verses [sic] RMI/DCOM/Sockets.

What? People think a one-line invocation of curl is comparable to 300+ line programs using D motherfucking COM? This guy is a bored Java programmer trying to troll dynamic language/functional programming fans, right? Right?I doubt he's a troll or a self-parody. Sadly, guys like this exist, they are for real, and they are wasting clients' money on a massive scale.[removed]Money quote:

&gt;*Right now, I can tell you that the Zune team is really focused on producing great innovation in the music device space*

BWAHAHAHA!  Snort.  Guffaw.Sketchpad (1963) and the Engelbart demo (1968) are older than Smalltalk-80.How does Excel meet the requirements for security, accuracy and reliability as required by Sarbanes-Oxley?Sorry but my understanding of *implementation* is, given a specification of something, a computer program able to behave like specified. If Ruby's and Lisp's specifications are not the same Ruby can't be an implementation of Lisp.

Too strict for this context? Ruby is not even a language with different syntax but with more or less the same semantic of Lisp (like for example is Dylan).I tried Smalltalk a couple of times, and I tell you, Ruby looks nicer. So I guess this is one of the reasons for its 
low acceptance. Also, scripting languages have always been 
open-source, which made them easier to integrate with Unix.
They are also technically easier to integrate. This is
probably the 2nd major reason why it doesn't have 
the acceptance you want it to have.

last not least, if you really set your mind to it, what
really holds you from using Smalltalk today? There are
even some free environments available, which wasn't the
case 15 years ago, as far as I know. Try Smalltalk/X!
&gt; If you have an OS written in a high level language you don't need "scripting".

I think that sort of 'turtles, all the way down' mentality is part of what has hurt Smalltalk and lisp.  Lots of energy was directed at the lower turtles rather than finding ways to coexist with popular realities such as Unix and Windows.&gt;**Some say the Zune project undermines your PlaysForSure partnerships.**

&gt;What you have to recognize is all the work we're doing around that sort of technology continues and we're going to continue advancements. We know that there is absolutely a market there for people who want to have choice of devices. We know there is certainly a market relative to the [cellphone] handset people and the operators who are interested in that technology. We've been very clear with our music and video partners as well as with the [device manufacturers] and operators that we're going to continue to support that.

&gt;What we did do is add to our arsenal, and we probably didn't do a very good job communicating the distinction between that, but we have the broad focus with our Windows Media technology as well as a very focused effort with Zune to compete with the vertical approach that Apple has taken. It turns out the market needs both of those. We're going to provide the technology on one hand and the product on the other.

Well I'm glad he cleared that up.&gt; While investigating the MLs I stumbled across a
reference (somewhat disparaging) to Haskell and lazy evaluation.  I
followed up on it (because the disparaging comment looked clannish to
me) and looked at Haskell more closely.

Fun!  I discovered Forth in exactly the same way.

&gt; At the time I rejected Haskell as being too "academic-oriented"
in favour of Dylan.

Dylan really interested me for a time, back when I had a p133 and
still lived with my parents.  I eventually rejected it then for
its unfortunate pairing of 'a freakishly slow compiler' and 'a
fairly slow interpreter that anyway did not implement as much of
the language as the compiler'.  And no, that I used a p133 doesn't
excuse these points -- I eventually put SBCL on that machine.
   
&gt; [O'Caml] also has two distinct list-like types: one that's lazy,
one that's not, and they require two distinct sets of functions
to work with.

A nicety of Mercury is that these functions would have the same
name (even without a typeclass -- although this isn't a pure
advantage), and still be distinct.  In Haskell of course this
-particular- issue does not occur, but even when you have
an analogous issue where you want to create a type with the
same interface as an existing one, you can in Haskell take
advantage of typeclasses which the core language and libraries
nicely already use.  (Mercury has typeclasses, but the Mercury libraries predate their addition to the language, and so still don't use them in useful ways that Haskell does.)

&gt; The syntax [...] felt very dated

&gt; It got me by syntax then by type system.

&gt; At the time I hadn't listened because I was sure that Eiffel was the future.
   
The ['elegant by repeated assertion'](http://www.adahome.com/FAQ/programming.html#elegance)
language?  Well, with the agreement above about how important
syntax is, I've little hope for this language with all-cap horrors 
like INTEGER and BOOLEAN.

EDIT: meaning ['Gwydion Dylan'](http://www.opendylan.org/) by 'Dylan'.  It's surely much better, now.No, it is almost impossible to write bad code like this this in strongly functional languages -- and even if you implemented this exactly in Erlang, it would look nicer for the pattern-matching.  In a Lisp the last example would translate to a FUNCALL with those three repeated arguments and a map -- instantly amazingly better, and -thinking- would only improve matters.

And sure, this code is messy -- but is it not Pythonic?A few more anecdotes are described here - [The Legend of John Von Neumann](http://stepanov.lk.net/mnemo/legende.html)&gt; [...] and no - CL does not have a simple syntax tree if every code walker needs to understand (loop ...).

Great confusion!

I have a better rule for you, taw: Don't criticize what you don't understand.[deleted]I'm aware that it's an HR issue and that it happens alot especially in larger companies, but I think it is telling when a company works in this way. I don't see a reason why large companies should enforce (or facilitate) a disconnect between their HR department and their IT department.Ah ah, ah ah, Java closures are going to lead to higher maintenance costs compared to..... Excel spreadsheets !!!

What is that guy smoking ??
Yeah, I agree, don't write AST interpreters in languages without pattern matching:

    data Exp = IntE Int
             | OpE  Op Exp Exp
             | VarE String
             | LetE String Exp Exp

    type Op = Int -&gt; Int -&gt; Int

    main = print (runReader (eval test) empty)

    eval (IntE n)       = return n
    eval (OpE op e1 e2) = liftM2 op (eval e1) (eval e2)
    eval (VarE x)       = do
        env ← ask
        return $ maybe (error "undefined variable") id (lookup x env)
    eval (LetE x e1 e2) = do
        env ← ask
        v   ← eval e1
        local (insert x v) (eval e2)

Using a reader monad style to encapsulate the variable scoping is probably a good idea too.&gt; it is almost impossible to write bad code like this this in strongly functional languages

Don't underestimate the [ingenuity of bad programmers](http://trevion.blogspot.com/2006/11/functional-anti-patterns.html).

&gt; In a Lisp the last example would translate to a FUNCALL with those three repeated arguments and a map -- instantly amazingly better, and -thinking- would only improve matters.

Just as in Python (except without the special FUNCALL treatment, of course).

&gt; And sure, this code is messy -- but is it not Pythonic?

No, it is not.  Really.That was a beautiful entry. He should consider renaming the blog to "sigfpe's seriously awesome shit". I already sorta knew the connection between logical negation and continuations, but I obviously hadn't explored it enough; as soon as he mentioned the identity a \/ b = ~a -&gt; b, it hit me.Interesting! Any performance numbers for what SSE2 buys you? How hard was it to hack in SSE2 support in the code gen?Yes, it does. The City of London, and probably Tokyo, Dusseldrof and the other big financial centers too. The exchanges themselves don't, and the quants do sometimes use all that heavy lifting gear you mention, and more. But the salesfolk and traders who actually make the markets and keep everything liquid use Excel. They use Excel to prepare the data describing their trades, the use it as a distribution mechanism to execute those trades (millions, billions, get moved around by emailing spreadsheets), and for much else.Are mathematician's really that stupid? An engineer that doesn't know the solution within two seconds is a moron.It doesn't. And SOX is not the only regulatory framework that it doesn't meet the requirements of. In the UK the regulators are beginning to turn their lidless, burning eye upon this issue: when your bank has thousands of traders in hundreds of locations all with home-brewed spreadhseets being used to, say, execute 40-leg fixed income trades with each leg worth many millions then this beings to look like a major source of risk. Bringing this under control is, as TFA hints, a major problem in enterprise architecture.

A lot of hard-core technical people don't like to hear this sort of thing, hence the dismissive title I suppose. But in the commerical world software is written by people for people who need to get their people's stuff done and will tend to use any damn thing to hand to help themselves. And quite right too.&gt; The performance is somewhat poor right now. ... we are anticipating equipment donations from IBM and Intel to help with performance...

No wonder they were waiting for hardware. They clearly blew their budget on that flashy logo.I believe everybody can criticize as much as he feels fit. There are no rights to be earned.

But this guy rightfully earns a "And then there's *this* _ASSHOLE_" Penn &amp; Teller introduction.

He compares Bjarne Stroustrup to Saddam Hussein. It's not funny. It's not witty. It's offensive and tasteless. What was the guy thinking when he wrote that?Good for the contractor I'd say.

I bet you also despise restaurants for selling $300 bottles of wine.&gt;I believe everybody can critize as much as he feels fit. There are no rights to be earned.

I disagree.  At the very least you shouldn't criticize something that you don't understand.  How do you think a Lisp programmer would feel if someone who had never used Lisp in their life said "all those parenthesis look stupid".  What about a non-Rubyist who said "Ruby is just a crippled version of Perl"?  If you're going to criticize something, you have to know what you're criticizing.Random info:

SSE2 just adds integer support, aside from a useful generalized packing/unpacking/shuffling instruction. The integer speed is about half that of MMX, but its registers have twice the width of the MMX registers (128-bit vs 64-bit); and of course the MMX register file aliases the FP register stack, and the context switch from using MMX to FP is a little expensive. If you're already using SSE for all floating-point calculations instead of the old x87 FP instructions, then the FP register stack stands idle, so it makes sense to use MMX for your integer SIMD needs.

Last time I worked on a high-performance software renderer I used MMX for the pixel pipeline and SSE1 for everything else, even the non-vectorized floating-point calculations. I don't think you can expect to take non-vectorized computations and magically make them faster by using SSE. One nice thing is that SSE and FPU performance is about the same, and SSE has a normal register file rather than the retarded register stack of the FPU, which makes good register allocation and code generation much simpler. So, if you can afford to require SSE (possible in my software renderer, but probably not possible for you), then it's a good shoe-in replacement for the FPU.&gt; He compares Bjarne Stroustrup to Saddam Hussein.

Oh, ick.  I read quickly past the Java part.He did say 'code walker', for such higher-level changes as turning (any-function a _ _) into (lambda (gensym:1 gensym:2) (any-function a gensym:1 gensym:2))

I've never written a code walker in CL; do they not need to understand LOOP ?  (so that this _ notation also works within LOOP)&gt; I've never written a code walker in CL; do they not need to understand LOOP

They do, at least partly. On the other hand, writing a code walker for Scheme is not a walk in the park either. For instance, suppose you want to write a code walker to find all variables bound by an expression or any of its subexpressions. If you could assume that lambda was the only binding form then that would be easy, at least assuming you could apply your code-walking macro after all other macros have been expanded; but while any binding form can in principle be expanded to something involving lambdas, most compilers make many of them atomic, so your macro would be need to be aware of all of them.

I guess my point is that robust code walking is difficult if not impossible in both Common Lisp _and_ Scheme and should thus be avoided at all costs. I certainly don't think it's a good criterion on which to judge their relative merit.Here is another one that maybe only Finns get: two Pendolino trains leave at the same time, one from Helsinki, the other from Turku (a distance of 149 km). The Pendolino from Helsinki travels at 150 km/h and the Pendolino from Turku travels at 125 km/h. Where do they meet?

[Ng gur Cnfvyn ercnve jbexfubc](http://rot13.com/index.php?text=Ng%20gur%20Cnfvyn%20ercnve%20jbexfubc)&gt; (except without the special FUNCALL treatment, of course)

That just means that Norvig should've written about Python's similarity to Scheme, instead of Lisp.  Ruby, by the way, has the FUNCALL thing :-)

&gt; No, it is not. Really.

Ah, OK then.  I'll stop saying so -- but I may quote you.Mathematicians and engineers are not that stupid and they would not solve such a simple problem in such a complex way unless there were follow-up questions which would benefit from that sort of analysis.I think you make yourself look like a fool when you criticize something you don't sufficiently understand. But you still have the freedom to do so, and that's a good thing. Criticism from people who have a poor understanding of the subject helps to illustrate the parts of the language that are hard to understand. This information is useful for the language advocates.

Usually, when criticism is based on misconceptions or flawed reasoning, people are kind enough to point out the criticism is unfounded. If somebody does not voice his criticism he will still think 'The language sucks because of the silly parenthesis', but nobody gets the oppertunity to point out why it's not really an issue. So ranting leads to a better understanding in the end. If the author is too stubborn to listen, other people who have similar gripes may stumble upon the essay and read the rebuttals with a more open mind.

Sometimes I'm annoyed to by a feature of a language. I specificially google for 'x sucks' in order to find out what other people think. Usually I find some kind of tirade in usenet, with thoughtful rebuttals, and the combination of the two is of real value to me.

Because rants are easy to recognize, you know you have to be sceptical about what you read. So it's all good.

What does bother me is when people try to educate others (articles, tutorials, whatever) on topics they don't understand well. Misinformation spread by tutorials is much harder to correct, simply because of the mindset of the reader.Thanks, nice to know.&gt; Yeah, I agree, don't write AST interpreters in languages without pattern matching:

Huh? Most programmers with good taste from the Smalltalk, Python or Ruby communities would implement AST walking via OO-style message dispatching, not endless if-then-else chains. This Scheme interpreter is _not_ an example of good style; please, all you guys, stop pretending this is an exemplar of Python style--talk about a straw man!In Common Lisp, loop is a macro, and therefore can be macroexpanded.  While it is true that any Common Lisp macro can be implemented as a special operator, it is ridiculously unlikely that loop be implemented as such.  No implementation I'm aware of does that, and no code walker I've ever seen treats loop specially.  (In any case, taw's statement is vacuously true, since there is at least one code walker that doesn't "understand" loop.)&gt; He should consider renaming the blog to "sigfpe's seriously awesome shit".

That title would have to be shared with the explosion of awesomeness that is [his homepage](http://homepage.mac.com/sigfpe/).&gt; In Common Lisp, loop is a macro

Wait, aren't you CLers always saying that your language is a "real language" on the grounds that it has a standard? :) Like you say, there is nothing in the standard requiring LOOP to be implemented as a macro. And even if it's a macro, it's possible it expands to code involving non-standard special forms specific to the compiler, or whatever.Implementing a code walker that works with every possible conforming Common Lisp implementation is no easy task.  Was that your point?&gt; Lazy semantics don't excite me at all, however. You can do neat things concisely with it, you can have automatic memoization and such with it, but ultimately I think that it is not very interesting.

I thought I was alone on this! It is often forgotten that (1) strict and lazy are interexpressible, so (2) which to use is a matter of personal preference and not of technical merit.That is the strongest form of my point, but I assume even supporting AllegroCL, SBCL and CLISP can be tough for a robust code walker. (My experience is limited to SBCL.)I can agree with that.  Still, your rhetorical question is irrelevant, and presupposes too much about me, my opinions, what constitutes a "real language", and the logical connection of not requiring macros qua specification to be implemented as macros to it.Cool - that's a good bit of history.  However, there's another reason that trumps this.

In Prolog, if you write code that looks like `foo(1,2,_,_,_)` the interpreter or compiler translates it to look like `foo(1,2,V001,V002,V003)` -- that is, each instance of `_` refers to a different variable.  To put it another way, `_` and `_` and `_` need never be the same thing.  I mean, they *can* be the same thing, as for example if you had a predicate like `foo(I,J,[],[],[])` and called it with the above query, then all three `_`s would be bound to the same value.  But they don't have to be.

So: given that `_` and `_` and `_` (and even `_` and `_`, and let's not forget good old `_`) all mean a new, independent and unrelated variable that will never be used, there's no value at all in ever having more than one symbol for the concept.  `_` is enough; a `_foo` or `_bar` meaning the same thing would be redundant.  So that's the real reason, and it's enough.&gt; They do, at least partly.

no, they absolutely don't.It is an insult to hardworking independent software people to call this guy a contractor. He's a market inefficiency.I thought the smiley gave away that I was joking. Chillax.&gt; I think it is not respectable to sneer and then say --oh, geez, you heard that?

What were you in a past life, an impresario for the Circus Maximus? I mean, you certainly sound like you're baiting someone into a fisticuffs. For the record, I apologized for being unnecessarily rude. I realized that, if I had a point, I forfeited it with my incivility.

I wrote a lengthy reply, saw a reply from dibblego to another post of mine in another thread, and realized that while I could defend my position it was ethically proper to concede. What should I have done instead, according to you? Delete the comment?Surely you don't expect the world to be fair?So, given that LOOP is a binding form and that it may be implemented as an atomic primitive, cf. the Common Lisp standard, how can a code-walking macro work correctly without knowing about LOOP? I'm genuinely curious, since I'm not an expert in CL in any way.&gt; Having said that, I have seen code that reads a Java InputStream to the end unnecessarily, or uses an (strictly evaluated) array when only a part of it is required to complete the computation

Parsimony in the use of resources is an orthogonal issue vis-à-vis laziness. Lazy data structures make it easy to express corecursion (unfolds), and often hide the need to perform Bird-Meertens-Meijer fusions on the morphisms. The issue of when to use it is one of pragmatics and not of semantics, [as Wadler shows](http://homepages.inf.ed.ac.uk/wadler/topics/dual.html#dual-reloaded).

&gt; I have worked for a corporation after all - I've seen code that is difficult to imagine.

Haven't we all.&gt; it may be implemented as an atomic primitive.

and the Moon may turn out to be made of green cheese, with about same probability.

(edit) no, let me rephrase that: it is perfectly possible that an implementation may compile LOOP natively, without macroexpanding it; but it is inconceivable that it would _advertise_ LOOP as a special form by making SPECIAL-OPERATOR-P return T on it and not providing a proper macroexpansion.&amp;nbsp; if for no other reason than because it would horribly break all the existing code-walkers! :)Rockets that are 30-40 years old, maybe. :)[deleted]Aye, but in the Erlang community there is a habit of:

    % use hxxp: here to avoid markdown issues.
    {ok, {_Result, _Headers, Body}} = http:request("hxxp://reddit.com/"),

as documentation, and Erlang implicitly supports this by not warning about unused _variables.&gt; What should I have done instead, according to you?

Ah, sadly, I'm still working on that facet of my ethics :-)

For instance, I have called PHP an 'obscene ball of mud', but if I found myself at the same picnic table with its inventor, and he happened to ask me what I thought about it, I think I'd use happer language like 'well, I prefer to use these other languages I already know.  Oh, you say everyone everywhere has it installed?  Why, that *does* sound like a useful thing.'  Of course, I want to call PHP an obscene ball of mud *and* have agreeable conversations.

But no, I wouldn't recommend deleting the comment.

&gt; I mean, you certainly sound like you're baiting someone into a fisticuffs.

It amused me, like I said initially, and it -does- seem like an instance of the Monkeysphere at work -- and so does my PHP example.Remember this is the author of ["My trolling on reddit"](http://programming.reddit.com/info/tzf7/comments)...
So I'd expect uninformed and provocative opinion only.For some reason I love it when programmers show that they have emotions and interests other than code. Also Everything, Everything is a pretty damn good album.No, taw authored [My "trolling" on reddit](http://programming.reddit.com/info/tzf7/comments), which actual blog post shows that his downmodded comments don't actually seem trolling.Here's some (totally untested) code to illustrate what I'm talking about:

    class Int:
        def __init__(self, value):
            self.value = value

        def eval(self, env):
            return self.value

    class Op:
        def __init__(self, op, left, right):
            self.op, self.left, self.right = op, left, right

        def eval(self, env):
            return self.op(self.left.eval(env), self.right.eval(env))

    class Var:
        def __init__(self, var):
            self.var = var

        def eval(self, env):
            return env[self.var]

    class Let:
        def __init__(self, var, exp, body):
            self.var, self.exp, self.body = var, exp, body

        def eval(self, env):
            bodyenv = env.copy()
            bodyenv[self.var] = self.exp.eval(env)
            return self.body.eval(bodyenv)
I don't think there's any way this can be overstated. People are not totally rational and subtle things like these are how they make decisions.

Firefox should learn this lesson by imitating IE7 in Windows, Safari in OSX, and in Linux it can just be itself ;-)
Let's see if I understand the chain of reasoning:

1. JavaScript, Excel, and Lisp are functional languages.
2. Lisp provides closures.
3. Adding features from Lisp to another language will make the latter Lisp.
4. JavaScript and Excel create expensive maintenance problems.
5. JavaScript and "Excel" are "just as powerful as Lisp" (thus equal?)
6. Adding closures to Java will turn it into Lisp (3), making it just as powerful as Excel and JavaScript(5), and create expensive maintenance problems (4).

I am running something of a calorie deficit at the moment so I may have misunderstood his reasoning. It would be easier to understand if he were more direct and clear about whatever he's arguing.

His conclusion might be correct: additional language changes may increase the cost of development/maintenance/so forth in terms of education/misuse or other factors that escape me. If he would like to support that assertion he would be better off proposing a study on the effects of language changes on the cost of software development.Deleting in protest of Reddit's new anti-user admin policies.&gt; it is important to recognize that a system architect or engineer that clamors on about closures really needs to realign themselves to the today’s modern functional platforms, JavaScript and Excel

And I was worried because I've never really taken up on Haskell...You can also try to get gcc to generate code for both x87 and SSE. With my track record of ICEs, I won't even think of doing it for a couple more minor versions ;)&gt; Pointing and laughing is not very constructive

Ah, but it is *so* statisfying! What has done more for the advancement of the arts than ridiculing the morons?

You would agree with the sentiment, though, that excellency and expediency are not at odds with each other, nor in any way trade one off the other. Sadly, I see that reflection and forethought are being sacked in the name of turnaround.&gt; I doubt he's a troll or a self-parody.

Go away and leave me to my self-indulging fantasy that it is. The alternative is too horrible even to contemplate...The last update on this page was in 2005. Anybody know the current status?Read the standard. It is required to be macroexpandable. Everything that's not one of the 25 special operators in table 3.2 of the CLHS (Section 3.1.2.1.2.1 [I'm not kidding]) must be macroexpandable into them, or functions. *My* codewalker explicitly only supports these standard special operators, and even macroexpands some of them itself. Porting it to clisp only required installing it.

EDIT: I however agree that, if you think you need a codewalker, you probably need to think on what you really want. Even if it's portable, the end-result will be brittle, and, if you're generating code, a mess that will be very different from the user's code, and thus a pain to debug.Thank you, that was informative! I remember you did a CPS code walker, so I was hoping you would chime in.&gt; He compares Bjarne Stroustrup to Saddam Hussein. It's not funny. It's not witty. It's offensive and tasteless. What was the guy thinking when he wrote that?

Is it more offensive to Bjarne Stroustrup or to Saddam Hussein? (Sorry, couldn't resist. I just regret my years spent as a C++ fan too much).Now, now. It's not because it's adding continuations that it's in CPS :p The whole point of using *ANF*+annotations was to avoid the interop problems inherent to CPS by keeping a normally-working stack. That way, catch/throw can work between transformed and normal code (both ways), you don't need TCO, etc.

Now, for the scheme codewalker. If you only want to deal with lambdas, there's a simple way to do that: macroexpand other bindings forms into lambdas yourself. It is literally a macroexpansion, after all. Or even macroexpand everything into core scheme!

PS the oasis is calling ;)[deleted]So, instead of working on functionality, developers should spend 99.9% of their time replicating every tiny bit of the GUI on each platform?  And then being repaid for their efforts by 3x or 4x the bug reports?  I think not.

However, there is nothing preventing someone from taking the shared code and wrapping the core functionality in a native GUI.  But, wait!  You say you don't have the time, that you are too busy, that you have a real life?  Well, the same goes for the original developers. (or so I hope!)  :-)

By the way, the example with the native-looking buttons on the Google page indicates, IMHO, breakage.  Their style should be mandated by CSS. 

If you agree to use a special syntax (for example, as I already said, `[]`) to denote underscore-ful expressions, you can easily get that affect without any codewalking. You can treat the input expression as basic lists and substitute symbols (just `defconstant` `_` to make sure it can't be bound to anything). Moreover, it is only that way that that notation can work with LOOP. Otherwise, you'd transform the tightest enclosing expression into a lambda, and find yourself collecting lambdas (or something in that vein) instead of having a lambda with a loop inside. 

+1 for minimal explicitness.&gt; My main “profession” is econometrician [...]

IHNTA, IJLTS "econometrician".&gt; At the very least you shouldn't criticize something that you don't understand.

Thing is, you can find 3 good things about a language without understanding it.  There are several examples in this blog post where it seems like he gave the language a cursory once-over, read the manual, and said "Hey, this looks cool."  Meanwhile, in practice the exact feature he picked out often has tons of pitfalls that make it a constant source of annoyances.  For example:

&gt; So the cool things about Java. How can I not say portability ? There was never a language as portable as Java.

I've actually had better luck writing cross-platform Perl and PHP.  Java apps usually have more dependencies (an unfortunate side-effect of pervasive OO), and it's almost inevitable that one of those apps will use hardcoded paths instead of File.SEPARATOR, or link against a native library, or use some functionality that works subtly differently on different OSes.  In my experience, Java is really Write-Once-Debug-Everywhere.  

I suspect I've had better luck with Perl/PHP because I only use them for text-munging and database-backed webapps, which have many fewer dependencies.  Still, for a language whose whole selling point was portability, and which makes significant sacrifices to achieve it, Java is pretty piss-poor at it.

&gt; Still, there are a few cool things about it, like arrays ! Perl introduced arrays and hashes as separate data types, and every single language followed. Perl had little choice, as only context determines whether "1" and "01" are the same thing or not.

I'm a little confused about this, as PHP *also* uses context to coerce "1" and 1, and has similar issues with "1" and "01".  Anyway, I consider PHP arrays to be one of its misfeatures.  Take a look at all the corner cases in [the array section of the manual](http://us3.php.net/manual/en/language.types.array.php).  

Personally, I think a better criterion for being able to criticize programming languages is having written at least one major (= used by people) project in the language.  You don't really get a feel for what's good or bad about a language until you have feature requests coming in and people banging on your program in unexpected ways.So, whatever happened to segfault.org?  Sysop get run over by a bus?Do any of Firefox's many themes that claim to be derived from OS-X address the issue in any way?

Seriously -- I'm using Linux so don't know how good or poor the many Firefox "OS-X" themes are...(?)Or, you can use Assembly today and get both the power and speed.

Can we stop with the language pissing contests?Because only irrational people wouldn't choose Firefox?  That's pretty goddamned arrogant.Based on what I saw interviewing at financial software companies, and what our customers currently ask for - yes, Wall Street runs on VBA and Excel.  APL is a really minor player - it's known as a financial language because most users of APL are financial institutions, but that doesn't mean that most financial institutions are APL users.&gt; Does this mean the Stock market itself runs on Excel?

The big order-management systems that electronically send orders to the exchanges seem to be a mix of C++ and Java.  I don't have firsthand experience with the stock exchange itself, but I have had to build stuff that integrates with various order-flow systems.[removed]Just because it's arrogant doesn't mean it's not true.There's no reason to use Safari when Camino exists. It has great Mozilla-powered rendering with the native Mac UI that Mac users like. As well, it's not as 'heavy' as Firefox. I keep Firefox around for its convenient web development features, but I use Camino for all of my every-day browsing.&gt;That is right, JavaScript and Excel are functional languages that are just as powerful as Lisp.

I just threw up in my mouth a little bit.It isn't true either.  Firefox has as many problems as any other browser.GHC Haskell does the same thing. With -Wall, _foo style variables aren't warned as unused. So they make good docs for default cases:

    case e of
       []     -&gt; something
       _blob  -&gt; otherthing[removed]Write your own instead of complaining. I don't create the website http://www.SomeProgramSux.com , I write my own version. STFU please.I am a professional developer, and the "user experience" is a HUGE part of any piece of software.   If it feels out of place on the OS, people are immediately going to have a negative opinion of the software before even really getting to use it.

&gt; Most of all, I find myself empathizing with Mac Safari users because I haven't been able to switch away from IE7 on Vista. Firefox feels so dowdy in Vista.

I can totally relate to that statement.  I have been using Firefox since it was called Phoenix and the installation procedure was "copy the files into any folder you want".   Using Firefox on Vista feels strangely out of place, because it doesn't use the native UI widgets.   It's still (by far) the best browser, which is why I still use it, but there is something tangible that's missing, and it's the non-native UI.

Thankfully, the Firefox devs seem to agree with me, and are working hard to get the browser to look native across all these disparate platforms.That's progress. The HOP website isn't really usable without JavaScript.
You believe adherence to a standard is more important than web pages that don't look ugly? Really?Does Camino support all of the plugins that Firefox has?No[deleted]Well, lets be fair, as any other browser except IE. Or maybe you didn't even count that one as a browser at all?[deleted]There's nothing a priori irrational about being superficial. People get caught in this trap frequently, but they are confusing being rational with being good -- per our cultural norms, which say it is immoral to value appearances over functionality (in an application) or over (to use a random example) personality (in a person).

Personally, I reject this, rationally. I care very much about superficial things like prettiness, because pretty things make me happy.

It's funny to think that a lot of the shitty UI out there exists because developers are morally reluctant to spend time aesthetically pleasing their users. 

(Perhaps the same phenomenon is why many geeks dress so badly -- are they afraid of appearing morally superficial if they spend the time to dress well?)So how exactly is Camino better then Safari? 

Safari has great khtml-powered rendering (more compliant then mozilla while using less memory) with the native Mac UI that Mac users like. As well, it's not as 'heavy' as Firefox. I keep Firefox around for its convenient web development features, but I use Safari for all of my every-day browsing.I'm a Mac user who still prefers Firefox to Safari.  Safari is a decent web browser but it doesn't support keyword searches (i.e. the ability to type 'yt Oozinator' in the address bar and have it search youtube for Oozinator), so I feel crippled when I try and use it.&gt; So, instead of working on functionality, developers should spend 99.9% of their time replicating every tiny bit of the GUI on each platform?

Yes, they should. If the UI is only marginally better, it makes up for far less functionality, because users get the opportunity to actually *use* that functionality.How do you figure that "more" always equates to "better"?&gt; it may be implemented as an atomic primitive

No, it may not. The Common Lisp HyperSpec refers to it as a macro, so a macro is must be. Macroexpand it and you can walk it just fine. This applies to all macros, so there's no problem here, and the author of this blog post seems too inexperienced with CL to provide a proper critique.No. OS X's interface is primarily about what it *works* like, not what it *looks* like.&gt; Thankfully, the Firefox devs seem to agree with me, and are working hard to get the browser to look native across all these disparate platforms.

Except the Mac:

&gt; First off, the use of Cocoa widgets in the new Mac builds applies mainly to UI elements like scroll-bars and is intended, as I understand it, to provide rendering speed improvements over the Carbon UI elements used by current version of Firefox. It does not mean that page elements like drop down lists and text fields will look like those in Safari (which uses Webkit).
 -- [Monkey Bites](http://blog.wired.com/monkeybites/2007/02/firefox_3_quest.html)Interesting news!  I believe the article can be summarized as a review of Linus's refusal to move from GPLv2 to GPLv3 and these two article quotes:

1) "In a speech and interview at the company's analyst summit this week, [Sun] CEO Jonathan Schwartz said he would like both Solaris and Java to be licensed under GPLv3"

2) "FSF Executive Director Peter Brown confirmed that Sun was moving in this direction (emphasis added): 'Sun has now asked for our thoughts on moving the Solaris operating system to GPLv3 and what they would need to do to engage the free software developer community.'"You're arguing a straw man here. I didn't say anything about superficial things being inherently irrational. If you'd like to argue that not using Firefox over IE4/IE6, purely based on aesthetics, is a rational decision then I disagree.

If Firefox and IE4/IE6 were condoms, the former would be 99% reliable and the latter 50%. Even if the IE condom *looked* way better you'd have to be *irrational* to choose it.But see Nassim Taleb's observations about the procedural flaws in _The Millionaire Mind_

http://cyberlibris.typepad.com/blog/2004/04/black_swan_skil.html

http://www.edge.org/3rd_culture/taleb04/taleb_indexx.htmlHere's some untested &amp; totally messy Haskell:

    if code !! 0 == Lambda
       then evalLambda continuation context code
       else if code !! 0 == If
           then evalIf continuation context code
           else if code !! 0 == Begin
               then evalBegin continuation context code
               else if code !! 0 == Define
                  then evalDefine continuation context code
                  else if code !! 0 == Set
                      then evalSet continuation context code
                      else if code !! 0 == Quote
                          then evalQuote continuation context code
                          else if code !! 0 == Load
                             then evalLoad continuation context code
                             else evalApply continuation context code

You can write bad code in any language.  This just happens to be bad Python.Yep. Right now I'm in the middle of a project that automates part of the process of putting together bonds. Not trading, but actually assembling them for original sale.

Currently it is a labor itensive process done mainly with Excel spreadsheets.

Excel and CSV are also just as popular as XML for sending bond prices and trade details right now.
Prove they are not.

Both JavaScript and Excel have the ability to rewrite themselves at runtime. That is a hell of a lot of power.upvoted for mentioning Inform 7.

It's a great thing. Spent a lot of time with it and had lots of fun.Every computer scientist knows that \sum_{i=0} (r)^i converges at (1/(1-r)) when r &lt; 1.  It comes up quite often in algorithm analysis.

edit: typo
For just getting the caller graphs without profiling 

http://pycallgraph.slowchop.com/

Just discovered it today and it's nice (though a little resource eating currently)Sorry, I used the common form of 'can' that refers to what humans will do, and not what is physically possible.  For instance, you can reimplement GHC in mostly ANSI Forth by way of a native-compiling implementation in Python by way of a Smalltalk implementation of Python3000 running in a Squeak system running natively in an x86 emulator.

Nobody can possibly ever write the code you've written :-)Python is actually one level above "foreach", because what Python "for" takes is actually an iterator, and you can feed it whatever you want as the iterator. I commonly use tree iterators, for instance, and often I even tell the iterator what nodes to follow.

In Python 2.5, there's even a formalized way to interact with the iterator while it is iterating, turning the "for" construct into a lightweight co-routine combiner. (You could already do this in 2.4 using an instance-based generator, but the feedback method wasn't formalized.)

So Python's "for" is a step above even Javascript's for (which iterates along the properties of an object, but does nothing else AFAIK, and can't be interacted with), and in some ways especially in 2.5 sort of leaves the linear scale in a different direction. I'm not going to say it's "better" or "worse" than map/reduce/filter, because I think it is just plain different.&gt; In the following article it says that MS has licensed the technology to lexar.

But it doesn't say why they did it. Was Lexar just being cautious or were they threatened? If they were threatened, why wasn't anyone else?

If you were the legal department for Lexar, would you even consider letting your company release a major product without covering yourself?

&gt; They were funded by MS.

Don't forget, they were funded by IBM as well. A big part of the lawsuit was about IBM not paying them enough royalities for IBM's Unix line.

What sounds like something the CEO of a major company would do?

1. Sue a company because they are not paying the royalites you think you are owed.
2. Sue a company because another company, with a competing product, said you should and tossed in a couple weeks of operating expenses.

Don't forget that 12 million is nothing. Orcale made more than that on one project, and the software didn't even work.

http://www.lsj.com/apps/pbcs.dll/article?AID=/20060125/NEWS01/60125005/0
Agreed.Ok. I know I was terse in my comment, but I never said the Kernels of the computers that run the stock exchange are excel spread sheets.

 I said the I've seen *a lot* of applications to compute financial information made using VBA and Access/Excel. This is not the same thing as frameworks for supporting orders etc.[removed][deleted]Exactly: I've seen many 'developers' that in my junior years I would have dismissed as being hacks. Except that these guys are actually getting paid doing financial work, and they're being paid a lot of money to do it. From time to time, they put on a 'hack developer hat' and write a little program for themselves, and before you know it, 500 people are using that program.

 Programmers educated in computer science are not the norm in that area.

 Also, I've found that these financial types are very doubtful of programmers. I was always greeted with a lot of skepticism. Now, I can't understand if it's because they're DIY kind of people, or if it's because they've had bad experiences in the past.No.There has been a lot of talk over the years about how one langauge is more 'powerful' than another. But a question still remains, how to you define 'power' in relation to programming langauges?

Is it defined in terms of sheer processing speed?
The ability for the code to rewrite itself at runtime?
Constructs that reduce the amount of code needed to express a concept?
Rich libraries that make common tasks easy?
Who cares.According to Paul Graham, who has a large epenis, power == brevity.

In before qwe1234.if desired, an even simpler problem can have an even longer solution. true mathematicians look for elegant methods.[Thanks!](http://programming.reddit.com/info/13nwf/comments/c13sh8?context=5&amp;style=nested#c13sh8)

This is the [future](http://programming.reddit.com/info/11gvx/comments/c012197?context=5&amp;style=nested#c012197) of [manual optimization](http://programming.reddit.com/info/111y8/comments/c011630?context=5&amp;style=nested#c011630) - doing even hand optimization at a high level of abstraction."(It was something of this kind: what is the smallest power of 2 with the property that its decimal digit fourth from the right is 7? This is a completely trivial problem for a present-day computer: it takes only a fraction of a second of machine time.) The machine and Johnny started at the same time, and Johnny finished first."

I did this in around 15 seconds, which might lose to both the computer and von Neumann - but just to demonstrate how it would be done in only a few computations:

I'll assume that you've memorized 2^10=1024 and 2^16=65536 (the latter being the number of rows in excel).

Start with 1024 which is the first power of two with a 4th digit. Now, the 4th digit is going to go 2, 4, 8, at that point the trailing 3 digits are around 8*25 &lt; 500, so the next number in the 4th spot is 6. At this point there obviously isn't another chance until 65536, whereby 2^17 ends in 1072. Now, multiplying by 8 (thus 2^20) gets in position again with an 8 as the 4th digit and the part to the right is obviously &gt; 500, so the answer is 2^21.

The more you de-emphasized preciseness, the faster it could be - you could consider 2^10 = 1024 and just recognize that multiplying by 16 (2^14) will fail, then just jump to 2^16 - 5536, 2^17 - 1072, and realize that multiplying by 16 (2^21) will succeed. Two steps in total, with about 2 or 3 logical deductions required&gt; Well, what would have happened? Elaborate.

There would be about a 100 more Lisp dialects that don't talk to each otherI wasn't disagreeing, I was contributing additional information that you or others may be interested in.&gt; You like dogs.

?

I like cats. *Some* dogs are ok but I don't like their lack of individuality.

&gt; competetive

Ah, how sloppy.Turing... yawn[deleted]When you're talking expressiveness of a language, there is a huge difference between the two.
if ui look and feel is so important, why is myspace so popular?

i love cool looking ui's, but im not convinced that having all my buttons look like gumdrops is what makes it 'correct'. i think it has more to do with what the app is trying to do, and if things are in the right place when youre working fast.

i dont think uniformity is as important as everyone claims. we all know what a button is.[removed]Nobody claimed anything in Python was new. It's just _practical_, without having to know terms like "hylomorphism", or [exhausting your category theory](http://etymon.blogspot.com/2004/08/bananas-lenses-envelopes-and-barbed.html), which for most people happens the moment you say "category theory".What sucks is that there is no way to sanely develop interfaces across the three major desktop platforms. Swing/AWT, wxWigents, GTK+, QT, XUL, Tk -- they are all messy, incomplete, and most are tied to specific languages or have other limitations.GrApple is even better.The claim that purity is necessary is a bit strange:

"Note that this is only valid in a pure functional language like Haskell! If we were working in ML or Lisp, then f and g might have side effects, and we couldn’t safely combine the two passes."

Side effect analysis isn't that difficult, and it has been implemented by decent compilers for impure functional languages for a long time.you're right i did see this and didn't make the connection. thanks.&gt; Side effect analysis isn't that difficult,

It can be if you want to eliminate false positives. And thanks to the way false positives propagate their effect over the codebase, the slightest false positive brought on by an non-automatically-resolvable ambiguity can trash the entire optimization process in the worst cases (think C or Python, both worst cases in their own way).

The exact difficulty varies from language to language, but it's hard to deny that pure functional languages are by far the easiest. It's not even a contest.Could you explain what you mean by 'Tool School'?

I've been in plenty of interviews and now run several myself. In order to bring a good team together it's necessary to have a process for working out if the two parties can come together successfully, and the interview serves in that process, also.

I don't agree that they're contests about fakeness - in fact a well-designed interview process is impractical to fake. So I don't think that's valid. Do you have any other ideas about why you might hate them?
It's true. If a compiler can detect a pure subset of an impure language, then a lot of these techniques can be used in ML, Lisp, or even in a portable assembly language (see LLVM).

But it's a non-trivial engineering effort to build such a compiler, and most such compilers don't allow the user to specify algebraic optimizations of user-defined functions.

Of course, there's another issue: Unless the language in question is generally pure (or supports something like the ST monad), the odds of 'f' and 'g' being purely functional aren't very high.&gt; *If-Elsif hell*

Oh god, I have to deal with this all the time at work. Our XML configuration scripting DSL was written like this on top of an oblique object model.

&gt; Surely this code is perfectly Pythonic

Dude, this is an antipattern in **C**. Has to be wrong in Python.As a non-Haskell programmer, this is one of the coolest things I've seen about Haskell. I've often noticed optimizations like this that I could do in other languages, but have had no clean and easy way to do it, even ignoring the possibility of screwing it up somehow.

(I notice this especially when I'm using Python at its most functional; sometimes when I stack generator comprehensions on top of each other, which at least isn't creating a list per statement, I find myself wishing Python could automatically combine them into one experission for me, and I've had my own tree moments like that too.)MySpace is so popular because people feel like they can improve on it. The same thing happened with Biz Markie.

Having one button that's different isn't that bad, but if buttons started looking different in all programs it would get very confusing very fast.And thus we see why _using_ case sensitivity as part of your language is a bad idea, since "case" is not even close to a universal concept. 

(I highlight "using" because the question of case sensitive identifiers is another one entirely; how you normalize English letters doesn't have to affect how you normalize the glyphs of other languages for lookup purposes.)Wrong on both countsWow, that really says it all.

(Seriously.)

Concise."...even ignoring the possibility of screwing it up somehow."

Validating the safety of the rewrite rule is, of course, critical.

dons explained a really neat hack to me last night: You can use QuickCheck to generate random input lists and random functions(!) for f and g.  QuickCheck will then verify that the LHS and RHS of the rewrite rule return the same value. Of course, you need to remember to turn off rewrite rules before running your test cases!

GHC rewrite rules plus QuickCheck give a you complete toolkit for writing and verifying your own optimizations, which is mind-blowingly cool.I am a reformed Windows/Linux user that started using Firefox long before i ever touched OS X, so maybe i'm a bit biased, but i use Firefox on OS X.

Safari *looks* spectacular, but i'm also bothered by some of its limitations.

For example, Safari's lack of "search for text as i'm typing" is a huge sore spot for me.  Plus, extensions i use in FF are either nonexistent or have some non-standard way to install them (at least compared to FF's "click the XPI and it installs automatically").  The lack of a well designed plug-in architecture is something that rubs me the wrong way.

i.e. the instructions for the "Safari Tidy Plugin":

&gt; 1. Installation

&gt; 2. Unzip safari-tidy-0.2.1.tgz

&gt; 3. Install SIMBL

&gt; 4. Copy safaritidy.bundle to the SIMBL plugins folder.

&gt; 5. Restart Safari

Safari is a great browser, i'm not bashing it.  But there are some things i need before i can use it as my everyday browser, fancy UI widgets or not...Visual Basic.net == power. Every other language == useless toy.An easy attack on a straw man.  I don't think *anybody* &amp;mdash; including the guy who emailed him &amp;mdash; is claiming that web standards are the most important thing in the world.

However, if you are going to write about the technical side of web development, it does your credibility a whole lot of good if you actually use the tools of your trade appropriately.  I suspect that is what the anonymous correspondent was getting at, but without the context of the whole email, he just comes off sounding like a troll.

Yes, there are plenty of important parts that contribute to running a website.  Complying with the relevant specifications happens to be a useful QA strategy.  Instead of looking at it from the perspective of "what's the relative importance of conformance?", you should look at it from the perspective of "why is bad code getting past our QA process?".  And that's the crux of the matter &amp;mdash; QA *is* vital to a good website, and any decent QA process would catch silly little things like invalid code.
&gt; Of course, you need to remember to turn off rewrite rules before running your test cases!

I haven't seen this as part of the official Testing Dogma, but I've been trending lately towards running my test both "optimized" and "unoptimized", _not just_ to make sure that they pass, but that they fail in the same way to the extent possible. I'm right on the verge of writing this directly into my test script.

("Optimized" can mean different things in different environments. I've got tests for my Python(/Django)-based website, but I can run them either with my aggressive caching, or without. Not only are failures clearly a problem, but even a difference in how they fail can indicate a bug in my caching code that perhaps my tests didn't catch directly, since the entire purpose of a cache is to be transparent.)&gt; In the UK the regulators are beginning to turn their lidless, burning eye upon this issue

HAHAHAHAHYeah, I've been wrestling with testing vs optimization issues in Ruby on Rails, too. Caching is generally hard to test well without being a bit sneaky.

As for the rewrite rules, I think there's a GHC flag that will allow you disable them on a per-file basis, so that you can actually test the LHSs against the RHSs without accidentally optimizing them to be identical. Which would kind of defeat the purpose of testing. ;-)Oh.. sure I agree but *Express Install is not supported by this Flash Player version*. I'm running Firefox on an Ubuntu box, nothing of strange, I don't what my users to see this message, ever.Finally a voice of reason. Clearly Excel is just as powerful as Lisp. But PowerPoint is even more powerful than Excel. It is the ultimate programming language, even more ultimate than VB.net.2121212121
55555555555555555555Camino still doesn't quite feel native -- it does an OK job of emulating native widgets in forms but doesn't quite get all of them right, and it's still based on Gecko which means it still has noticeably different font rendering from Safari (Zeldman has [some good examples](http://zeldman.com/2006/11/27/safari-beats-firefox/), and still doesn't handle some CSS niceties that Safari does (like `text-shadow`).This comment was the best:

&gt;The truth is, only developers care about these issues !
&gt;
&gt;Look at MS Office, Ad-Aware, Winamp, Windows Media Player, your own beloved MediaMonkey, Picasa, all the web sites with image buttons, etc... All of these use a different look and feel from what could be called "standard windows" and yet they are very succesful. Why? Because most people don't care ! My dad doesn't care whether a button is squared or round, as long as it is recognized as a button and behaves as expected, he's happy. As so should you... Usability over "Pixel Perfect Copy" I say. Just like JGoodies java apps can be perfectly usable, if it weren't for Java's installations issues.

Those animated gifs are really annoying when you're trying to read the article...Shhh. They wont forget if you keep reminding them.&gt; At the very least you shouldn't criticize something that you don't understand. How do you think a Lisp programmer would feel if someone who had never used Lisp in their life said "all those parenthesis look stupid".

...or, how would a Haskeller feel if someone who (by implication) barely has a grasp on Haskell says this:

&gt; It's a bondage and discipline language - either you think the way it thinks, or it will make your live [sic] a living hell.

...since any moderately seasoned Haskeller knows that, if anything, it does quite the opposite.I think it goes further than just button controls and the like. For instance there's a noticeable difference in rendering of fonts in firefox on ubuntu when companred to native apps. Tahoma 8pt looks crisp and  clear on gtk menus, but ugly as hell on firefox's. Maybe there's some way to configure this, but I've never found it.So "Enjoy your scrums" is the contrapositive of "Enjoy your waterfall"?

Your ignorance is fucking staggering.  Why don't you and your sockpuppet...

1) Buy a dictionary

2) go blow yourselvesOne of the things you'll often hear Python people say, including Guido, is that Python steals good bits from other languages instead of trying to invent new things. But it does so in ways which don't require multiple post-graduate CS and math degrees to understand -- when Python steals something, it generally doesn't bother with mentioning that "this is an implementation of a Type 3 hyperphasic polycombinated quadri-linear co-sieve"; generally, Python folks are more concerned with getting things done than with getting the technobabble :)

Incidentally, the sheer amount of impenetrable jargon (and my degree is in philosophy, so believe me when I say I know impenetrable jargon when I see it) is one of my biggest beefs with the functional programming community -- when I read stuff about FP, I keep expecting Geordi to show up and tell me that if I just reconfigure the phase emitters to produce an inverse tachyon pulse, everything will work. Figure out a way to lose the jargon, or at the veary least make it more approachable, and FP will have a chance with the general programming population.&gt; Both JavaScript and Excel have the ability to rewrite themselves at runtime. That is a hell of a lot of power.

MacGyver can build an explosive out toothpaste and a small quantity of nugat, but that don't make it C4.Camino doesn't support any Firefox extensions. I can't see why one would use Camino over Safari. It's the extensions that make Firefox.The same as it was in 2005?Obviously. The point is that your post adds nothing. If you had said, "Almost all Lisp implementations implement as much of Lisp using Lisp" then I'd up-mod and keep my mouth shut. But all you said boils down to "Lisp is better na na na".Side effect analysis is not very difficult to do. Even Steele's Rabbit compiler, which was the first real compiler for Scheme, could do it. One difficulty might arise in heavy usage of first-order functions, but if you are going to attempt an optimizing compiler for a functional language in the first case, you are going to have some way of getting an approximation of the call graph, and it would usually catch these sorts of issues. The interesting case would be when there are two kinds of f's and g's being passed to mapping function. In that case, you would need more sophistication, like a compiler that would realize it should split the code for the function involved due to the savings of the optimization.Although I partially agree with that point (Haskell does force you to program in a functional style), I wasn't really defending the article in my original post.[removed]"The exact difficulty varies from language to language."&gt; Define power

Troll.Heck, `some people` would probably use smart constructors to transparently have what would require guards with pattern matching ;)(I linked a footnote from the essay back to this discussion.)

It's true that side-effect analysis has a much better chance of working in a mostly-functional language, like Scheme. The problems that jerf complains about are very real (adding a single line of code and killing the optimization of your entire program, for example), but they're greatly lessened if everybody on your project feels that SET! is unclean.

What's really neat about the Haskell model is that there's an insanely robust theory for dealing with side effects--using the ST monad, we could have a nasty, stateful algorithm down inside the first argument to treeMap, and the compiler can trivially prove it to be pure. Here's some more information:

http://citeseer.ist.psu.edu/launchbury94lazy.html

(And I recently encountered a code generation bug in Visual C++ that was caused by faulty side-effect analysis, so having a robust theory for this stuff is valuable.)
Do you have anything to offer besides sarcasm?I haven't found this to be the case. On the contrary, certain languages such as Python as specified in an imprecise, inaccurate and self-contradictory manner, it amazes me people get things done (eg, look up the type coercion rules in the Python manual. It basically says "this is too complex to document, just try examples in the REPL to get a feel for the herustitics used")

If you want to be a programmer, there are just certain things you have to learn. FP doesn't use category theory, it uses a tiny subset, just like data structures are built on a tiny subset of discrete math and logic.

If you want to see some far-out and wacky applications of category theory, don't look at FP, learn some algebraic topology.&gt; It's the extensions that make Firefox *slow and buggy if  you add too many good ones or just one bad one.*

There, fixed that for you.I'm confused. Is Lisp or JavaScript/Excel suppose to be the imaginary one that only works on TV?Why do you say that?Hey, don't knock MacGyver. He's da man.It is a bit of a tradeoff. The forced usage of a monadic style does reduce the expressiveness of your programming language (in the sense of Felleisen, as he proves that Pure Scheme can not express set!), which makes perfect sense. You can not decide to use side effects in your program locally if you are forced to use a monadic style. The conversion of side effects to a monadic style is a global transformation, just like the CPS transform. If you are interested in having a language that is as expressive as possible without telling the programmer how to program, i.e. you let him use side effects if he wants, this will obviously complicate your analysis somewhat. However, all of the requisite techniques are already being used in actual compilers and there is nothing preventing these same sorts of optimizations from being used in the presence of side effects.

I agree with jerf that the difficulty varies from language to language, and I was going to reply to his post essentially echoing your comment that it depends a lot on coding style. In Scheme, for example, unnecessary usage of side effects is generally not encouraged.Because the entire goal of this exercise is to have various people attempt to define 'power', while you proceed to shoot down the ones which don't agree with your already made up opinion that VB.net and Excel are the ultimate programming languages.Well if all that talk about MVC, domain driven architecture, and seperation of concerns actually worked, then it should be easy to bolt on platform specific GUI libraries.No, the purpose is to encourage people think about the terms they use to defend and promote their favorite languages.There is no "sane" way because the very principle of cross-platform interfaces is flawed. It's that simple. If you wish to produce a "sane" interface, make it platform-specific. You'll not only please users more by giving them a more familiar look and feel, but also by providing them with OS-specific features.There's no doubt that fusion is cool and the analysis involved becomes much easier if your language enforces referential transparency.

On the other hand, are the performance gains enough to outweigh the hit?You raise good points.

Lack of unified extension architecture is definitely a limitation of Safari.

As for find-as-you-type, that's coming in Safari 3.0 in 10.5 Leopard.

It's a good thing we have many browsers to choose from; they all have their pros and cons, and people all have their preferences and specific needs. Firefox isn't for everyone, and neither is Safari.I shall point you to Inquisitor, which is an excellent extension for Safari's search that enables searching custom websites and also adds predictive search through Google Suggest and such. Alternatively, there's also AcidSearch.Of course it works.

Skype, for instance, has a common library among all of its platforms, but a Mac-specific, Cocoa-based GUI on Mac OS X, a Qt-based one on Windows XP, and a slightly different Qt-based one on Linux. It has Mac-specific features, Windows-specific features and Linux-specific features. The non-Windows versions lag behind at times, but they're all Skype clients all the same, and they share all the important stuff.

Same for browsers. Gecko and WebKit work on multiple platforms, and anyone can write their own front-end for them, be it Safari, Firefox, Camino, Epiphany or something else entirely.

Even Microsoft can write platform-specific software while maintaining compatibility, cf. Office:mac and Office for Windows. Very, very little shared code in the higher layers.

What makes you think it doesn't work?Depends. GHC actually outperforms GCC on some benchmarks, largely thanks to rewrite rules and fusion. There's a few links scattered towards the bottom of the essay. In particular, take a look at the ByteString paper.

Your biggest pain in high-performance Haskell programming will often be forcing certain data structures to be fully computed at specific times, to minimize memory footprint.
&gt; The point is that your post adds nothing

My post points out that while Python is close to Lisp in many ways, instead of waiting for PyPy, you can get not only what Python is approaching ability-wise, but you can get it at an *average* of 50% the speed of C (PyPy's long term *best-case* scenario for a very small subset of code paths).

Your post is just agumentative without offering anything new.  If you don't want to get into an argument about it, then don't start an argument.  Saying "shut up about stuff that's similar but even better!" sounds like insecurity.
I've done wxWidgets in C++ for a small graphics application, and from my experience, it's really really nice. Of course, the documentation sucks, but the wxWidgets api is really nicely thought out. I was plesantly surprised when I got my program to recompile on MacOsx, GTK/Linux, and Windows - without changing a line of code, and getting native widget sets. 

I don't get what you are asking for?[deleted]I am surprised the author overlooked [SWT](http://www.eclipse.org/swt/), a library that attempts to overcome the java shortcomings mentioned in the article. One high-profile application is the bit-torrent manager [Azeurus](http://azureus.sourceforge.net/index.php), that has native widgets.That's a worthwhile feature.  The Prologs I've used don't warn about anything as esoteric as unused variables.  &lt;klingon&gt;If it was hard to write it should be hard to read!&lt;/klingon&gt;But look how rare it is in practice. If it really was effective, why don't we see more cross-platform commerical applications?[deleted]Easy digestible tutorials on various Unix programming topics. Not very in-depth, but good for getting started.I find this whole discussion unpersuasive. It is neither necessary nor, in general helpful, for the compiler to prove that the functions are side-effect free.

Why can't the programmer just declare that the functions are side-effect free? They may occasionally make a mistake, but more but often they will identify that something is functionally side-effect free despite being technically side-effect causing. For example, a process may do a SELECT against a database or GET to a web service. The remote server and any intermediaries may log this and therefore cause a side-effect. But the programmer knows that this is irrelevant to the algorithm. The compiler does not.

Note that the whole thing was originally motivated by a discussion of Google's MapReduce which is already implemented in a very side-effect friendly language. Furthermore, the kinds of algorithms that MapReduce implements are highly likely to involve network communications that cannot be proven to be side-effect-free (and quite possibly are NOT strictly speaking side-effect-free).

The whole thing is similar to the equality discussion. Only the programmer has a useful definition of side-effect-freeness for his domain, not the compiler.
It's not true that conversion of local side effects is a global transformation.
For global side effects, it's a global transformation.

But (as the Launchbury link explains), you can use side effects locally and have the type checker ensure that the effects will not leak out of a limited scope.no offense - but when judging the computation power of a system, one doesn't usually distinguish between forms of data storage.  turing tape is different in that it is allowed to hold infinite storage.  not like a hard-drive or the internet or devices in our universe at all.Why not just use the [for-switch](http://thedailywtf.com/Articles/Switched_on_Loops.aspx) paradigm?Yeah, lets all start smoking crack and begging on the street... we'll become millionaires.VLC media player used to use wxWidgets on Mac OS X. They eventually switched to Cocoa.Here we go - this exposes the great problem of stats - the connection between correlation and causation. Could it be that more intelligent people measure self-worth in less financial ways, therefore are less likely to chase after money to become millionaires. People with High grades and advanced degrees simply do not want to do the jobs that make you a millionaire, they want to do jobs that stimulate their intellect, and those jobs don't usually pay that well. It may be that it doesn't take intelligence or creativity to become rich, it requires greed and ruthlessness. or luck.I believe it would work for both, but I think it's mainly targeted at developers at present. There's not much in HackageDB at the moment which is of interest to non-developers.

You won't find much on CabalInstall yet because it's not finished. Looking at the [source code](http://darcs.haskell.org/packages/Cabal/cabal-install/), it looks like it must be pretty close to completion though. It's now in the main Cabal darcs repository which is a good sign too.good links!The basic win of fusion isn't to make tightly-written code run really fast. The basic win is that it lets you eliminate the abstraction penalty for well-factored code. Now, you can write clear, understandable code, and it won't be any slower than the tight way. 

For this to really work, the optimization has to be part of the performance model of the language, and when it is invoked has to be predictable by the programmer. (An old, familiar example of this is mandatory tail-call optimization in functional languages, which gives you asymptotic space savings and which is very predictable in its effect.) 

Enforcing purity is actually an important part of the story for fusion, because it simplifies the performance model for the programmer. Even if you could do a very fancy whole program dataflow analysis to detect side effects, a programmer couldn't easily reason about when the analysis succeeded or failed, so the optimization would be less useful.

I am personally very slightly skeptical of the specific GHC mechanism (the rules pragma), because I'm not quite sure when ghc's rewrite rules will be invoked. Will the compiler figure out it can apply a rewrite to a locally let-bound or curried function application? This is unclear from the manual (because the application of a rule can apparently depend on what other optimizations have been performed). But it's still pretty good!Good point.  Before I even knew what haskell was about, this was a feature of my 'dream language' - explicit markings of pure functions, annotations which propogate to functions that only use pure functions.

In defense of haskell, though, rarely when you deal with IO will side-effects truely have no way of affecting the function's result.  For example, lets say the logger routine errors out, or continues indefinitely.  Technically, this means the function returns bottom, undefined.

I agree somewhat though, assertions of believed behaviour (giving the compiler a more detailed 'understanding' of IO) might allow some interesting things.That was a good move on VLC's part. 

Let me put it into context, if you are doing something really graphically intense, you need to get as close to the hardware as possible, in that case, the wrapper that wxWidgets provides, will cause a noticeable degradation in performance, especially for video.

Otherwise, wxWidgets is fine. Eg [Audacity](http://audacity.sourceforge.net/), which is the open-source audio-editing software, which doesn't really need intensive graphics.

The other reason is, if you want to allow extensive customization of the themes (thus voiding a consistent UI with the operating system). Given Firefox's modus operandi of defining everything in XUL, they have to draw everything from scratch.

Well there are two issues: can a failed IO cause the computation to fail and does the IO cause side-effects that makes reordering of operations dangerous. For the most part, Haskell deals with these separately (as it should, and as other languages do). Even side-effect-free functions can fail because of divide-by-zero or "head" of an empty list etc. Similarly, side-effect-causing functions can be made to always succeed with sufficient error checking and fallback behaviours.I'm not arguing against a straw man -- you continue to maintain that having a preference on the basis of aesthetics is irrational! And I continue to disagree. 

If the pretty IE condom gets you laid, and the ugly Firefox condom doesn't, is it irrational to choose the IE condom? 

Are you really, really sure? Or are you making judgments about the relative values of avoiding babies, looking good, and getting load that are personal to you?

Put another way, you are saying *functionality* is **better** than *appearance*, which is a moral argument, and has nothing to do with words like "irrational". 

Moreover, in a lot of cases (though not condoms, I admit), I personally think the right choice is to value appearance. But that's just me. And that's a different argument.SAP opens a new content area for its SAP Developer Network (SDN) and Business Process eXpert (BPX) community members. You can subscribe to video recordings and other premium content from the latest SAP TechEd conference.support costs are too high.Nice post. But how does this work with Winamp and iTunes? These don't use native UI on Windows, but I don't know anyone who would choose anything else on that platform.All abstractions leak. 

That which does not leak is not an abstraction. 

Adding another abstraction layer therefore multiplies leaks. And usually is just plain ugly, slow, and sucky. 

Even ".net", a uni-platform virtual machine designed by insiders to the OS, is only prettier, not any less balky, unreliable, and nightmarishly versioned. 
&gt; that businesses will need far fewer computers in the future to fulfill their demand

To fulfill their demand of the present, but demand increases with what computers and technology can provide, I thought everyone knew that.I once saw a paper titled "Continuation-Concious Compilation" where a compiler technique was described which could infer that (map f (map g xs)) == (map (compose f g) xs) without the compiler being aware of 'map' or requiring any rewrite rules. Basically it would first transform your program into CPS form and specialize compiled functions based on continuation, much like many compilers specialize functions based on type.

Such an approach might be better than explicit rewrite rules. On the other hand, it might not work for anything other than the most trivial examples.We do need better documentation. It's just that features being added to FP languages are so bleeding edge often the only reference for them is research papers themselves.[deleted]Every discipline has a specialized jargon.  It's what makes communication efficient; you know that certain words stand for certain well defined concepts.
FP is no different from other fields, except maybe you are more familiar with other jargons as the FP one leans somewhat more towards mathematics.
Yeah, it will work with let-bound or curried applications. It's fairly robust in practice (particularly since GHC 6.6).The technology is important, not it's interface.  Searching using Google is just as easy and useful in lynx without a gui.Keep in mind that this is a work-in-progress.  Just because the next version of Firefox isn't 100% Mac native doesn't mean they aren't working towards that goal.Plus, they didn't even write it in Haskell...or is Erlang popular this week?[removed][deleted][deleted]Formally, the best start is Matthias Felleisen's _[On the Expressive Power of Programming Languages](http://citeseer.ist.psu.edu/felleisen90expressive.html)_, and John N. Shutt's two [abstraction theory](http://web.cs.wpi.edu/~jshutt/abstraction-theory.html) papers.

The gist of it is that you can arrange languages (considered as the whole of their features and libraries/libraries) into a partial order, where each one can be losslessly mapped to the ones above it, but not the ones below it.MacGyver is real dammint!Thanks for bringing this into consideration.

Since the basic parallel unit is a parallel function call to achieve better parallelization all the heavy computations should be done inside of this functions, rather than in the main program. 

I agree that callback feature may simplify the task of programming and sometimes even improve parallelization efficiency. 

And finally, parallel python since version [pp-1.3](http://www.parallelpython.com/component/option,com_smf/Itemid,29/topic,19.msg40#msg40)  supports callbacks:


And here is an [example of parallel python aplication with callbacks](http://www.parallelpython.com/content/view/17/31/#CALLBACK)

Thanks for a great feature request!That sounds really interesting!  Was it [this one](http://citeseer.ist.psu.edu/queinnec93continuation.html)?

Addendum: [This](http://programming.reddit.com/info/14a3f/comments) is highly related, too.On a related note, here's a literate Haskell program that implements a simple JPEG decoder:

http://www.imperialviolet.org/binary/jpeg/Okay, I'll bite. This is not the sort of question one usually likes to answer, because the meaning of "power" is extremely context-dependent, and you really should be asking someone in particular. I'll tell you what it tends to mean when I say it, though I usually prefer to use more specific terms, or use it when talking about more specific parts of a programming language.

I would say that direct expressive power mostly has to do with the ability of the language to *eliminate* design patterns as they arise. That is, to factor out common patterns in code and turn them into, say functions, or other named constructs. In this way, they can be reused by plugging in only what differs between their instantiations.

On top of that, the language itself will generally factor out a bunch of patterns for you, relative to some of the languages which came before it. The quality of these abstractions is also important.

I don't usually consider the present existence or non-existence of libraries as having too much weight when evaluating a *language*, as they're something which can be constructed, but the *potential* existence of libraries of various classes is pretty important. Not being able to construct various abstractions in an elegant way as needed for certain libraries is a bad sign. The availability of powerful libraries is certainly something to be considered when choosing a language for a project, it's just that I tend not to mix that idea up with the power of the language itself.

In addition to this direct form of expressiveness, there's also the matter of how good a language is at permitting the programmer to express what programs *should not* do. This is often overlooked, but really is quite important. Having what is essentially a theorem prover go over your code and check that it absolutely will not go wrong in a variety of ways saves you piles of testing and debugging, and aids you quite considerably in actually thinking about your program, and also in guiding your thoughts when it comes time to use or extend someone else's code. Of course, you have to be careful about this, and there are many design factors to be considered, but I generally count good type systems as contributing to the expressiveness of a language rather than taking away. There are obviously many design factors involved, and it's possible to design type systems which get in the way more than they help, but good type systems really do help quite a lot in expressing ideas effectively. (Type inference helps immensely with this, as it permits the programmer to express only what is very important, while allowing the rest to be inferred and still checked for consistency.)&gt;it bangs huwey

??? I think that should be "hangs a U-ey".  Unless the bee was into some kind of kinky train sex or something.&gt; NO SIR... I am right on both things...

No

&gt; I don't tolerate this kind of crap.

I don't care

&gt; I'm going to have a public name someday too, I hope....

Not likely

&gt; This is segfault's weak attempt at humour

You wouldn't know humor if it threatened to kill you

&gt; which thinly veils a death threat.

No, it doesn't. You are a paranoid schizophrenic. Kill yourself.

&gt; A death threat, in no way shape or form, means that the person who commits verbal assault intends to follow through with it.

threat

–noun

1.\ta declaration of an intention or determination to inflict punishment, injury, etc., in retaliation for, or conditionally upon, some action or course; menace: He confessed under the threat of imprisonment.

2.\tan indication or warning of probable trouble: The threat of a storm was in the air.

3.\ta person or thing that threatens.

&gt; But that doesn't make it any less what it is.

Ah, but it *does* make it more of what it isn't.

&gt; Hold yourselves, your acquaintances, and those in your society to social standards, please.

I do. My social standards require that I call you a retard now.

&gt; Good day, sir.

Eat shitIt's still not so robust that most people should be thinking about authoring rewrite rules. The interaction with compiler optimizations like inlining can make for some hair pulling. But for core libraries that really stand to benefit from rewrite rules, it's definitely worth it!&gt; I'm confused.

The solution to that is beyond my or anyone else's means

&gt; Is Lisp or JavaScript/Excel suppose to be the imaginary one that only works on TV?

Binary illogic -- your native tongueWow. I actually relearned more spanish in the few seconds reading that page than in the last couple months.Like I said, I did my degree in philosophy. Got a ton of formal logic from that, and I almost pulled off a minor in math while I was at it, so I'm not without some experience in this stuff.

I fully understand the utility of jargon. But there's this big push lately by FP people to get it out where more people can see it, and that's going to fail horrifically unless somebody can sit down and sort out the jargon problem in a way that people who don't have math degrees can understand.Your reply has been superseded by a newer, more efficient, linux-specific one.Yeah, it's either ivory tower or the crack pipe. No middle road.Once I figured out the dumb stuff (turning off the profiler, and so on), things generally worked the way I expected. All in all, I think GHC's support for rewrite rules is in pretty good shape.

But I agree that, at least for now, this is something better left to library authors (and people goofing around with optimizations).True . . . but I still like Ruby a heck of a lot more than Python, and the Python community has its own aggravating hang-ups.He never does.&gt; Why can't the programmer just declare that the functions are side-effect free?

Aye, Mercury allows this, through its (really neat) notion of [purity, semipurity, and impurity](http://www.cs.mu.oz.au/research/mercury/information/doc-release/mercury_ref/Impurity.html#Impurity). Mercury, a purely logical programming language, is at least as amenable to this optimization as Haskell.The initial impact of your software/applications is the design and ease of use.

The functionality is more important thereafter.

Google has alot of ease of use going for it.  It won the search battle because it wasn't a crowded portal.  

&gt; You can not decide to use side effects in your program locally if you are forced to use a monadic style.

It's the other way around:  allowing things like localized state/side effects is pretty much the *point* of monadic style.What I meant is that you can not just decide to assign a value to a global binding in the middle of your code without restructuring your program, so that the decision to use global side effects can not be made locally. I made the mistake of placing the 'locally' too far away from 'decide'. See Felleisen's paper on expressiveness for more details (and a formal proof).

I made an analogy above to the CPS transform. One can use a "local CPS" transform to implement call/cc as long as one is guaranteed of the scope of the continuation. Stalin uses this technique to implement call/cc, and I believe MLton uses this technique to optimize nested loops. However, that doesn't mean that the CPS transform is a local transform. It is only as local as the extent of the continuation, much as the conversion of side effects to a monadic style.&gt; The ultimate goal is nothing short of replacing GNU/Linux with "GNU/Solaris".

Well, that's not a bad thing. Solaris 10 has a lot of innovative stuff in it and doesn't have a prick like Linus Torvalds at the helm. So long as Solaris is under a free license, this is a good thing.I understood part of that, but not the rest. Specifically, I understood how continuations/throws correspond to ~a... but I don't quite follow how catch, aka call/cc corresponds to ~~a.

Could anyone explain that? thanks.You need to relax a little. Take a few deep breaths. Go for a walk or something.
That's one issue I have, and then there's the other: no REAL buttons in Gmail. I can't hit tab-enter to send an email. It may sound like a retarded thing to bitch about, but it's a hassle to switch myself from doing one thing in EVERY BROWSER. I know it's probably more Google's fault than Apple's, but still. I love Firefox on Mac. With the Del.icio.us plugin.jEditThat is a bullshit moral of the story. What evidence is there that confirms that this happens "often"? Way to go. Overgeneralize everything.Yeah. I was actually thinking of the library writer's point of view. The library dev writes the library and knows deeply the algebraic transforms that hold. So they provide rewrite rules to be inherited by application developers using their lib -- self optimising libraries, in essence.

It's a great tool to have in the data structure library author's kit.[deleted][removed]Well, I agree that the interview process is an important one when assembling any type of team, but the kind of typical cliched questions such as the one that is the topic of the article aggravate me.

When they are asked so often, people learn to give prepared (fake) answers that will most appease the interviewer, such as the three options you stated.  In my opinion, that defeats the purpose of an interview which should be to get to know the candidate quickly in order to judge whether they would fit into the group.  I don't know... I guess I just hate any corporate management process out there, starting with interviews.  That's one of the reasons I dropped out of business school.

Edit:  Oh, and as far as Tool School, I mean a school for tools.
tool (n.) One who lacks the mental capacity to know he is being used. A fool. A cretin. Characterized by low intelligence and/or self-steem.
From UrbanDictionary.comYes, and because of that abstraction, it's something which [the library authors can do](http://www.cse.unsw.edu.au/~dons/papers/CSL06.html), rather than having to hand optimise things yourself.Press the Esc key to stop them. Works in Firefox, IE, and possibly others.[removed][removed]And that relates to the conversation how?So long as the UI's follow certain basic standards like communication with the clipboard, I've never really cared whether my programs all look the same. Sure, it looks pretty, but doesn't usually buy you all that much more. I'd take a system with lots of different UI's that all work great for their particular application over one in which every program had the same bad UI.

That's not to say that the standard OS X UI is particularly bad, or that Firefox's UI is any better, but I've never found the difference between them so great as to actually care about it. Most of my time spent using OS X, I was using emacs and/or vim and a whole bunch of terminals anyway. I really couldn't care less which widget library my terminal or text editor uses.

The bigger issues I'd complain about on OS X would be simple things like not having point-to-focus and multiple desktops built into the standard UI at least as options. (Exposé is great, but it doesn't replace multiple desktops as a tool for keeping windows organised -- repeatedly hunting through a mosaic of similar-looking windows for the ones you want is a pain.)

[Desktop Manager](http://desktopmanager.berlios.de/) and [Quicksilver](http://quicksilver.blacktree.com/) help improve things a lot, and I'd recommend them to anyone using Mac OS X on a regular basis. Quicksilver in particular is interesting, since it allows you to get to windows, files, applications and operations you commonly want with an ever-diminishing number of keystrokes as it learns which things you're more likely to be looking for. It's a good complement to Spotlight, which seems better at looking for lots of resources when you're not completely sure what it is that you want, rather than getting at a particular resource instantly.

I never found a free solution to the point-to-focus problem.Authors have always had the ability to hand-optimize their own libraries. What is new here is that:

1. Others have the ability to give the compiler hints about optimizing libraries for a specific context.

2. The hints are *readable*. Optimization is a [separate concern](http://en.wikipedia.org/wiki/Aspect-oriented_programming) than implementation.&gt; We don't know that anybody else was not. We don't know how many people are licensing this patent.

Well, lets look elsewhere. Has anyone claimed they were actually threatened by Microsoft for any specific patent? Have there ever been any lawsuits started by Microsoft over licening or patents?
That's what makes me think it doesn't work. While it may not be harmful, it doesn't seem to be beneficial enough to overcome the difficulty of cross-platform development.

Of course, I'm just talking about it from the "abilty to swap out the view" perspective. The other claimed benefits may actually exist.They had the ability to internally optimise their functions -- they didn't really have the ability to optimise the ways in which you combined their functions. This sometimes resulted in either suboptimal performance from using a library, or else needing to rewrite functionality otherwise provided by the library so that it can be cleverly intermixed.Thank you.Try [Quicksilver](http://quicksilver.blacktree.com/).Ah, yes you're right, libraries can now design their interface purely for abstraction rather than, for example, having a bunch of variants for different contexts.

Hmm, I wonder what the relationship is between map fusion and partial evaluation.This is the whole point of monadic encapsulation with the ST monad (and others). Encapsulte your side effects locally, inside the monad, so they don't leak. The ByteString library in Haskell uses this to do mutable updates of buffers in the style of Clean's uniqueness types, while on the larger scale, presenting a pure interface that is indeed pure.

Encapsulate your impurity!You've just been added to my killfile.  Is that considered a death threat as well?&gt; Could anyone explain that?

As he says, throw has type a -&gt; bottom, which is just a synonym for ~a. An application of catch looks like

    catch $ \throw -&gt; ... (throw a) ...,

so catch's argument must be a function consuming an ~a. Since this argument function should produce an a in either case, whether it returns or throws, it follows that it must have type ~a -&gt; a. Thus catch has type (~a -&gt; a) -&gt; a = ((a -&gt; bottom) -&gt; a) -&gt; a = ((forall b. a -&gt; b) -&gt; a) -&gt; a. This is just [Peirce's law](http://en.wikipedia.org/wiki/Peirce's_law) of classical logic expressed in type theory, which is known to be equivalent in power to double negation elimination, ~~a -&gt; a. I can easily derive (~a -&gt; a) -&gt; a from ~~a -&gt; a by instancing one of the quantifiers, but I'm having trouble with the converse--I do know that Peirce's law and double negation elimination should be equivalent, so there must be something obvious I'm missing.

So, I think maybe he's wrong about catch's type being ~~a, because it looks to me like it should be more like Peirce's law; or maybe ~~a -&gt; a, assuming that equivalence above can be demonstrated.[removed]Assuming you have a way to encapsulate effects, then there's [no need for there to be a performance hit](http://shootout.alioth.debian.org/gp4/benchmark.php?test=all&amp;lang=ghc&amp;lang2=sbcl).

And if the language is pure by default, people tend to *try* to write pure code, leading to more opportunities for fusion and related optimisation (at least in my experience).
He may have some valid criticisms there, I didn't spend the time to read it in depth, maybe later, but he doesn't seem address the lack of school performance among the millionaires sampled.  If it is just luck that they become millionaires, you would expect academics to receive an equal share.  Instead, they only show up in those professions which require it, law and medicine.&gt; libraries can now design their interface purely for abstraction rather than, for example, having a bunch of variants for different contexts

this is *exactly* what we found in practice when implementing the Data.ByteString library.

For example, the general function "filter" has a nice fusible implementation, but we used to provide "filterByte" for filtering a specific byte. This was faster for this special case, than the generic higher order version. 

However, adding the rewrite rule:

      "specialise filter (== x)" forall x.
           filter (== x) = filterByte x

Totally removed the need for the filterByte function to be exposed! It was removed from the api, since now users need write only "fliter (== x)" to transparently get the fast "filterByte x".

Domain specific optimisations like this, that are invisible to the user, greatly simplify the special cases in the API.But, I thought ~~a is supposed to correspond to type ~a-&gt;a

not (~a-&gt;a)-&gt;a

and he was making the associacion of catch and ~~a?

Or wait... are you saying it's catch's _argument_ that corresponds to ~~a, and not catch itself?

(Sorry, I'm not actually a CS person or anything, so am probably missing obvious background info and stuff..)

Oh, and thanks&gt; Programmers educated in computer science are not the norm in that area.

Can't say I'm surprised. The best programmer I have worked with actually has a degee in marketing. The CS professors I worked with, on the other hand, got so caught up in the framework that the project failed.

&gt; Also, I've found that these financial types are very doubtful of programmers.

Can you go into more detail?
Well, all I've totally convinced myself of is that catch has type

    ((forall b. a -&gt; b) -&gt; a) -&gt; a,

because I arrived at this independently through the reasoning outlined in my message. Only once I was done did I realize that I was staring right at Peirce's law. That's way too unlikely to be a coincidence, especially considering its link to double negation! But I'm not sure how to go from it to ~~a -&gt; a right now.Well, your reasoning has convinced me of it as well, thank you. Well, almost. I'm not too familiar with the "bottom" concept. It seems to be a symbol that's analogous to "the set of every false statement", if I understood it correctly.

but you seemd to transelate it to "every statement"... ie, you turned it into "for all b"

And thanks again.&gt; but you seemd to transelate it to "every statement"... ie, you turned it into "for all b"

From the vantage point of logic, you should think of bottom as meaning "absurdity" or "contradiction". The idea is that if forall b. a -&gt; b, i.e. for every b you can prove that a implies b, then in particular a implies a contradiction. And conversely as well, for if you can prove a falsehood then you can prove anything, i.e. forall b. bottom -&gt; b. By composing that with a -&gt; bottom you get forall b. a -&gt; b. I think that's right, anyway. :)Oooooh, I see. Thanks.People who prefer
Gentoo to Ubuntu
are 9.0 times more likely to prefer
outdoors to indoors .

All that compiling keeps them outside.I wonder, do hints such as this further the case for there to be only one way to do the next level of complex tasks? For example, are there other idiomatic ways to filter byte that byteString needs to keep an eye out for?&gt; ...since any moderately seasoned Haskeller knows that, if anything, it does quite the opposite.

This is obvious nonsense.  Did you mean to say something like this?

&gt; ... as lamenting about this restriction is almost as silly as someone lamenting that Lisp 'forces you to think in parentheses'.  Haskell isn't as restrictive as you think it is, and morever the flexibility and tools it gives you afterwards more than make up for the 'bondage and discipline' of preferring that you write pure functions.&gt; Ah, yes you're right, libraries can now design their interface purely for abstraction rather than, for example, having a bunch of variants for different contexts.

This general concept of separating core meaning from optimizations deserves much more attention from programming language researchers than it has received so far.

One real-world instance of the concept can be found in relational database systems. Various kinds of indices and hints on access paths and query planning are specified on the side in order to influence performance, while queries themselves are written in a purely declarative style. You still see people mangling their queries and data models in the name of performance--denormalization is the usual suspect--but that is more a fault of currently available RDBMS implementations than any inherent limitation.

In an ideal world everything related to performance tuning should be specified "on the side". That might be an unachievable goal, but we can certainly get much closer to it than we presently are.[removed]_"In an ideal world everything related to performance tuning should be specified "on the side"."_

One of the ideas I've been toying with recently is a language where we separate a purely functional specification from the 'memoization strategy'. Swapping in different memoization strategies can change time and space complexity without actually influencing correctness. All data structure selection decisions are part of the memoization strategy, of course.[removed]Does anyone else think it's wrong to have a computer sitting at
the "imperative" end of the line and a human brain sitting at the
"declarative" end?
No, I didn't mean that. I meant what you think is 'obvious nonsense', but I'm not prepared to defend the position - not on an internet forum anyway.&gt;So, instead of working on functionality, developers should spend 99.9% of their time replicating every tiny bit of the GUI on each platform?

No, they should use native UI widget implementations. TFA does not claim what you argue against. Strawman ;-)&gt; I meant what you think is 'obvious nonsense',

Then you've emitted obvious nonsense, as I said.

Nonsense because -- duh! -- Haskell -is- a bondage and discipline language, and you -can not- program in it unless you make personal adjustments.

Obvious because -- contrary to your qualification -- these attributes do not require 'sufficient experience' with Haskell to perceive.  (My kind suggested-response *does* require experience with Haskell, to see that these restrictions are reasonable and cheap.)  'Haskell is a bondage and discipline language that requires -- not allows, but requires -- me to structure programs in a new and alien way for whatever reason' is as perceptive an assertion about programming languages as 'that apple is green' is of apples.

&gt; but I'm not prepared to defend the position - not on an internet forum anyway.

If you've such contempt for this 'internet forum', don't fill it with obvious nonsense.  You certainly won't persuade someone against taw-like dubiousness about Haskell with such.1. Because huge user base is Windows-only.
    2. (distant second) because it's expensive to do.

Combine the two and there we are.
I agree that MVC does not works in practice, for "economy" reasons. On the theoretical merit, there is nothing wrong with it, though (as you probably think, too).The very first criticism addresses the idea of "equal share."
Well-educated people may make up a small fraction of millionaires,
but what fraction are they of non-millionaires?

It wouldn't surprise me if becoming a doctor or lawyer was the most
common way of turning an advanced education into money, but why do
you feel it doesn't count?
&gt; unless you make personal adjustments.

My son would disagree - never having to make any so-called 'adjustments', since Haskell is his first language. My contraposition first requires you to define 'discipline', but you seem to be set on the conclusion anyway, so why bother arguing? Who's to gain?

&gt;If you've such contempt for this 'internet forum', don't fill it with obvious nonsense.

You might mean, with 'what at least one person' believes (and another doesn't) is obvious nonsense. In any case, I won't yield to your command. Your authority is not recognised by me.

I hope it's over now. Neither of us is to gain here.No screenshots?&gt; My son would disagree - never having to make any so-called 'adjustments'

Your son, not being a programmer, is not qualified to disagree with, as I've said, a perceptive quality of programming languages.  After he grows up and learns more than his father about programming languages in general, he may look back and agree that Haskell is such a language.

Please go to #Haskell and seek their opinion on 'haskell is a bondage and discipline language'.  Someone in that diverse channel can surely shed more light on this subject that you

&gt; I hope it's over now.

sulkily refuse to investigate.  -- to the detriment of any discussion like the one above, where someone makes a plain observation and subsequently writes you off as a cultist after you say 'nuh-uh!  The apple is not green!  I gave my 2-month-old son these apples and he did not say anything about them!'No. The Von Neumann architecture is imperative, and the human brain is declarative. I acknowledge that the second statement is far more debatable than the first.&gt; But it does so in ways which don't require multiple post-graduate CS and math degrees to understand

Amusingly, the most obvious case of this is in the Python 'dictionary', modelled after Perl's hash but without the terrifying(?) prospect that somebody learning Python might get lost in Wikipedia looking for hashing algorithms and O(1)-access associative lists.

Perl newbies manage to use hashes without needing to understand those things, however.Well, I've more respect for the ability of tthe human brain to grow and learn to simulate imperative thought than I have of the computer to do such a thing.  So this little-bothers me.  If it bothers you, please [look for something interesting here](http://en.wikipedia.org/wiki/Category:Declarative_programming_languages) -- particularly in the subcategories.
I'm a happy Basecamp user and I don't think it sucks; it's very simple but that's the point.
Would you setup a cokesucks.org website to complain that it doesn't taste like Pepsi?
Best analogy I've read all day.What a bunch of bullshit. He knows some vaporware non-existent language that the clickfraud empire that is Google is cooking up and we're all going to use it because Google made it or something? We're supposed to blindly accept his completely empty subsequent claims as fact?

When did he get a magic 8 ball?Well, I've narrowed it down to Perl6 or possibly Pike.

But -- ECMAScript?  Well, it's not awful.Gosh, he doesn't mention Google anywhere.  Also, we aren't 'supposed' to do anything.&gt; Exercise: a ∧ b = ~(~a ∨ ~b). What does that mean in the above context? (I haven't got my head around this yet.)

I think I figured this out.

Write out the right-hand side type in detail, expanding the negations:

    ((a -&gt; bottom) \/ (b -&gt; bottom)) -&gt; bottom

An (x \/ y) -&gt; z is actually the same as a pair of functions in disguise. One is of type x -&gt; z, the other of type y -&gt; z. In the above context that lets us massage the right-hand side into

    ((a -&gt; bottom) -&gt; bottom) /\ ((b -&gt; bottom) -&gt; bottom),

Apply catch to the left component to get an a and to the right component to get an b. There's your a /\ b!How else could he have a 'tip' or any other kind of inside knowledge? He works for Google and claims a language that becomes big has to have a big corporation backing it.

Anyway, isn't dynamic typing with optional static checking impossible?'And there are two other platforms that NBL will run on which, more than anything else, are responsible for its upcoming dominance, but I'd be giving away too much if I told you what they were.'

The web browser and the shell.How fast is Perl 6?They _imagine_ it's pretty damn fast. Har har har.Dynamic typing with optional checking is possible. Not in all circumstances (e.g. static checking of code contained in a string, to be passed to eval()), but when you look at duck code (ruby/python/php) most of the time it's pretty easy to 'guess' what types will exist at what point.

That's a good enough approximation.Inside knowledge? Did you read the same article? He's clearly acting this way because people ask him his opinion on the languages he's working on, and the sane reply 'I have absolutely no idea' is just so very boring. So he presents himself as a very fake authority on the subject, illustrating how silly it all is.&gt; Anyway, isn't dynamic typing with optional static checking impossible?

No, see the python (no relation) inferencing compilers of CMUCL and SBCL, and Common Lisp's own support for such things.

FWIW, the downmods on schwarzald's comment here (-5) are absurd.  Please try to downvote -posts- when they -really deserve it-, and not -people- when they -happen to make an inflamatory comment anywhere this reddit post-.This is an anonymous OpenID server where everyone can create an account without signing up or giving a password. By using an OpenID URL such as http://www.jkg.in/openid/1234, you will be able to log in into OpenID secured apps without authentication.

It's a bit like [BugMeNot](http://www.bugmenot.com/), but for OpenID powered sites.Assuming that NBL is ECMA/Java/ActionScript 4, one of the platforms is surely [Adobe Apollo](http://labs.adobe.com/wiki/index.php/Apollo). The other one might indeed be the browser, but it might also be some kind of mobile software stack providing a lite version of Apollo.No, it's pretty much spot-on, the Von Neumann model is completely imperative and is the one on which our computers are based on while the brain is much fitter to handle declarative structures and match patterns.I always saw flames about Dynamic Typing Languages vs. Static Typing Languages. But very rarely have I seen people giving REAL evidences. All I see is some kind of "talking", that is, they just **assert** that Dynamic Typing Languages is better, agile, being able to accommodate change more quickly, and so on. Take ruby for example, I know that ruby has create a huge amount of fuss, and I always see people talking about how good it is, comparing it to static/traditional languages such as C/C++. But, problem is, almost never have I seen even a line of code, not even to mention use-cases, real development senarios, etc. And watching people do that again and again just make me feel sick. I would really want to embarace something if you can convince me how good it is. So, let me be clear, I want a REAL debate, in the sense that every judgement shall be backed with bloody code - real case, reasonable code. No vacant talks, no flames.
Tk sucks hard on look and feel in older versions, but newer versions using Tile are quite ok on the eye, see:
http://tktable.sourceforge.net/tile/

But your right that a lot is missing, one fact is that some things are really hard to even define cross platform. One example: modal vs. non-modal dialogues, Windows uses modal dialog boxes for very many standard features like color or font selection, OS X uses non-modal dialogs. You cannot implement this feature in a sane, cross platform way, without violating the platform guidelines for one platform, unless you make your app behave totally different on two different platforms.
The toolkits cannot help you in this case, other than providing easy to use access to the functionalities you need, but you need to decide how you adapt your app to the platform differences.

From my POV Tcl/Tk does a really good job at abstracting away the things that can be abstracted, but stops at the point where an abstracting would be too leaky, like forcing modal vs. non-modal, or trying to fake fork() on windows and other non-portable stuff like that, which is by design forced to be in platform specific extensions like TclX, Twapi and the various OS X packages.
I don't disagree with you that asking cliched questions defeats the purpose of an interview. But if you're on the receiving end you need to know how to deal with it in a way that won't ruin your chances of the job. The fact they ask it doesn't necessarily mean it will be a bad workplace. Interviews are high-pressure and small organisations don't get very much practice at them so they're likely to have rough edges.

The business school bit is interesting. I discovered a harvard management course on my workstation at my new job at BigCo. I've been flicking through that. I doubt I'd have the patience to do a course like that though."look up the type coercion rules"

What did you do that for?  Why are you coercing types?  

"it amazes me people get things done"

They get things dones cause they don't waste time looking up things that don't matter.irb: (10..1000).each{|i| break i if ((2**i).to_s[-4] - ?0) == 7}
    =&gt; 21

A one-liner in ruby.Well, part of the reason "static vs. dynamic" is the new Emacs vs. vi/Mac vs. windows/whatever is because it's so ill-defined. Different static/dynamic languages implement a general typing strategy differently. 

Popular static/dynamic languages often have certain sets of features that have nothing to do with when types are determined. These features are often a significant part of what makes people like/dislike something.

Also, when people argue, some of them evaluate something just on the merits of how it presumably helps programmer productivity. Others are only concerned with how that would play out if done today in a large corporation with high turnover.

So the debate is all fucked up to begin with. People are thinking all these other things besides what's being said.Huh, almost nothing about concurrency.

In the next 10 years, we're going to see more and more multicore machines, simply because clock speeds are static while transistor count is still increasing exponentially.

I predict that the chip vendors will try to keep their fingers in this dike as long as possible. But once it breaks, we'll be dealing with 40 cores before we know it.

So, while we're making predictions, I'll throw in my two cents: The next big language will deal much better with concurrency than Java, C#, etc.

Yegge's proposing a very small incremental improvement in the current state of the art, basically a Dylan with more conventional syntax and a better compiler story. I can see that happening if the language is a new revision of EcmaScript. But otherwise, you'll need at least a factor of two improvement beyond Java and C# to get any adoption.&gt; So NBL will have to do something clever about eval.

Look at Lisp. It's not going to be the NBL, but it's dynamic (including eval) and fast.You also get crappy syntax for free!

I'm firmly in the "school" that programs should be written for programmers not computers. 

Also speed is so very overated for 90% of applications.  

Typically if you need speed, you need all of it. Which means 100% of C.  If you don't need speed then it doesn't matter how fast you are (within reason, say 10x slower than C)

I totally don't understand why you don't send them the autogenerated password instead of futzing with the token and link in email.How about a simple Python interpreter in Python?Yeah, the whole thing is pretty specious.  

The choice isn't between *a-great-site-with-great-content-but-nonstandard* on one hand and *standards-compliance* on the other.

The only real choice is between clean html and not-so-clean html.I first started using Ruby about 4 years ago, and since 2 years ago, Ruby has been my main programming language. I can't discuss everything that's possible with Ruby here, as it's a very broad subject. But a couple of differences is that with Ruby, my files can reside anywhere in the file system and they don't need to be compiled to work. Also, Ruby code can be created at runtime with the eval command. That is, you can adapt code at the last moment, and cache it in memory when it's ready to be reused, as I do in my web framework. When I edit the file again, the code is updated automagically in the next request, by the last modified date of the file. Even for GUI programming, I don't need to compile anything. BTW, I generally hate XML, so XML is nowhere seen in my applications/libraries, generally, except for things like RSS feeds. :-) Ruby has a very flexible syntax, which when coupled with the other properties like the ones above, can provide plenty of flexibility for creation of supporting files, like configuration, MVC's View, Model, Controller, etc, all from Ruby itself. It's dog fooding all the way! If you allow me, eating our own dog food is the best way when trying to create something.

My whole site is based on Ruby, and it's very modular, even though from the outside it looks just like anyone's:
http://www.ralip.com.br/jp/blog/

Here are some screenshots as well, for GUI programs:
http://www.ralip.com.br/jp/wiki/?p=screenshots

And here's a listing of my main project directory:
dewd@corvina:~/workspace/pilar$ ls
account_rules   bookmark_rules  conf         gtk_rules       report_rules      web_rules
bayesian_rules  business_rules  db_admin     gui             ruby_editor       wiki_rules
bin             command_rules   db_rules     project_rules   session_rules
blog_rules      common_rules    forum_rules  question_rules  translator_rules

Ruby allows for new and sometimes improved "Design Patterns". Things can go beyond simple OO in Ruby. My own programming goes beyond OO when it makes sense. Ruby is going to get a speed improvement this year, with the officialization of the YARV (Virtual Machine), hopefully with the support for native threads!!! Also, Matz has been working on the multilingualization (with Unicode) support for Ruby, which might be ready in the new Ruby 2.0 as well.

But after Rails, one of the most impressive changes in Ruby has been in the community: it's bigger than ever, and speaks more English than ever. Portuguese is my first language, but for computing, English is fundamental, and the head-start Python, Perl, Java, etc, had in their English support has been mostly offset in the past two years, with all the blogs/books/articles/talks in English we have now in Ruby.

Ruby is not perfect, and it differs from other languages when it adopts a very dynamic approach. Some errors can hide in Ruby code because it does not get compiled/error checked all the time. Ruby in its default form will never be quite like the static typed ones, even though more tools might be developed for Ruby to help with the error checking for instance. One should consider Ruby a paradigm shift from the mainstream languages, which might work better under some conditions (web, scripts, etc) than others (proprietary desktop applications). If someone can make Ruby cross these boundaries, maybe with encryption or even some very smart native compilation, I am sure there will be a market for it. Just don't expect Ruby to fix itself for your needs. :-)It's hard to tell weather this was a satire piece or not.  On the one hand, it seems very unlikely the Steve Yegge would have the ability to know exactly what the next popular language will be, but on the other, the language he describes has a lot of features that would make it acceptable to the general programming populace (C-like syntax, GUI, etc).This technique is not exactly new. It's done via compiler macros in Common Lisp.Do-syntax is important. Conciseness of code is one of Haskell's mayor selling factors. If operations had be be written out every time, it would make monads less attractive. And I've also seen lots of 'proc's in nearly every code which used Arrows.

And the 'scrap-your-boilerplate' approach for example is no pure typing. Sure, you can create all the boilerplate by hand instead doing a deriving(Typeable, Data) - but wouldn't that negate the idea of scrapping the boilerplate if you have to write lots of it yourself?

Also changes/extensions on the Type-system are impossible to do in Haskell without changing the compiler. To do message passing in a way that it's both concise and high-performance, changes to the type system are quite likely because only this way the compiler can do optimizations and detect errors by reasoning about the program. While it's always possible to create simply ad-hoc message-passing based on a lib such an approach is comparable to doing functional programming in C++ or OOP in C and thus inferior to a language where such a paradigm in build right into the foundations of the language.
[removed]No, but I might analogously put up a cokesucks.org if coke were run by sneering jerks who don't want to program even the simplest utilities for paying customers, and who resent the request.  That's the point this site tries to make; it doesn't say anything about how Basecamp differs from some other service in a trivial sense.No, it keeps them outside, because they use such heavy optimizations and such hurried concurrent builds that they can't use their computer for its intended purpose of -compiling more programs-.  What is a computer so loaded on compiles that you can't load compiles on it?  Something you come back home to in a few hours or days, after you've run a marathon and battled with ninjas in the park.Tell us what you use, in what circumstances, and we can begin to start to think about how to consider the propect of calling you a bloody fool who will die on a deserted planet because you decided to load a custom module into your hyperspatial navigation system written by semiskilled mass labour in your awful language of choice.

-- oh, wait.  You want a debate on 'dynamic versus static typing'?  Gosh.  Well, you still need to narrow it down -- if anyone mentions Java or C++, everyone else will just sneer.  Morever, the space is more complex than {dynamic,static}: what do you think about CL systems with their dynamic language with type inference and static analysis?

I'd say, oh, that you want a debate between the merits of:

1. CL-style typing - mostly dynamic with floating islands of very statically optimized code

2. Haskell|O'Caml|Mercury|&amp;c typing - mostly static with floating islands of dynamic code -- and very rich, extensible type systems

3. Perl|$your_favorite_nonPerl|Erlang|&amp;c - limited set of core types and then a jungle of ad-hoc types with no compiler support or inference -- although Erlang and possibly Python, that I know of, have good static analysis tools here

4. and, oh, Q [(q-lang.sf.net)](http://q-lang.sf.net/), because it has much the same sort of typing that Haskell has, but it handles them dynamically, with no static analysis.

FOR VERY LARGE VALUES OF 'TYPING'.  I mean the whole bundle that you'd need to show any kind of 'betterness', and what people will refer to anyway as soon as any discussion happens.well, when you look at the output SBCL spits out when compiling those benchmark toys, especially the amount of warnings about missed possible optimalizations, you will know that SBCL can do a lot better. unless somebody fix that sources, these tests have no valuable information.The phrase: "Programs must be written for people to read, and only incidentally for machines to execute," comes [straight out](http://mitpress.mit.edu/sicp/full-text/book/book-Z-H-7.html#%_chap_Temp_4) of the Lisp tradition.

(I realize the post you're responding to may be taken as an invitation to a flamewar, and if you personally prefer Python's syntax, that's absolutely fine by me. However, I should say that I used Python before I heard of Lisp, and now overall prefer Common Lisp by a wide margin, in terms of readability.)
Signs are on it that NBL is ECMAScript 4 with (some version or descendant of) Adobe Apollo as the main client-side platform. If so I would guess that that Steve Yegge has heard about one or more Google teams tooling up to deliver Apollo applets (probably versions of [Maps](http://blogs.warwick.ac.uk/stevencarpenter/entry/adobe_apollo_demonstration/), Calendar and so forth). If that *is* the case, then you wouldn't need a magic 8-ball to see JS 4 making it big on the client side. The AJAX experience suggests that the first applet system that both doesn't suck too much and can quickly get a critical mass of installs (see Google, Adobe and probably others) is going to be a big hit. If the package also includes a gentle migration path from AJAX and Flash in the browser and (just guessing here) the first mobile application platform that doesn't suck too much and isn't proprietary to Apple, then it's an even better bet.

&gt; He works for Google and claims a language that becomes big has to have a big corporation backing it.

Actually, in the article he claims that the language needs a whole cabal of big companies acting in unison. Which is exactly what JS/Apollo would have in this scenario.The only thing visible is a little "m" in the corner of the browser... there isn't really much to screenshot!CONDENSED VERSION: passion == traditional artistry. (You people who read 'Zen and the Art of Motorcycle Maintainance' and think -- yeah, there *is* something of great beauty in the designs of good bridges and software?  Pick up a crayon and draw something, or you'll waste your life away.)

If you don't mind me (ayrnieu), I'll go back to trying to pick up Mercury and Mozart, to do beautiful things with.However, compiler macros are a lot more ad-hoc and don't include any kind of type-information.  deftransform may be closer.
[PyPy](http://codespeak.net/pypy/dist/pypy/doc/news.html).

Or actually, I think you mean [eval()](http://docs.python.org/lib/built-in-funcs.html).From this review, the middle section that held any interest at all should be discarded and replaced with [a free copy of the latest re-release of Thinking Forth](http://thinking-forth.sourceforge.net/), which has an excellent history (up to then) on the evolution of programming methodology.This is my first encounter with rewrite rules in Haskell, so I'm somewhat talking out of my ass here:

&gt; However, compiler macros are a lot more ad-hoc

Why? From reading http://www.reed.edu/~carlislp/ghc6-doc/users_guide/rewrite-rules.html my impression is rather that rewrite rules are ad-hoc. You know, compiler macros are standard CL, not a nifty optimization technique of one implementation.

&gt; don't include any kind of type-information. deftransform may be closer

OTOH, the compiler macro gets a sexp and can inspect and massage it in every conceivable way, whereas with rewrite rules you are restricted to pattern matching. Also compiler macros are written in plain Lisp, whereas rewrite rules are "outside" of the language (less flexible?).um.  

yes it is.  are we talking theory or practice here?  any finite volume in our universe has a finite number of unique states.  i.e. finite storage.[deleted]Here's an open source clone:

http://www.activecollab.com/

Incidentally, seeing as how we're on the subject, I wrote something of my own that I'm pretty happy with.  It's quite simple in that it's just a priority queue that lets you share stuff with other people, and keeps track of the time you spend on tasks, but they've been happy with it where I work precisely because it is so simple, quick to use and unobtrusive:

http://stufftodo.dedasys.com/&gt; GHC-style rewrite rules do have one advantage, though: they run after first-pass inlining

Could be done via macroexpand, no?

&gt; but the notion that we should start thinking of libraries as algebras, and use that viewpoint to build better optimizations.

A noble goal, indeed :-)

_Edit_: macroexpand doesn't inline function calls, my bad.Just to preface: if you hadn't mentioned compiler-macros, I probably would have.

Now about compiler-macros: since environments were never usefully standardized, there is no portable way to extract type and context information about terms in a form; if it is even available.  compiler-macros provide a source-level interface; the s-exp you massage "in every conceivable way" is precisely the s-exp the programmer typed.  This is not so good in the cases like

    (let ((foo #'1+)) (my-map foo list))

All the compiler-macro for MY-MAP sees is "FOO".

Secondly, compiler-macros are bound to symbols so there is a limitation on them similar to the difference between single-dispatch and multiple-dispatch methods.  Rewrite rules are usually specified on combinations of functions; not on a single one.  So, you may have a wonderful way to rewrite (map ... (my-fun ...)) but you can't do it because map is CL:MAP.
I found that over the years I have grown to trust theory less and less. I try to no longer confuse theory with proven fact.

RANT

Consider Test Driven Design and Pair Programming. In theory they both result in faster and more accurate development than classic build then test methods.

When I researched it for my master's degree, I found that ALL of the papers I came across supporting pair programming were bogus. They would say stuff like PP was more efficent because it took 20% less wall-time, hiding the fact that it took 60% longer if you look at developer-time. It was as if they believed the second developer was free.

The only paper I read that did appeared to do real staticial analysis on defect prevention showed there was no significant difference pair programming and solo programming, except that pair programming took nearly twice as long as solo programming. TDD was shown to be less effective that build then test development for defect prevention. So much for theory.

The one flaw in EVERY study on pair programming I found was that the experiment was always 2 pair programmers vs 1 solo programmmer working on a single task. In reality it should be 2 pair programmers vs 2 solo programmmers working on two tasks. Otherwise you discount the opportunity cost of the second developer.

END RANT

Good point, thanks.Are you willing to spend money on it?I use some home-made scripts with tar and scp, works pretty well. All the other solutions I tested were either no good or too complicated for my use. It's pretty easy to program something that gives you history, is secure and automated.If it's so easy, I keep wondering why I can't find a program created by someone who's put just a bit of extra effort into it to make something that's generic enough for others to use.  Not idiot-proof, but something I can set up in 15 minutes by specifying the machines to back up, the directories to include, those to exclude (in a simply way...rsync loses here), and how much history to keep.&gt; Could be done via macroexpand, no?

As you noted, not without some extra work. But you could try something like:

    (defmap double (lambda (n) (* n 2)))
    (defmap increment (lambda (n) (+ n 1)))
    (deffold sum #'+ 0)

With a certain amount of macro wizardry (and explicit contracts about purity), you could probably get the same optimizations working in Lisp. It would be a totally sweet hack.

Overall, I'm starting to suspect that purity is the best default, and that side-effects should be implemented by the standard library, so that we can replace them with whatever crazy things amuse us. :-)Surely he is talking about JavaScript 2 or EMCAScript 4 (as others have already mentioned, but I bring slides! - that I found in the comments on Steve's post).

http://developer.mozilla.org/presentations/xtech2006/javascript/

According to that set of slides the spec is due by the end of the year, JS2 will be in Mozilla by the end of the year, it has what looks like optional static typing, it is supported by a large amount of companies (Adobe, Mozilla, Microsoft), and is 'optimized for space and speed'.

The only other possibilities from the way he is talking is some internal language used by a large company, such as a Google, but surely we all would've heard about it now, and it is pretty well known that Python is Google's scripting language of choice, so I don't think it is something by Google. And it most likely isn't an 'old' language like Perl or such. That doesn't have major backing in the way he is talking and its also damn hard to try and market something which people have seen before.

Not sure how I feel about this. I mean JS just isn't that exciting, but who knows. Would be nice to get the speed of Java and also get a majority of dynamic language programmers finally working on libraries in a single language instead of having them all spread out over Perl, Ruby, Python, etc. And with the items it would be supporting like continuations, you could get some nice stuff implemented like a Seaside-style framework.

Is there anything that he mentions which makes EMCA4 not sound like the NBL?Horrible article.As I was reading Yegge's post, I kept thinking NBL is already here, and it's called [C#](http://en.wikipedia.org/wiki/C_Sharp#C.23_3.0_new_language_features).  So, OK, C# 3.0 doesn't meet every one of Yegge's criteria, but it meets many of them, and it clearly illustrates that C# is a rapidly evolving language.  If C# 4.0 throws in ideas from [Polyphonic C#](http://en.wikipedia.org/wiki/Polyphonic_C_sharp), then that at least partly addresses your concern about concurrency, as well.
[rdiff-backup](http://www.nongnu.org/rdiff-backup/)Home: SuperDuper!
Work: Cobian BackupI switched to Google because it's search results were markedly better than Yahoo's or AltaVista's.  I'm pretty sure that's why everyone else switched too.  The uncluttered page was nice but it wasn't part of the decision.

(Side note, I know that alltheweb.com was approximately as good but their page was as empty as Google's so that's not relevant.)
&gt; You have no right to criticize a programming language, unless you are able to point three things in which it excels compared to your favourite language.

My personal corollary to this is _"you don't really know your favourite language/tool if you can't name its warts"_.Agreed.  I doubt sucking less will provide sufficient motivation for people to adopt a new language.  Changing languages is a pain, so there must be a compelling reason to do so -- a "killer app".  A better concurrency model could be that killer app.

An argument to this effect is made [here.](http://toomuchcode.blogspot.com/2007/02/part-4-killer-app.html)"Only wimps use tape backup: real men just upload their important stuff on ftp, and let the rest of the world mirror it."   -- Linus B. TorvaldsIf Sun fully GPLs Solaris (i.e., including the 'or a later version' language), then without a doubt it will be a massive boon to the free software/open source community. Linux is a very cool OS, but most of its coolness comes from two things: the GNU userland and its hardware support.  Well, [GNU/Solaris](http://www.gnusolaris.org/gswiki) has the GNU userland by definition--bye-bye, brain-damaged BSD-era utilities, hello GNU toolset!  As for the hardware support, I'm informed that many files in the Linux kernel _are_ licensed under the full GPL, and thus can be used under GPLv3, so the porting effort shouldn't be that difficult.  Even for those which aren't, the code's there, so it should be easy enough to figure out what it does and write a Solaris version.

Linux has never been a particularly amazing kernel. I don't say this to be disparaging, just accurate.  It does do some neat things (things which hopefully will make it into OpenSolaris), at least at the user level, e.g. /proc and /sys which are actually useful. And Linux LVM seems ahead of the Solaris stuff (I've not used the latest from Sun, though, so I could be talking into my hat).  But overall, the Solaris kernel is relatively rock-solid compared to Linux; GNU/Solaris should be at least an order of magnitude more stable than GNU/Linux (although perhaps not so much as SunOS/Solaris; unfortunately the GNU userland has its own minor stability faults).

Essentially, a properly GPLed Solaris kernel will be what the FSF wanted decades: a free kernel on which to run their free operating system.  Properly licensed, it can be relicensed as the legal climate changes.

And perhaps the computing industry will finally start to move forward, after twenty years of wasting time...I think there may be a couple of important benefits to pair programming beyond simply reducing mistakes.  The first is much faster dissemination of knowledge (especially project specific knowledge) through the development team, something that has been a problem on every project I've worked on that had more than one person on it.  

The second is adherence to process rules.  For example, in an XP environment one of the key rules is that tests are written first.  If everyone has to pair program, then chances are very good that everybody is going to be on their best behavior and is going to write the tests first, pretty much every time.  However, if the team was trying to do all of XP *except* the pair programming, you'd likely see a lot more backsliding about writing tests all the time and writing them first.  Pair programming will definitely cost you time over the short term, but a rich set of automated tests will save a whole lot of time later, for at least some types of projects.  

Finally, it's probably hard to goof off and read Reddit when you're pair programming.

So although 2 pair programmers on one well defined task are unlikely to be twice or more as fast as a single programmer on the same well defined task, if you look at the overall picture in a normal team environment on a multi-month project, I can imagine that in at least some cases pair programming might be a net win.  This probably wouldn't show up in the relatively simple pair programming studies that have been done to date.

Probably not (to borrow PG's phrase) to [axiomatize computation](http://www-formal.stanford.edu/jmc/recursive/node1.html).He misses it, it's most likely JS2. See the [comments to this earlier story](http://reddit.com/info/13ycl/comments)It's not mentioned in the article, so:

For Linux, set with: `ethtool -s eth0 wol g` wakeup with: `wakeonlan [ -i ip ] macaddr`

For FreeBSD, set with: `ifconfig xxx wakeon magic` ([patch](http://stsp.in-berlin.de/)) wakeup with: `wol [ -i ip ] macaddr` (ports/net/wol)I love Erwig's Dist monad. It's a sweet demonstration of why monads are useful for a lot more than just IO.
All the discussion boils down to one thing.  Static typed languages help you catch mistakes quicker, for the cost of having to write more code and being unable to do some things easily.  Whether that is a trade-off worth making depends upon your individual circumstances and methodology.

If you're the type of organisation that requires fast development for changing circumstances, you're probably better off with dynamic.  If you're the type of organisation with relatively stable requirements and plenty of programmers, you're probably better off with static.  Anyone in-between (which is most people), it's a grey area, which is why the debate rages on.I think 37 signals got it right; simple and easy to use apps. Thats what most people need. A lot of apps become a huge pile of hard to use or to understand features that no-one really needs or use.

What is wrong with "No, sorry" answers?[rsnapshot](http://www.rsnapshot.org/).  It uses rsync as its backend, keeps a "history" on the other side (you define how much).  You can either install it on each system and backup to a central location, or install it on the backups system, and have it ssh out to run rsync remotely.  Setup requires an edit of one conf file + additions to crontab.

You can use ssh without root passwords; simply create ssh keys using `ssh-keygen` (without passphrases), and use them for rsync.  ssh lets you set things up so a particular key can only run one command (`/usr/bin/rsync` or whatever, in your case).&gt; I'd like to be in going after the GPL community, because there's an awful lot of really bright people who think that's the license they prefer. That discussion is incredibly central to recruiting more developers around the world.

Not the worst strategy to "out-hire" Google.I do a moderate amount of programming for research. Since I open-source the code, I svn to code.google projects for backup.So, just don't use it...[removed]Newsflash, NBL is Standard ML.
I'm ok with your question but I would prefer to see it illustrated using a pertinent example and not an example where constraints forbid such transactions.I don't read Yegge anymore. He's a nerdier John Dvorak.I'm sorry, I can't point you to some online documentation (I hope Google will be more helpful): I learnt this from a former job.currently i use my .mac backup on my notebook

manual off site backups for my windows machines and full daily tape backups for servers.

History is realized with svn to some degree (e.g. some servers actually have /etc under svn control). I also use svn locally when i work on about anything (also binary stuff)Parsec is another good example.I use unison. It's open source and it runs on most systems. It uses the rsync algorithm. Check it out here: http://www.cis.upenn.edu/~bcpierce/unison/It IS Java at its heart. All Processing adds to the mix is a few macros to easily specify color values (they add a pseudo-native type called color which maps onto int), and a handy applet class that includes all the Processing API methods.

If you want to access the Processing libraries through Java so you can debug (which the processing environment does not support), it's really quite simple. Add core.jar to your build path, and subclass PApplet. You'll be good to go.SyncBackSE: http://www.2brightsparks.com/syncback/syncback-hub.html

My code is incrementally backed up to another internal drive nightly, with a full backup every sunday.  This is on top of most of the code being stored on various svn servers.

I also use this to backup a remote server (apache config, htdocs, mysql).  This runs on the remote server, and prepares zip files, my computer downloads those about an hour after they are created.  It's been working fine so far.My first thought was perl6 too. And my second was "but steve yegge doesn't like perl". (The "two platforms" in this case would be parrot and the perl5 engine).

But I think the people saying JS2 are probably right ini thinking that was his intention.

perl6 will be/is technically brilliant, but it's success or otherwise will be determined by the plusses and minusses of perceptions inheried from perl5.I can't help but feel this topic has been done to death, and other articles give it a better treatment than this one.I keep code and configuration dotfiles (linux) all in subversion. I have a separate directory that is synced across all my systems via ssh-agent+rsync+cron that has pdfs, photos, and other binary files that I don't want in SVN. So, most of my important stuff is redundant just because I have a couple computers I use regularly.

For serious backups I use amanda (amanda.org).Around once a month, I copy everything in my home directory to an external firewire. (…this probably isn't a good method…)ECMAScript doesn't have call/cc or first class continuations which the article says will be in the NBL.[deleted]I use Subversion for *all* files (including, my ~, /etc and even /opt directory!), which I mirror locally using svnmirror.sh and I also have a post-commit hook that symmetrically encrypts the revision and rsyncs to three remote locations (Australia, Germany and US).

I have yet to find a superior solution and I've considered setting it up for others on the side before.If you have type inference as well, then the increase in code you have to write is basically proportional to the number of constraints you actually want to express about the types in your code. It still checks the consistency of the rest of the code, and that the programmer-supplied signatures are consistent with the inferred signatures.

So for the most part, the use of your static type system becomes an additional freedom to express something which you lose with dynamically typed languages. (Or else go to a lot of trouble in implementing.)

Mind you, limitations in expressiveness of those static type systems can result in making impossible some techniques with which dynamically typed languages have no problems. However, there's almost always a not-too-inelegant way to rephrase things so everything works out. To paraphrase the metaphor used to explain duck typing, "If it quacks like a duck, then we'll keep the quack and forget the rest". If you have a bunch of different kinds of data objects which share an interface, and you need to treat them as if they had the same type, then simply record the interfaces to each, which really do have the same type. Automating that idea gives you existential types. The only thing you lose is the ability to "upcast" back to the original types.

Such an upcast operation is pretty unsafe anyway, but, it's possible to support it safely by other means if you're willing to represent failure. It's quite possible to embed dynamic types into a statically typed language, for example like Haskell does with [Data.Dynamic](http://www.haskell.org/ghc/docs/latest/html/libraries/base/Data-Dynamic.html). However, the way Data.Dynamic does it is *just* unwieldy enough that you wouldn't want to use it unless you really had to. It would be possible to build something like it into the language at a lower level, and perhaps make it somewhat more flexible (for example, in reflecting polymorphic values).

Another place where some level of dynamic typing appears necessary at first is in compiling code at runtime. Surely, if you're building code at runtime, you can't statically check that it has the right type when you compile the program as a whole. However, one thing that you can do relatively easily is to have that runtime compilation check that the expression/program it compiles has the right type(s), and explicitly represent the failure to compile if it doesn't. (Or just throw an exception.) This is similar to the dynamic to static conversion I mentioned above. In Haskell in particular, it actually buys you quite a lot of security, since you can insist that there are no untoward side-effects involved. (Except perhaps nontermination or large resource usage, which are comparatively easy to defend against.) A string is a string, an integer is an integer, you won't have users sneaking by code that erases your hard drive.

So perhaps one ideal is to permit convenient mechanisms for dynamic typing (or the things which dynamic typing buys us) within a system that is otherwise statically typed and has type inference.[deleted]The fact that implementing queue with worst case O(1) operations requires "tricks" says a lot about Haskell.
&gt; Virtual Machines are exactly what they seem to be. The ability to create a virtual computer on which any and all personal computer applications (as well as higher end apps) can reside.

Does this guy knows what a VM is? That's the most stupid article I've ever read on computers, it looked like an old sci-fi comic from the 40s...

&gt; If the heavy bandwidth apps are on gaming consoles (...)

What? Here is how bandwidth works: you either need bandwitdth or you don't. Gaming consoles are maybe 1% of the bandwidth used in the world, most applications pumping stuff on the internet are used by companies whose servers are located in different countries or places (read GBs of numbers crunched for private purpose, not a 15 years old WoW noob in his bedroom)
&gt; And I havent even begun to discuss the role of HDTVs to replace personal computers

Ok I understand now, this article was a joke, I've been trolled.well, try implementing persistent O(1) queues in any language without some trick.Most "software engineers" are fucking morons who *shouldn't* be allowed to choose their own tools, because if they were they would make retarded choices that would end up costing their employer boatloads of money.

This is the Best C++ tutorial I've seen[removed]I use SVN for important stuff, have it on my server and then on all my pcs so lots of redundancy, its free, easy to set up. Then for the rest just on an external HD and maybe another PC.Yeah, I took the article as saying something similar to: I know who the next President of the United States will be.  He'll either be a Democrat or a Republican, and slightly cooler than the current guy.  Sure the Libertarians or Greens or Reform Party  might be 'better' from your perspective, but it ain't gonna happen.&gt; Why all this excitement about UNIX, and will UNIX become the next MS-DOS?

Bwahahaha... ;D
&gt;? : is confusing

&gt;    use them a lot

Yess[deleted]Or even with JavaScript if you're using Safari (contrary to their assertion).Back in the day (by which I mean 1997-1998) I had the misfortune of helping to replace an ugly, imperative Java-based multimedia runtime with an FRP-based one built on top of Microsoft's [DirectAnimation](http://msdn.microsoft.com/archive/default.asp?url=/archive/en-us/dnarmulmed/html/msdn_directan.asp).

The DirectAnimation design looked vastly cleaner than the crude Java runtime, but, as we all know, surface appearance can be misleading.  It turned out that it was dirt simple to write event handling code in Java and to get good performance, and it was an extraordinary pain in the ass to get DirectAnimation to do anything other than toy mathematical animations, and the performance just sucked.

Now no doubt part of this was due to serious design deficiencies in DirectAnimation, but a big part of it also had to do with the FRP paradigm not being nearly as great as everybody thought it was.
[But](http://lambda-the-ultimate.org/node/1784]), [And](http://www.mozilla.org/rhino/).The ECMA4 reference implementation will reportedly be written in *ML extended with first-class continuations, so even if ECMA4 itself doesn't have them the fact that they're on the list is probably due to misunderstanding.&gt; and, char (*(*x())[])() is
&gt; a function returning a pointer
&gt; to an array of pointers to
&gt; functions returning char

And who said C's not a FPL!It could be fun If you like math and money.&gt; You mean Python has a Lua-like syntax I guess?

Actually, Python was first (1991 vs 1993), I believe.

&gt; No, nothing's converted. This guy should learn a few more things about languages before trying to create a new one 
&gt; from scratch.

Thanks for the correction. I've noted it in my post.

Always learning...   :)
JI have a hard time listening to anyone who spells as poorly as he does.

I do think that casual users who don't really do anything serious probably won't use a general purpose computer in the future. Philip Greenspun thinks they might use a glorified phone.

However, I doubt it will be a bigass general purpose entertainment console. The Wii's success shows that people are also perfectly happy to just have a dedicated gaming machine.I said "Eat shit," not "Write shit."Backup to /dev/null, restore from /dev/random.Don't you think this is a rather silly point to split hairs over? What I was getting at is that traditional computers can have their storage space increased to match the needs of the computation they are performing. This isn't true with quantum computers unless you can build non-deterministic IO.I realize 95% of all programmers are using Eclipse, but this still isn't reddit-worthy.I looked over the examples.  It seemed more like taking Java's OpenGL binding (jogl or something) and shoving it in an applet.  The command structure is very similar to OpenGL too.[my house burned down a month ago](http://billmill.org/fire), and my computer with it, so I'm looking for the answer to the same question. Right now, I'm backing up my laptop to my work computer, but that's a point of failure.

Ideally, there would be a service I could use to back up my stuff (rsync rocks) and wasn't ridiculously expensive. Right now, everything is too expensive.

1) the price point I want to hit is something like $10/month for ~60 gigs. If you're getting economies of scale, this should be possible. I'd go up to $20, but we're talking ideal right now. I don't need lots of bandwidth, one upload plus ~5gig incremental/month and a max of 3-5 (total) downloads per year.

Right now, I'm finding prices more like $2/gig/month, which just won't cut it.

2) for bonus points, I would be able to play my mp3s/avis/mpegs/oggs via an online player from any computer. I would pay an extra $10/month on top of the backup cost for this.

Anybody got anything?As far as functional programming is concerned, C's procedures are not functions at all.  (Not that there's anything wrong with that!)You mean... languages with constructs like side-effect free maps can easily parallelize them? I'm shocked. I've never heard that before.For storage on the net take a look at;
  http://www.jungledisk.com/ 
  http://www.bingodisk.com/

Of course you've then got to get the data onto these services, for which most people end up using rsync.Sounds like [Time Machine](http://www.apple.com/macosx/leopard/timemachine.html).&gt; Static typed languages help you catch mistakes quicker, for the cost of having to write more code and being unable to do some things easily.

They don't - at least, not under typical "best practices" development processes for both static and dynamic languages.  When I write Haskell code, I type "make", GHC compiles my code, and I have a reasonable assurance that the program works as expected because of the type system.  When I write Python code, I type "make", a test script tests my code, and I have a reasonable assurance that the program works as expected because of the extensive unit test coverage.

A lot of static vs. dynamic typing discussions completely ignore unit tests.  You *can't*, because most dynamic language programmers run with 100% test coverage.  In this case, dynamic typing tests just as much as static typing: any type error will show up as a test-time assertion failure instead of a compiler error message.

Now, one advantage of static typing is that you have to write *less* code.  Typically, type annotations are less verbose than extensive unit tests, though doctest makes this a litle more even, and type inference does away with even those.  This assumes a *good* static typing system, however, like Haskell or ML.  Java's type system has so many holes that you have to write the unit tests anyway, so you need both the type annotations and the verbose unit tests.

I see the real benefit of static typing as programmer tools.  It's very difficult to write autocompletion and refactoring tools for dynamically-typed languages, because the IDE doesn't have as much information.  Type-inference has a similar problem, however, because often the type-inference algorithm doesn't have enough information to determine an expression's type until the expression is complete.  IDE algorithms need to work interactively on an incomplete program, which is an area of ongoing research.Obligatory Haskell one-liner (avoiding intermediate conversion to string):

    Prelude&gt; head $ filter (\x -&gt; rem (div (2^x) 1000) 10 == 7) [10..]
    21[removed]No, that wasn't a joke.  Processors are basically dirt cheap.  It's only a matter of time before someone puts a processor, wireless and bluetooth in an HDTV, and there you go... surf the web, email, do all your basic stuff with a bluetooth keyboard from your couch.No, what you're asking for is arbitrary behavior.

I said Standard ML because it has a formal semantics.  Scheme would have done, as well.

Both certainly have a runtime exception mechanism.  With well-defined semantics.  Big difference between that and the attitude of "bats fly out of your nose -- ha ha."
C and C++ are have ISO standards. They still have many implementation-defined behaviours (for example, casting overly large floats to integers in C). Same with Common Lisp, scheme, etc. The difference is that these issues are now officially recognised, and, hopefully, good coding practices will navigate around them.

EDIT: The point might not be clear, but I'd rather have explicitly ill-defined than implicit ill-understandable but well-defined behaviour.Chomsky is oooooooooolllllllldddddd&gt;rather than a language standard that leaves implementation to other people

...or one done by committee.  See how wonderful that has worked for Java.

I'm a bigger fan of the idea of a standard implementation that everyone else can follow.  Standards written down become outdated by the time they hit the paper and people want to use it as a weapon to beat down those who want change.The Next Big Language will by definition suck, because it will bring in all the sucky programmers. I hope Java continues to hold on to a large segment of the market, just so that the shitty enterprise developers who religiously identify themselves by the language they use stick with it and leave the rest of us alone.I was thinking the same thing about ECMA script even before i read any of the comments. In fact, I have been thinking it for some time now. 

It seems the biggest barrier here will be Microsoft. As far as I know, IE 7 didn't really progress much in terms of it's Javascript/ECMA script support. Although I could be mistaken.i do the same thing, only mine is more or less once every time i get scared that my computer is going to die. more or less 4-5 times a year.Ditto. I have two external drives and just rsync my home dir to them periodically; one at work and one at home (do keep an offsite backup if you can).

Judging from the question though it sounds more like a production environment backup being sought.[removed]worth it!It's starting to look like a real hodgepodge of ideas, and it doesn't look very pretty because of it.  I am not, however, convinced that that's a practical problem.  I like elegance as much as the next guy, but I think it's overrated in terms of actual usefulness.  I also think that the C# designers are adding stuff to the language to address actual pain points, not based on some ivory tower notion of what a language should  look like.  This is not a recipe for elegance, but it is a pretty good recipe for success, at least if C++ is any guide.Many people would not consider C++ a good recipe for success...

(Actually, I should qualify that.  "Old style" C++, before templates and RTTI and many of the other new features, was a success.  It hit most of the pain points of C and yet was still manageable enough to learn without going mad.  "New style" template-metaprogramming C++, despite being vastly more powerful and probably more elegant, has grown beyond the bounds of manageable complexity, so many people deserted it for Java, Python, C#, etc.  Perhaps the lesson is that languages can grow up to a certain size and then just collapse, with people leaving en-masse for a simpler language.  Perl also followed this pattern, as did Common Lisp.  I don't know if C# 3.0 has exceeded the point of maximum complexity or can still productively add features.)I do not see this happening, simply based on the location of a computer versus the location of a gaming console. While a gaming console certainly has the potential to run a spreadseet or word processing application, I do not foresee people abandoning their desks and offices for couches and tvs.&gt;&gt;&gt; def f(p, k, nmax=1000): return [n for n in xrange(nmax) if ((2**n)/(10**(p-1))) % 10 == k]
    ... 
    &gt;&gt;&gt; f(4,7)[0]
    21
    &gt;&gt;&gt; f(4,7)
    [21, 24, 27, 32, 40, 46, 56, 62, 73, 85, 94, 141, ...etc
    &gt;&gt;&gt; f(3,9)[:10]
    [29, 38, 53, 54, 55, 56, 60, 61, 62, 67]

Lack of lazy evaluation makes the nmax thing obligatory.  Generators obviate it, but you do need a few line breaks (with \n as a block delimiter Python will never win a one-liner contest).That's cool, but what a pain in the ass! Why didn't the people who designed the text renderers realize that everybody needs outlined text sometime. And that goes for VRML too!Very cool.[removed]ode |ōd| noun a lyric poem in the form of an address to a particular subject, often elevated in style or manner and written in varied or irregular meter. • historical a poem meant to be sung.

Also, Ode to Joy doesn't sound like a song about never being happy again.C# 3.0 seems to be all about LINQ and I'm not at all sure how I feel about an embedded sublanguage that is the bastard child of C# and SQL.  But on the other hand, I already don't like writing SQL syntax in SQL and trying to glue it to C# code.  If LINQ makes that easier it might be worth it.  As weird as LINQ looks to me, I don't think the added complexity will kill the language, though, because I think anybody that doesn't like it will ignore it.  But maybe I'm wrong about that.  If they're planning to add as much more complexity in C# 4.0, though, that is pretty scary.If it Javascript I'm just praying that it holds onto to its fundamental prototypical nature. Prototype OO is awesome, but has basically no mainstream use except in Javascript. Even there people have managed to contort it enough to make it look like an uglier version of traditional OO.lack of "using namespace std;" ?[deleted]On Windows it is still hard to beat RoboCopy. I tried several backup programs tonight for my laptop, and in the end some RoboCopy scripts beat out everything else for ease of use. Remote hosts can use mapped drives so long as you don't mind having passwords in your batch files.

I can't say anything about Linux, I just let my sysadmin handle that.
[deleted]It's amusing to see how the link to those slides got posted as a [comment on reddit](http://reddit.com/info/13ycl/comments/c13ypa) then to the comments on the original blog post, and now back to reddit. Full circle :&gt;I don't think so. C# is very anti-dynamic and I don't see it changing any time soon. VB is a much closer match in terms of the optional static typing requirements. But VB has a Fortran heritage rather than a C heritage, so it isn't even an also ran.All studies are bogus (just covering my ass here).

According to the studies, writing the tests first is better than no tests, but slightly worse than writing the tests afterwards. So maybe backsliding isn't all that bad.

But since studies suck, we don't really know if that is the case. 

Personally I would rather have a solo developer/programmer teamed with a solo tester/programmer whos goal in life is to break the developers code.

The possible problem I see with pair programming is that the programmers will begin to get "in sync". If they start thinking the same, then they will start overlooking the same issues.
Creates graphs of all your python code functions[deleted]The title is a bit misleading:  the tool only optimizes GCC compilation options.[deleted][removed]One of the biggest 'implementation defined behaviour' problems is the order of evaluation of nested function calls. e.g. in `foo(bar(blah()),baz());` it isn't defined whether blah or baz can be called first.  This has some strange implications in many unexpected places--initializing smart pointers in an exception safe way for one.Some of the commentors on this blog entry got it right: if you need both an array item and its index use enumerate() that is present in Python for almost a decade.

for i, item in enumerate(array):
  BLOCK

That would be better than 100 incompatible Algol dialects we have today (Java, C, C++, Perl, Ruby, Python, PHP, .....)I wonder what the ALGOL programmers have to say about this. Which ALGOL dialect should I pick? Old fashioned C? Flavor of the month Ruby? Corporate standard Java? Why can't any of them interoperate? Why the duplication of effort in the ALGOL community?Perhaps you're thinking of an elegy?spamIt's not for work, there we use bacula, which seems to work ok, but is a hassle to set up.

The ideal solution would provide for my home systems:

* Linux PPC
* Linux x86
* Wife's Linux amd64 laptop, when it's connected locally

As well as some remote systems I run (this the requirement that it doesn't gobble bandwidth).  Would be nice if it could handle windows, too.ALGOL community?
[removed]It will feel just as snappy/not snappy as anything else in C# or Java. Snapiness or lack thereof is not the point of looking at these languages.See F# for the MS version of a functional language without a C-like procedural/OOP hybrid underneath. These guys are not "eggheads", MS employs a bunch of damn smart language designers. Can't say the same for Sun; they have a bunch of 80's has-beens playing the role of "thought leaders" and thousands of interns cranking out junk.When did Perl and Common Lisp "just collapse"? I just have missed the memo.There are already too much blogs and online articles about startups. If a couple of guys are smart and have good ideas they will find their way to work. It does not require an article or a book to tell you that you need to work even at night and be totally focused for hours in the way to solve your application problems, nor that you are not intested in wearing like a corporate zero-work-done guy, so what's the point? Instead to read this book make a call to your programming friend and start writing code today.

I think that almost the *only* thing that can help in a book like this is relations with investors. That's the thing you really don't know and where some advice can be valuable.[removed][removed]If you don't want/need the encryption, [rdiff-backup](http://www.nongnu.org/rdiff-backup/) (a sibling project of duplicity) is great.&gt; Anybody got anything?

If you can make finer distinctions than '60 gigs or so of whatever', I have these suggestions:

1. put photos that really matter to you online, with some stable service.  LJ, for just one example that I happen to be familiar with, gives 1G of space to normal paid account holders who pay 25/year.  LJ is also a decent repository (with a client protocol quite sufficient for accessing your own stuff) for very important emails or small histories of writings or such, that you can post mechanically and fetch mechanically -- even keeping FS-like data in a single post to find the others, with all of them made private and then back/futuredated to never be a bother.  Of course, I mean documents that you consider very important without considering fodder for a subcutaneous multiply-encrypted IR-communicating nodule with a small embedded explosive.

2. put all of your code (and books, and blog posts, and any such thing) in a VCS.  With svk and probably one of the other post-svn systems you can trivially operate on everything locally and then push it to foreign repositories.  Good, free services exist for svn, which svk uses as a backend.

3. take your embarassing love letters, encrypt them, and then stuff them in gmail.  Shy of 3G and increasing of free space that you use as a filesystem.  On a bad day, you can just attach the files to an email and send them to yourself.

4. if you want to backup the entire state of your computer -- fine, have a ball.  But don't you use a package manager of some kind anyway?  Don't you have islands of personal stuff amidst the 'oh, I can just wipe all of this for this new FadLinux disto'?  Don't you at the least have an easy diary of what you do to your system that you can follow later on?

5. You're willing to pay $20/month (supposing online play) for some fraction of 60G for music?  20*1year = 240 -- buy an ipod.  Hell, plan ahead and buy two.

6. So, what do you have left?  A fraction of 60G of space for video and pictures you wouldn't feel comfortable putting on flickr?  Video and pictures infrequently have updates -- you only add to them, eternally, until your house burns down (sorry).  [TigerDirect.com](http://www.tigerdirect.com/) has cheap harddrives.  Buy an ATAPI&lt;-&gt;USB bridge (20USD or less, from ebay) and use to it to -- monthly, if you care that much about delta{1month} of video, hook up one of these harddrives, copy everything to do it, and then drop it off at your bank.  These are pretty low on my list of 'things to immediately restore' after a disaster; I think I can stand the non-immediacy.

7. Oh, and put your bookmarks on del.icio.us -- this web2.0 stuff is all a secret plot to make your computer less of a point of terrible loss.

If this list very doesn't apply to you, please give us one that does.  If you -can't- get more atomic than '60 gigs or so', sorry.  I understand that at least 1/3 of my suggestions are things only a geek would enjoy setting up.Apps like these are great, but I wonder what it will look like when you have functions returning functions, and it can't be decided at compile time which functions will be returned? In short, how to make this work for haskell?[rsync.net](http://rsync.net/) offers 2 classes of service starting at $1.60 GB/month

I've used a VPS from their sister company and been happy with it.rhino, jruby and jpython are dynamically typed. scala and F# are statically typed. The JVM and CLR are designed to run statically typed languages efficiently.PyPy is far from simple, and `eval` isn't in Python. :)

Glyph Lefkowitz's [ouroboros.py](http://twistedmatrix.com/trac/browser/sandbox/glyph/ouroboros.py) comes to mind, however.Shows how to get out of tough corners when deploying with FastCGI and railsAwesome! Really cool for debugging and understanding other people's code.

EDIT: just tested it, it works well even for recursive functions, but it seems to slow down execution a lot.A standardized official implementation (de facto standard) would be enough only if it was bug free.

As Sam Ruby (http://www.intertwingly.net/blog/2007/01/25/Pro-Choice) recently stated: there are open standards, standards which have at least one open source implementation, and there are standards. Favour open standards.I take it Python code must be really messed up if you can't figure out which function calls what.Even if your code is pretty straightforward, a visual representation helps. Don't forget the visual cortex is one of the most developped part of our brains.Have you read the book?  It's full of cool stories from the founders themselves.  Worth the read for sure.  Doesn't beat cranking up TextMate of course, but then again that's not its purpose.
It uses Python's [tracing hooks](http://docs.python.org/lib/debugger-hooks.html), so it also works with C functions and exception flow, for example.[deleted]I think the point was that it might include cool stories but it's not going to help you found a company.I love this argument about unit tests.

Would you rather live in a building that was designed employing mathematically-derived physics, or one that was "unit tested" with progressively larger and thicker cardboard models?Yep, no progress happens whatsoever, anymore.

1. [lanl.arXiv.org](http://lanl.arXiv.org/)

2. [Mercury papers](http://www.cs.mu.oz.au/research/mercury/information/papers.html)

3. [Mozart/Oz papers](http://www.mozart-oz.org/papers/)

4. [Erlang - Master's theses](http://www.erlang.se/publications/master.shtml)

5. [Erlang papers](http://www.erlang.se/publications/publications.shtml)

6. [Haskell papers](http://www.haskell.org/haskellwiki/Research_papers) - you'll need to look in the subcategories.
&gt; but I wonder what it will look like when you have functions returning functions, and it can't be decided at compile time which functions will be returned?

This is already the case in Python.  (In fact, even if it wasn't, Python's dynamic nature would still make it hard to tell much at compile-time.)

For this reason, pycallgraph doesn't operate at compile-time, but via run-time tracing.

&gt; In short, how to make this work for haskell?

The problem with Haskell is actually lazy evaluation:  function "calls" can happen arbitrarily far removed from the place that initiated the call, so simple tracing doesn't tell you much.  The Haskell debugger `buddha`, for example, gets around this by instrumenting your code to construct an "[Evaluation Dependence Tree](http://www.cs.mu.oz.au/~bjpop/buddha/onlinedocs/UserGuide/node5.html#SECTION00520000000000000000)", from which it can (among other things) extract call graph diagrams.I found unison quite cumbersome when I tried it as a two-way synchronizer. For the first runs I checked what it was doing and I came to the conclusion that I couldn't trust it to do its work fully automated.

So I just dumped the two-way synchronization and now I use a combination of bzr/svn/rsync/rdiff-backup.I dunno...I haven't read *Founders at Work*, but I just read Cal Henderson's *Building Scalable Websites* and Jeremy Zawodny's *High Performance MySQL* (which describe what they did at Flickr and Yahoo Finance, respectively) and found both very helpful.  *Founders at Work* is a bit of a different type of book, but if it has actual descriptions of problems that the founders faced, it's likely helpful.  It's much more productive to learn from others' mistakes than your own.

Also, these sort of corporate histories are often really good for setting realistic expectations.  It's interesting to read about Microsoft in 1980, for example, and realize that they were just a handful of guys in an office (I think the MS-DOS team was like 6 people).  Or to realize that Microsoft got the DOS contract because Gary Kildall of CP/M fame was out of the office and his wife refused to sign an NDA without their lawyer present, and so IBM went to Microsoft.  Oops.

Many startup founders think they're going to get rich with their first big idea and cash out with $100M after a year.  It's nice to hear about, say, Flickr/Ludicorp having to try out 4 different businesses (the first 3 of which had *nothing* to do with photos) before they found one that people would buy.Or even eulogy. :)[deleted]You're considering C? Are you mad? Or an academic? C does not have objects, it does not have templates, and it does not know that functions are different when you overload the args. They brag about being concise, which translates to not having much of a language. of course, it has a fine core so I can begin my application project by first erecting a language like C++ atop C, but what's the point.

Excuse me if I just use C++ and just get to work on my application /this/ year.[deleted]I don't know if it's affordable for you, but there's backing up to [Amazon's S3](http://www.google.com/search?q=backup+%2Bto+S3)Great article on language choice, optimization, and program design with business in mind. Interesting spin on scripting languages and C.Explanation for non-Algol programmers, please?You'll need to do more than point to papers since 1974 to convince anyone that progress is taking place - generating a lot of research papers is what academia does, and it doesn't signify that new big ideas (as big as the ones that went before) are coming along. There *is* progress though, in types and concurrency and all the other areas discussed in the LtU thread.I'm messing around with that now; the main problem is that I can't rsync to it, but I'm working on solving that.I have flickr for my photos, an ipod, my bookmarks on del.icio.us, and my code in svn at billmill.org.

The problem is, I've got ~60 gigs of music + dvds + personal data that needs storage. The iPod is not a solution beceause it's not offsite. I want a commercial, stable company with fire protection to host my data.Jungledisk is something I've been playing around with, it shows some promise. It is kind of sketchy though.This is great. Python applets, someday.Because the human memory is infinite!11!Placebos work even with computers apparently.Or really large and not written by you.[removed]&gt; I don't know any type systems that aren't a total pain

Wow.I think Slava's point is that all those languages, widely considered to be very different, are really descendents of Algol, while Lisp, Forth, Haskell and Factor aren't.I'll repaste here what I said there:
---
Two points:

1) The calculus can be only invented once. In that sense, no decade ever surpassed the 1660s in science and technology. Of course, that's not true. Where were computers concretely in 1972? Where are they now? What do we know about, say, relational database performance or strictness analysis for lazy languages?

2) Intellectual / academic / scientific progress often takes time to trickle down to the concrete computing world. How long since some form of OO has been massively adopted? It could be argued that the discovery -&gt; application time is increasing in computers because platform effects are increasing as user base increases -- network externalities, switching costs, etc. But, as you said, type inference? 1978. Lazy evaluation? 1980s. Bird-Merteens formalism? 1980s. Monadic programming? Late 1980s. Theorems for free? Early 90s. More general categorical programming? 1990s. Maybe it'll take longer until these things make a concrete effect, partly because in the late 60s/early 70s universities drove a lot of the technological adoption and now we basically depend on what Microsoft does (it *is* embracing FP though) and partly because some new ideas, like categorical programmng, require a stronger background to grok.

As far as I'm concerned, there has never been a better infrastructure for writing programs that can be formally thought about. Golden age schmgolden age, it doesn't hold a candle to the 1660s when Newton invented the worldI want to make a comment about Digg users here, but I'm sure it's only a matter of time until somebody submits the same thing here and it makes it to the hot page.Duh, this is very similar to the technique described [here](http://thedailywtf.com/Articles/The_Intentional_Slowdown.aspx)&gt; you trust your data in some foreign world

On which planet are they located? :)Why all this hatred? I can't understand it. Did a Java developer stole your girlfriend or something?

Wtf does it matter if some bad programmer is using the same language as you do? Why always this 'we against them'? Is your ego is really so weak that you need to define yourself by using a certain programming language? If I find something useful it absolutely doesn't matter for me, if I'm the only one who uses it or if everyone on the world uses it.
[deleted]On the other hand, I upgraded to Python 2.5 and found I could not install TurboGears.  At all.

Since then I have been developing applications using the built-in WSGI runner and Selector... :-PWell, I'm wrong, but not for the reasons you point out.  I checked the [Wikipedia entry](http://en.wikipedia.org/wiki/Gary_Kildall#IBM_dealings) after posting.  Gary *was* out of the office when IBM came calling, but it's debatable whether that was the reason he lost the contract.  There were a variety of possible excuses: Digital Research might not have been able to meet IBM's schedule (personally, I think this is the most likely) or IBM may have been pissed off at having to wait for him to get back.

Gates was the one who suggested that IBM approach Digital Research in the first place, though, so I really doubt that he got the contract because of his mom.  I suspect he got it simply because IBM thought it was more likely that he could deliver on it.Of course, Unix still isn't "huge" in the business market that they meant. You don't walk into businesses and see people using Unix/Linux boxes on their desktops--other than the programmers and some scientists. Which were exactly the people they said were using and would continue to use Unix.

Server space is completely different and several times in the video they mention how much penetration there was already to servers.

Also, it's important to remember (like foonly apparently didn't), that "Computer Chronicles" was targeted toward home users and small business users. From their perspective, DOS was where it was right then.

The prediction that soon LANs would replace minicomputers with terminals, by the way, was exactly right. From 22 years out, it ages pretty well and it's a pretty neat historical document. Not really worthy of any attempts of mockery.

(It also had the hot news about Commodore's new 128 model. And the news about the roll out of Atari's CD-ROM drive in a few months at $500 ($936.80ish today) with 500MB was fun to see.)I wonder what the cpu/memory usage profile of this thing is. For instance, if I were to start instantiating a million or so objects in Python, I'd kinda know what to expect performance wise.

Time to climb into my suit, board the rocket and take an architectural trip to the moon. :-)

I wonder what relation-oriented-programming would look like, and I wonder what it would look like to an prosaic old OO person like me. The code they showed doesn't mean anything to me unfortunately, given I'm a prosaic old OO person. (POOOP for short.)
I sincerely doubt this is limited to Digg users.priceless :)Similar here.  I put a little extra effort into maintaining each top-level home directory to contain less than 700MB (or can be compressed down) so each can each be copied to CDR often.  Then, take copies of the CDRs offsite!And while we're at it, why did the Lisp-organization fork that language into so many mutually incomprehensible languages such as Common Lisp, OcaML and Haskell?&gt; ...this implies that Java programmers can be something other than useless.

If anyone is going to make Java programmers look good it's going to be mainframe programmers.

(mostly j/k :D)The ramifications of putting Alan Kay and Bertrand Meyer together in a small room for a few hours would be quite interesting...Some people will find ways too polarize just about anything...  But seriously, isn't this "Us vs Them" mentality on just about everything getting to be a little much?I've been using it for about a week to synchronize my hosted web stuff with a local archive - a bunch of static files plus a file-based wiki - and after the first run through, it hasn't done anything questionable.  I've been running manually every time just to make sure, but I'm pretty confident at this point that nothing will get clobbered when I start running it in batch mode.

The only real issue I see for this level of usage is a situation with conflicting file changes, but for what I'm doing this is less painful than rolling my own solution with rsync. It'd also be pretty easy to add a version control layer of some sort.via
http://ifacethoughts.net/2007/02/12/command-lines/
Command Lines on iface thoughts
I first saw this back in the 90s, well before the date on this message. Does anybody know who actually wrote it?heh, welcome to my world. I came into a job at a behavioral healthcare organization that had a very similar setup.

The good news is that by merely upgrading the system to something that approximates what most organizations take for granted, I have become the IT god. 

One phenomenon I did run into though that really burned me came when I improved many of the achingly slow reports that were run out of the thing. It turns out that many of the employees LIKED that it ran so slow. It gave them time to chat and catch up with their shows. Uh, sorry ladies.Change happens when the playing field changes.  In the golden age, researchers saw three directions that had almost historic inevitability 1) networking, and 2) GUI and 3) personal computing.  Much of the golden age research mined possibilities in those directions and produced new ones.

If you want another golden age, find different modes of human / computer interaction or a completely new way of using computers.  If you do, the researchers will come.[removed]I like domain models but hate the data access object (DAO) pattern. This blog entry makes DAOs slightly more acceptable.One man's peevishness is another man's cynicism...Obligatory:  Erlang, Haskell, Mozart/Oz.[removed]&gt;or random exceptions happened randomly

How else would they happen?Erlang would probably be the best to starts with (I'm not sure Haskell should be used for industrial applications yet as the STM support is fairly recent, and the language is also extremely brain-crushing; and I don't know Mozart/Oz at all): parallelism is at the very core of the language, SMP has become automagic since R11B-1, the language is fairly simple and message-passing concurrency is... liberating.[removed]i took a look at the source, looks like its just creating a cPickled file.  so it wouldnt be very useful for anything beyond a few thousand rows.I think he meant "random exceptions at random times".you wont find anything better than SQL for efficient relational operations on datasets of arbitrary size.  also curious how many years experience you have with SQL such that you think its "insane" and "hideous".Yeah - what I read was that all IT people are stupid - ESPECIALLY java progs.

Get a real job - like management.He had hair!This is why we substantially more erudite people peruse Reddit rather than Digg. I find that site highly annoying. Used to love it until everyone started flaming each other for the OS they use. Der. Safaris used to be all about killing beautiful animals.

Still plenty of dumbasses trolling around teh reddits tho. Me especially.His comment is just a consequence of the Curry-Howard Isomorphism and Goedel's Incompleteness Theorem. ;-)You could do a few things. You could have separate spaces in the call graph for distinct closures, or you could merge all closures with the same block of code together, but both are not really satisfactory. This is actually a practical problem in control flow analysis of languages with first class functions, because you have to decide on some scheme of identifying distinct closures (as there are potentially infinitely many possible bindings of variables at each point) that does not identify too many closures.Perl 6 has parallelization syntax [(which works)](http://pugs.blogs.com/pugs/2006/10/smp_paralleliza.html); it's built on Haskell. 

More realistically, writing a task manager like KDE's [ThreadWeaver](http://api.kde.org/cvs-api/kdelibs-apidocs/threadweaver/html/index.html) is probably a more practical place to start. 

I've read that an earlier version of Oz had implicit parallelization (a la Perl 6? is it automatic in Haskell?), but it was a bad idea, because moving small bits of work to a new thread or process, can hurt performance because of the overhead. And, it's hard for the runtime to know how long any given statement will take to run (impossible even). So, I don't know about Erlang, but in later versions of Oz, threading must be explicit (though it's still convenient to do).[deleted]Haha. But can a language without macros really be considered a Lisp?I've seen a lot of talk about how with many-proc architectures, functional languages are the way to go.  Does someone have a good resource that explains exactly why?

I like functional languages, but am by no means an expert, which may be why I don't see why things are possible in them that aren't in other languages.[deleted]I think the question you are asking is the wrong one.  The true reason why people can leverage multicore's with languages such as Haskell easily is not because it is functional but because of the `referential transparency`.  Typically, functional programs will have bigger parts of the code that are referentially transparent, but if referential transparency was introduced into imperative programs (perhaps at a coarser level) they too could easily leverage multicore. 

The main issue when dealing with multicore is detecting where the dependencies (usually data-dependencies) reside, as these will force tight coupling and therefore either high costs in locking, or the choice to keep the code sequential.  When your code is referentially transparent, you will have fewer places where you have this data-dependency coupling, thus giving you more freedom to split the code over multiple cores.Windows claims 2 gigabytes of virtual address space for itself, on 32bit systems. (Regardless of the amount actually needed.) This is just a performance optimization: by looking at a single bit any address can be classified, which is really fast.

So programs cannot have a process heap larger than 2 gigabytes, and that is a pretty bad limitation. Computers sold come with 2 gigabytes of ram already on the medium end, and with 4 gigs on the high end. So moving to a 64 bit architecture is an absolute necessity.

Also, some versions of Vista don't allow 32 bit and 64 bit installs from the same CD. So unless you plan on buying Vista twice, I recommend the 64 bit version - even if you have only 1 or 2 gigs of ram. (Assuming you have a 64 bit processor, otherwise the choice is obvious.) This way you can at least upgrade later, or get a new PC without windows and transfer your copy.[deleted]&gt; Although my experience with databases is limited, SQL does feels akward.

All powerful tools are awkward at first. Don't give up on SQL too soon; no matter what other solutions you try, you'll almost certainly come back to SQL some day.UIs are to be built upon. How do you build upon search?

And i would describe his idea as a convergence of GUI and command line, combining the strengths of both approaches. Spatiality and pretty pictures via GUI and a rich command language of text.

A nice first step is the Plan 9 "editor" ACME. It has rich text commands and spatiality, but no pretty pictures.[removed]The surprising thing is just how ordinary this story is.  I'm constantly amazed at how few really good database guys are around.  There are a lot of people who know who to write a basic query, but when you are looking for someone that can design, architect and ultimately build a solution that will be fast against a relational database, the pickings are slim.  Personally I think all anyone needs to do to guarantee themselves a high paying job for the next 10 to 15 years is really learn how databases work.  The few really good database guys I know, don't know much about programming.  So if you know programming well and you know databases well, you will never have a hard time finding a good paying job.or:

-is a really bad-quality video

-contains a linkjacked article

-praise for liberals fighting "neocons" (after siding with them)

-praise for Apple/Mac OSX

-attacks Microsoft

-attacks any other Big Bad Corporation
I started off in 1998 as an AS/400 RPG programmer, and I know what type of environment this guy is describing. (I got  out in 2000)

These developers are trapped in the stone age. They where taught the AS/400 way of doing things and have never looked at anything else. The big reason for this is that everything else is alien. 

An average AS/400 programmer has never:
1) Installed software
2) Created a database (They only create physical and logical files)
3) Opened an IO port
4) Used a file system (Seriously the AS/400 only has libraries, and objects, thats it)
5) Used a custom IDE
6) Used a regular expression
7) Written a shell script

So even if you have 15+ years of experience, you are a total noob on anything outside of the system. 

They know this, and fear it.
&gt;I say that without having the slightest frigging clue what "chaff" is, but let's assume it's some sort of inferior wheat substitute, possibly made from tofu.

Worth the read for that line alone.Vincenz's answer points out the theoretical advantages. Generally, the argument is phrased in terms of "automatically parallelizing code".

The flip side is that a paper was written analyzing how much of a functional program could be automatically parallelized, using real programs to feed the analysis, and it was shown that there was generally no more than 3 or 4 core's worth of work. It's become evident it's going to take a long time for these results to sink into the "automatic parallelization" community; I for one don't pay much attention to automatic parallelization claims unless you can directly reference that paper and tell me why it doesn't apply to your scheme, hopefully with a naturally-written program that does something that isn't embarrassingly parallel.

On the more realistic front (i.e., "existing real-world programs"), functional programming can make shared-nothing architectures reasonably efficient and safe. Erlang is based on message-passing, and if you can guarantee all messages are immutable, then you can freely share them between processes without making copies, yet still retaining "shared nothing". Trying to do that with mutable languages would be much, much harder. (I've tried to work out how to do a shared nothing architecture in Stackless Python, and while I think you can do it by choosing to limit your messages to immutable data types, it's something you have to try to do, and the compiler/runtime wouldn't really understand that's what you were doing.)I don't know of anyone major but Opera that uses Pike (at least openly).  

I don't know if Perl6 would be C-like enough, but who knows.Where are you located that it's so difficult to find good people?
The performance problem was not enough memory. That should always be the FIRST thing you look at, since it's cheap and easy to remedy. What's the first thing you do to speed up an x86 system? More RAM.
The other problems he mentions show a lack of undetstanding of the AS/400 architecture (by the author) or a lack of good practices (by the AS/400 team).I've got a couple of ideas that people might be able to shoot down:

1. Tree-based query systems - the sort of thing that XSLT does

2. ORM is still evolving and has yet to reach elegance. Sqlalchemy appears to be at the edge at the moment.

3. Business-logic abstraction still has a long way to go in the mainstream yet, and this is what I've been working on.

Note that these are all 'adaptor' concepts that link one paradigm to another.rsync has a bandwidth-conserving option:

 --bwlimit=KBPS          limit I/O bandwidth; KBytes per second

rsync works with Windows filesystems too; do spend a few minutes skimming the rsync man page. It's quite a fabulous tool.Doesn't [SQLAlchemy](http://www.sqlalchemy.org/) already make Python relational?Stevey has written some nice articles.

Can we please not inflate him to the status of demigod?  Lest the usual fanboyistic drama play out.
As one who primarily works in SQL, I find [C,C++,C#,Java,etc] awkward. It is an expected sensation.
&gt; you wont find anything better than SQL

[Relational algebra](http://www.cs.sfu.ca/CC/354/zaiane/material/notes/Chapter3/node7.html), which is what SQL was supposed to be based on.  Actually, SQL is the bastard child of the relational algebra, relational calculus, and COBOL, which explains some of the antipathy towards it.  Many of the people who dislike SQL dislike it because it strayed too far from the relational algebra's mathematical roots.In this case though, if you read his article, he hints that he knows it'll get a lot of backing. And he works at Google.On a side note, when I hit submit on this previous comment, Firefox crashed.
Modern Lisp, Scheme, and Haskell are descendants of Algol in the sense of block-structured programs.
I dunno about him, but my coworker's former employer (Boston, financial software) went through 3 DBAs that didn't know jack before they found one worth his salt.

It may not be that they're rare so much that they're crowded out by people who claim to be DBAs but really don't have the skills.  It's so easy to install Oracle, learn SQL, and then claim to be a database guru.  And if the apps don't run fast, everyone blames the programmers.Many of the works at http://www.complexification.net/gallery/ ([reddit](http://reddit.com/info/5928/comments)) use Processing.&gt; extreme late-binding of all things

Now that is something totally missing from C++, Java, and C#.Programmers come and go. You're always going to want to use popular languages so that you can find replacements. The NBL matters even on the server side.With Bjarne Stroustrup as well :)Five Things You *Don't Want To* Know About C++
Jerf, would you have a pointer for that paper?I agree with you that the unit tests should be self documenting (which in a way is what makes this work). A couple of extra @ tags in the comments just helps fill in the word docs.

I really don't think they knew what they wanted (or the impact) when the statement was made. I actually pointed out how voluminous this would be during the class but, I don't think they got it. Maybe the topic will be re-opened.

But, I figured rather than try to fight a crazy requirement, by fulfilling it they could discover the absurdity for themselves.It is called progress. Ask Schemers, they know :)
After the golden age came:

1. [BLAST](http://en.wikipedia.org/wiki/BLAST)
2. [Pagerank](http://en.wikipedia.org/wiki/Pagerank)
3. [GPL](http://en.wikipedia.org/wiki/GPL)&gt; I'm not sure Haskell should be used for industrial applications yet as the STM support is fairly recent

There's a [lot more](http://haskell.org/haskellwiki/Libraries_and_tools/Concurrency_and_parallelism) (and [more](http://haskell.org/haskellwiki/Research_papers/Parallelism_and_concurrency)) to Haskell's concurrency and parallelism support than just STM.

&gt; I don't know Mozart/Oz at all

Like Erlang, [Oz](http://en.wikipedia.org/wiki/Oz_%28programming_language%29) is considered a "concurrency-oriented" language, but it's also very notable for its strong, integrated support of basically all the major programming paradigms, including OO, FP, eagerness, laziness, shared-state, message-passing, dataflow, logic, constraint-solving..."Visual Studio coddles the programmers enough."  It coddles *programmers*, but I wasn't talking about them. I was talking about *occupational programmers*, i.e. people who program maybe 10% of the time. And actually, Visual Studio is the antithesis of coddling because it has far too much stuff; I think my point if far closer to your view than you realize.  As an occupational programmers what I'd far rather have is a really tool that has a built-in intepreter.

As for annoying professionals, why does it have to be either/or?  I wasn't arguing that Microsoft should dumb-down Visual Studio.  I was arguing that they needed a completely different track for occupational programmers.

You'd argue occupational programmers ramp it up, eh?  Understanding reality not your strong suite markedtrees? That's like saying to you:  Got a pain in your stomach?  Learn the medical reasons why and quit bitching.  Don't understand your taxes?  Well, dive into the tax code and learn it!  Got a lawsuit pending?  Better head down to the law library.  Outsourcing got you down? Well tough! Just accept less pay!  

The problem is that occupational programmers are professionals getting a job done, but that job isn't programming. If programming is easy enough for them, then they will use it. If not, they won't. But it is a huge market. Learning the framework in the depth that a professional programmer needs is just not an option, and completely foolish and irresponsible of markedtrees to suggest that it is.

Yes, people like markedtrees can gloat all to himself that he's better than the occupational programmer, but the *potential* occupational programmer comprises more than 99% of the population and many of them have expertises he could only dream of having.  It's too bad he wants to deny them the ability to automate things.If your developers are x% more productive with their language of choice rather than the NBL, then you owe it to yourself to do a basic cost-benefit analysis and see whether the ability to treat your developers as replaceable cogs is worth it. If you're coding E-Enterprise Java Business Solutions, then it may very well be worth it. But if you're actually doing something interesting, then the productivity gain of using a less-popular language may very well outweigh any amount of ability to throw a dart out the window and hit a Java/C# developer in the eye.Curiously, most of these are the same skills you need to succeed as a professional software engineer.  The only one I can think of that's *not* applicable is LaTeX (and the specific advise about graphing, diagramming, and bibliography software).  I would've expected there to be more of a gap between academia and industry.Really? I kinda fit that description.&gt; but if referential transparency was introduced into imperative programs (perhaps at a coarser level) they too could easily leverage multicore.

Referential transparency is not enough, you also need out-of-order execution.

And ensuring that a piece of imperative code is both referentially transparent and out-of-orderable is very difficult, to say the least.Ummm...don't diss all AS400 programmers. The ILE stuff lately is very robust language. Plus, with Websphere, true AS400 programmers now need to know Java component development to 'webify' the mainframe environment for browser usage. I have never seen a more stable backend system than various AS400 deployments. You just gotta know how to build around them.When you get type errors in Haskell, error description can be longer than the program you are writing :)&gt; Like Erlang, Oz is considered a "concurrency-oriented" language [etc...]

I at least know that much about it, but that's just the marketing stuff, I don't have *any* experience with the language while, on the other hand, I've done some basic stuff with Erlang and Haskell, and have a feeling of both languagesI really hope so. Solaris is pretty damn good. ZFS even cooler.QuicksilverI hope someone copy-edits that before it's published in ACM.&gt; how often do you see code that looks like this?
&gt; template&lt;typename T, template&lt;typename T&gt; typename I&gt;

I see it often. If you use a lot of templates, you're lazy and write a lot of typedefs with them so this kind of construction is not rare.

&gt; mutable members and pointers to members

Everyone know about these things, it's the basics of C++, it's not obscure.

Nothing on this list is hidden, it's the basics of C++. The strange stuff of C++ is the order of initialization in multiple inheritance, protected inheritance or the parameters you can give to the new function for example.&gt; UIs are to be built upon. How do you build upon search?

by adding additional commands?

&gt;  convergence of GUI and command line,

So my terminal window in Linux is a convergence of GUI and command line too just because i can launch graphical applications with it too? I don't think so.If a piece of code is referentially transparent then it is by definition out-of-orderable, at least that is my gut reaction.  By my definition of referentially-transparency, some code-block will not affect the outside world except by the result(s) it returns.  Therefore it is perfectly out-of-orderably executable with only one top-lock to wait for the result (unless you have a good scheduler/scheduling).  Basically you have a partial-order between your different blocks, anything which is not comparable to another is therefore transparent.

I think the main difficulty is making imperative code referentially transparent as it sort of goes against the grain of the language.  Nonetheless, and this is also in part in reply to Jerf's excellent remarks, I think referential transparency would be MUCH more interesting at the coarse level.  Indeed if the bottom-level leaves are referentially-transparent you get some parallelisation but not much.  HOwever imagine coarse-level referential transparency (perhaps by defining separate heaps ala separation logic).  This would be much more interesting as  

a) you don't have an explosion in blocks that you have to schedule (as scheduling is a hard problem)

b) you still have out-of-orderable execution between those blocks except at very specific and well-defined points where the results from one grain is required by another.There are definitely a lot of people who **think** they know databases really well.  Unfortunately most of them have never really built a big system or had to support it.  I do think a big part of it is what nostrademons says too, the amount of people who are really good at it, is vastly smaller than the number of people who really are good at it.&gt; But if you're actually doing something interesting,

This is a critical predicate.  You've now excluded 99% of what companies want programmers to work on.
The IBM AS/400, iSeries, i5, whatever-they-call-it-today, already has a simple command UI... hit "F4" to prompt the command - which normally provides all the information you need to know ( and you can create your own with set values, help text, restrictions, etc.) I am unaware of other OS' that use this form, but they might learn from the example.It's the persistence that got rewarded. If he had only presented the facts he would've been laughed out of court.I believe good developers are never replacable just like that. 

What you really want are developers who are good enough that they can learn whatever language you are using in a few weeks anyway and if a few weeks is a large percentage of the time you employ them you probably should solve some other problems first before you think about the language you are using.&gt;If you developers are x% more productive with their language of choice rather than the NBL

Do I have a ready supply of programmers with those skills in the non-NBL?  No.  That means training.  Training costs $$$.  People come, people go, so more training has to be factored in.  Take that cost over a decade and that time saved using the non-NBL ends up being a huge loss.

&gt;If you're coding E-Enterprise Java Business Solutions

Stop writing E-Enterprise Java Business Solutions and just write solutions, with Java or C# or whatever, and move along.  I have yet to see a cost saving from porting an existing system from language X to language Y.(For those interested, the distinction is roughly that an elegy is in (solemn) mourning, while a eulogy is in praise.)I for one cannot wait for the command line resurgence. Sometimes it is just much quicker, plus it makes it easier to automate tasks.if this limp rant had been thought through, it might have been worth reading.&gt; We should be careful not to confuse a manager's zeal for killing trees with process.

We shouldn't confuse it with progress either. Sadly, they often do.Thanks, I was in fact not aware of the word 'elegy' and assumed that cale meant 'eulogy'.  You learn something new every day :)

**Note:** It's curious how the score of our posts are exponential (at least atm: 10,4,2,1).  I guess readers' interest declines with depth ;)SQL is:

1) Effective, efficient, and powerful if you take a few dozen hours to try to master it.

2) ubiquitous

It is my opinion that most of the hatred towards SQL is irrational.  It is a tool that helps developers be productive.

I would never claim it is perfect, though.[deleted]Actually, I found that a bit curious too.  It's very focused on the tools (not the tips) and very specific tools.  He does admit that, near the end, I suppose.  There's a lot of skills which are more general, such as the ability to read an academic paper, to chase down references, to decide heuristically what is worth your time, successfully work together with fellow students and professors, etc.

And there's plenty of room for LaTeX in industry, if you care about nice looking documents!  I hate reading specifications written in MS Word, especially if they do try to throw in formal language.  It's so crude, harkens back to the 60s when people penned in symbols the computer couldn't typeset well.  (I was reading a paper in an old (definitely pre-TeX) journal today that had hand-written curly brackets -- in a published book, no less.)
Seems like the sort of things one could achieve by scheduling implicit parallelism around explicit futures. The problem of sharing could be avoided by restricting the sort of functions that can be called in a future setting; e.g. only immutable closures. &lt;/baseless speculation&gt;\*cheers\*I'm guessing he was referring to "Limits to implicit parallelism in functional application", http://www.detreville.org/papers/Limits.pdf .My gut matches with that paper, that automatic parallelization isn't really possible without _some_ compromise on the part of the programmer.

I _do_ still hold out hope that a system can be built on top of referential transparency that with _relatively little_ annotation or rearrangement still lets us get a lot of bang for the buck. We know Erlang scales, but it also requires a programmer to explicitly break the system up into pieces themselves, and you can still argue that is something the programmer shouldn't have to do. I think at the extreme ends of performance that will always need to be an option, but I hope it is someday seen as something akin to writing assembly by hand, an optimization last resort or something for restricted domains.

I have no idea what such a system would look like, but I think it's clear that building it on top of traditional imperative languages isn't going to happen anytime soon. I don't know if it's possible; call me 50/50 on the possibility.What area lacked enough throught?  What would you have liked to see me expound upon?&gt; 4) Used a file system (Seriously the AS/400 only has libraries, and objects, thats it)

that sounds great, actually
Bingo, and bookmarked.Is [this](http://programming.reddit.com/goto?id=wqry) it?Haskell and Erlang are the newest languages in town (worth using), and they are from 1990 and 1989, respectivelyI think that fundamentally we are saying the same thing.  It would be automatable once the coarse-grain co-referentially transparent blocks are designated with some programmer notation.

Suffice to say that it -is- possible to do this on top of imperative langauges and there's a lot of research going into this direction at the moment, but yes, it is not totally automatic.  It is programmer-driven but not programmer-written.And here I thought the GPL was the reason for the fall of mankind, shucks!Only if you plan on gassing the room.

*hides*But is Goedel's Incompleteness Theorem still true under intuitionistic logic?Yea, in Java you have an application class an main method at least, so you don't have to be afraid of that in Java.[deleted]Does anybody actually get code reviews for their side projects?too long. java uninteresting.you make a good point.

perhaps a bigger problem for this guy is he is not focusing his resume toward the type of work he wants to do.
IE, if he wants to work as a system admin, there is no point even mentioning VB on the resume, better to use the space to talk what a great unix admin he is. Or, if he wants to be a VB programmer, leave off the admin experience.
Nobody wants to look at a kitchen sink resume (and most people don't believe them anyway). Better to tailor the resume to show how your experience can be applied to the job you are interested in.Functional programming favors constructs like map, filter and fold, which can be easily written in a distributed way. Object-oriented languages (as far as i know) don't offer constructs to send/call the same message/method to multiple objects at once.

Personally, i don't believe functional programming is the silver bullet for parallel programming. The language level support for FP you get in Python (and maybe C#?) is enough and you don't have to sacrifice classical OOP like Haskell.

A little virtual machine support to distribute code or ensure data immutability as in Erlang would help, though.I think someone should write a real document on tips for getting a PhD...  like learning how to live on chips for a month in the computer lab, how to present empirical data to  prove your idea is worth squat, the method of using lots of greek symbols so that journal reviewers will have no choice but to accept your paper, and the right way to choose a professor that just wants to churn out PhDs in a short time."Ideal" is a very strong word.  What you're really asking is why they're better than the alternatives (e.g., C, C++, Java).

The answer to the latter question is: because they work at a higher level of abstraction, in the literal sense, than those languages.  The functional style specifies less about how the computation needs to be carried out than do the imperative, "close to the machine" languages derived from C.  

Many programmers (myself included) quite enjoy being close to the machine, but the sad fact is that machines are getting to be complicated enough that programs (i.e., compilers) can often do a much better job writing efficient code than humans can.  

Because functional languages tend to make many fewer guarantees about exactly how the program will be executed, compilers for functional languages have more latitude in using new processor features than, say, a C compiler typically does.

This property is not completely restricted to functional languages -- a very smart C compiler might well be able to take advantage of these features.  The key point is that the functional languages have definitions that often [make the compiler's job easier](http://wwwuser.cs.rose-hulman.edu/~froydnj/archives/2006/07/28/high-level-languages.html) in this respect.In the case of intuitionistic logic, you have a less favourable situation. Even pure intuitionistic logic is incomplete, unlike pure classical logic, which is one possible completion of intuitionistic logic. If you have first-class continuations, you get the law of the excluded middle, classical logic returns, and then Goedel's Theorem applies.Congratulations, somehow you've managed to turn an article about a command line (r)evolution into a contemptuous comment about Vista.First off let me say that the AS400 system is without a doubt the most stable system I have ever worked with. So I'm not out  to diss.

The problem is that they are locked into the IBM way of doing things.&gt; The calculus can be only invented once.

Do we need a new calculus for quantum computing?http://en.wikipedia.org/wiki/Judy_array

http://en.wikipedia.org/wiki/Skip_list

http://en.wikipedia.org/wiki/Elliptic_curve_cryptographyIt's also not about Haskell, or functional programming of any kind. Though I'd expect to see a few comments suggesting that the feature could be implemented in Erlang to offer better concurrency...Quantum computing is usually expressed in terms of quantum information theory, which is formally developed in terms of operator algebras and completely positive maps.&gt; If so I would guess that that Steve Yegge has heard about one or more Google teams tooling up to deliver Apollo applets (probably versions of Maps, Calendar and so forth).

Nope, Steve Yegge has now updated his post with a note claiming that it's not based on any Google insider knowledge, so presumably I was wrong. I'll still be unsurprised if Google releases some Apollo apps sooner or later.From my point of view, it suffers from a serious case of category error.

Solaris is an OS, consisting of its kernel and a bunch of user-land utilities, some of which are strongly linked to the Solaris kernel, some of which are basically interchangeable except for command line switches and other historical baggage.

GNU is the user-land utilities and Hurd, and the overarching project/goals of RMS.

Linux is a kernel, and from the point of view of RedHat and other distributions, a bunch of other user-land stuff, mostly part of the GNU project.

If you mean GNU user-land is going to magically be replaced by Solaris user-land, the big question is why? If you mean the Solaris kernel is going to replace the Linux kernel, the question is what user-land will go along with it---GNU or Solaris or some mix? And who is doing the replacing? Why will they do so, what will they gain, and how does that affect them and anyone else?

The only significant differences you mention are strengths of the Solaris kernel and a GPL revision. Very few people pick their software purely on the basis of GPL revision or licenses or what RMS wants them to do. You also mix in some baloney about GCC being replaced by an Intel compiler, when GCC runs fine under Solaris. 

RMS can't take all his code back and restrict it to Solaris or anything else. At best, he can re-license it under GPL v3, and everyone who doesn't like it would fork the latest GNU under GPL v2-or-later, and keep it under GPL v2. Realistically, he will continue to lobby for full GPL v3 systems, and people will listen when convenient, and ignore him otherwise, much as they do today.[deleted]You'd think download.microsoft.com could provide more than 20-30KiB/s. It must be swamped right now.

Edit: It's giving me ~250-450KiB/s now.[deleted]Solaris is an excellent unix; if it's GPL3'd, then that'd mean there'd finally be competition inside FOSS.  What's to lose?

&gt; Well, Jonathan Murray thinks it may have something (actually, everything) to do with IBM’s business model and its fiduciary duty to its shareholders...

Well, Jonathan Murray would just have to be spot-on the money.[deleted]I can already smell the mimeographs!What the hell is the "pearl"?[removed]I don't intend to convince anyone today that anything today constitutes progress.  People looking back 40 years from now will have a better perspective -- and may benefit even from this discussion, when they come to lamenting about how little worthwhile research they get anymore that benefits declarative programming.[If you want data parallel arrays](http://haskell.org/haskellwiki/GHC/Data_Parallel_Haskell), there's no where else to look other than Haskell at the moment.

GHC is quite the playground for multicore concurrency and parallelism:

* [Very light threads](http://haskell.org/ghc/docs/latest/html/libraries/base/Control-Concurrent.html) (i.e. [see the shootout](http://shootout.alioth.debian.org/gp4/benchmark.php?test=all&amp;lang=ghc&amp;lang2=hipe))
* [Shared state locks](http://haskell.org/ghc/docs/latest/html/libraries/base/Control-Concurrent-MVar.html)
* [Transactional memory](http://www.haskell.org/haskellwiki/Software_transactional_memory)
* [Beautiful concurrency in STM](http://research.microsoft.com/~simonpj/tmp/beautiful.ps) (postscript)
* [SMP support](http://www.haskell.org/ghc/dist/current/docs/users_guide/sec-using-smp.html)
* [*implicit* parallelism](http://haskell.org/ghc/docs/latest/html/libraries/base/Control-Parallel-Strategies.html)
* [Message passing channels](http://haskell.org/ghc/docs/latest/html/libraries/base/Control-Concurrent-Chan.html)
* [User land threads](http://www.seas.upenn.edu/~lipeng/homepage/unify.html)

[removed][removed]From Google?@sickofthis....

I don't think you got the point of the article.  Also, did you read the link that the article refers to?  I thought it was clear that what is being contemplated is replacing the Linux kernel with the Solaris kernel in major distributions that use the GPL v3 licensed versions of the tools owned by the FSF (GCC, GLIBC, Samba, etc) and that Linux distributions would be relagated to using forks of those projects that are still based on GPL v2.[removed]Lecture 4's slides were all over the place, messy. Lecture 5 is fairly interesting.What's the difference between Haskell and intuitionistic type theory that allows you to use continuations? Type classes? Whatever it is, it's basically reducible to classical logic.Oh, so this is really a Lisp-joke then. Yeah I'm also not a Lisp-programmer... Thanks man.yes, show me the non-SQL relational algrebra engine that works efficiently across millions of rows.[deleted]a little bit.  this thing can go alot further since it has the advantage of not needing to translate into SQL (as can any tool that isnt mapping to a SQL database).

duhBut it's sooo fun to sacrifice OOP to the lambda gods!I don't believe typing commands and options comes back. Just too much to memorize. Instead little dropdown boxes with alternative selections - at least for all the options the system provides - as for modern IDEs, will be used to submit requests. This leads to just another standard dialog available for any GUI toolkit. This however does not contradict the authors assumption since it is the moral equivalent of a command line, wrapped into a GUI control. Note that I would appreciate this very much if for nothing else than simplifying the creation of user interfaces for programs that don't require advanced GUIs. A command line interface might than automatically be mapped to such a dialog without any additional work.I added linuxer to my 'friends' and your name sure does get highlighted a lot.  Not a bad thing, just an observation.

Post more of your own then!I am not saying it is bad.  It just hit me, that linuxer shows up a lot.

I guess we'll never know why Microsoft took out the very feature you describe... In Vista Beta1 you were able to use the GUI to create a search or filter a search... [Screenshot](http://www.winsupersite.com/images/showcase/winvista_virtualfolders_04.jpg)Most new ideas in in Haskell and Erlang are older ideas combined in new way. For example Hindley-Milner type inferencing was invented 1978. Of course there is nothing bad in this, but new inventions just don't seem to be as revolutionary as older.&gt; I thought it was clear...

I found it quite muddled also.  You need to do more to distinguish between the OS, kernel, and userland.  

&gt;replacing the Linux kernel with the Solaris kernel in major distributions that use the GPL v3 licensed versions of the tools owned by the FSF (GCC, GLIBC, Samba, etc) and that Linux distributions would be relagated to using forks of those projects that are still based on GPL v2.

Having a GPLv2 kernel doesn't prevent you from having a GPLv3 userland.  Having a GPLv3 gcc doesn't prevent you from compiling a GPLv2 kernel.  etc.
[removed]Well I don't think he's too specific, but he's in a position to hear things.The point is that Stallman has already made statements that the Solaris kernel could very well be a replacement for the Linux kernel.  That is a shot across the bow of Torvalds who is a staunch supporter of GPL v2.  Stallman wants ALL GPL v2 projects to migrate to GPL v3.  

The same arguments that the FSF are trying to use to stop Novell from using any GPL v3 code in their SuSE Linux will get applied eventually to all distributions containing GPL v2 projects.  Wait and see, Stallman is playing for keeps in his effort to completely kill intellectual property.yeah its not a bad idea. Just really really old in implementation. Most of the new good ideas are really really old ideas with new flesh on them.class PrettyAdHoc a b c where
        foo :: a -&gt; b -&gt; c
        
    instance PrettyAdHoc Int [Char] Bool where
        foo a b = a == (read b) 
    
    instance PrettyAdHoc Double Double Double where
        foo = (+) 
    
    instance PrettyAdHoc Char (IO ()) (Integer-&gt;Integer) where
        foo _ _ = succ 
What is the big bonus of the GPL v3 stuff vs. the widely available GPL v2-or-later version? (Apart from ideological purity.)

If this is truly enforceable, GCC under GPL v3 will probably be about as widespread as the Hurd. The world will coalesce around some other distributor of GNU-derived software that doesn't let ideology get in the way of running the kernel you want.
dforbes, you can help by submitting something.Hehe

&gt; A type class can be seen as defining a semantic relationship between its members. It is bad practice to introduce a type class just in order to overload function names when there is no such relationship.

;-)

Though, clearly, since *foo* must at least have the same polymorphic type across all instances, that makes it less ad hoc than a system that allowed arbitrary types for *foo* (i.e. true overloading, where you reuse the same name for completely different concepts).An intro to using the widget both with the extended HTML and programmatic techniques. Also, info on customizing the look and feel and the obligatory RPC-AJAX population.
Feedback for improvement is always welcomeYou really should go read what Stallman is out there saying about Linux, Novell, and GPL v3 vs GPL v2.  I know you disagree with my beliefs on all this, but Stallman really is acting like a wanna-be Carl Marx, but he's trying to use Joseph Stalin style tactics to accomplish his goals.Also, ad-hoc polymorphism, at least, as far as I see it, selects a meaning for the overloaded value based only upon information available *at the call site*.

The bounded parametric polymorphism which typeclasses give you allow some more flexibility. Consider:

    bar :: (PrettyAdHoc Double b c) =&gt; b -&gt; c
    bar = foo (5 :: Double)

Depending on the type of value the function bar is applied to,  different values for foo might be used.

This is sort of a poor example though. A better one would be
   
    sort :: (Ord a) =&gt; [a] -&gt; [a]

The sort function is not a class method of Ord, it merely uses the comparison operations Ord provides.

While you can abuse typeclasses to get arbitrary ad-hoc polymorphism effects, that's a waste of their expressiveness, and also tends to be quite awkward. (It's easy to end up with ambiguous types if you start abusing it in that way.)This article bothered me enough to both join reddit and write two long comments on ittoolbox. The description of this tripe as 'a limp rant' was being polite - Byrd demonstrates a complete lack of understanding of his subject matter and a functionally negligible lack of literacy. I'm normally civil (but almost never genuinely nice) and try to avoid bashing people simply for having a different viewpoint, but this is a poorly researched and poorly written article that does nothing but spread inaccurate and confusing information about Free software.The only thing Stallman can do is talk.  He has no power over you if you don't listen.  If his moral authority is all that is needed to kill off all "intellectual property", then it seems like that was a pretty weak concept that needed to be culled from the rest of the hurd (ba-da-ching!).

You should really visit with your doctor about upping your meds.It's actually quite an interesting video, it's more about Brian Beckman's work as a whole even than specifically about VB or Haskell. That's just a half-joking remark made in the middle of it.

He missed part of the description of a monoid, but I can forgive that. I found the discussion about monads somewhat confusing, as someone who knows very well what monads are about. Most of it is correct, but there are places where he was confusing, or identifying the right fold with monadic bind, but foldr doesn't even approximately have the right type to be bind for a monad. I suppose you can write the bind for the list monad in terms of foldr and the monoidal operations and unit (concatenation and the empty list). Perhaps that's where the confusion came from. Bind in the list monad basically maps a list-producing function over a list and concatenates the results. 

    x &gt;&gt;= f = concat (map f x)
            = foldr (++) [] (map f x)
            = foldr ((++) . f) [] x

It's nice to hear him talking about the monads as containers perspective though. :)There is an alternate calculus for quantum _mechanics_ (quantum calculus), where limits, derivatives, and integrals are re-formalized based on the quantization of time, etc.. but it boils down to planck's constant, and all the results are basically the same.&gt; ... Instead little dropdown boxes with alternative selections

I can't stand this kind of stuff. Anyone else hate entering dates into a webform, where you have to operate three dinky little dropdowns to get the day and month and year?

Google's got the right approach with the "one box" plus operators plus suggestions model. (eg, why would you ever use the gmail address book, when you can just type the first few letters into the to-field, and it already knows which contact you mean?)

Single text fields are great for the same reason that CLIs have stood the test of time; they're blisteringly efficient.General Interface is an AJAX GUI design framework, with it's IDE also published as a web application.Great resource for website building help.  Anything you need to start building your website today.[deleted]You'd prefer your type errors reported like this?

    zsh: segmentation fault (core dumped)  ./a.out

;-)CLI is making a come-back?  When did it ever leave?
[deleted]Who thinks `blockquote` is obscure? It's only used by every blogger in every blog post ever…Q: Space, mixed case, slash, backslash, question mark, colon, asterisk, quotation mark and control codes. 

A: What are, things that shouldn't be in file names?Stallman has the stroke of authority at the FSF which owns many of the moving bits of the Linux OS (not the kernel, but the rest of the OS), so if he decides to go GPL v3, well then they are going GPL v3.  How long do you think it will take the distributors to replace those bits if they want to stay with GPL v2?  It will be quite an effort.  Most likely we are heading into a new phase of FOSS software that's going to further fragment the FOSS OS landscape.I'm happy for the guy...but man, did it take a long time. Surprising, considering the guy seems to suggest that he's big into compiler-type knowledge.&gt;Most of the work since then seems to have been in consolidation and
&gt;integration (combining the power of the different ideas).

Don't knock consolidation and integration.  Or, for that matter,
testing and refining.
Unhandled exception.Agreed.

I think we need to first as ourselves what we want to do, then think of paradigms to solve the problems.

The author suggests commands that are closer to natural language.  Natural language will probably play a role in future UIs, but they lack the conciseness of GUIs and CLIs.

CLIs and GUIs both treat the computer as a tool that can do something predictable.  The complex tasks the author suggests (well, kinda mentions) seem better served by a nondeterministic, "intelligent" natural language system that itself uses the computer like a tool.

Personally, I think the turning test is wrong.  We will achieve AI when a human perceives a machine that acts nondeterministically.  It doesn't necessarily involve speech.  For example, when you do a search with ambiguities, the machine will act more like a human, removing some of the "why did that come up" results, but missing some obvious one.  The machine will try to predict what will be useful, not just what matches.[Previously seen on reddit](http://programming.reddit.com/info/wxa9/comments).

My answer is still [no, not really](http://programming.reddit.com/info/wxa9/comments/cwyp4). The "real" problem is the n-dimensional nature of the discussion, not the tree; there's only a narrow window where the interface might improve the situation, then the simple complexity of an immense conversation overwhelms the human mind no matter how you gussy it up.You probably want to look at [the numeric prelude](http://cvs.haskell.org/darcs/numericprelude/), or one of the other [math libs](http://haskell.org/haskellwiki/Libraries_and_tools/Mathematics)Many thanks to everyone who answered!  This clears up a lot of things for me and it's an interesting discussion.&gt;the FSF which owns many of the moving bits of the Linux OS

    The whole point of Free Software is that it doesn't have owners.

&gt;How long do you think it will take the distributors to replace those bits if they want to stay with GPL v2?

Distributors?  You mean tivo?  Maybe you should state which clauses of GPLv3 you find objectionable.  And why should developers who aren't satisified with GPLv2 care about how long it takes for a GPLv2 fan, or a BSD fan, or an MSFT EULA fan to reproduce their features?There's always [OpenMP](http://www.openmp.org/) if C or C++ is your thing.  I like the graceful degradation in that you just annotate the parallel sections with #pragmas which a non-OpenMP compiler will ignore to generate the serial version.  It's a fairly elegant approach that's already supported by Intel C/C++ and Visual C++.  GCC will finally be supporting it in 4.2, whenever that gets released.I don't just disagree with your beliefs, I believe them to be unconnected to facts.

There is no way RMS can take away all the GPLv2 software from people who already have it. You show no evidence of understanding that fact.

All he can do is lock up any *improvements donated to the FSF after a project moves to GPLv3*. And preach. Neither is likely to have a major effect on people who want to stay on GPLv2 software. You show no evidence of comprehending that either.Hey, that's pretty awesome. I should spend more time browsing the Haskell wiki, it seems to have everything. Thanks!You're considering C++? Are you mad?Do you hate our freedom?&lt;blockquote&gt;The whole point of Free Software is that it doesn't have owners.&lt;/blockquote&gt;

This just proves you know nothing about Copyright.&lt;a href="http://news.com.com/FSF+says+Novell+could+be+banned+from+selling+Linux/2100-7344_3-6156037.html?tag=nefd.top"&gt;Link to FSF says Novell could be banned from selling Linux | CNET News.com&lt;/a&gt;[removed]You don't email the new password because you don't want a plain-as-day copy of it sitting in the recipient's email records, a copy of which might fall into the wrong hands should, for example, his laptop get stolen.  The idea is to make sure that an attacker combing through old emails would find only expired account-recovery links, which are worthless.Um, sorry, this post appears to contain no actual content.Hmm, his example of command lines used by average people are good - search engines, however, you find that extending 'natural language' to command is a bad idea.

With search, if you don't get the results you want, you try again - it's no big deal.  Misinterpreting actual commands is a much bigger problem.  Sure, it could verify the intention with a clear version of what the human said, however, people would just get in a habit of hitting 'y', and not reading it at all.  It'd probably generally look right, anyway - the details would be most prone to error.

This could work out all right for opening files, running applications, etc (I use the gnomebar for this), but using ambiguous systems for actions with possible reprecussions is a bad idea.

I suppose something like sudo might help...*Verr iz zee microfeelm?*Coming up next on Ask Reddit: Why do Java Programmers EAT BABIES???I'm surprised nobody has mentioned CLIM yet.&gt; This is why multiple subclassing is not a problem, in contrast with classes in object-oriented languages. For example:
&gt;
&gt; [diamond inheritence]
&gt;
&gt; There is no conflict between B's, C's and D's function names cause they are required to differ.

Phooey;  [C3 linearization](http://www.webcom.com/haahr/dylan/linearization-oopsla96.html) (circa 1996) elegantly solves this problem in object-oriented languages (without requiring the classes' names to differ).&gt; Does Stallman have any loyalty to the single most successful FOSS project?

Why should he? Why should anyone have "loyalty" to any particular piece of software, especially on the basis of popularity alone?&gt; So my default answer to questions such as the one I got at last May's Ajax Experience, "When will you add threads to JavaScript?" is: "over your dead body!"

Coroutines, please!  That's what a lot of people actually want when they say "threads":  not a way to _"scale up to handle the megacore teraflop future"_, but a way to write more complicated, cooperative, stateful code/callbacks without going insane.

Just look at Lua for inspiration.What is the algorithm actually doing, and what is its running time?And with reason!The lack of any non-proprietary version is probably a big factor.  (Users have been [petitioning about it](http://wiki.axiom-developer.org/FreeAldor) for over a year now.)&gt; Both camps support spaces in file names, but it is generally frowned upon in the Unix camp

No it is not frowned upon.

&gt; Including spaces in a file name is a pain because they’ve have to be escaped

No it is not, bash has command and file completion with the tabulation key, learn to use it.I believe they're considering adding (delimited?) continuations. That still leaves you in a slightly uncomfortable position when you want to mix continuations with coroutines...[The initial post](http://news.gmane.org/gmane.comp.lang.haskell.cafe/cutoff=19472), 
and related [discussion thread](http://thread.gmane.org/gmane.comp.lang.haskell.cafe/19380).This is not what you think it is, and Factor does not do any SIMD stuff. It uses SSE2 for ordinary run-of-the-mill floating point on x86. Unlike what psykotic said, SSE2 adds support for double-precision FP which SSE lacks, thus completely eliminating the need for x87 instructions (which run at half speed on Pentium 4).

Factor doesn't attempt to do floating point with x87 at all, because going forward it appears use of the x87 unit is discouraged. Also the SSE2 intrinsics can be used unmodified by the AMD64 backend.

There is not a lot of code involved, http://factorcode.org/repos/Factor/core/compiler/x86/sse2.factorspamthere's a comment about filenames that start with -, but that is resolved by:

    rm -- -whatever
    rm ./-whatever

(though having to do that can be annoying)What would you say are the minimal expectations one should have for a DBA?

Mine would include

* being able to explain the physical layer and what implications this has for setting up a machine for a database (interview question: for the disk volume containing the tables, which is better raid1 or raid5 ?)

* composite primary keys ( iq: you're tasked with setting up a database to record events (a bounded set of char(8) ) coming from multiple entities, is it better to set up one table for each entity, or one table for events ?; how would you index it.)

* transactions should be used when...?

_nominally i am a system administrator, i do DBAish things to multiple large (4-18 gb ) databases in multiple dbms formats (postgres,sybase and mysql), i wish we would hire a competent DBA, until then, I'll muddle through_Errors resulting from STL abuse are their own special world of pain.&gt; Unlike what psykotic said

Ah, you're right about the double precision. The stuff I work on can almost never afford to use double precision, so the only things about SSE2 that registered for me were integer support and the new combined shuffling/packing/unpacking thingamajig.I seem to be of the understanding that FSF projects, unlike say any ol' GPL project, have all the code given to them (just like anyone writing code for a company). In this case should you write, say, GCC patches, by submitting them it is understood that you are attributing them to the foundation. But yes, no one can take the stuff away from folks who are distributing the old version, of course.STL implementations are like cute little puppies compared to the crimes against nature that are the Boost libraries.vincenz is right that "referentially transparent" == "out-of-orderable". Beyond that though, be careful when you use the phrase "out-of-order execution" because that generally refers to processors executing instructions in the instruction stream out of order to hide the high latency ones and extract instruction level parallelism.I agree. Check this out:
&lt;blockquote&gt;
Where most programmers who are accustomed to imperative style would naturally use an array, &lt;b&gt;a variable&lt;/b&gt;, or a mutable object, a functional programmer will often use a list.
&lt;/blockquote&gt;
What? A list instead of a variable? What's this guy smoking?[Counter-screenshot](http://img454.imageshack.us/img454/6669/scr00497nq1.png). (No results simply because I don't tag pictures.)Another Snap user - you'll want to [kill that](http://www.downloadsquad.com/2007/01/31/how-to-get-rid-of-snaps-obnoxious-link-previews/).As far as I can tell, he basically wants first-class patterns.  The problem is that it's very hard to type the ~= operator unless you somehow make it a compiler construct.  Afaik, though I am not an expert in the field, research in how to make patterns first-class is still very open.It gets messy really quickly when you trail 7-8 files together, some of which need to be quoted, or have some characters escaped/unescaped from quote, etc. to **wget** where tab completion isn't there to save the day.This is extremely cool. It would be very nice to have this in GHC.

One thing which the paper doesn't really talk enough about is exactly what isn't captured by the technique. It would be nice to know exactly how far this gets us toward ideal fusion. It seems to do very well with all the common examples, it doesn't depend on the particulars of any data structures, and works with functions of multiple parameters as well, so it's already fairly general.[removed]There were some issues with IE but I've fixed these now (for IE 6 anyway). Hopefully this works on all the major browsers now.All lowercase and no spaces? Makes for pretty ugly filenames, I'd say...Great example of the new intuitive interface. Where by "intuitive" I mean "every application does it differently".Ah, the good old days of #pragma warning(disable: 4786)...You might be surprised to know that the 'space-delimited tag:notation' Microsoft chose to *adopt* is compatible with [most](http://del.icio.us/tag/system:filetype:mp3+remix) [tag-](http://www.flickr.com/photos/tags/fall+snow/)[intensive](http://danbooru.donmai.us/post/list/tags/cute+score:%3E5+rating:safe) applications, including [Google](http://www.google.com/search?q=breakthrough+inurl%3Adiabetes).This has been discussed a bit on Haskell-cafe.

Here's a link to [my idea for syntax](http://www.mail-archive.com/haskell-cafe@haskell.org/msg12432.html) and [the top of the thread](http://www.mail-archive.com/haskell-cafe@haskell.org/msg12404.html).ls [query] | xargs wget

For query you have tab support, works perfectly. Use grep or find if ls is not specific enough.

It's really no big deal.I'm sorry I read it too.

And I think some general ranking for data structures is justified. Some data structures are just more versatile, and therefore obvious choices in most circumstances. Speaking about 'favorite' data structures implies it's an emotional issue, even though there are only 3 classes of data structures that are usually even worth considering. And within these categories some data structures are more versatile than others, making them the obvious choice.

In nonexceptional circumstances, prefer a deque to an array (same big-Oh performance, cheap push/pop at both ends). 

Likewise, prefer a doubly-linked-list to a singly linked one. This way you don't have to come up with complicated caching tricks to compensate for the fact that you can traverse the structure in only one direction. Even if you don't really need the backlinks - they're almost free.

And you should also favour a hash over a generic (balanced) tree (unless you care for the ordering of the items). Hashes are very predictable in their performance, almost regardless of data contained in it. With generic trees you can get horrible performance when you update the definition of equality of the data contained in it.

It's probably really easy to build a decision-tree, advising the programmer which data structure to use, by answering just a few questions. It's not that difficult, really.If you use good dynamically typed language you don't get core dumps when you get type error. You get type error. 

How it is possible that when I program in dynamically typed language I don't get type errors when program is ready and delivered. The errors I get come from wrong assumptions of outside world that my program has, or there are bugs in logic of the code. Yes, of course there must be some type errors. But hey, if they are less than 1% of the errors, why to put so much effort on these.[deleted]You should be [ashamed](https://addons.mozilla.org/firefox/1865/) for even knowing what snap is.I'd say the bug is in the shell, rather than the allowed characters.I was talking about URLs, say, copied and pasted from your web browser one-by-one. (Hmm, it looks like xargs will help here as well, so thanks for the tip!)or Emacs, for that matter.Interesting - I use AdBlock, not AdBlock Plus, and I'm not aware of the difference.  Does Filterset.G or an equivalent work with Plus?

Actually, I've deliberately not blocked Snap using the link in my post above, because I'm trying to spread the word about the block.  Can't tell people about the solution if I never notice the problem.Of course. For those who don't know how:

    cat | xargs wget

Then you can copy paste the urls in one by one, hit EOF (C-D) and wget will merrily go on a download spree.uhm? I didn't refer to spaces or tags at all... I was referring to menus, toolbars, buttons and icons scattered everywhere, always different.With Adblock plus you can subscribe to a blacklist (for free). Zero maintenance, and a very low percentage of false positives.

It's significantly better than the original adblock.No.

So if the filesystem allows a user to put a bell character in a filename, what's the shell supposed to do? Not display the file? Prematurely wipe your harddrive? Discard the special characters from the filenames and create filename ambiguity in the process?

The shell has no choice: it has to deal with whatever the filesystem feeds it.I use unison for two way synchronization between two computers in a home network. Parents and other family members don't know and/or care on which of the two they stored their files, and having to remember not to turn off a pc if someone is using it's files on the other pc didn't work either. 

Unison has been working fine for the last year or so. It's never lost any files. Ocasionally it can lose track of whether a certain file has already been updated, but if that happens it leaves a 'danger.txt' file in it's homedir and refuses to do anything further untill the problem is solved manually. I've written a small script to throw an error if this danger file exists. 

Unison sometimes finding itself in this inconsistent state and requiring manual intervention is a bit of a pita, but according to their web page this is due to how windows handles the files, and unison can't really do anything to correct that. It however happens infrequently enough not to be that much of a problem. 

Unison is also able to keep a history of files it updates, but I haven't gotten it to work right because it won't delete old versions, so it'll just fill up the entire drive. Maybe it's just a config problem on my side, though.There's a visual studio plugin that re-parses the error messages so they become more human-readable.

I admit the compile errors are a bit cryptic, you quickly sense what part of the error message is meaningful. It's not a deal-breaker for me, Boost is incredibly valuable to me.I've seen a bunch of programs and scripts, in both Cygwin (under Windows) and recent Ubuntu Linux, that aren't guaranteed to work with 100% reliability if your *directories* include spaces.  Apparently there's a lot of `cp $1 $2` in code that should really be `cp "$1" "$2"` (or am I getting my shell-scripting syntaxes confused again?)Nice, if just for the architecture versus features spectrum.I guess that's why Firefox's entire UI hangs whenever anything in any of the tabs is busy.
i deactivated that snap thing. thanks for bringing it to my attention.The code lacks decent concurrency, but threads are not the answer.The shell doesn't do anything with a bell character, the Terminal does. Let's see what your browser displays: "•"

What shells do is interpret commands and it is hard to interpret ambiguious things like "mplayer a movie.avi" and it's hard to connect things like that with pipes.

Once again a Plan9 reference. Their ls goes like this:

    ; ls
    something.avi
    'a movie.avi'
    and_more.avi

I don't know what Plan9 does with bell characters, but i suppose displaying some wierd symbol.Almost makes me want to write in Java |:^)Interesting. What do you propose a better, more consistent UI could be? The layout, though not perfect, actually makes a lot of sense to me.So who's to blame? The filesystem. Not the shell (or the terminal).

The filesystem returns a filename with unprintable characters. That's the bottom line here. Filenames are for humans to read, and bell characters, nulls and tabs have no place in a file name.

The shell/terminal cannot handle the problem gracefully, so I don't think they're to blame.I'd blame the person who would put those characters in the file name.I love when Oleg K. steps up to solve a problem. The man doesn't have a mind, he has a machete.

Every one of his contributions is a gem, and a testament to human intelligence. Can you tell I'm envious? `:-)`Absolutely, though it would be nice if he did that in 2D graph too (with architecture/features on each axis) to make it clearer that some languages simply lack both elegance and features as well.Mercury, which has typeclasses as well as type inference (and mode inference, and purity inference), also has appearances of polymorphism in that:

1. functions/predicates are named by arity -- so `foo(1,2)` and `foo(1,2,3)` have nothing to do with each other (unless the first evaluates to a closure for the second -- which Mercury also allows.)

2. functions/predicates are named by module.  So `rtree.is_empty/1` and `bag.is_empty/1` have nothing to do with each other.

3. Although you can `:- use_module` to include a module but with the restriction that you always call its exports by their `full.names`, you more normally `:- import_module` , where you may then have code with `is_empty(X)` applying to bags or rtrees in different places.  Mercury (and you) tell them apart through static analysis.

4. With typeclasses, of course, you can have a `can_be_empty(T)` defined over both bags and rtrees, and so can operate willy-nilly on either -- to an extent, as you'll declare what typeclass the predicate/function operates on.

5. Mercury also has existential types, which work like typeclasses but reverse some responsibility in some way that makes them -- sorry, I don't understand these yet.  But Mercury has them, and they also give you apparent polymorphism.

6. If you want to get really crazy, you can turn things into `univ` types (which is apparently trivially defined with existential types) and then resolve these issues at runtime.

7. But you can still have limited dynamic dispatch, just like Haskell and O'Caml have, by using discriminated unions: `:- type string.poly ---&gt; f(float) ; i(int) ; s(string) ; c(char).`  , with one-letter constructors you don't even notice polluting your namespace, through the 'seeming polymorphism' qualities already covered.

-blink-, wait.  -overloading-?  OK, Mercury absolutely has that -- you need only include the operators from separate modules, and let inference do the rest.  You actually have to import the `int` module to get addition over integers; that module's interface exports:

    :- func int + int = int.
    :- mode in  + in  = uo  is det.
    :- mode uo  + in  = in  is det.
    :- mode in  + uo  = in  is det.
    % ...
    :- func + (int::in) = (int::uo) is det.

Although if Haskellers ever begin to wail and gnash their teeth at lamenting what they might remember from eeeeevilll OO languages, they can sooth themselves by creating `+++&lt;&gt;+++` and such operators, which flexibility Mercury does not yet have.I see your point and I wonder why I haven't seen it before; so I apologize.

Anyway, this thread seems to be full of little conversational misunderstandings.. whatever.
This is driving me completely nuts, too. A long time I just assumed these were some focus-issues, but the truth is that FF is a completely fucked-up piece of shitware.Support them with unsafeDestroyTheWorld! :D  That'll teach people who use your vectors in silly ways.  (Or you can do APL and distribute the abs over the matrix, have * rotate the matrix, etc.)I agree.  Basically it is a 2D graph and most languages tend to (or hopefully do) fall on the pareto-curve of architecture vs feature.  However linearizing this directly fails to show those that just plain fall short.  For those not knowing what a pareto curve is, hereunder you find a rendition.  The dotted curve is the pareto curve while some  points just fall under it, (the x's).  Notice that a pareto curve will always be decrease and that it is not possible for a point to fall above the pareto point, if such were the case, then this would give the new boundary for the pareto-curve  (It never truly is a curve but more of a step function, but looked at from a coarse level, it tends to look like a decreasing (usually concave though it can be convex) curve).
 
    Arch
     |  .
     |   .
     |    .
     |x      .
     |  x       .
     +------------FeatHmmm...that won't work for files with spaces in the name:

mkdir tt

cd tt

touch "foo bar"

touch baz

ls | xargs echo

\# prints 'baz foo bar'

ls | xargs -n 1 echo

\# prints:

\# baz

\# foo

\# bar

but this is what '-print0' on find and '-0' on xargs were made for:

find . -print0 | xargs -n 1 -0 echo

.

./baz

./foo bar

HTH. HAND.The falseness of this dichotomy should be evident by the examples he has to support it.  Lisp and Haskell -- 2-dimensional grand designs?!  COBOL - somehow not as '2d grand design'++ as his 'omigosh let us just make everything pure'?  No, none of these languages have such a trivial basis -- and whatever their motives, -all- of them have continued to grow and develop (and, oh dear, pick up features and design changes) with time.

++ For those of you who have, like the author of 'grand architecture versus feature collections', forgotten what little you might have accidently known about COBOL, please do some research.  Being 'awful' doesn't mean that nobody put any thought into it.A fugly implementation in OCaml:

\ttype 'a stream = { hd : 'a; tl : 'a stream Lazy.t; }
\t
\tlet rec repeat x = { hd = x; tl = lazy (repeat x); }
\t
\tlet rec take_while p l =
\t  if p l.hd then l.hd :: take_while p (Lazy.force l.tl) else []
\t
\tlet replace_every n l =
\t  let rec replace i l =
\t\tif i == 0
\t\tthen { hd = false; tl = lazy (replace n (Lazy.force l.tl)); }
\t\telse { l with tl = lazy (replace (pred i) (Lazy.force l.tl)); }
\t  in replace n l
\t
\tlet primes =
\t  let rec loop n l =
\t\tif not l.hd
\t\tthen loop (succ (succ n)) (Lazy.force l.tl)
\t\telse { hd = n; tl = lazy (loop (succ (succ n)) (replace_every (pred n) (Lazy.force l.tl))); }
\t  in { hd = 2; tl = lazy (loop 3 (repeat true)); }
\t
\tlet list_take n l =
\t  let rec iter i = function
\t  | [] -&gt; []
\t  | h :: t -&gt;
\t\tif i == 0 then [] else h :: iter (pred i) t
\t  in iter n l
\t
\tlet p = list_take 10 (List.rev (take_while ((&gt;) 10_000) primes))
That's solvable from the Firefox team. Just make each tab have its own thread to load the page and run JavaScript. Problem solved(tm). (Note: I realize that this probably is a huge undertaking at this point in Firefox's life, but it is something that's feasible if they really care about it.)Ia! Ia! C++ fhtagn!
Real hackers don't write blogs, they just email gems and let the fanbase propagate it :)COBOL was a poor example. I think PHP is a better on as it really was accreted based on features.Err, or just `xargs wget` (or, for that matter, `wget -i-`).  Don't forget to collect your [Useless Use of Cat Award](http://partmaps.org/era/unix/award.html). :)

For what it's worth, you can also say:

    xargs -n1 wget -nv

to start `wget`ting individual URLs immediately (reporting them as they complete), instead of waiting for the end of the list.Additionally, it's much harder to prove that the patterns are exhaustive, which is a very useful tool.&gt; It gets messy really quickly [...], some of which need to be quoted, or have some characters escaped/unescaped from quote,

You can just use the `M-"` (`quote-region`) command in `zsh`, at least.  It automatically escapes whatever needs escaping.&gt; Apparently there's a lot of `cp $1 $2` in code that should really be `cp "$1" "$2"`

This one of many great things about `zsh`:  it doesn't re-expand substitutions by default (unless you request it), so the first version Just Works.http://www.wired.com/news/politics/0,1283,1484,00.html

Bill Gates never said "640k should be enough for anybody".&gt; ls | xargs echo

&gt; \# prints 'baz foo bar'

Though it appears like that, if I'm not mistaken, echo should see them as two arguments (not three).

edit: markdownWell, for those interested, quantum computers cannot in general solve NP complete problems in P time. This has been proven for all possible designs of quantum computers. It would only be possible to solve NP problems with a NON-LINEAR quantum computer. Of course, we have no evidence of a non-linear quantum system yet and they don't use the term in their briefs. So it makes sense to assume that while they may have made a 16 bit quantum computer, they probably have not made a QC that can solve NP problems easily.I've been waiting for this for a long time!  I've been using TurboEntity, but encountered some bugs which weren't going to be fixed (because of the impending release of Elixir).  This looks like a really solid project, and much needed for those who want an easy way to use SQLAlchemy's great features.You're right.  The answer, at least for JavaScript, is a robust scheduler.  Just expand the current job queue manager that's already in place to manage `window.setTimeout()`.

Just add an extension on the Global DOM window implementation to add a prioritized job to the queue.  The scheduler would then have to figure out when to run a timeout job or a scheduled job or to just go back to the event loop (very important to keep from starving the message pump, and the reason you can't, at least in mozilla, set a *real* timeout value for less 10ms).

Something like `window.addTask(callable, priority=undefined)` would be enough.  Then JavaScripters would add the concurrency (tasklet system) layer on top of that call instead of `setTimeout()`.  (check Rico animations for examples of tasklets)Reminds me a little of AS/400 architecture. The virtual instruction set is translated to native instructions in hardware.You're right, you *are* nuts.I think we were talking about Firefox's code itself, not webpage-embedded code.Nope...look at the "xargs -n 1" output. The '-n 1' ensures the command is run once per input value, and demonstrates that you get three input values.

And from 'man xargs':

"xargs reads items from  the  standard  input, delimited by blanks (which can be protected with double or single quotes or a backslash) or newlines"

It'll work for URLs, since spaces should be encoded in URLs (bad Internet Explorer - no cookie) but any sysadmin should know to use -print0 to 'find' and '-0' to xargs, otherwise they can be in for a nasty surpriseThere are a lot of things a DBA has to know, and unfortunately you can't boil it down to a couple of specific bullet points.  But on those bullet points there are a couple of things to note:

* Understanding as much of the machine as possible is really important for the DBA.  However for this question, you'd get a big fat "it depends".  First off, I don't like raid 1 or raid 5 for data tables.  Raid 5 comes with a relatively large penalty hit compared to say Raid 10.  Another thing is that you need to make sure your raid is implemented with a good controller (I like Perc), and not done in software.  If you are doing software raid, it doesn't matter what you are doing, because it won't scale.  That being said, I like Raid 1 for the log files, and Raid 10 for the data partitions.  The problem with Raid 5 is the performance hit for one, and it starts to have serious critical failure problems when you get to very large arrays (for instance 16+ disks).

* Composite primary keys are interesting.  There are a lot of people and DBA's who flat out refuse to use them, and there are others who use them for everything.  A good dba should know what they are, what the trade-offs and advantages for them are, and when you would and wouldn't use them.  For instance, Rails currently can't handle composite primary keys, so if you are building a system for Rails, it wouldn't be good to use them.  But I would ask them a lot more than that.  Find out what their thoughts on natural keys versus synthetic keys.  Generally I'm not a fan of natural keys, but again this is a point of contention among dba's.

* Understanding transactions, and when and where to use them is very important.  Transactions are one of those things you have to find a good balance point with.  Most operations are done with implicit transactions on anyway, the key thing to understand is when you need your transactions to span multiple operations within the system.  If you try to do it everywhere, you'll have deadlocks and contention all over the place and your system won't scale.  If you don't use them anywhere, you're system will end up with inconsistent data in some places.

* Understanding how and why deadlocks occur, and how to resovle them.

* Understanding Indexing.  What is the difference between clustered and non-clustered indexes.  When should you use a clustered index?  When should you use a non-clustered index?  What are the disadvantages to having too many indexes on a table?

* What is the advantage to having your keys the same size as a word on the machine?  What is the advantage to having a table with all fixed width columns, versus variable length columns.

* How well do they handle data problems and normalization.  Can they fix a broken structure and make it work well?

* How do they handle disaster recovery planning?  What type of backup schedule do they use?  Do they do mock recoveries on a separate system to verify their backups are working?

* How good are they at SQL really?  They should be the guy the developers bring those queries they just can't get performing to.  He should be able to understand what they are doing, and rip the query apart and put it back together so it's **fast**.

* Does he know how to identify poor performing queries on the system, and then work with the developers to speed them up.

I could probably keep going for another 20 or so bullet points.  And each one deserves a lot more detail and explanation than I've given here.  I hoped to post some of these points to help you in your search for a good dba.  The most basic test I can recommend is to use the BrainBench ANSI SQL Fundamentals test.  It costs something like $45, but it's a very revealing test.  Someone who really knows SQL well can typically ace that test.  A score in the high 4's is someone who really knows SQL well.  In the low 4's indicates advanced SQL understanding.  High 3's is advanced mid-level SQL, low 3's is beginning mid-level.  High 2's is pretty noobish at SQL, and low 2's is generally someone who knows only rudimentary SQL.

I've administered that test over 100 times now, and those numbers hold pretty true.  There are the people who don't test well for some reason, but we decided to accept that.  It's better to let a qualified candidate walk than to hire the wrong candidate.  Remember this test really only tests basic SQL skills.  Most developers will score somewhere in the 3's.  Any DBA should be in the high 4's in my opinion, if not a flat out 5.  The test is really easy and they should only miss something that's a trick really.  If they don't fundamentally understand one of the concepts, it might be time to worry.

I hope this helps in your search for a dba, if you have more specific questions drop me an email at: dba dot closure at recursor dot net.

* How can you not have typo's with a post this long? :(But remember it was even more fucked-up, when it was called (Netscape) Navigator.I think you mean: Why does George Bush take money from the Java lobbyists??? A blogger follows THE MONEY!!!Last year April Fools' joke.I know, that's why I qualified the answer.Making array similarity testing a breeze (in rails)These guys have too much time on their hands.[deleted]Singletons? In Python??? This is not progress.

The Martellibot solved the "singleton problem" beautifully with his [Borg pattern](http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/66531) many moons ago.edit: 2004 April Fools' joke.I apologize for the misunderstanding.  A variable can track state over time.  A list or stream can be used to track state over time where each element of the list or stream represents a the value of a variable at a given time.  This still preserves purity but does allow a simulation of state change.
This just proves you haven't actually read the GPLv2 or v3.nitpick: Fortran appears twice in the spectrum.Besides, one thing that might be causing the "hanging" you're talking about is because of complicated inlined or onload JavaScript that I assume is respected, read "executed", during the actual loading of the page.  If you notice, opening a bunch of simple reddit links don't block the UI (even if the server is unresponsive).  But, if there is complicated JS (any web 2.0 heavy site) you'll notice the blocking even if you're on a different tab.

This is due to the fact that JS code is executed in the message-pump thread and isn't handled the same way as the render-flow process *because* it's custom JS.Azul is taking an interesting risk.  Historically whenever software features move into hardware for speed they move back out again later for flexibility when the general purpose hardware gets fast enough.  Symbolics and Lisp Machines machines are two examples where the underlying hardware got fast enough in less than a decade to allow all that LISP required be handled in hardware rather than in a relatively expensive special-purpose machine.I agree that some data structures are generally better than others.  The problem in at least in an interview setting is that the interviewer is gauging how well the candidate makes tradeoffs in design and whether they are appropriate for the problem.  Using the same data structure over and over again even when the design constraints change drastically (and the constraints don't justify the tradeoffs) is a tip-off to me that they just don't have that many tools in their belt.

The post itself was not so much about that issue as about at least one reason why lists are used so much in FP.  I then tried to address why C# 3.0 uses IEnumerable&lt;T&gt; instead of some list structure by showing how they are isomorphic.

So I apologize if the title of my post led you to believe it was something that it was not.  And I hope this clears things up a bit.ANSI Lisp, "consistent"?It is funny, but it's not uncommon.

Once I got email from a US-based multinational's IS department, asking about software - and because I could tell from the nature of the problem (we don't put a flashing red warning light saying "you are using the software illegally", but if you know what to look for, the effect for us, but not the user is the same) - I knew that they were using pirated copies.  When confronted about it, their initial response was "it's okay, we're only testing your software internally".Apologies for being harsh - if I had know my reply would reach you, I wouldn't have put it like I did.No apology necessary.  I should have been more clear about explaining things in my blog.Thanks for pointing to your comments on the original site.  I found it very, let's say, "sane" and objective and recommend people to go there and read it.

Edit: with "it" I meant the comments from "SickOfInaccurateArticles".  The article itself, oh well...&gt;Haskell 98 + Hierarchical modules is pretty much feature complete as far as I'm concerned. 

[If society fits you comfortably enough, you call it freedom.](http://www.quoteopia.com/famous.php?quotesby=robertfrost)

&gt;However, there is one operation that I think would be useful - pattern equality.

[Does Haskell need macros?](http://neilmitchell.blogspot.com/2007/01/does-haskell-need-macros.html)Frankly the gist of the article isn't progress, it's how to do things you did in C++ in Python.

I'd rather do things differently.  Here's a good [blog post](http://blog.plover.com/prog/design-patterns.html) from a ways back that went over some of this.
&gt; Architecture Orientation in nutshell! That would explain why the R5RS is only 50 pages long. The draft R6RS — now with added modules! — is 150 pages long, but it still opens with the same statement. I do hope the authors appreciate the irony.

I thought about that too, until I saw that the language specification is only about 50 pages long -- around 65 if you count the formal semantics. The rest is just libraries.I rsync to Amazon S3 using [Jungle Disk](http://www.jungledisk.com).&gt; So here's a promise ... I will keep ...: JS3 will be ready for the multicore desktop workload.

Good to hear.  I'm skeptical about the ability of a language to really solve these problems, but the more tools the better.

The idea that Erlang or Hermes holds the key to scalable and safe programming for, say, 32+ core SMP systems raises an eyebrow.  Anyone have concrete examples?[removed]Thanks for the link.  The documentation there is much more detailed.This paper is ancient. I don't see anything that actually wouldn't run, but there are things I wouldn't do the same way today. (For instance, use of \_\_getattr\_\_ has gone way down since "property" came out.) I don't recommend it.People might also like the development of GCD in Stepanov's [Notes on Programming](http://www.stepanovpapers.com/notes.pdf)(section 10.2).&gt; The code lacks decent concurrency, but threads are not the answer.

Yeah, I hate it when applications use my entire processorRemember that multi-core processors are an implementation of symmetric multiprocessing (SMP), which has been around a long time and had a lot of research around it.

There are established techniques for partitioning and replicating data to avoid interference (locks and inter-processor interrupts) and to verify absence of deadlocks.

But can language features or compiler optimizations really open up the performance of highly scaled SMP systems to the average programmer, or average program?  That is, something that doesn't already lend itself to a parallel solution?  
Another option, from the [Irrlicht](http://irrlicht.sourceforge.net/) graphics engine folks, is [irrKlang](http://www.ambiera.com/irrklang/).Again, I'm not here to propose layouts or to criticize this one in particular, my point is that MS has chosen to deliberately encourage ad-hoc UIs, while most of the world has always strived to have consistent UIs across applications.

This UI is not consistent with the rest of the system: IE, Office, Media Player, etc all have wildly different layouts.

I think this is not an improvement. I might be wrong and Vista's UI might be a refreshing breakthrough in user interfaces. We'll see how it will work out (nevermind the fact that it's nearly impossible for MS to "fail", for many reasons).True, but I cannot help but think there is better middle ground. Reflection code sucks.Good stuff. I'm slowly making the move from programmer to DBA and I appreciate your input.I was wondering what's special about the number of cores mentioned in the article - 24, 48, 384 or 768. I've come to expect powers of 2 in computers so is there any particular technical reason why a factor of 3 slips in? Can anyone help?Well I don't want to be too controversial and I don't mean to disrespect the guys behind Django, but I would like to suggest that if you are looking for a Python web framework, look elsewhere. And I post this because my *honest* desire is that developers use the best tool available; I'm not posting it out of some grudge. 

There are some really nice things about Django, and they were what initially drew me in, but in the long term I don't think its flexible and I don't think its a very efficient way of development.

I'll just list a few of the features of Django and why I found them lacking:

* admin interface

This is probably the most talked about feature of Django, but just like another very talked about feature of Rails, scaffolding, it just isn't that useful. The current system is very inflexible and unless you are using it explicitly as a back end for managing data, it won't get you anywhere. Now you might have the same idea that I did that it would be easy to modify and let external users manage data from it, but that's simply not the case. The admin interface will get you 80% to where you need to go, but no further. You are much better off just using the lower-level framework. Thankfully this will be getting better with the 'newforms admin branch', but that 80% feel was something that to me existed throughout Django. It feels like just about everything in the framework will do 80% of what you need to do, but it is lacking that very last feature which would make it flexible enough to do exactly what you need. As a consequence, you are often told "that's how it works" and either you use a feature as it was expected to be used or you don't use it at all.

* regex-based URLs

Okay, initially these look simple and they are simple (initially), but very quickly you figure out that unless you really know regexs, they don't scale. Regexs are error-prone and complicated, why do your URL setups in them? Unless you are supporting a legacy site there's just really no reason to use them. Using something like [Routes](http://routes.groovie.org/) is a lot simpler and for most people a lot more powerful.

* generic views

Generic views were neat, but to be honest the whole reason they were useful is because creating custom manipulators was such a PITA. This is another one of those things which is getting better through newforms, but generic views aren't really that big of a win because they are another one of those things which lacks flexibility and controller code usually isn't that complicated anyway.

* function-based views

Initially all of your view functions are put into a file named 'views.py'. This is simple, but its another one of those things that doesn't scale. Quickly you'll realize that you've got a set of view functions for 'users', and a set of view functions for 'records', and a set of view functions for 'awards' (or whatever your app has). You can break these out into separate modules, ie (userviews.py, recordviews.py, etc.), but why not just used a controller/class-based design from the beginning? That's just a much more natural way of organizing code.

I agree with Rails, Pylons, and TG that you might as well have a UserController, or a RecordController, and then have unique methods defined to each one. And of course by using classes you have the advantage of inheritance and defining methods or attributes that should be available in every controller in the superclass.

* the templating system

I got myself to like it, but the more I used it, the more I realized that it was just getting in the way. Now I'm sure the templating system is great in situations where you there are designers working seperately from the programmers, and in that situation you probably want something limited and easy, but in a situation like mine where I am the designer and the programmer, the templating system just doesn't buy you much. I few weeks ago I had to display the counts of an item on all of my pages. To do this in Django I had to create a folder + a file + a 'tag'. To do this in any other Pythonic templating system I could've just created a function.

Like I said, it just got in the way. There are features of the templating system I liked, like the way inheritance works, but thankfully something like Mako has that feature along with a lot more power and utility for your everyday Python programmer.

* the packaging system

I say the packaging system, but Django doesn't really have a packaging system. Django separates web programs into two separate things: projects and applications. A project is often composed of many applications. The idea is that you can plug and play with different apps in each project. This is neat in theory, but I found it just didn't work well in practice. First of all, you'd think that this concept of applications would help reuse and there would be a lot of applications floating around the community. There really aren't, and what only outside apps that you'll tend to be using are the ones supplied with Django. Second of all, I find the distinction between project and applications very murky and redundant. Usually my application IS my project.

Finally, and this isn't something I learned from using Django as it is something I learned while using other frameworks, but the entire system just isn't a good one for packaging and deploying apps. What I'm really a fan of is how in Pylons every project is wrapped up in an egg. When you are developing you say 'setup.py develop' and because it is just a Python egg, you get the advantage of a virtual install, and you now have it available on the Python PATH. Second, when you are done developing your app, uploading it the Python Cheeseshop for others to use is a piece of cake. And finally, when you find a Pylons app (or any other WSGI app) that you want to use, you can "easy_install app* off Cheeshop and then plug it right into your application.

I just found that because Pylons used a standard like eggs, it was much easier to actually plug and play components. There's not a real easy way to do that in Django unless you want to manually go through and setup the eggs yourself (and again, nobody in Django is really doing this), and you just don't see the code reuse you might expect.

* flexibility and code reuse

And that's what leads me to my final point: flexibility and code reuse. People give Django a hard time about having a "Not Invented Here" mindset. I don't think its completely deserved, but I do understand why people think that. The nice thing about Pylons and something like TurboGears is they aren't based off their own stuff. They actively embrace other libraries (they really are composed entirely of separate libraries), and they are made to be very flexible. Pylons and TG both support a plugin interface for templates so you literally can plug in 5 or 6 different templating languages by changing a single line of code. Ditto for things like ORMs.

There are two big advantages to this. First of all, because they are using outside libraries they often are just more stable. Django had a bug in its file upload system a few months back which made it to where uploading a file would take 24 seconds compared to a few seconds when running the exact same application ported over to Pylons. Now I did do all kinds of work and I verified that the problem was a result of Django using its own way of uploading files. I didn't have this problem in Pylons, and its probably because Paste, the underlying library that Pylons uses for uploads, has lots of eyeballs on it through different projects so problems get fixed.

Next, while I like Django's basic simplicity, it doesn't scale that well. If you have a complex legacy DB, Django's ORM simply isn't going to cut it. Now Django allows customization, but it doesn't go about trying to help you do it, so while you could plug in SQLAlchemy to handle that DB, you are better off just using a framework that from the beginning is designed to be flexible. Ditto for things like the templating systems. You can wrestle with Django to get it to be what you want, or you can just use a framework that from the beginning is designed to be what you want. And again, this is just another one of those areas that where because Pylons and TG use outside libraries, you get a lot more power. While Django's ORM is simple, it is also limited, and a better option might be to use SQLAlchemy with a declarative layer like [Elixir](http://elixir.ematia.de/index.html), which gives you a very simply syntax like Django but then let's you drop down into the very powerful SQL tools if you need to.

One more advantage to this is because Pylons, TG, and others use a lot of the same libraries, you are just getting more code. Both will soon be using ToscaWidgets, which means that you'll see interface widgets developed separately in each framework and used between them. In Django you have newforms, but only Django is using newforms, so there's just a lot less code that will be available. This also is really the case for WSGI.

Now again, I don't mean to disparage Django, and their docs are still some of the best, but when it came down to it I just feel like there are a lot of sub-optimal design decisions made in it. I do think as a Python developer that you owe it to yourself to check out other frameworks. After using Django for several months, I found myself very frustrated, and I could've avoided that frustration by using something that was designed to be more flexible from the beginning.Just as an example of Django's mindset which I feel permeates the whole project...MVC. We all know what that is. Model, view, controller. Rails uses this terminology, TG uses this terminology, Pylons uses this terminology, damn, everyone who has a framework pretty much calls it that (Catalyst, Symphony, Java frameworks, GUIs, etc. But not Django. Django calls it 'MVT', or model, view, template. Is this a big deal? Not really, its very minor, but why do that to begin with? When people discuss Django with other frameworks the terms are always getting mixed up because to Django what are views are considered controllers in other frameworks. Its just another one of those areas where Django does things differently for whatever reason.Let me be clearer: the kind of threads rallied against in the article are not the answer.  There are simpler, more efficient and scalable ways to fill your cores.[deleted]When I reinvent wheels I try not to reinvent the standard library functions ;-)

    import System.Environment
    import Data.Char
    import Data.List
    import Numeric
    
    main :: IO ()
    main = do
      args &lt;- getArgs
      let result = case args of
                     ("encode":xs ) -&gt; encode . concat $ xs
                     ("decode":x:_) -&gt; decode x
                     _              -&gt; "Bad arguments"
      putStrLn result
    
    encode :: String -&gt; String
    encode str = concatMap encodeChar str
    
    decode :: String -&gt; String
    decode [      ] = ""
    decode ('%':cs) = fromHex (take 2 cs) : decode (drop 2 cs)
    decode (c  :cs) = c                   : decode cs
    
    encodeChar :: Char -&gt; String
    encodeChar c
      | isLegalChar c = c   : ""
      | otherwise     = '%' : ( toHex . ord ) c
    
    isLegalChar :: Char -&gt; Bool
    isLegalChar c = c `elem` (['A' .. 'Z']++['a' .. 'z']++['0'..'9']++"-_.!~*'()")
    
    toHex :: Int -&gt; String
    toHex x = showHex x ""
    
    fromHex :: String -&gt; Char
    fromHex xs = head $ [n | (n,"") &lt;- readHex xs] ++ [0]OK, now it just feels like you're trolling. I've responded to a bunch of your complaints about Django in the past so I won't repeat myself here. You obviously don't find Django to your taste, and that's fine, but with 382 sites listed on DjangoPoweredSites it's pretty clear that there are literally hundreds of developers who are doing just fine and getting Real Work done with the framework: http://code.djangoproject.com/wiki/DjangoPoweredSitesThe standard library functions I take it meaning showIntAtBase and concatMap?  I hadn't found showIntAtBase but will go looking for it again.  What do you use for reference?  I just stumbled across zvon via a google search.

Thanks for taking the time.  Brand new to Haskell, so I'll pull apart what you posted to learn from.

edit: I also see the readHex.  I had wanted to use that but was having difficulty with type conversion.  Show's I've got a dynamically-typed-language background :)[removed]Here we go again...

&gt; As a consequence, you are often told "that's how it works" and either you use a feature as it was expected to be used or you don't use it at all.

Only thing I've ever said that with is the old version of the admin app, because it was bundled as an "as-is" app. The newforms-admin branch aims to add a lot of flexibility, and I honestly can't think of any other component you can say "use it as is or not at all" about.

&gt; Generic views were neat, but to be honest the whole reason they were useful is because creating custom manipulators was such a PITA. 

No, the reason they were and still are useful is because you shouldn't have to write view code just to display a list of stuff by date, or an individual object, etc.; why write a paginated date-based archive for every one of your models when you could just hand off to something designed to do that for you? And they're just plain old Python functions, which means you can wrap them with other code to get as much flexibility as you want.

Manipulators? Manipulators matter in _two_ generic views, out of the _fourteen_ Django ships with.

&gt; Initially all of your view functions are put into a file named 'views.py'. This is simple, but its another one of those things that doesn't scale. Quickly you'll realize that you've got a set of view functions for 'users', and a set of view functions for 'records', and a set of view functions for 'awards' (or whatever your app has). You can break these out into separate modules, ie (userviews.py, recordviews.py, etc.), but why not just used a controller/class-based design from the beginning? That's just a much more natural way of organizing code.

Subjectively more natural to you, maybe not subjectively more natural to someone else. Hence, it's an opinion thing. If you like something that defines controller classes and methods off them, use that. But don't sell it as objectively better than a module of functions, because it isn't -- MVC isn't about "must have x number of components with these exact types", it's about clean separation of concerns.

&gt; I got myself to like it, but the more I used it, the more I realized that it was just getting in the way. Now I'm sure the templating system is great in situations where you there are designers working seperately from the programmers, and in that situation you probably want something limited and easy, but in a situation like mine where I am the designer and the programmer, the templating system just doesn't buy you much.

Again, subjective. In a lot of situations backend and frontend code are done by two different people or two different groups of people. It's worked well for a lot of folks to maintain a system which cleanly separates their concerns, but you're perfectly free to use something else if something else suits you better.

&gt; I say the packaging system, but Django doesn't really have a packaging system. Django separates web programs into two separate things: projects and applications. A project is often composed of many applications. The idea is that you can plug and play with different apps in each project. This is neat in theory, but I found it just didn't work well in practice.

Works pretty darned well for me, so I'm inclined to say it's another subjective thing. I've got a personal project I'm working on which is just a couple apps I've developed plugged together, and at work we basically run everything off mix-and-match combinations of Django apps.I'd like you to do something for me: go read Martin Fowler's "Patterns of Enterprise Application Architecture", which defines the MVC pattern (and, notably, which DHH made great use of in developing Rails -- where do you think the name "ActiveRecord" came from?), and see what he has to say about applying it to things that aren't desktop GUI apps. Strict MVC doesn't make a whole lot of sense in a web app, so you're down to adhering to the pattern for the sake of adhering to the pattern when the _point_ is to cleanly separate the components of your application.Right, the `Numeric` functions `showHex` `readHex`, `Data.Char` functions `digitToInt` `intToDigit`, `Data.List`'s `elem` as well as using guards and more pattern matching.No mention on what character encodings to use.Vista, OSX, Quicksilver, and now Enso. 

Yah, I think the author is spot on. 

However, there still exists a significant problem without a solution for all the computer-illiterate. How many people avoid features of their favorite application just because they didn't take the time to look (or even RTM). 

A return to CLI is very much a "good thing" in that it will provide the standard efficiency power users are expecting without sacrificing usability. But there still needs to be an easy, and obvious, way to learn the system and discover new commands. 

The main reason I've dropped Enso (for now) is that I couldnt find a central location describing the commands available. And my own explorations came up with way too many "Command not available." 

The new CLIs will need to have a VERY robust suggestion engine for truly widespread adoption.[deleted]Near the end of the article I think it is mentioned that they'd "rather eat glass" then go this route.Simon, I've really only started to openly criticize Django online within the last few weeks, and while ubernostrum has responded, I don't believe you have.

Besides, you can say that I'm trolling all you want, but I have valid complaints. I'm not saying Django doesn't work, I just don't think its the best available. You can address my criticisms or not, but don't just say I'm trolling.

It was only a few months ago that I was praising Django, but after a few months of heavy use I'm just not happy with it. You can look at my complaints for what they are worth, or you can ignore them. But I don't believe I'm the only one that feels this way.Hmm.. wonder whether the author reads [comp.risks](http://catless.ncl.ac.uk/Risks/23.23.html#subj8).Design Patterns are Design Defects.  The Gang-Banger Foursome peddled a dreary collection that way too many people bought in to as gospel.Like I said the terminology isn't that big of a deal, I just think its an example of how Django does things differently for no really good reason.

What is the point of naming it 'MVT' when literally everyone else calls it MVC? Its just seems like using different words for the sake of it, and again, its just confusing.

A 'view' in Django is different than a 'view' in every other framework. Why?you will want to check out hoogle. in your particular case, i guess that would be [http://haskell.org/hoogle/?q=hex](http://haskell.org/hoogle/?q=hex).[deleted]Wow! Lisp on haskell already shows how powerful it is.
While haskelers waste their time in long discussions on how to achieve it, lisp allowed to achieve it in 3 lines macro!
Except for the inventor of C#, who just happens to be the best language designer of our time.  If he thinks C# is good, then perhaps, just maybe he's right and you might be wrong?[deleted]I think this makes sense in general, although he may got a few languages wrong.

I noticed a similar thing in my kitchen: if I keep things in cupboards and drawers and neatly arranged, the kitchen countertop looks really neat and clean, but as soon as I need to cook something, I have to fetch stuff from all over the place. If I leave stuff scattered on the countertop, it looks messy but so it's much more practical, everything is within easy reach.

Unless you're a neat freak, you pick feature over architecture on a daily basis, except when you're trying to sell the house, then you want to present a clean countertop.

So C++ and VB is the way to go, and Haskell is for neat freaks who want to memorize where they put their vinegar.I believe they are using a simple copy.
Which hurts you, when you rename or move a file.

That writer needs to learn about the delete key. That stuff could probably have been condensed to 3 paragraphs.jesusphreak: I apologise; on looking through my posting history I've responded to leonh, not you.Features and elegance-and-simplicity are orthogonal. Granted, the latter is often sacrificed in order to improve the former, but you can have languages good at both, or (much more commonly) languages bad at both.I imagine it is (hopefully) a bot, but I don't mind, the links it does submit are very good :)We use different terminology because we're _not_ MVC -- we don't really have an analogue for the "controller" part (and, I think, Fowler does a good job of explaining that the controller doesn't always make sense outside of a desktop GUI app). So we'd be using _inaccurate_ terminology for the sake of using terminology.And with more interesting tools.  Java jobs are a dime a dozen and look very tempting but leave a lot to desire in terms of using productive software development tools.

Are there a lot of unposted research type developer positions(for those without a post graduate degree).

Do you just work your java job in the day and work on your other projects at night?  Just walk away?Also, on regexes for URLs: again, it's a subjective thing. Regular expressions are pretty good at describing complex patterns in a concise way, and Python's named groups make for a really useful mapping of keyword arguments out of a URL.

Also, JWZ aside, knowing regexes is something I think is pretty fundamental for a programmer; sure, you can screw up a regex, but you can also screw up pretty much any other syntactic construct ;)Great site to search cool images.. You can search, select and send images as mails. Images will be from FlickrOk, I _know_ some Haskeller is going to set us all straight as to how this Liskell thing isn't panacea.

Until then, I'm a rabid panting fanboy. Liskell!Are there many Asterisk users reading this?A possible solution is to quit the software industry, find another job you like and keep programming as a hobby, on your free time and with whatever technologies you like.

I'm really serious.
Quit first. That's easy.

Then it's just sink or swim.

That's easy too.&gt; Do you just work your java job in the day and work on your other projects at night?

That's what I'm doing.  Java web development by day and Python startup on nights &amp; weekends (actually, often mornings, as I'm more productive then).

If you don't want to take the risk of a startup and don't want to spend all your free time doing one, then I'd ask your professional acquaintances if they know of shops using other languages.  In the Boston area, ITA uses Lisp and MetaCarta is mostly Python.  There're probably lots of other startups I don't know about too with exotic languages.  Silicon Valley has Google, YouTube, and Reddit all using Python.

Stay away from the want-ads: companies that hire through recruiters or job sites usually want commodity programmers for commodity programming languages, because that's who usually frequents the job boards.  The type of tech companies that'll take a risk on an unusual language usually hire through friends &amp; acquantainces of current employees.

Also, hang out in the IRC chatrooms of the languages you want to work in.  Someone from Galois (high-assurance development with Haskell) was recruiting in #haskell the other day, and I always get Google ads for Jane Street Capital (Ocaml) because I'm subscribed to Haskell-Cafe through GMail.Dang, should have told me that earlier:

"Stay away from the want-ads: companies that hire through recruiters or job sites usually want commodity programmers for commodity programming languages, because that's who usually frequents the job boards."
It's a classic (non-Turing) tar-pit. You start out saying, "Hey, this technology is too complex. I can create a useful subset of it by doing _this_..."... except that either you or your users turn out to need that other stuff, so you start gluing it on, one bit at a time, and in the end, you've replaced a reasonably (but not perfectly) clean and well-thought out language like HTML with your horribly mutated, thoughtlessly-grown replacement that makes easy things easy and anything else either impossible, or much harder than the technology you initially thought would be _so easy_ to replace.

Document layout languages seem to have a magnetic attraction to people.

(All of these "easy" markup languages are make me grumpy; thanks to the proliferation of them, I never know when my long_identifier_names or Python __setattr__-type method names will have some horrible formatting applied to them, and whether or not &lt;b&gt;formatting&lt;/b&gt; will work. Maybe one of them is easier than HTML, but the sum total of them is emphatically not.)There a lot more people using Java, even just on servers, than used Lisp at the time of the LM.That's probably true but, still, some job boards are better than others (jobs.joelonsoftware.com comes to mind).This seems so similar to reStructuredText (http://docutils.sourceforge.net/rst.html), which I'm already using... is there a compelling reason to switch?Nice language: http://www.99-bottles-of-beer.net/language-mumps-416.html
All the numbers are 24 \* power of 2. If you look [here](http://www.azulsystems.com/company/faqs.htm#whydidazul) you'll see that each chip has 24 cores but upto 16 can be put on the same board. This isn't what Intel are doing AFAIK, they're doing 80 on one die/chip.

The new Azul chip has 48 cores (which is where the 16 * 48 = 768 cores in TFA comes from) as also mentioned in the FAQ link above.[deleted]You're lying. You have got to be lying. Please tell me you are lying.Compare and contrast:  [Continuation Conscious Compilation](http://programming.reddit.com/info/143ep/comments).[deleted]Compare and contrast: [Lightweight Fusion by Fixed Point Promotion](http://programming.reddit.com/info/14a3f/comments).[deleted]I don't know, [what do you think?](http://groups.google.com/groups?q=chuq+von+rospach+group%3Acomp.risks&amp;start=0&amp;scoring=d&amp;num=100&amp;lr=&amp;as_drrb=q&amp;as_mind=1&amp;as_minm=1&amp;as_miny=1981&amp;as_maxd=13&amp;as_maxm=2&amp;as_maxy=2007&amp;safe=off&amp;)

The blogger has been on the net a long damn damn and is the principle author of of the canonical usenet netiquette post 
[A Primer on How to Work With the USENET Community](http://www.faqs.org/faqs/usenet/primer/part1/).
Haskell has a lot more expressiveness within easy reach than either C++ or VB.  You have a curious definition of what features are important.  Basic things like higher-order functions and pattern matching are features that no amount of cupboard-searching will net you in VB or C++.
Don't miss the subsequent conversation.[removed]If only we had a standard generalized markup language...oh, wait.[removed]Actually, that's why I used job board aggregators like http://www.indeed.com and http://www.simplyhired.com -- when you are looking for something more rare, they work pretty well.  If you do a search, they both give you an RSS link to the search so you can monitor it over time.  I found my current job that way -- it took months of keeping up with the feed to find what I was looking for, but I would never have found it otherwise.holy crap, I've never seen as many google adwords on one blog.  Is that a store or a blog:)Thanks.  I am not implying that I just 'wont' work with java.  It is more the culture surrounding J2EE development that really doesnt encourage innovation and users and management are in complete control.I wonder... how is IE 7's concurrency support?http://programming.reddit.com/info/10c60/comments/c10ckpHe also improved the pattern names:

Singleton = Highlander ("there can be only one")

Monostate = Borg ("we are all one")Thanks for the recommendation, but Mr. Byrd has deleted all of my comments from the site (4 comments over 3 posts),with no explanation or response.

&gt; Prospect’s Response: I was hoping for a way that didn’t involve paying. Do you know any sites where I can get cracks* for your software?

Best way to handle this one... "Well, if you like I can forward your email to the FBI Software Piracy Unit.  Maybe they can help you out."At least he *had* a job. I wouldn't throw that away.
I've written a number of simple markup translators that work like AsciiDoc.  They work great for simple stuff, which tends to be 90+% of what you need to do.  For the remainder, you really need a way to embed traditional markup -- just HTML in my case, but arbitrary XML would work just about as well and would give you more flexibility.Fry's 10th law of Text Based Document Typesetting:

* Any sufficiently complicated typesetting system contains an ad hoc, informally-specified, bug-ridden, slow implementation of half of TeX.

TeX may not be perfect, but at least you know people really put some thought into it.

Which brings me to Fry's 9th law of typesetings:

* Judge a document about typesettings by its typesetting.

Look at the PDF userguide.

1. On page 3 you see that the chapter title is on the last line of the page. (Badness!)
2. No hypenation
3. No text justification

I like the idea of AsciiDoc, but to suggest it's suitable for writing a book (see homepage) is ludicrous.To be fair, I've hacked a couple of things together too.

Where it seems to go off the rails is if you try to release it... pressures you didn't even feel as a tiny little project for your own use for some distinct purpose suddenly amplify into irresistably large ones when you actually try to make it work for some value of "everyone", driving you inevitably into the sticky goo.Agreed. So?It seems to have an EVAL!

If you find yourself stuck with the language, perhaps you can try to [Greenspun](http://en.wikipedia.org/wiki/Greenspun's_Tenth_Rule) it into something more pleasant?Am I the only one who got about halfway through the description of MUMPS features without seeing anything significantly different than Perl?  After that, it got a little uglier, but for a while there...Nope.Thanks. You just wasted 2 hours of my time...[deleted]It has a lot better chance than Lisp Machines et al did. Even if Intel and AMD are selling chips that're on par performance wise in a decade this has a chance to get the toe-hold it needs to stay in the game.Do you work for internal IT or a software company?  I think environment differences have to do more with what the focus of the company is rather than what technology it uses.

When I was working in J2EE as in in-house developer, the project was actually pretty ok and well run.  The rest of the company was pathological and made the environment unbearable.  That same exact project in another company would have been totally fine.  

Now I work in .NET for a software company -- we sell software, so we really care about it -- software is an asset and developers are investment, not a cost center (the way non-software companies think of them).  The difference in environment has nothing to do with the differences between Java and .NET.Well, here is a simple way - just create longer passwords for your more important accounts.

You will not be able to remember any of these passwords :-)

    def makePassword(length=8):
        import string, random
        special = '!@#$%^&amp;*()_+-=[]{}&lt;&gt;?|;,.:'
        characters = list(string.digits + string.letters + special)
        return ''.join(random.sample(characters, length))
&gt; So we'd be using inaccurate terminology for the sake of using terminology.

Inaccurate though it may be, it has permeated the "culture" of web dev frameworks. No, conventional usage here does not fit the definition, but I would be more worried about confusing another web programmer (lots of those) than confusing a desktop GUI programmer trying to do some web work (comparatively, not so much).[removed][deleted]If you were using Ad Block Plus (Firefox extension) like me, you wouldn't have known that page had any ads!I'm still puzzled about the key-term "architecture" in this context. We might better describe it as *principle-based* or *axiomatic*. That's interchangeable: in an axiomatic design you can add lots of new features as long as they do not violate the basic principles. The mentioned compositionality of features in Scheme is a good example of a reductionist approach. That's also why languages like Haskell ( or Epigram ) are considered as *mathematical*. Ruby on the other hand as an explicit *user orientation*. According to its creator it is dedicated to make users happy and its design is *harmonious*. These are subjective criteria and they can persist only by community reinforcement ( faithfull believers ). Other scripting languages are indeed domain specific languages turned into general purpose. They have a definite focus. Perl and PHP are two such examples. Note also that C++ is far more axiomatic in this respect than its successors such as Java or D: object oriented, full compliency with C and no speed compromises. 

The architecture as the *visible* structure might more clearly emerge from axiomatic languages but using monads ( as a pragmatic / problem oriented feature ) or list comprehensions do not spring off from purity and lazyness considerations alone. In a later stage, when ever being achieved, we might also see that this will creep in Epigram.Funny, but this is NOT the concept of shareware. The concept is to try a full uncrippled version, and pay for the software if you find it useful.

If you take the concept of the author, then almost any app is shareware.Only 'tmh' had anything I saw worth reading.  The rest was point/counter-point noisy chatter.  And I'm not going to read them all to find any more potential nuggets.This is one of the things I like about Markdown: it allows (by default, not here on reddit) you to embed arbitrary HTML.[removed]interesting read&gt; Remember - every script written for Perl waaaay back
&gt; to 1.0.0 is still executable in 5.8.8 - backwards 
&gt; compatibility is a major concern for p5p.

Take that Ruby and Python![Design patterns are missing language features](http://www.norvig.com/design-patterns/ppframe.htm).Mr. Byrd - 

I responded to your misleading article on your blog, but my comments have since been deleted. You can respond here, on your blog, or not at all (the latter being your intention, I suspect).

It's unfortunate that you decided to erase my comments without a response, because I was genuinely interested in whatever counterargument you might have offered.

On a final note, you toss the name Stalin around with impunity, likening Richard Stallman to him and comparing the politics of freedom to the politcs of the Soviet Union. I'm in the unique position of being familiar with the Free software movement and having family who was directly affected by Stalin's murder of 8 million Jews and displacement and repression of 20 million dissidents.

I would like to think that your far-right ramblings are merely the sign of a fool who doesn't understand Free software, but your constant comparison of altruistic software engineers with one of the worst dictators the world has ever known shows you to be a fool who doesn't understand the history of the world in which you live.

Your denigration of Stalin's victims is contemptible beyond words.That's interesting.  So, which languages do you think lie on or near the curve, and which under the curve?

As a quick hypothesis, I'd argue that *all* adopted languages are on the curve at the time of their design (I don't count hobby projects that everybody realizes are idiotic).  However, the curve moves outward over time, as people discover new ways of doing things or ways of simplifying existing feature sets.    Languages with small installed bases adapt to match the new curve, so that they are always on or near the curve.  However, languages with widespread industry adoption get "stuck" at their original point, so they fall behind.  That's why everybody thinks that COBOL, PL/1, Java, C, and C++ suck, while Python/Haskell/Lisp/Erlang remain cool.NoScript works well too. :)I am really glad you two guys had this conversation, because I just realized that until now I had thought you were the same person.
I had the same reaction.
Epic Systems of Madison, WI (Verona, WI) is the company being referred to. They prefer (nearly demand) all their new employees be straight out of college with no programming experience so they can train you for MUMPS without any "pesky" programming habits you might have from other languages. They pay well for entry-level, which is how they getcha.Hell no. Raganwald actually knows stuff about closures and the like. My mantra is invariably, "That's cool and all, but how it will it help me build software?".(I'm not trying to discredit all of your points and particular experiences, but I just want others to know that Django isn't the nightmare you depict, in reality there isn't much consensus about what's better.)

I've also used both Django and TurboGears heavily (including contributions to both) and I disagree. TurboGears and Pylons are only "better tools" if you're dreaming of a utopian future.

More stable because they're built on third-party libraries? In practice this has made TurboGears such a moving target that it's a huge pain to keep up with. SQLObject or SQLAlchemy? Kid, Genshi, or Mako? CherryPy 2 or 3? CherryPy's dispatch or Routes? These are questions that should be ANSWERED by a framework, not asked by it! And it would be smart for developers to choose Django for its (actually existing) documentation alone.

I still use and appreciate both, but when using TurboGears I often think "damn, this is already a solved problem in Django..."I know what you're saying.  My contention is that in some cases you can avoid the sticky goo if you're careful (well, careful and disciplined).  In this case it's recognizing the limitations of stylized text as a substitute for markup, and not letting your users pull you over the line and into the weeds.  One way to do that is allow them to escape into true markup when the stylized text can't do what they want.  Think of it this way -- build the tool around, say, XML, then introduce stylized text as "syntactic sugar" for all those basic things like paragraphs, italicized text, simple bulleted lists, etc.This isn't new though, special syntactic sugar for patterns can already be achieved with other preprocessing/macro tools, like [harp](http://www.cs.chalmers.se/~d00nibro/harp), and of course the existing Haskell macro system: Template Haskell. 

The point of the extended discussion is to consider what would happen if patterns were first class values in the language, and not just syntactic sugar.We'll he also came up with one of the first pascal compilers for the PC as well as one of the fastest of the time.  He did Delphi and was a pioneer for visual development.

Why do you not think he's the best of our time?Thanks for the tip!No problem :) I think I also see your point, it's just to see this stuff from different point of view I guess, and there are true things in both sides, that Ruby is not an implementation of Lisp, and that Ruby allows for many abstractions like Lisp.I sent you a response via the email address you left with your comments.The Trojan horse that pumped up spam volumes in January is at it again, researchers said today, and is now spreading over instant messaging and engaging in attacks on rival malware.

Symantec Corp. researchers said that the "Storm Trojan," aka "Peacomm," is now spreading via AOL Instant Messenger (AIM), Google Talk and Yahoo Messenger.Good lad!

The crusade continues![deleted]&gt; I've also used both Django and TurboGears heavily (including contributions to both) and I disagree. TurboGears and Pylons are only "better tools" if you're dreaming of a utopian future.

Shouldn't we be working toward a 'utopian future' instead of continuing to use non-standard parts? I mean if everyone thought that way, we might all still be coding in C, because for the time it was good at what it did. (note, I'm not making a direct connection between Django and C, I'm just making a point).

There will be a time, very soon (and I think it has already started) in which you will be able to 'easy_install' a piece of middleware off of the Cheeseshop, plug it into your application, and then have it running. Perhaps that middleware was initially coded for TurboGears, but even when using Pylons you'd be able to plug it in. The same goes for apps. And widgets.

What about Django? Well, things will likely stay as they have and everyone will create their own widgets and apps, and some people will stick the code up on the wiki for others to check out, but there won't be the widespread code reuse that there really could be, because Django doesn't really embrace standards like eggs, and it doesn't share libraries that most other frameworks are using.

&gt; More stable because they're built on third-party libraries? In practice this has made TurboGears such a moving target that it's a huge pain to keep up with.

TG 1.0 is different from 2.0. TG 1.0 is a set platform with SQLObject, Kid, and other items, which allows you to swap other components out. TG 2.0 will use SQLAlchemy and Genshi. Regardless, its not like they are changing between every iteration, and the core of the framework itself will retain a very similar feel.

&gt; SQLObject or SQLAlchemy? Kid, Genshi, or Mako? CherryPy 2 or 3? CherryPy's dispatch or Routes? These are questions that should be ANSWERED by a framework, not asked by it!

Okay, so create a Paste template which suits you and then call it everytime you want to create an app (paster create app --template=my_template'. Pylons and TG give you options, but they also give you defaults. Its not like they are leaving you completely to yourself.

&gt; And it would be smart for developers to choose Django for its (actually existing) documentation alone.

I think Pylon's docs are underrated. They are actually pretty good. Django does have excellent docs, but there are also parts of the framework that aren't documented so well. To be honest I think TG's docs are a bit messy but they seem to be working primarily on those right now. You must also keep in mind that a lot of times the external libraries Pylons and TG use are documented very well. The documentation for Mako and SQLAlchemy is probably better than the documentation for Django's templating and ORM.&gt; Only thing I've ever said that with is the old version of the admin app, because it was bundled as an "as-is" app. The newforms-admin branch aims to add a lot of flexibility, and I honestly can't think of any other component you can say "use it as is or not at all" about.

Well, another example would be generic views. For example, all of the create/update/read ones give you an option to specify a URL that you redirect to after success. Problem is, you often want to do more than that. One thing I was wanting to do was say "Job 'name' deleted" after an object and been removed. Django does provide some default messages, but they weren't what I was hoping for. An easy solution to this would be if Django allowed you to pass in a callable to the generic views and thus do whatever you wanted after create/update/delete.

As it is, if I really wanted to use my custom messages, I couldn't use generic views (and yes, I tried *everything* I could to get them to work for me). One of those 80% things. They almost would do what I needed, but they lacked that final bit that really would make them flexible to accomplish exactly what I needed. Therefore it was use it as is, or don't use it at all.Dang, that was a really depressing read. I hope dude can start hacking away at some Python (or whatever) open-source project and get himself a gig. I mean, a lot of us have the Java or .NET golden handcuffs, but that's not *nearly* as bad a situation...&gt; Shouldn't we be working toward a 'utopian future' instead of continuing to use non-standard parts?

Non-standard, eh? In what way are ANY of the pieces of TurboGears or Pylons a standard? Just because the pieces have different maintainers? The only piece that even came close to being "standard" in Python web development for a while was SQLObject and hey, look, a replacement is here.

&gt; TG 1.0 is different from 2.0.

In practice, do you really think this is how developers see it? Or is it more like "shit, if I develop this now, everyone who decides to go with 2.0 in a few months won't even want to look at my now-legacy code."
&gt; Non-standard, eh? In what way are ANY of the pieces of TurboGears or Pylons a standard? Just because the pieces have different maintainers? The only piece that even came close to being "standard" in Python web development for a while was SQLObject and hey, look, a replacement is here.

Eggs. WSGI for middleware. The templating interface. Now something like the templating interface isn't a literal written down standard, but I'd say it is becoming the defacto standard in Python web development, and frameworks that don't support it are just hurting themselves, as it makes it easier for users to plug in whatever templates they want. And SQLAlchemy is quickly becoming the defacto ORM, especially with something like Elixir coming out that makes it act a lot like SQLObject.

&gt; In practice, do you really think this is how developers see it? Or is it more like "shit, if I develop this now, everyone who decides to go with 2.0 in a few months won't even want to look at my now-legacy code."

Hey, I don't think the process is perfect, but the TG guys are doing a good job of making sure things are easy to swap over to the new platform. At its heart will still be CherryPy; the, Genshi can be seen as an 'improved' Kid, ToscaWidgets are essentially the TurboGears widgets made portable for any framework to use, and even though SQLAlchemy will the ORM, it'll probably have a declarative layer very similar to SQLObject.

The TG guys have definitely thought about this and its not like they will be making people transition from Zope to Rails. I mean, hey, you look at Django, people are having to switch over from custom manipulators to newforms. It really is an unavoidable process and it is happening to all these frameworks because they are young.I second this.  If you want to develop software for a living and you don't want the job to suck, work for a company that develops software as their primary business or at least regards software as a critical portion of their primary business (e.g. Google).  If you don't live in an area of the country with a lot of high-tech (Bay Area, Boston, Seattle, maybe Austin or RTP), start thinking about moving.So Ruby is going to get some lovely Java-style UTF16 strings where some code points use one slot in a char[] array and others use 2 slots? And remember Java only has braindead strings like this because the designers went with fixed-width integer types and were too short-sighted to ponder that the Unicode space might expand past 16 bits. JRuby is ruining Ruby. The removal of continuations and the addition of variable-width code points just to please the Java monkeys is not progress. What are they thinking? They need native compilation, better GC, a cleaner implementation, and real Unicode support. They do not need to slavishly ape Java with its oh so enterprise design flaws and moronic community.You're limiting your definition to the honor system, which is one type of shareware.  But it's not the only type.  I think you're being overly pedantic.

If you look at what shareware libraries list, it includes software where you get something free, but paying gets you more time, more support, more documentation, more functionality, etc.

You'll also find that a wider definition is commonplace, for example: http://foldoc.org/index.cgi?query=shareware&amp;action=Search

You're right on the WSGI front, and Django being able to make use of that would be pretty sweet.

Eggs and templating interface though aren't things people want in Django, so why count them against it? Django is distributed as a whole, what eggs would it use? And I see maybe one person a month ask how to use a different templating system with Django, whereas it was such a big complaint with TurboGears (due to the popularity of Cheetah and discomfort of XML based templating, for example) that of course they had to add it!

Re: Elixir - like I said, in an ideal future this entire stack will be great, but citing a project that was just released *yesterday* as a win for TurboGears? Gee, I wonder what will be released tomorrow that the code I'm writing today could use...ASTs as source code makes source transformations simpler. With infix you might have to encode precedence into the grammar that leads to long nested chains of nodes in your parse tree. However when transforming parse trees, these chains can usually be abstracted away or one transforms parse trees into ASTs prior to the transformation.

Source transformations as a convenient programming tool are neither strongly investigated nor encouraged outside of the Lisp community at the moment. In effect having a monopole on them  leads to lazy thinking and strong assertions.[deleted]If you actually had java (as opposed to MUMPS) experience, you wouldn't have an issue with quitting MUMPS and doing something else.

Java has many issues that prevent it from being a fun language to program with, but a lack of high-paying jobs is the one aspect that isn't a negative for a Java programmerOn the other hand, because of the sorry state of the Ruby VM people will be more eager to try out YARV, Rubinius and JRuby. In the python crowd the consensus seems to be that CPython good enough, so alternative Python VMs will have to do a lot better in order to get some traction.Flexibility wins over special purpose hardware as the Symb. &amp; LM stories show.  A big market for special purpose hardware is also big or bigger for the general purpose hardware that eventually performs just as well.You can try to Greenspun straw into gold, but one prick of the finger and it's lights out.It's not the main point of the article, but:

&gt; it compiles the Ruby code down to Java bytecodes (therefore you get speed)

Wishful thinking?What the hell does any of PyPy's success or failure have to do with Rubinius? PyPy's proposed feature set lead me to believe it would be scoped either to death or extreme development length. Rubinius' scope and team are quite a bit smaller, which leads (applying Mythical Man Month logic) to a greater chance of success.From a programmers point of view (rather than the OSes) what do other people mean when they say threads?

I've always taken thread to mean a concurrently running subroutine which has access to the full program resources with some limitations. Specifically it means for me common access to the memory, so moving data between threads could happen no overhead (highly unlikely but possible).

Am I missing something?Ruby 1.8 is goddamn slow. *Goddamn slow*.Let's see.

1. In the original code, the interviewee uses explicit variables instead of a hash-map. Result: lots of typing and magic constants are necessary. (Not to mention that the ordering of the columns in the DB will cause the program to fail, which is not the case for the hash based solution)
2. fetch row without checking whether there are any rows to return
3. the code doesn't clearly show that $customer_id is a numeric. Why not cast it to an integer, or use sprintf("... %d ... ", $customer_id) to make sure typos at other places cannot wreak injection havoc?

As for the 'new and improved code':

1. Lots of magic without documentation.
2. clobbering of existing variables without warning
3. clobbering of variables if two columns in the database differ only by nonalphanumeric characters
4. still no checking whether $customer_id is numeric
5. all columns are retrieved, not just the ones that are needed. What if there are a few binary-large-object columns  are added later? Then this piece of magic will actually do worse than the original version.

Frankly, I think that the first author gets -1 for being a bit sloppy, and perhaps another -.5 for cramming so many statements on one line (that's pretty much a give-away of poor code). And -1 for magic numbering without obvious reason.

The second version gets a -10 'what were you thinking' award. The refactoring changes the semantics of the code, it clobbers stuff without warning and it makes no effort to prevent injection.&gt;So, I began searching for alternatives. Spent a couple years with a
language that requires everything to be an object. Can someone hand me
a hammer, I have a round peg here and a square hole there. Didn't have
good numeric support, but based on other perceived advantages, I had
hashed out an object system that would have provided numeric support.
As I'm implementing the numeric stuff, I'm getting very annoyed with
changes in the language that are requiring redesign of my objects. Plus, performance, while not bad, is not the best.

Anyone know what language he's talking about? Maybe Ruby?I received this response from you via email after posting my previous comment here:

"I had your comments removed because of the personal attacks in them.  Regardless of what you think of me, it's my blog and I don't have to let comments stay that personal impugn my character or competence.  It's one thing to disagree and us have a civil conversation, but before I even had an opportunity to engage you in debate you started making false accusations and assailing the fact that I was homeless.  You showed a complete lack of respect and civility and I simply
didn't appreciate it.

Payton"

Since people have no opportunity to read what I wrote on your blog (which is unfortunate because the reason why I took time out of my day to respond there was to offer clarifications to your misstatements about Free software), I  wanted to post the comments here on Reddit as well, but it's too long. So the ball is in your court, Mr. Byrd. You are welcome to reinstate my comments and let the court of public opinion decide if I wrongly assailed your character. I stand behind everything that I wrote and am more than willing to defend it.

I'm surprised that someone with a such a self-professed "thick skin," who compares Richard Stallman to Josef Stalin,  would have such a problem rebutting my comments in the forum of public debate. I welcome your attempts to show me where I have made false allegations in this or any of my other comments on your site. Why don't you replace the deleted comments and let the public see what I wrote, and you can take this opportunity to defend yourself and correct any mistakes in my remarks? 

You're right, it's your blog and you can do what you want, but why have a blog if you are going to spew falsehoods and then refuse to defend your claims? You accuse me of lacking civility (and I admit that even the minimal level of courtesy I extended in my comments was a strain for me, given the level of offense I took at your insistence on defaming Richard Stallman and the Free software movement), but your responses to other commenters include gems like:

"That's as basic as you can make the situation. If you don't understand this then I suggest some remedial courses at your local community college." 

and

"Your arguments make no sense in the real world, you are highly uninformed, and I suspect that you are uneducated as well. You've done nothing to prove yourself otherwise. "

You demonstrate a lack of knowledge of the Free software movement, disrespect for Richard Stallman, Linus Torvalds and every developer who contributes to open source software, labelling them as "Communistas," then delete comments which present a dissenting viewpoint, all the while claiming Free software proponents seek to undermine capitalism and demanding immunity from attacks on your own character.

You claim that "Stallman really is acting like a wanna-be Carl Marx, but he's trying to use Joseph Stalin style tactics to accomplish his goals" then take offense when somebody has the gall to question your character and competence.

For anyone who is interested in the comments that I posted on Mr. Byrd's blog, you'll have to ask him to to reinstate the comments. Hopefully he thinks his positions are defensible and will take this opportunity to engage in debate.

If any Redditors feel so inclined, they can upvote the article and this comment thread so people can express their own opinions on the issue.

I'm not going to apologize for hurting the feelings of someone who publicly equates the leader of the Free Software Movement with a murdering dictator.
Come on, are we still to the "Java is slow" myth?OMG, you can outsmart a PHP-programmer. You're clearly a genius!You're trying to hire a PHP programmer. What do you expect from dollar-sign land?

It looks like both the new and the old versions of the code have potential SQL injection vulnerabilities, so it's a moot point anyway.That's not about Java. Last time I checked JRuby was significantly slower than the C implementation.I can't tell.  There's weird line noise all over that page.&gt; The concept is to try a full uncrippled version, and pay for the software if you find it useful.

That turns into *if-only-you-had-this-ware*, where users say they'd gladly pay your $15 registration fee if you just supported their 1988 dot-matrix printer or some other esoteric feature.The first item on the list is standard to nearly all programming languages, the second (and third) item on the list is not found in Perl (or any other mainstream language that I'm familiar with).  So unless you define 1/8 as "about halfway," I'm not sure how you can claim that you read halfway through before seeing feature divergence from Perl.[removed]Hmm, not to be paranoid, but this looks like spam. A site about shareware, with links to shareware in question, from a user with one link, this one, who's name is "SharewareFun"...Java is fast on the JVM because it is statically typed.  The problem with Ruby code on JRuby (or Python on Jython) is that they need to do dynamic dispatching on a platform that was not designed for that.  Historically JRuby and Jython (as I understand it) relied on reflection to get the necessary dynamism, and historically reflection on the JVM has been really slow.  I think Sun has put in some added support for dynamic languages with Java 5, but I don't know any specifics, or if JRuby or Jython have tried to exploit it yet.

I still think it would be an interesting exercise for someone to try implementing a Ruby or Python runtime on top of a commercial Smalltalk runtime, which would be a much closer match in terms of run time model.Here on the programming subreddit we prefer actual content to slavish fanboism.Eh, just to clarify, I mean speed in a relative sense. I'm not suggesting that JRuby is gonna be as fast as Java or anything like that, but the latest benchmarks I saw of the compiled JRuby had it running at nearly twice the speed of CRuby (was only a fib method, but still).[deleted][quoth](http://www.martinfowler.com/eaaDev/uiArchs.html) Martin Fowler:

&gt; The presentation part of MVC is made of the two remaining elements: view and controller. The controller's job is to take the user's input and figure out what to do with it.

That's only the mid-level view, of course, but I think it's enough to lump webbish quasi-MVC in with gui MVC. Additionally, some web MVC is starting to look more like gui MVC what with AJAX allowing for partial redraws.The JRuby guys, the RubyCLR guys, the Cardinal guys, and the Rubinius guys are all getting together for the mountain west ruby conference next month.  Should be interesting to see what happens.

http://on-ruby.blogspot.com/2007/01/mountainwest-rubyconf-implementors.html&gt; Django is distributed as a whole, what eggs would it use?

With Pylons (I don't know about TG) you automatically get eggs built for your own application, not just the framework.(was only a fib method, but still).

No.  No 'but still'.  Some guy wrote a python to ocaml 'compiler' (there is even a google tech talk video on it on google video) that got better speed than cpython.  It turned out he implemented hashing by ID... well no shit it runs faster.I'll second. If progress is being made on Arc and Paul wants to share it he will.&gt;Source transformations as a convenient programming tool are neither strongly investigated nor encouraged outside of the Lisp community at the moment.

Prologhttp://www.ridiculousfish.com/kay/prize.htmlInside info: Paul Graham has joined 3D realms and they are now together rewriting Duke Nukem Forever® in Arc!

Other interesting facts:

1. Target platform and development environment is GNU Hurd operating system. 
2. All internal and external documentation is made using [Xanadu](http://xanadu.com/)
3. Project motto: **"It's done when it's done"**
4. Physics engine will use completely rewritten version of Maxwell equations.
I'll tell you what worked for me:

I'm not sure exactly how he put it but Paul Graham mentioned something somewhere about how real hackers are always hacking as if by impulse and should therefore always have working examples of what they're capable of. This was actually true in my case.

It's through working, web accessible examples that I got my first and second jobs with a startup. If you're lucky, the guy doing the hiring will actually be qualified enough to evaluate your work and make some sort of determination as to how talented you are.

So If you haven't done it yet, work on building something to showcase your talent. In my case it was a user-driven music site which demonstrated (to the trained eye) my competency in programming as well as UI design.

Take full advantage of the web to market yourself through unconventional channels. For example, all of my work has come through craigslist. If you live in a po-dunk town, you can always telecommute but im not sure if this would work over the long-term (never done it).On another note; reddit seems show 22 comments but I bet there are only 11 actual comments.

I wrote in M for almost 4 years.  In comparison to what we have now, it's pure crap.  But compared to what was available 40 years ago (and the amount of storage that was available then), it's a pretty neat language.  Remember, you're talking about a system that was designed back when megabytes of memory was pretty much unheard of.

It's almost like COBOL now - if you know how to write well, you won't have any problems finding a job.  I got out of programming and am now a happy system administrator.

The Department of Veterans Affairs spent *years* writing M code to run their hospitals, and almost all of it is available via the public domain.  Companies sprung up that took the PD code, modified it a bit, and turned around and sold it to hospitals. The next time you're at a hospital and someone is using a dumb terminal or CLI interface to something else, it's most likely an M database in the back end.  I remember seeing it at my local hospital maybe two years ago.An interesting paper. Maybe I should try writing something non-trollish in this thread:

One of the examples in the paper is the factorial function which is typically written like this in functional languages:

    fact(n) = if n == 0 return 1 else return n * fact (n - 1)

which is efficient because of tail recursion, but is actually a horrible algorithm from a parallelizability perspective. In fact, factorial parallelizes very well if you implement it differently:

    prod(n, m) =
        if (n == m)
            return n
        else
            let mid = (m - n) / 2 in
                return prod (n, mid) * prod (mid + 1, m)

    fact(n) = prod (1, n)

which would allow all the recursive calls to proceed in parallel so at the deepest recursion level, this program could make use of O(n) cores.

A compiler could automatically schedule this program very well on a multi-core CPU given assumptions about purity, though there may be issues wrt. to limiting the overhead (ie. you need to stop the parallelizing when each function is operating on a suitably sized range).

So it seems like programmers will have to understand parallel algorithms, but maybe pure functional languages would be better suited for implementing them.[deleted]&gt; In practice this has made TurboGears such a moving target that it's a huge pain to keep up with. SQLObject or SQLAlchemy? Kid, Genshi, or Mako? CherryPy 2 or 3? CherryPy's dispatch or Routes? These are questions that should be ANSWERED by a framework, not asked by it!

SQLObject, Kid, CherryPy 2, and CherryPy's dispatch, respectively. This is plastered all over the TurboGears main page, mentioned in practically every discussion about these alternative on the mailing list, and any part of the documentation dealing with everything else is conveniently under the "Using Non-Standard Components" section, just to make sure the context isn't lost. If that doesn't count as "ANSWERED" then evidently nothing short of removing all mention of the alternatives will.

Next time you feel like knocking down straw men try to pick ones that aren't immediately obvious to anyone who has actually *looked* at the subject.

Hell, TurboGears doesn't even *work* with CherryPy 3 yet.They can be done in Prolog, but they're not elegant.  `assert(foo(X) -&gt; Y(X))` is possible (where Y is previously bound to some other value) but it's fiddly and therefore somewhat frowned-upon.  Similar things can be done in Forth (`CREATE` ... `DOES&gt;`) and even in C# with reflection and all those code-generating classes, but it's never pretty.

Lisp makes is more part of the language, which is why it's relevant.  Remember: you can do anything in machine code, given infinite development time.In an attempt to rescue this thread/post:

I think the best environment to embed a new experimental Lisp in would be [SISC](http://sisc.sourceforge.net) because SISC is the most efficient JVM Scheme interpreter I know of and, living in Java-land, it has Unicode baked in in addition to a convenient interface to Java libraries, giving it a chance of actually being pragmatic.I assumed Smalltalk.  Ruby is pragmatic; it doesn't require that everything be an object, from what little I've seen.Same is true with TG ... pretty much has it reduced to filling in a few fields and hitting "GO". (Which I assume is where Pylons is as well)&gt; Silicon Valley has Google, YouTube, and Reddit all using Python.

Google *also* uses a lot of Java. If you have really good Java  skills you could get hired at Google and work on both Java *and* python as part of your day job.&gt; They prefer (nearly demand) all their new employees be straight out of college with no programming experience so they can train you for MUMPS without any "pesky" programming habits you might have from other languages.

Which basically makes them a cult&gt; Haven’t heard back from him yet.

Hmm, can't imagine why!I wonder if any of this can apply to web applications. The web is event driven, but for certain applications, it's easier to program it as threads, as is done in Seaside and other continuation-based webservers.&gt;  I think Sun has put in some added support for dynamic languages with Java 5

There is talk about adding _one_ opcode (for method dispatch IIRC) specifically for dynamic languages to the next major version of the JVM.

I'm skeptical that JRuby will ever be much faster than C Ruby. It's past 2 am here already, I'm too tired to elaborate in detail, just a few thoughts:

* Ruby grew with its C implementation.

* The C implementation can use low-level tricks that don't translate to Java, e.g. a fixnum is simply a tagged processor word, requiring no memory management overhead.

* Every JVM instruction directly maps a Java language construct (rather high-level compared to what C offers). Thus every mismatch between Java and Ruby semantics (the devil is in the details) means a huge performance and/or generated code size impact.

* `1 + 1` is not necessarily `2` in Ruby. One example of dynamism that probably destroys many JVM optimizations.[deleted]This is programming-reddit. I think we all know which language is better than both.Is your first name Ben? :)MUMPS is a very cool language. I worked with it for a few years in the early 80s. It's great for doing data management (it was designed to facilitate patient records management). It has only one intrinsic data type: string. The ubiquitous string was such a natural pleasure to work with that to this day, I consider all strongly typed languages to be unhelpfully coercive, forcing me to make things easy for the machine even when it's of no value to my business.

The MUMPS language is interpreted, and its persistent data store is arranged as a giant disk-based asymmetrical sparse tree. Daunting though that may sound, it really acts like an unlimited dynamic array that uses any string (or combination of strings) as subscripts, and whose elements are strings. So...

&gt;   set ^customer("187","NAME") = "ABC COMPANY"

...would be a natural way to store away the name of company number 187. And this would execute faithfully without any prior declarations or schema management.

Every MUMPS statement, such as the "set" statement above, began with a different letter of the alphabet, so it was proper idiomatic style to abbreviate by using only the first letter of the statement, e.g...

&gt;   s myvar = "Hello world"

As a lazy programmer, I loved how much you could do with so little typing.

And, yes, you could use such a language to create an impenetrable mess. But good programmers make good structure of any language, and good MUMPS code was no less a pleasure to work with than any other useful language.

I did, however, move on from MUMPS. I believe that was before many (perhaps most?) Reddit readers were born.Wow, I forgot just how bad PHP really is.At the risk of beating a dead horse, how does a new experimental Lisp have anything remotely to do with pragmatism?I can't wait to see what he does with Arc. But I think it's really lame to nag him about something he's working on for free. Bugging him for status reports isn't going to speed anything up.

"Daddy, are we there yet?"


"One good example is that rubinius is all driven by bytecode and will even have hooks to further help companies protect their ruby intellectual property. To companies, that means ruby is much more enticing to use."

Eh? Does this mean Rubinius is building in some sort of DRM?Uh no, that was certainly *not* a straw-man argument, this is a real problem. Like I said, I've developed with TurboGears and even written [my own tutorial](http://docs.turbogears.org/1.0/TodoList). Yes, it is clear that TurboGears chose those components as its "official" pieces, but it's also pretty clear that those components are being phased out; what's not clear is what a developer should do about this. CherryPy 3 was released almost two months ago and kicks ass. I want to use it. SQLAlchemy is way more powerful than SQLObject. I want to use it. Genshi is like Kid but better, I want to use it. Given this state of affairs, do you really think choosing SQLObject, Kid and CP2 is "immediately obvious to anyone who has actually *looked* at the subject"? Because those certainly aren't the tools I would recommend as a knowledgeable developer.It already supports loading archives of compiled byte-code files. Perhaps they mean some sort of obfuscation to prevent people from disassembling the bytecode. There's something called ZenObfuscate or something for CRuby that compiles to obfuscated C or somesuch aimed at that. 

What does developers for the CLR and the JVM do to protect their intellectual property?[removed]I wanted to hate you.  
I did hate you.  
Then I got to number 4.No, he's [Tron](http://c2.com/cgi/wiki?AlanKayIsTron)!If I may ignore the ongoing multicore concurrency religous wars for a moment:

Brendan's blog gives a fascinating and no-bullshit view into software engineering in the real world.  I hadn't seen it before but he talks about a lot of neat stuff that comes up with a codebase that large.  It's worth reading through.

Thanks for posting this.These guys deserve each other.Wow, you're insane. I was on the fencepost about that for awhile, but this last post really pushed me over the edge.I think the newforms module is going to be awesome.  Since I need to use some features that are not yet available in it, I must use oldforms and it sometimes pains me to see how easier it would be with newforms if only it had [insert missing feature].Does Java support Unicode, or just semi-broken UTF16 strings?I suspect your shitometer has fallen out of calibration.  May I suggest the following as a reference for "completely fucked-up piece of shitware":

http://www.codinghorror.com/blog/images/wgetgui-screenshot.pngBecause it’s partly an experiment in pragmatism.You don't even have to quit your "job" to quit your J2EE programming job, depending on where you work. Just tell your management you want to do something else before "just quitting" -- who knows maybe there is a group where you can work with Python or PHP (in my case both such positions were available in a huuuuuugely  pro-Java shop; now I work primarily with Python/Jython).Arc will never be released, and in fact PG hasn't started work on it yet, and never will. Now stop caring about PG and let him write his inane essays in peace.Mr. Byrd -

I thought I would post a few quotes from your blog so Redditors can get a feel for your understanding of Free software and the world in general:

from http://blogs.ittoolbox.com/visualbasic/dotnet/archives/linux-lacks-innovation-13721

"The problem is that FOSS companies force volunteers to assign copyright away from themselves without any kind of tangible compensation, some of which have no right to assign away copyright because they are under age and some of which don't have a concept of intellectual property."

"My problem is that large, international for-profit companies are using non-hobbyist open source projects to make super-big bucks and the developers don't get paid for it."

from
http://blogs.ittoolbox.com/visualbasic/dotnet/archives/gpl-the-flesh-eating-virus-of-open-source-14273

"The fact is, the FSF is trying to use the fact that they actually do infringe on IP to keep Novell from indemnifying their customers from the repercussions of that IP violation! The whole purpose is to devalue IP and thus further along Stallman's communist agenda. "

"So, if the FSF thinks that they can block Novell from releasing GPL v3 code because of intellectual property issues that would conflict with their pact with Microsoft, then doesn't it hold that the FSF is acknowledging that most Linux distributions do in fact violate patents covered in the Microsoft-Novell pact?"

"So *WHAT* if MS doesn't want to play fair. Fair is a socialist utopian concept that has no bearing on the real world anyways! It's a fairy tale that we tell to 4 year olds to explain why they have to go to bed when their 18 month old brother has to."

I recommend anyone who is interested go read the articles from where I took these comments to get some context. If you're interested in reading an intelligent and articulate response to this and other articles on his blog, ask Mr. Byrd to reinstate the comments I left there and defend his position in debate, either here or in his blog comments.

Actually, that's much more comprehensible than the nested, tabbed and sub-dialog-spawning FF-preferences-dialog.Here's a link, you can take whatever you want from it.

http://headius.blogspot.com/2007/01/jruby-compiler-in-trunk-and-ready-to.htmlReal men use only byte[].Ruby doesn't require that you *know* that everything is an object. Everything still is.I can personally attest to the fact that you're wrong.  Go spread your bullshit elsewhere.Not sure if you noticed, but the excerpt of code was just that -- an excerpt, written merely to demonstrate how to reduce redundancy. All of the points you raise are obvious flaws that would be addressed in a real application. Obviously (and as is explicitly stated), it's a contrived example. This code would not be used as-is in a real piece of software.Since you brought it up, $customer_id is actually cast to an integer prior to the code excerpt, so there's no possibility for an SQL injection.

Edit: Additionally, re your first point, I expect to find someone who, despite the fact that they are working in dollar-sign land, takes an interest in improving their coding ability. For myself, yes, I write PHP for a living. By night, I'm hacking with more expressive languages for fun.

PHP is a less-than-perfect tool (no shit), but if you know your stuff, there's no reason you can't create great software with it.[deleted][removed]_"It also shows that the codebase to PyPy didn't really end up being a lot simpler as originally hoped."_

Where does this idea come from?

PyPy is a lot, *lot* more ambitious than CPython:  the expectation is that it's more flexible, modular, extensible and powerful than CPython;  anything but simpler.I want to know, but I also think that Graham should release what he's got when he's ready to release it.

Designing a programming language is a lot like sex-- it's best when performed privately and between fond friends. And, while it's fascinating to watch public displays, the results are far from satisfying.[deleted]--&gt;- Smalltalk --&gt;-
      /                    \
     /                      \
    |                      PHP
     \                      /
      \                    /
       ----←--Ruby---←-----
My sincerest apologies, there was a great bug preventing a useful chunk of its useability. Now it has a dedicated web page with features, screenshots, and instructions, here: http://www.hoovy.org/HaskellXcodePlugin/

Again my apologies for lack of formal release schedules. Please enjoy![deleted]&gt; PyPy's proposed feature set lead me to believe it would be scoped either to death or extreme development length.

The upcoming 1.0 release is [targeted](http://codespeak.net/pipermail/pypy-dev/2007q1/003507.html)  to happen tomorrow (2007-02-15), actually.You build software??upchuck if you don'tI'm very curious about Arc's progress.

I especially want to know if PG's still accepting/soliciting suggestions...[Template Haskell](http://www.haskell.org/hawiki/TemplateHaskell)?I share the skepticism. People underestimate the work necessary to make things as good and compatible as another complex thing.

Even though JRuby has been under development for several years, it's still far from being a complete replacement for Ruby in C. And the speed is the least of my worries. There are hidden semantic differences between them, for example, which can mean some hard to find bugs and such things.

Python is a little simpler than Ruby, but its several implementations still differ one from the other in the little details, for sure.

The good news, is that SASADA Koichi, the man behind YARV, has proven himself another genius by being able to create the virtual machine mostly by himself, with total compatibility with the default Ruby in development, as YARV reused as many things as possible from the current implementation, even though it meant that he needed to synchronize the trees all the time before they were integrated.

The hare and the turtle. Ruby came after Perl, Python, and many other languages, but has been progressing well, with little resources. Remember that Perl, Python, and other languages had a lot of investments during the DotCom bubble, while Ruby was a foreign undertaking that could enjoy little out of it. :-)

To an inexperienced Ruby core developer, Ruby is very hard to follow altogether. But for the insiders, they know a couple of spots which are really hard to understand, while most of it could be explained more easily. They haven't been able to solve those issues because Ruby requires the way it is now, probably because there isn't an alternative yet. Ruby was grown organically, but with a certain consistency which us Rubyists enjoy in it. How consistent is it? Well, I don't complain of not understanding my own code after 1 month, 6 months, 1 year, 2 years... And I have old code that's that old!The choice of categories for the tabs in FF certainly isn't very clear:  what belongs in "Main" vs. the other tabs?  Why is History under "Privacy"?  Any hierarchical taxonomy requires you to adopt its viewpoint, and only makes sense once you've arrived there.

Have you used OSX's System Preferences dialog?  It seems to be the best take on "expose 1 million options" that I've used.  It helps that it includes a searchbox.  Then again, so does about:config ...Yeah, but you will be stuck with Java for as long as there are jobs for it. Learn from that MUMPS guy and don't let Java be the only thing on your resume...because a) he got called on his bullshit or b) he has such a huge ego that he cannot handle criticism. 

Anyone who says they have not refactored code because "they always get it right the first time" (paraphrased) is obviously lying or they have never worked on anything large.9 years later and still waiting...more interested to know if he is having breakfast regurlaly[deleted]And what kind! because, if he eats Cheerios, then by golly I've got to throw away my Wheaties.&gt; Everything still is.

...except for methods.Dr. Harrop's book on Ocaml is a treasure: he's probably not the only ML master of his calibre, but he's the only one I know who gives secrets away in such great detail.

If he dropped the price on that to reasonable levels and got a mainstream publisher I can see the Ocaml userbase increasing an order of magnitude.I just walked away. j2ee developer 4 years and I just quit one day, went back to school to do a phd in AI. writing up my dissertation now.I guessed Java.You mean even if Java hardware is crap we will be stuck with it because so many people are using it?sisc is still slow and they have admitted that in the past.Lamer. `byte[]` is only for those too lazy to use `bit[8]`.Yep. Currently I am designing a system to clear bond trades using robots and OCR readers. It's great. The traders type out the trade using any word processor, text editor, or type writer. They hand the print-out to the robot, who carries or mails it to the other robots that handle the books and records. Since there are no electronic communications, there is very little risk of a virus or other malware infecting the system. And if it does, the spread should be slow enough to contain.

We are having some performance issues, but I'm sure we can sort it out with time.
My favorite:

&gt; On Lisp Macros:
&gt;
&gt; I think they are overrated, and in general cause more harm than good.
&gt; It's the reason I find Lisp-like programs difficult to grok, maintain
&gt; and extend. Cos every smart ass wants to needlessly write his own mini
&gt; language to the point of absolute obfuscation. Naturally, I'm supposed
&gt; to be awed by his mischievous cleverness.&gt; What does developers for the CLR and the JVM do to protect their intellectual property?

Mostly write code so crappy nobody will bother to decompile it because it would take longer than writing the same stuff yourself and writing it yourself requires less skill.Things that shouldn't be in filenames… if you use a command-line. Everybody else—that is, regular users, as opposed to geeks like us—would wonder why they have to name their file “2001_aspaceodyssey.txt” rather than “2001: A Space Odyssey”.I have to agree with slava on this one. Real unicode support is a must have if you want to use Ruby outside of the English speaking world.Does this have anything to do with global warming and rising sea levels?[deleted]Why is it that we think we can give examples that aren't in themselves examples of good code?

This is not meant entirely as a criticism of your stuff, more about the concept itself. I'm sure my web site has plenty of examples of code that doesn't belong in production systems. If so then why am I using it as illustrative code for anything I wonder?&gt;Would you be interested in moving to a low cost area of the U.S. (read: the middle of nowhere) and working for less money, but having a better quality of life with no traffic and cheap housing?

I live in Alabama.  If you find yourself online most of the time (when not working), want to spend as little time shopping as possible, and heavily use Amazon and similar sites to buy stuff to be shipped to you, then this may be for you.  If you like going out to the night life of the city you live in, probably not, unless you can live with going out to one of the 4 places in town that stay open after 9pm. :)
Nice concept, but extremely outdated. An update would be highly interesting.8 months?Yea, I really want to go back to school.  I took the GRE and did pretty well on the analytical part, could get into Georgia Tech, but I always miss the deadline; get depressed and 'dont' apply the year after year.

Hi, I'm Ken Silverman. I've rejoined the Duke Nukem team and Paul is gone.

In order to rollback the game from "Duke Nukem Forever in Development" to just "Duke Nukem Forever", the new Duke engine will be written in C and x86 assembly language, it'll based on the Duke Nukem 3D engine with a large amount of code re-use.

Furthermore, the game will be release when it's only half-done. However the game will be so hard no one will be able to finish the first half before we release a patch containing the second half, in 2020.we can't be sure until he's collected two of every animal.This "trampolining" sound like the technique used by David Mertz for implementing the same in python (June/2002):

http://www-128.ibm.com/developerworks/library/l-pythrd.html

Anyone knows the origin of the trick?Any way you could do some of your work in Jython or JRuby?  Jython kept me from going insane (till I could get a full-time Python job).This is a valid question. My reply here was perhaps not entirely accurate. I don't think the code I give in the example is bad, merely that it's not complete in itself. In a real application, you will have things like robust validation, proper documentation, and mitigation of impact on surrounding code ("clobbering" as fry says [BTW, incidentally, there would be no clobbering -- all the variables created would be local to the function that the exerpted code is within, not global]).

For the purposes of an example, all of this stuff can dilute the essential point -- in this case, that tedious, repetitive code can be shortened and, more broadly, that code can always be improved. It's not saying "this is exactly how you should code", rather "this is what you ought to think about when coding".

For what it's worth, I spent all of 5 minutes on the example code.Do you have any evidence for that assertion?  Cos it's looking like the Reddit equivalent of Intelligent Design at the moment.But there's some kind of autoboxing similar to the .NET CLI, isn't there?  So that if you declare an array of a thousand two-byte integers, it takes up two thousand bytes of space, not one thousand times the size of an `Integer` object?  I'm just making assumptions here.Ooh, good call.  I'd forgotten that.  Could be, indeed.No and um no.* Robust validation
 * Proper documentation
 * Mitigation of impact on surrounding code

These things exist in real applications?I've always believed that pragmatism in programming has nothing to do with the language, and everything to do with the libraries.

I haven't been following Arc - is Paul Graham attempting to build up better libraries for a Lisp, or is he just trying to make A Better Programming Language?If you really want to do something interesting and different (read: doesn't need to make money) school is the most likely way.  Research grants, fellowships, and teaching, and you can basically work on whatever you want!  (if you are convincing enough to the clueless administration).  And the most important thing is that's where you make the connections that lead to jobs that would never post on craigslist.Also ridiculously too long. It has to be short enough that people will actually want to take it.On one hand, presumable fans:

&gt; Graham should release what he's got when he's ready to release it.

— [kanagawa, elsewhere in this thread](http://programming.reddit.com/info/14fmn/comments/c14gic)

&gt; I think it's really lame to nag him

— [staunch, ditto](http://programming.reddit.com/info/14fmn/comments/c14g5m)

On the other hand, Graham:

&gt; 1. Release Early.

— [*The Hardest Lessons for Startups to Learn*](http://www.paulgraham.com/startuplessons.html)

This is an unfair comparison. Graham was talking about startups, not languages, and Y Combinator is probably much more important than Arc. He’s got a great excuse.

But it’s an *easy* comparison, and I think a lot of people are making it. Having said people should work hard and deliver often, Graham will get complaints whenever he’s not squeezing unicorns and daffodils out of anything he’s said he’ll work on. It’s not fair, but he should have seen it coming.

If hundreds of people paid attention when I announced spare-time projects, I’d keep a stiller tongue.No, George Bush is stopping him.[removed][removed]Apparently, reddit *loves* Paul Graham.

Who'd have thought!Actually that sounds right. Everything is an object... technically no, actually in Python everything is, but Java forces OO down your throat much more than Python does so I could see that. Bad numeric support, that's the truth. Changes in the language -- release of Java 1.5 most likely. Performance not the best, especially for heavy number-crunching.[removed]_"Rubinius has a huge uphill battle ahead of it, as it is being worked on by a much smaller team,.. with a small fraction of the funding.. [than Pypy]"_

Perhaps pypy's (alleged) lack of success is *because* of the money and the lack of constraints rather than in spite of it? My counterexample is [pugs](http://www.pugscode.org).The 2nd graph is priceless.

Entropy and the arrow of time are orthogonal!

The arrow of time axis is continuous! Shouldn't there only be 3 values?I'm upvoting because I want to know what this even is."the latter is often sacrificed in order to improve the former" - seems that this disqualifies it from orthogonality, eh?  Sure, they aren't directly related, but this shows that there is some connection between the two.&gt; pragmatism in programming has nothing to do with the language, and everything to do with the libraries

But the pragmatic quality of the libraries you can write depends on the core language. Can a library function return a function? Return a pointer? Return an arbitrary number of results? Can it take an arbitrary number of arguments? Can you make security guarantees about it? Threading guarantees? Can it return a new type? A custom error? Can it return an integer bigger than 2^32? A Unicode string? Can it define its own calling syntax? These things can all matter to the real-world speed and quality of development, and they all depend on how the language itself is written.

If you could find an unbiased bilingual developer to write the libraries you need in Cobol and in your favorite language, would you really have to flip a coin?

&gt; is Paul Graham attempting to build up better libraries for a Lisp, or is he just trying to make A Better Programming Language?

Arc is a new language – new syntax, new basic functions and types, new conventions – but the plan (as far as I know) is to keep it small and put a lot of stuff in libraries.He's trying to shrink the core language and move nearly everything
into the libraries.  I don't know whether you'd consider that good.
Download shareware software for PC, Mac, and Linux categorized into categories[deleted]We're about to launch some new dynamic stuff on the Y Combinator site that's written in it.

Actually, the application form for [startup school](http://startupschool.org) is written in Arc.  But it's just a little throwaway thing I wrote in an hour.From the [Arc FAQ](http://www.paulgraham.com/arcfaq.html):

"If you want us to notify when we release something, send mail to tryarc@paulgraham.com."Like, say, putting the search box on the upper right corner?My aunt Gerald has an orange non-sequitur that she drives to the abattoir every Tuesday.Girlfriend.What's the interpreter written in these days?  At last year's startup school you told me it was Common Lisp, but then you mentioned on USENET that you had switched to Scheme48.  Are you still using that, or have you started doing the "real" version in something like C?[removed]Looks more like a newbie than a spammer to me.All languages in common use today can do most of the tasks you listed. 

In response to COBOL vs Insert Language Here - I would generally not be interested in writing a library if it meant reinventing the wheel. I would just choose a language which had the library already written and publically available - even if it was COBOL. Usually I switch between R (when I need a statistics library), Python (for most other non-trivial libraries), and Lua (when I can get away with it).

The problem I have with Arc then, as you describe it, is that there will be a massive amount of libraries not written for the language.He's [this guy](http://www.codehallow.com/), apparently.It was a joke.  Squint a little and tilt your head 35 degress to the left.  Done correctly, you'll start to get a not entirely pleasant feeling as you realize that allowing programmers to abbreviate commands with a single letter is a particularly Perl-ish thing to do.NEWS RELEASE: Google just announced that it is sharing it’s AdWords program with local individuals who want to earn money from home on their computer.I don't understand the point of this post. First of all, I am unsure that this is what Norvig is really saying (he does say that design patterns can be used to /address/ missing language features, but not all patterns are a result of limitation), but secondly, would you really say that it's any better to make the language so semantically complex that it encapsulates any number of design patterns that I would wish to use? Should every software project somehow be unique in every way?Well, if you still want to use it I know that Coventry is desperately looking for MUMPS programmers right now, and has gone so far as to start assimilating some of the COBOL programmers here in Salt Lake City.  I don't remember the pay rate, but I do know that if you mention that I'm the one who steered you to Coventry that they'll give me a nice bonus on my next paycheck ;)

Personally I'm no fan of MUMPS or COBOL, but hey, whatever turns your crank.[removed][removed]No. First go do something interesting, then quit your J2EE job. IMHO, it really is that simple.In other words, it's a front-end for nethack.

I'd better start saving my money.Yes, but sadly no.See, there you go... ;-)I think it's better outdated.  It gives you an excuse for a low score.
But you have to admit, even though this isn't the true pattern guard, it did go a ways to address the problem, and might be enough in many situations.

I think what really rang my bell when I first saw Liskell was how much the code looked like idiomatic Scheme. Template Haskell and Harp look bolted on. Liskell looks like Scheme.
Bah, it's all about http://mathworld.wolfram.com if you want to waste (really, is it waste?) time on math.Think what you will of the language in question, but the point is spot-on.  One of the differences I notice between good programmers and others is that good programmers recognize when they've done something that's "not quite right", and want to fix it.  The other guys don't notice, and happily go on to fuck up more code.&gt; Is your first name Marla?

Not to the best of my recollection.
via
http://conal-elliott.blogspot.com/2007/02/separating-io-from-logic-example.html
ConalBlog: separating IO from logic -- example
NEWS RELEASE: This is spam, and we don't want it here.[PlanetMath](http://www.planetmath.org) is even better at that, because you can post articles too.Meh.  Unfinished site... No time.Google is going after the same segment with Google Apps.

I'm not counting Microsoft out yet, but the malaise has set in, and the monopoly may be on the way down.(please)I haven't laughed that hard in at least a month.  At least I didn't pee myself -- it might have forced him to up the timetable.[deleted]I think I'd rather write in the blood from my fingers after having my fingernails pried off with a pair of pliers than write anything in COBOL -- but I haven't tried the former, so I'm not sure.[deleted][removed]The point was that the Java server market is very large, probably in the billions of dollars range.

As good as the Lisp Machine and Symbolics tech was, the market was pretty small. The could never get the investment to maintain their margin.

Which features are that?I'm not doubting that specialized hardware has it's work cut out. As you point out it's still within the more general market.

Part of the LM, Symbolics failure was Lisp though. It's a great language (a lot better than Java) but the market was tiny, they never had a chance to make enough profit to turn into investment to keep up with the generics.[removed]&gt; Or is there some subtle and deep property of the placement of operators that I'm missing?

Yes, there is.  Prefix operators, employed consistently in a language, allows for a greater elegance and simplicity to the interconnected rules of syntax within that language.  Precedence confusion, dirty hacks to chain operations, and grouping ambiguities are pretty much impossible with a consistent system of prefix, vs. infix, operators.

Realize that what the aerospace engineer who wrote that really meant, whether it consciously occurred to him or not, was not that prefix operators are more elegant than infix operators: it's that prefix operators *and everything they imply* constitute a more elegant way to do things.

Ultimately, there are two important factors in determining the strength of a language design (as opposed to a language's features, which is another subject entirely): syntax and semantics.  "Semantics" refers basically to the superficial aspects of language design, like "Are we going to call our function constructor fun or defun?"  "Syntax" refers to the form the language takes, the kind of thinking the (successful) programmer will do, and how programs fit together.  That's the deep magic of a given programming language, as manifest in its design.  The choice of prefix or infix operators is a part of that.[removed]So Tron is not Linux?The release of Java 5 hardly required the redesign of anything...It's probably as old as the hills.  Historically, Lisp implementations have used the technique;  not sure what else did, before them.Are you sure?  I seem to recall seeing Ruby methods called on Ruby methods.A few minutes of math and a couple of hours on lesbianism in erotica.Besides eminently sensible reasons like everything is an object, collections have both size and length (Come on Sun please can you make Java arrays and collections work in the same way!), most everything is an object etc.. There is this magnum opus by why.

Like all the best programming books, i.e. Donald Knuth's The art of computer programming, it is unfinished and new sections arrive infrequently (however you can't use it as a doorstop, The art of computer programming is a much better doorstop trust me). It is also possibly more mindbending than those little 'insert name of lisp like language' books but doesn't quite have their level of socratic dialogue or sausages but makes oblique references to their oeuvre in its crunchy bacon sequence. 

When I learnt C in university back in the late eighties we all had a copy of K &amp; R; all novice ruby programmers in the naughties should have a scraggy printout of the poignant guide beside them at all times.Those are wrapper objects, à la Java's reflection API.  (That's why you need the extra `.call` or `[]`.)This site helpfully gives links just shy of 1MB of unreadable markup.  Yay, smart servers?[deleted]Besides the syntax it didn't seem that bad or that worse than other popular languages.

Specificaly 0,1,2,5 are bad. Although precedence is no big deal (I quit wasting my time learning precedence in each language and started using parens long ago)

but, Scope of IF and FOR is "remainder of current line." if I understand it correctly earns the label "write once, maintain never"

The worst thing they did for maint was to put the code in some db.I bet this kind of shit in Java 6 will kill the rest of performance. Great. Makes a perfectly acceptable Linux-PC feel like a bloated OSX shitbox.ARC is a remake of, and comes from, the 1981 Raiders of the Lost Ark movie [ http://www.imdb.com/title/tt0082971/ ], but the spelling is changed to avoid any trademark issues. Paul Graham is Harrison Ford but in programming-land and the Lost Ark (ARC) is the distilled universal truth of programming languages - LISP refactored by god, basically. The Nazis are programmers who use lesser languages and are trying to find the ARC to use for evil purposes (to make their bad languages slightly better) but little do they realize that when they see the pure light of truth of LISP, it will melt the flesh right off their bones. 

The film has had some production problems and was sidetracked by a series of other films where Paul (Jack Ryan) is the owner of a venture capital company that doles out $3000 - $18,000 for a 90% stake in start ups. There is a further distraction from the main film, a "mutual admiration society", "behind the music"-style film called "Look at us, we're rich and famous" http://www.foundersatwork.com/index.html - fanbois and other rich and famous people highly recommend this piece and the narrow columns of text on the propaganda pages used to pimp this work have become a signature of Paul's production company.

Does that help clear things up for you?[removed]I guess I'll find out about that next month.  I'm finishing up a book about another language, and picking up a Ruby book next.

thanks for the response[removed][removed]It's my Lisp in a box! It's my Lisp in a box, girl...I'll only care about Arc if it has a free, stable, complete, cross-platform GUI library (that includes Windows).  Every other language under the sun provides this; heck, even Prolog provides it!  I fail to understand why Lisp is so far behind in this area.

OK, rant over.  I really am interested in seeing Arc again; I just want to know I'll get something out of its use.Befunge!At that time it was fun. I know some people that love things like MUMPS and ditch things like PHP "*because it's always a moving target, you cannot be sure what happens in the next year to the language, since it's open source*".I'll see what I can do. . . .Especially as g has a stem like that (though usually it's shorter than the strawberry's stem)I think you got syntax and semantics mixed upYeah, great news for W3C-fanbois and SemanticWeb-asshats.don't buy a huge expensive house and a new gas guzzling car.  learn to cook and realize how bombarded you are by signals to buy, buy, buy.  get catastrophe insurance and treat exercise and diet as part of your healthcare plan.  to pay the bills, just take a java contract every 3-6 months out of the year.  the rest of the time is yours free and clear to go to school or better yet - just buy the books and find collaborators and mentors online.[There's a Wikipedia entry](http://en.wikipedia.org/wiki/Arc_programming_language)Cool, thanks.Bit, Ben is my dad and Bea my mom :^)Nope.  [Syntax](http://www.bartleby.com/61/8/S0970800.html) relates to how "statements" (typically lines of code, blocks, et cetera) are composed.  [Semantics](http://www.bartleby.com/61/81/S0248100.html) relates to what terms mean.

Examples:

syntax = *foo(bar)* vs. *bar.foo*

semantics = *foo(bar)* vs. *baz(bar)*Dumb server, smart page. AFAICT, you just downloaded *all* of his tiddlywiki in a single 1 MB chunk. I like tiddly, but I'm not sure a 1 MB one is suited to general consumption.&gt; [...] the money and the lack of constraints [...]

As far as i know, the EU funding came with fixed goals and deadlines, not as angel money.[removed]That does not matter if Arc is going to have strings as lists.If you're like raganwald, and like speaking and writing more than actually working or validating the stuff you speak and write about through experiments on someone beside yourself, any language or idea that isn't already saturated by products from people who can speak, write, reason and validate their work better than you, is welcome.

Hence, Ruby on Rails, Extreme Programming, etc.Speaking from experience, there are some python jobs in non-profits / universities / gov't / public sector. Universities in particular offer a place to work with leading edge and/or more interesting stuff without having to work insane hours. I think any decent programmer should be able to find a python job if they look around a bit and are not only motivated by the almighty $ / £Speaking of which, look at the jewel I found today:

        if(strlen($postcode)&gt;5) return "??";                
        if(strlen($postcode)&lt;5) return "??";                        
        $postcode=(integer)$postcode;
        $str=sprintf("%d",$postcode);  
        if(strlen($str)&gt;5) return "??";
        $postcode=sprintf("%05d",$postcode);        

I really like the third strlen... you know, "just in case".

Whenever I finally get the hell out of dodge, thedailywtf is going to have a steady stream of new material:-(AJAX doesn't change a thing, and web MVC and desktop MVC are quite different.

In a desktop GUI app, it makes sense to have controllers for the various parts of the application, so that they can catch different types of user interaction and dispatch appropriately to views.

On the web, not so much. Every "user interaction" is actually an HTTP request to some URL or another, so the "controller" becomes nothing more than a URL dispatcher with a funny name. So why keep it around as such a major part of the framework?

In the print edition of PoEAA Fowler touches on this exact point.An interesting talk, but it seems that as soon as anyone implements anything truly widespread and actually practical, Alan Kay ridicules it.  His dismissal of C++, Java and HTML, for example where absurd.As I recall, you wanted messages to be displayed for anonymous users, and Django's `Message` object can only attach to an authenticated user. I'm not sure how that points to a problem in generic views, or how passing a callable to them would change anything (you do _know_ that, as much as you seem to dislike this, views are Python functions and you can wrap them, right?).

There are a couple tickets out with ideas for implementing a more flexible `Message` object, btw.What? Just because a.foo auto-applies? Try a.method(:foo), there you have your method object.[deleted]Arc could actually appear. It might even be a "practical" Lisp, with cross-platform support for TCP/IP, GUIs, databases, etc. But it could still be cause for great sadness: we might discover (or at least come to believe once and for all) that practicality and ugliness inevitably go together. IMHO, much of the ugliness of existing lisps comes from the inevitable hacks used to harmonize with the decidedly non-lispy outside world: this includes things like optimization for a machine architecture that was designed for C compilers, dealing with HTTP, and so forth.

I predict that the Time of Sadness will *definitely* come, unless Paul Graham has invented a means to fully "lispify" decidedly procedural things like GUI drawing and network access. Monads won't cut it. We need something entirely new and outlandishly elegant.&gt; Non-RIA Web applications are page-based.

Ugh.  The *best* RIA web applications are page-based too.  Whenever I use web applications that try to do away with the page metaphor, I trip over countless kludges where basic features of my web browser simply stop working.  Ever try to open an email in a new tab in GMail?

Why am I not surprised that this guy is a Flash developer?

&gt; There are hundreds of libraries, toolkits, and control sets that give you the impression that AJAX applications are cheap to develop and strategically safe since there’s no vendor lock-in. Actually, there is vendor locking because you won’t manually write JavaScript code and will have to pick an AJAX library of some vendor. Now think about it: starting from the ground up you need a communication layer, messaging and remoting mechanisms, an HTTP sniffer, a library of UI components with shared objects and event models, a visual IDE that understands these components in design time, and a debugger that accommodates all this stuff.

Wow.  What a completely alien mindset.  *I* use a text editor with syntax highlighting.

It's sad that he's so used to needing special software to create his Flash applications that he's become totally convinced that these are necessary tools for web development.  They aren't.  They might be necessary tools if your framework is built on binary blobs and proprietary software, but all you need to create Ajax applications is a text editor.

I could go on, but it's that long that it would take ages to respond to each point.  I'll summarise by saying that he seems to have fixated on Flex as the ideal, anything that deviates from this is considered bad, and he doesn't seem to have experience with *good* JavaScript development practices.  Hint: if you are worried about somebody stealing your business logic by using "view source", you are doing something wrong.&gt; It’s not fair, but he should have seen it coming.

True, but I'm sure he's not all that worried if some guys on the interwebs are carping.Just wanted to announce a new beta service we've released and get feedback.

We released it last fall, got some good early reviews, then, based on that review feedback and user suggestions, recently completed a redesign release.

Please feel free to comment below or email us on our contacts page.Won't work in the USA: suddenly developing an expensive health problem and dying from lack of health insurance will not be fun. Check out the price of an individual policy that actually covers anything.Over at my Mac laptop, lisp in a box. Downstairs in the server room, it's lisp in a box...It was written before the advent of the Web 2.0 Attention Span (tm).This code does not look for an email header. It looks for the start of a message in an mbox file. Besides, I am not sure using a loop would result in cleaner, easier to understand, code.I downloaded their code and found the function in src/com/sun/javaone/mailman/model/MailBox.java.  Here is the complete function:

    private boolean parseToNextMessage(InputByteBuffer buffer) throws IOException {
        while (!buffer.atEnd()) {
            if (buffer.get() == '\r') {
                if (!buffer.atEnd() &amp;&amp; buffer.get() == '\n') {
                    if (!buffer.atEnd() &amp;&amp; buffer.get() == 'F') {
                        if (!buffer.atEnd() &amp;&amp; buffer.get() == 'r') {
                            if (!buffer.atEnd() &amp;&amp; buffer.get() == 'o') {
                                if (!buffer.atEnd() &amp;&amp; buffer.get() == 'm') {
                                    if (!buffer.atEnd() &amp;&amp; buffer.get() == ' ') {
                                        return true;
                                    }
                                }
                            }
                        }
                    }
                }
                buffer.rewind(1);
            }
    //            int size = buffer.get(TMP_BUF, 0, 7);
    //            if (size != 7) {
    //                return false;
    //            }
    //            if (TMP_BUF[0] == '\r' &amp;&amp;
    //                    TMP_BUF[1] == '\n' &amp;&amp;
    //                    TMP_BUF[2] == 'F' &amp;&amp;
    //                    TMP_BUF[3] == 'r' &amp;&amp;
    //                    TMP_BUF[4] == 'o' &amp;&amp;
    //                    TMP_BUF[5] == 'm' &amp;&amp;
    //                    TMP_BUF[6] == ' ') {
    //                return true;
    //            }
    //            buffer.rewind(6);
        }
        return false;
    }

Looks like an infinite loop to me, if '\r' is not followed by '\n'.For the interested: grammars and rules are explained here:

http://dev.perl.org/perl6/doc/design/syn/S05.htmlThey really don't know whether they're talking about choosing one or the other regarding teaching or "industrial" programming. All the arguing about SICP and TAOCP seems to be centered for the teaching crowd, but the style of the arguments often seems to suggest that ML is better for pragmatic reasons.

Sometimes I'd like to see arguments that really go deeply into more "real-world" issues. Implementing something like [Frag](http://www.haskell.org/haskellwiki/Frag) in Erlang/Haskell/ML/Common Lisp and where each language would help or hinder such a project is something I'd like to hear. No actual code required, just laying out the strengths and weaknesses.

Then again, the old adage that functional languages are mostly used to implement compilers for said functional languages still rings true.If I understand correctly, the `buffer.rewind(1)` call undoes the byte read done in the `buffer.get() == '\n'` test, so that parsing resumes on the byte that follows the `\r`.&gt;Besides, I am not sure using a loop would result in cleaner, easier to understand, code.

But I'm sure a simple regular expression would have been.Here is a non-LISP example of the same movement from special purpose hardware to software implementations on general purpose hardware - your cell phone.  Every phone used to be entirely hardwired.  We have seen those hardware features move out of expensive special-purpose hardware into software as the general purpose hardware got cheap.  Now you have phones with an OS (various levels of sophistication), and you can actually program them in software and some of those you can program in Java as well as other languages.  

The market for general purpose hardware is so much larger than for special purpose hardware that the economies of scale and the relentless pressure of shared research relegates special purpose solutions to high-margin niches.  

Azul seems to be gambling that something about a JVM in hardware will beat a software JVM.  Maybe they can find optimizations that will give orders of magnitude performance jumps (which only means about 7 years in hardware performance increases), but I doubt it.  

It is an interesting gamble.A regular expression against what? The whole mbox file? That would be truly bad. Against a buffer? How do you handle buffer boundaries, or the end of the file? Note that the commented code above contains a crude attempt at pattern matching, and the authors abandoned it.

I'm not a regexp guru, just a regular user. Feel free to enlighten me.Here is a non-LISP example of the same movement from special purpose hardware to software implementations on general purpose hardware - your cell phone.  Every phone used to be entirely hardwired.  We have seen those hardware features move out of expensive special-purpose hardware into software as the general purpose hardware got cheap.  Now you have phones with an OS (various levels of sophistication), and you can actually program them in software and some of those you can program in Java as well as other languages.  

The market for general purpose hardware is so much larger than for special purpose hardware that the economies of scale and the relentless pressure of shared research relegates special purpose solutions to high-margin niches.  

Azul seems to be gambling that something about a JVM in hardware will beat a software JVM.  Maybe they can find optimizations that will give orders of magnitude performance jumps (which only means about 7 years in hardware performance increases), but I doubt it.  

It is an interesting gamble."Explained" is perhaps a strong word to use. All I see is line noise, and that's *not* taking into account the usual ugly - er - terseness of Perl code.Yep, I see that now. So there are at least two gets per rewind. However, if '\r' is the last character in the buffer, it only gets one read before the rewind, and will loop forever.while language itself is ancient, the concept of persistent hash-tables (moreover, it's even distributed) is very cool in my opinion. i think it's much better than SQL RDBMS approach, at least for some uses. and why it can't be used just as an interface to RDBMS when there's no need for complex query? like ORM stuff, but less burdened.

it's a pity that these feature is not available in other languages. i've implemented a kind of "library" for C++ that offers MUMPS-like persistent arrays, that are stored in files (for example, db["udo"]["idio"] -&gt; /db/udo/idio.value), but i didn't use it much..

&gt; 0097 Have you solved the halting problem?
&gt; 0098 ... Correctly?

Of course! ;)[deleted]Here is a rewrite. Since InputByteBuffer is a class they wrote themselves, I put skipUntil and match in there. If not, they would be functions.

    private boolean parseToNextMessage(InputByteBuffer buffer) throws IOException {
        while(!buffer.atEnd()) {
          buffer.skipUntil('\r');
          if(buffer.match("\nFrom "))
             return true;
        }
        return false;
    }[deleted]&gt;A regular expression against what? The whole mbox file?

Per line.  See BufferedReader readLine()

&gt;I'm not a regexp guru, just a regular user. Feel free to enlighten me.

^From\sI do agree with you, mobile phones are a great example.

[Yes, just what we need. Another mail and calendar server.](http://www.hula-project.org/Hula_Project)Visual Basic is widespread and practical, is it absurd to ridicule it ?I notice that most of his digs against C++/Java aren't much appreciated by the audience. Smalltalk's real-world successes are not great:

http://c2.com/cgi/wiki?CthreeProjectTerminated

...C++ and Java have a pretty impressive record. (Well, in 1997 not so much Java)This is not a BufferedReader. InputByteBuffer is their own class. Though of course they might have implemented readLine themselves and run a regexp on that. Their way is very likely much faster though, and I believe that's the whole point of their custom-made buffer implementation.Incorrect. ;)

Matching the `\r\n` is very much part of finding the start of a message.any chance you'd publish the snippet so we can see what the current state of syntax is?How many sources are you tracking?

I did a search for "Iraq" and got 47 sites (I would have expected more).Configure Multiple LDAPs[deleted]&gt; Try a.method(:foo), there you have your method object.

No, that's a (non-first-class) wrapper object.

The difference between it and first-class method objects is the difference between (for example) the Java/C# reflection APIs\[1\] and Python/Dylan/CLOS methods.

\[1\] `java.lang.reflect.Method`, `System.Reflection.MethodInfo`If you're not using assembly, you're not a real man.Cool. :)

But isn't it just moving code around? How would you implement buffer.match()?1) Cut a hole in a box
2) Put your Lisp in that box
3) Let her open the box

It's my Lisp in a box!huh????Ok, so i like the fact than in Ruby some methods have two versions, one that returns a copy of the object you called it with and one that actually modifies it (usually ends in '!', as in

s=s.chomp

vs

s.chomp!

but how would i define a method myself that will do this 'in-place' modification?[deleted]With a loop, something like this:

    public boolean match(String chars) {
        for(int i=0;i&lt;chars.length() &amp;&amp; !buffer.atEnd(); ++i) {
            if(buffer.get() != chars[i]) {
                buffer.rewind(i+1);
                return false;
            }
        }
        return true;
    }No fair counting sh, bash, and zsh separately.Lisp has a very good type system.

Qi has an even more advanced type system that can create types via logical deduction.He's talking about the type system. Type checking in Qi is turing complete. The type checker can perform arbitrary computations during typing.[deleted]Yeah, pub picked a bad title.

Qi's type system is turing complete.The title is misleading.

The TYPE SYSTEM of Qi is turing complete. Any Lisp in general is turing complete.You definitely got the idea.

&gt;when I finally understood that the half page of code on the bottom of page 13 of the Lisp 1.5 manual was Lisp in itself. These were “Maxwell’s Equations of Software!” This is the whole world of programming in a few lines that I can put my hand over.

-- Alan Kay

[source](http://acmqueue.com/modules.php?name=Content&amp;pa=showpage&amp;pid=273&amp;page=4)Haskell also has a fast, native DFA regex implementation.  It supports captures!  See the announcement at  http://www.mail-archive.com/glasgow-haskell-users@haskell.org/msg11442.html .  I think this is now in the libraries distributed with GHC 6.6 as Text.Regex.Posix.
Catastrophic health insurance is about $265/month (for a healthy 24 year old).  It's not cheap, but it's easily affordable for a contract developer.Well, it is fair, because the program has different behaviour uder each shell.Wow. A stereotypical blub programmer, in action.I would prefer semantics, constructs, paradigms and new ideas that are in the language. It easy to change surface syntax, rip off features and rearrange things little. As I understand Arc is going to be extremely compact Lisp implementation. Putting something really new into something small is going to be truly hard.Interesting that sizeof '2' returns 4 in C and 1 in C++.&gt;I am not sure using a loop would result in cleaner, easier to understand, code

It's more a question of aesthetics ... and dignity.Did you follow up on it?A good way to test the language is to write the "real" version in the language itself. Of course, this would have to be a compiler instead of an interpreter, but the extra work is well worth it.Good god, c# is ugly (says a C# coder who voted it up).You really don't get it, do you?

The law says very little to nothing about the semantic web (which is something entirely different from forcing developers to use meaningful elements for content in markup), but it forces authors to make sites that have a decent and predictable behavior when assistive devices are used.

&amp;lt;font size="+7"&amp;gt;This pretends to be a heading&amp;lt;/font&amp;gt;

has a different meaning, and a different presentation with assistive devices than:

&amp;lt;h1&amp;gt;This is a heading&amp;lt;/h1&amp;gt;Well, the GNU project is already started.  It's called Narc - 'Not Arc, Really C'Yes! Persistent hashes ("globals") in MUMPS are the most notable distinguishing strength of the language. Surprisingly, I haven't seen them in any other language. It's a sign to me that very few language designers have actually experienced this paradigm.

Syntactically, globals would fit easily into almost any conventional programming language. In MUMPS, the only syntactical difference between a global and any other variable is that its name begins with a caret ("^").

Though an RDBMS works well as an embedded engine for storing and retrieving MUMPS-like persistent hashes, the hashes are by no means a replacement for a conventional SQL RDBMS. Why? Because a rich/capable query language like SQL tends to be easily confused by the asymmetry of MUMPS-like persistent hashes. For example, in the world of globals, not every 'customer' element has to have a 'name' sub-element (read: field, column, attribute...). That flexibility to be sparsely populated and asymmetrical is a strength, but not if you want to use a generic querying engine to pull stuff out; a query language will tend to degrade into a more conventional language when forced to contend with a non-tabular shaped data store.

Touche to you for your stab at a c++ implementation. Unfortunately, a MUMPS-like persistent store is *very* easy/convenient to use but quite a challenge to build well (and hide).What i'd like to know is what will set Arc apart from the established Schemelike Lisps.Link?Godwin's Law of programming forums[^W^Wreddit]:  As online discussion grows longer, the probability of a comparison involving stereotypical Blub programmers approaches one.&gt; There is talk about adding one opcode (for method dispatch IIRC) specifically for dynamic languages to the next major version of the JVM.

My guess would be that adding [invokedynamic](http://blogs.sun.com/gbracha/entry/invokedynamic) to the JVM, plus the kinds of runtime type feedback developed for languages like [Self](http://www.cs.ucsb.edu/~urs/oocsb/papers/type-feedback.html)  would allow JRuby and Jython to run pretty fast when JIT'ed.Real programs don't have GUIs.I'm not so sure. If `\r` is the last character in the buffer, there will be a rewind, and then the while() condition will not be met, and the loop will be over.of course it's easy to do that, but PG has stated that he might provide some sort of syntax as long as it is similar enough in nature that it can be converted to s-exprs. so, by asking for a snippet, we get to see what he has in mind with this (assuming he's using it of course). [http://www.paulgraham.com/ilc03.html](http://www.paulgraham.com/ilc03.html) shows us some examples, of arc code, but it's not a full blown usable program nor is there any of this magic "s-expr" stuff i've read about. btw: i'm not complaining about s-exprs. 

*edit*: yes, those snippets on that page are *real* examples, but they are only language building blocks, not an *application* of them in use.So you think buffer.atEnd() will return true *after* the buffer is rewound?

If you're right, I think that's a wtf too.It works better if we have meta-info for files. Title of file? "2001: A Space Odyssey". Filename? "1239812391823.txt"If one were writing a "100 year language", would you really target your GUI library to a platform that can only decline in usefulness?  Every interesting GUI app in the foreseeable future is going to be written for the One True Platform (the web, of course, where operating systems don't matter).

Sure, you still have a quagmire of incompatibilities across browsers, but at least they're similar and speaking the same language.  You don't have COM, GTK, KDE, Cocoa, etc. all using utterly different models...just JavaScript, the DOM, and CSS (all of which are roughly friendly to each other).

A Windows (or any other OS) GUI lib would be a huge time sink for a rather dubious benefit.So, a guy points out some obviously hideous code -- regardless of what it does -- and the Java apologists come out yelling that this is the best way to do things?

You can't win.  If this is truly the best way to do things, then Java has serious problems.
In the case of Lisp, that's a one-liner :-).  But yeah, been there, done that with the parser-generator I wrote.Oh, math with an A... I imagined an E there.Err no, I wasn't thinking clearly.Yeah, sort of.  But you could equivalently #ifdef GCC, etc, and be more specific about your C compiler.  csh would at least count as a different language.

I still think the eight-language [polyglot](http://ideology.com.au/polyglot/) is a bit more impressive - it has fewer "languages" but the languages themselves are a lot more diverse.

Anyway, nevermind my sour grapes.It appears to be fixed now.[removed]&gt;Matching the \r\n is very much part of finding the start of a message.

Again, see BufferedReader readLine():

"Read a line of text. A line is considered to be terminated by any one of a line feed ('\n'), a carriage return ('\r'), 
or a carriage return followed immediately by a linefeed ('\r\n')."something similar (sparsely populated attributes) can be found nowadays in 'triple stores' -- RDF databases, or whatever they are called (certainly is an old concept, now promoted as a part of Semantic Web).

normally SPARQL is used, that is not very convenient, but it's possible to make wrappers..

a shameless self-plug: http://abcl-web.sourceforge.net/rdf.html

it's SPARQL wrappers for Common Lisp which can make queries as easy as using hash-tables or CLOS. at same time it's also possible to do more complex queries..&gt;Their way is very likely much faster though

I have to wonder if the performance gain, which has to be very little, is really worth all the hackish code.  If we dropped using regular expression (which is slow) and used String startsWith("From ") with the BufferedReader, I don't think we would see much of a performance difference at all.  In fact, I would guess this later implementation would actually be faster.IronPython (Python on .Net VM) runs faster than CPython in some benchmarks.

Granted, those are exactly the benchmarks that de-emphasize dynamic coding styles.  The coding style that runs fastest on IronPython is not really Python.

There is more interest now in having the .Net and Java virtual machines work better with more dynamic languages.  So performance will only improve.

For Ruby, using the JVM is not a slam-dunk guarantee for better performance, but I think they have a better chance of success than you imply.

&gt; Ruby grew with its C implementation

Granted.  Obviously, Python grew with its C implementation.  But Ruby is more dynamic than Python, so this is a good point.

&gt; C implementation can use low-level tricks that don't translate to Java

Yes, but a particular JVM targeting a particular processor architecture can use assembler tricks, and JIT compiler techniques.  With JIT, longer running processes would benefit greatly, with shorter running processes suffering.

&gt; mismatch between Java and Ruby semantics

&gt; dynamism that probably destroys many JVM optimizations

Granted.  The coding style that will end up running fastest on JRuby will probably not really be Ruby coding style.Only on reddit. Nowhere else exists such a collective graham crush.Mac's real name was Timothy Hart.
Ok. All my comments up to this point were written under the false assumption that one should look for a *blank line followed by a start-of-line From_* when looking for messages in an mbox file. However, experts say it is safer to just look for a *start-of-line From_*, in which case your approach is the clearest and the simplest for the programmer, though not necessarily the fastest (which matters in this case, too).[deleted]Well we confronted them.  We did what we could.Who "yelled" this is the "best way to do things"? You, sir, wear strange googles.Is there a file browser somewhere that lets you list files by title rather than filename?[removed]It's from actually researching policies when I was deciding whether to start a business straight out of college or get a job.  Quotes depend on specific demographics, but Google for "catastrophic health insurance" and there'll be Google Ads for all sorts of quote services.  Unfortunately, many of them require personal information that I'm not willing to give until I'm actually looking.

The links I've been able to find have been even more affordable:

* [$29/month](http://www.insurance.com/Article.aspx/Understanding_Catastrophic_Health_Insurance_/artid/43) for 29 year old female.
* [$29/month](http://info.insure.com/health/catastrophic.htm) for 21 year old female

(And actually, I may be misremembering my own quote on the high side.  It may've been as low as about $120/month.  $29/month seems absurd though...I wish I were female.)[removed]You sir,

&gt; Besides, I am not sure using a loop would result in cleaner, easier to understand, code. What do you propose, Mr. Feeling-Kind-Of-Shitty Man?

and someone else,

&gt; The code does indeed look awfully repetitive, but I thought for a moment, and without allocating Strings/StringBuffers/StringBuilders, I think that the code I’d refactor it to might actually be harder to understand.

\+ a little bit of hyperbole
Hmm, the thought of a moving curve is definitely an interesting proposition.  Either way, having rethought the pareto-curve, while I think it to be true, I think it is probably a much higher-dimensional pareto-surface with many different axes.  As such, I do not know the axes, and trying to map this higher-dimensional curve to a 2-dimensional curve would be very subjective and highly inflamatory :)  I guess this is the reason we have so many flamewars, everyone has a different many-D -&gt; 2D mapping and presumes it's the same for everyone else.

The exception to that rule: very smart PhDs from smaller schools tend to use those boards for their first job when there's literally nothing available in their hometown. So if you're brilliant and willing to move to New York/Silicon Valley/Chicago to write Java or C++ for a hedge fund, they're a good deal.

Otherwise, stay the hell away.I think it's wanker-esque to pick apart people's code like this.

Sometimes you just slap something together based on assumptions you know are true. It works. It's fast. Move on.

When faced with a problem you just need to solve and move on, it's stupid to be over-obsessive about aesthetics.Great. Now, let's see the Perl community written in the Perl community. Better yet, Larry Wall written in Larry Wall!An imperative hamming solution is actually quite short:

    hammings = [1]
    ms       = {2 =&gt; 0, 3 =&gt; 0, 5 =&gt; 0}

    20.times do 
      n,m = ms.map{|m,i| [hammings[i]*m,m]}.min
      hammings &lt;&lt; n if hammings.last != n
      ms[m] += 1
    end

    hammings # =&gt; [1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 15, 16, 18, 20, 24][removed]Yep. This is the kind of code I see every (ok not every) day from people working in Accenture. It's consultant code. You write it, if it works once, you ship it. No big code reviews.  You never see the code again. Maintaining code is not a problem. Custormers have to pay for it. You work in next customer project already.No. what you say is true to a certain extent, but if you find yourself writing 6 nested ANYTHING you have to rethink.In your original post, i think you got them the other way around, though.

Syntax is form, or _"the superficial aspects of language design, like 'Are we going to call our function constructor fun or defun?'"_.  Semantics is meaning, or _"the deep magic of a given programming language, as manifest in its design"_.

The choice of prefix, infix, or postfix syntax is ultimately a surface issue, as demonstrated by the various infix syntaxes for Scheme, and Haskell's support for switching arbitrarily between prefix and infix application.Sun's Java implementation is full of terrible code.

For example, there are actually two regex implementations, and several ad-hoc bytecode generation frameworks. Plenty of copy and paste coding.Depends of your object, if the object is immutable you can't and you always have to return an altered copy of the object (the non-destructive version), if the object is mutable you just change what you're supposed to change.

For example, using Ruby's strings (which are mutable)

Let's assume that we don't have access to any Ruby String method but [] and []=.

    class String
        def my_chomp!(separator=nil)
            separator = $/ if separator.nil?
            if self[-1] == separator[0]
                self[-1] = "" 
                self
            else
                nil
            end 
        end
    end
No its not "wanker-esque". Sometimes you have to cut through the bullshit and marketing and show people how can Sun's Java implementation really is. Anybody who thinks Java is suitable for anything appropaching "mission-critical" is a fool.Writing a JIT compiler or a self hosting language is not hard. The reason PyPy failed is because Python is too complex and poorly specified.

I think Rubinius holds a lot more promise than YARV or JRuby.Does ANYBODY REALLY care about this bullshit?Now imagine writing a syntax highlighter, auto-indenter, code completion or refactoring against this grammar. Or even try to hold it in your head.Java may be retarded, but I don't think I would go so far as to accuse it of being a cult. Programming in Java is like eating at McDonald's: everybody does it, they think it's normal, it feels good to them, and it's incredibly bad for them. You wouldn't say that people who eat at McDonald's are one big cult, would you? 

(I might accuse Agile of being a cult, akin to Scientology, though.)"Rich List". LOL. Why is everything "rich"? Don't say Application, say Rich Internet Application. Don't say List, say Rich List. Don't say Piece of Shit, say Value-Added Rich B2B Integration SOA.

Ugly application BTW. Total waste of cycles with no usability gain whatsoever.That's a bit misleading in my opinion. Ruby makes a difference between function/method names and variable names. If f is a function name, then writing f will call that function. You have to do extra work to get the actual function instead of calling it (&amp;:f if I remember correctly). On the other hand, if x is a variable name than writing x will only return that variable, and you need write x.call to call the function contained in x. The rule is a bit confusing, but internally consistent (function names always apply, variable names never apply).

In Python or Javascript, there's no distinction between variable names and function names. Thus, f will always return whats contained in f, and writing f() will call f. This means that parentheses are required for function calls -- in Ruby they are optional.

I like the Python and Javascript syntax better -- it's easier conceptually and you don't have to jump through hoops to do any higher-order stuff. But then I never liked Perl's bracket-less function calls anyway. In terms of power or semantics, there's no real difference.

BTW, the downmod was not by me.Yeah but these technologies are hardly state of the art, nor do they harness the full potential of computers.[removed]How is this different from FreeBSD, where you become root before running pkg_add? Or Redhat, where you need to be root to install RPMs? Apt-get? Right, same thing.

When you install software as root under Linux, it's A-OK. But when you do it as an administrator under Vista it suddenly isn't?

Sniff sniff. What do I smell?This is a pretty ironic statement.  Ruby is at least as complex and poorly specified as Python is.

I also think rubinius, JRuby, and YARV all hold a lot of promise.  I'm most interested in rubinius, but I don't think it's the only horse in the race.Note that the referenced article's method (Thompson NFA) apparently doesn't work for backreferences, one of PCRE's big "selling" points.That should be c++ seems reddit didn't like the + characters.&gt; Sniff sniff. What do I smell?

I'm to polite to say, actually. You do realize that in OS X people have dozens of applications installed and few (if any) are installed as root, right?

They are in the *user's* application directory -- installed by the *user*. This is much different than switching to root and installing with the privileges thereof -- which is the default in Vista.

See?&gt; When you install software as root under Linux, it's A-OK. But when you do it as an administrator under Vista it suddenly isn't?

You are confusing the functionality of Windows Update with Joe-user downloading and installing fungame.exe. The former is similar to updating a *nix system using root privileges, the latter is not.i mean honestly... the only dick you guys suck more than steve jobs is paul graham.

So insteaed of Jobcock, we have Grahamcock... we haven't seen any Kawasakicocksucking in a while.Properly written Windows applications don't _require_ elevated privileges either. If applications don't muck too much in the registry and store their files in \Documents and Settings\\[username]\Application Data\ all is fine.

Some applications want to store stuff in \windows\system. That requires admin rights. Or an application may want to install files in /usr/local/etc/ or modify /etc/rc.conf, and then you've gotta be root.

Under linux, at least 90% of all applications you install require root. Under freebsd it's 100%. And freebsd and linux are considered relatively secure despite of it.

Calling Vista security a joke because it needs to maintain backward compatability (and thus permit elevated privileges for installers) seems a tad hypocritical.[deleted]How exactly is Python "unsafe"? It is as pointer-safe as any other high level language in its class, such as Ruby, Java, Lisp, etc.Which is why you don't need to be administrator just to run fungame.exe. You can run it as a limited user - like in Windows XP.This story omits the fact that in the pre-macro stage, Lisp programmers still had FEXPR's, which give the same kind of syntax abstraction ability, but in a way that is much less friendly to the compiler.

FEXPR's are described in [The Lisp 1.5 Programmer's Manual](http://community.computerhistory.org/scc/projects/LISP/book/LISP%201.5%20Programmers%20Manual.pdf) and on [Ward's Wiki](http://c2.com/cgi/wiki?LispOnePointFive).

The easiest way to play with FEXPR's is to run L. Peter Deutsch's Basic PDP-1 Lisp (available for [the simh emulator](http://simh.trailing-edge.com/) from the [software kits page](http://simh.trailing-edge.com/software.html), documented in [The Programming Language Lisp: Its Operation and Applications](http://community.computerhistory.org/scc/projects/LISP/book/III_LispBook_Apr66.pdf)).

For the historically minded, it's fun to read [the original macro paper](ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-057.pdf) and [the paper that describes why macros beat out FEXPR's](http://www.nhplace.com/kent/Papers/Special-Forms.html).Why Blame Vista? I have seen many people install Linux and happily use root accounts to install everrthing that they need.Just what we needed, a naming conflict between a parser-generator and a binary data exchange format.
Character constants are ints in C, char in C++. This is not new and it's needed for function overloading.http://www.google.com/search?hl=en&amp;q=%22sudo+make+install%22&amp;btnG=Google+Search

http://www.google.com/search?hl=en&amp;q=synaptic+package+manager+sudo&amp;btnG=Search
[removed]So where was the simple question?[deleted]&gt; You are not authorised to view this resource

Great tutorial! Anyway, if you want the greatest free book on C++, try this instead: [Thinking in C++](http://www.mindview.net/Books/TICPP/ThinkingInCPP2e.html)ok, thanks.

sorry if this is a dumb question (i have googled!) but...

does this mean that if i want to make a method that does this to strings i have to create a subclass for string?

I can't just have a global method and call it like

my_chomp!(s)

?
[deleted]FUD? Just because I posted some of the difficulties the Rubinius team faces doesn't mean that I think its a bad project or it doesn't have hope. As I said, I'm pretty interested in it myself.

Really, it wasn't a personal statement on the abilities of Rubinius team or anyone, its just a difficult project. That's all.not to mention a sprintf that doesn't return length?  what were they thinking?To use the package managers under Linux and FreeBSD requires root (normally), but to install an application from a source tarball or otherwise, almost *never* requires root. Any program that uses autoconfig (that is, you use ./configure to set it up before compiling) allows you to change the install prefix. To /usr, to /usr/local, or, yes, to /home/fry/.

In addition, my understanding of source article, [Running Vista Every Day!](http://theinvisiblethings.blogspot.com/2007/02/running-vista-every-day.html), is that even if you're running as a normal, privileges restricted user, Vista will *detect* that you're running an installation program and say "You're installing a program. You can either run it as Administrator or not run it.", more or less. I don't use Vista, so I can't confirm that.Thank you for your insightful comment.The current version is not an interpreter.  It compiles Arc
programs into Scheme programs.  Currently we use MZScheme.Any such tool which can take any formally specified grammar could take this one (subject to machine translation if necessary).

Writing syntax highlighters in terms of regexes is always going to suck[*], ideally your editor should do the same parse as  the language. And to do that you need...a grammer.

[*] - ok, lisp and scheme perhaps get a let-out here, but even then you don't want regexes, since you can't match parens.Emacs isn't a real program?Going along with the examples given in that link, I would love to see a Microformat that would enable a browser to know how to search a given web site.  (ie to move the functionality of something like the reddit searchbox up into the generic firefox search bar)As I understand it, JRuby doesn't *yet* compile Ruby down to Java bytecodes - it's a Ruby interpreter written in Java.  Hence I interpreted he "wishful thinking" as referring to the first part, not the second.

My understanding could be several months out of date, though.  I know that bytecode compilation was on the roadmap the last time I looked at JRuby, and they've made significant progress since then.Well, no one's going to have a choice about using YARV or not.  If you're using 1.9, you'll be using YARV.Has anyone asked yet, "why is this code so hard to reason about and why do I keep making mistakes?" 

(hint: loop, destructive update)Listen - sweetie - when you think you're gettin a come-on *even when you're told you're not*, you really need to seek help before there's a nasty accident. Go to it. Please.

PS. This is not a come-on.*little bit* is quite an understatement.&gt; How many programming languages of 12 years ago do you keep using?

Me, personally? C, C++, Lisp, Haskell, Python. For other people, add Fortran, Basic, Smalltalk, Scheme, Eiffel, Ruby, PHP, etc. Nothing special about Object Pascal (Delphi's underlying language) in that respect.NO SHIT! That's what THREE FUCKING QUARTERS of the article was about! Did you read the article? Did you read Slava's insane post? *They have nothing to do with one another.* I thought this was obvious. Since it is not, let me go into furthur detail.

&gt; So Ruby is going to get some lovely Java-style UTF16 strings where some code points use one slot in a char[] array and others use 2 slots?

That is what that UTF-16 is. That is what it does. That is how it works. A code point can be 1 or 2 16-bit words in length in UTF-16 the standard. I should hope that's how Ruby's UTF-16 is going to work, because THAT IS THE FUCKING STANDARD. 

&gt; And remember Java only has braindead strings like this because the designers went with fixed-width integer types and were too short-sighted to ponder that the Unicode space might expand past 16 bits.

Which is covered by they fact that code points can be either 16 or 32 bits in the UTF-16 standard. Expanding the underlying chars to 32 bits would turn the encoding into UTF-32. None of the underlying implementation should matter, anyhow, as the overwhelming majority of string operations should be happening at the more abstract character sequence level, which is exactly what Matz said he was going to do in the article.

&gt; JRuby is ruining Ruby.

Insane

&gt; The removal of continuations

Continuations were dumped for the time being in the mainline C ruby implementation on account of being a pain in the ass to implement. JRuby had nothing to do with this. You are insane.

&gt; and the addition of variable-width code points just to please the Java monkeys is not progress.

Variable width code points **ARE HOW UTF-8 AND UTF-16 WORK**. You are insane.

&gt; They need native compilation,

No, they don't.

&gt; better GC,

Duh. All GCed language can use that.

&gt; a cleaner implementation,

Duh. All software projects can use that.

&gt; and real Unicode support. They do not need to slavishly ape Java with its oh so enterprise design flaws and moronic community.

You don't even know what Unicode is.The battle between gui, cli, and search is a none issue to me, because I have made an advance keyboard that enables/empowers the user to point, click, type, and scroll in any order simultaneously and instantly without taking your fingers off the home row.  With my keyboard it is possible to design advanced interfaces that use gui, cli, and search all in one interface that is customizable for each user. From my research, development, and prototyping for new keyboards and papers for my PhD requirement in hci, we all work differently and we should not be locked in or constrained to one way of working.  The interfaces today are still designed around the stand alone keyboard and mouse, this is old technology.  Advanced keyboards and interfaces are on there way.  

from the ”father of the perfect keyboard”
Not according to her research; the very point of what she's saying is that when Vista detects an installer it wants to either install as an administrator or not install it at all.There's no magical function or operator that does 'in-place' modification vs. safe modification.  It's just a general strategy.  Here's what I do sometimes:

    def my_method!
      # -- insert your own code to do things to this object
    end

    def my_method
      #  -- insert your own code to create a copy of this  object

      copy.my_method!
    end

On a side note, it's super-obnoxious that the base libraries of Ruby don't mark every destructive method with '!' (e.g., my_array.pop, my_array.shift)  

Consistency is an aspect of beauty too.
[removed]Google Apps probably won't allow a company to set its own policies about how email gets handled from its users and so forth.  I really think Google is going after the personal use segment of that market, not the organizational use.

The Exchange product has lots of features and is easy to administer (though, when it goes wrong, it seems to go ape-shit-crazy-end-of-the-world-call-$400-an-hour-tech-support wrong).  It'll be a hard product to try to compete against.&gt;Which is why you don't need to be administrator just to run
&gt;fungame.exe.

According to the Polish researcher in the article, Vista detects that
fungame.exe is an installer and forces you to run it as administrator
or not at all.  That is, in fact, the entire point that's being made.

Looking at your other replies, you seem to have completely
misunderstood what this "feature" is preventing:  The running of an
installer (not Windows Update ~= APT, and not the program itself) with
privileges *in between* those of a normal user and the administrator
(~= root).

This is something that can be done in XP but is (according to the
article) impossible on Vista.  I repeat, this is possible in XP
but not in Vista.  (It's possible on other OSs as well, but there
you usually do something completely different: run the install with
normal privileges and install it in your home directory (~= Documents
and Settings\username) so no extra privileges are required.)
next if s/^ $ws //;   # reward consistent tabbing

I smell a Python jokeYou can do global methods too*.  It's just hard to program well when there's no structure.

For example, you can paste this into irb:

    def my_chomp!(s)
      s.chomp!
    end

    a = "Hey!\n"
    # =&gt; "Hey!\n"

    my_chomp!(a)   
    # =&gt; "Hey!"

* Although actually, there is no pure global environment.  Everything is an object, so defining my_method without wrapping it in a class, means that my_method exists as a method of the top-level object ('Object'?)
Come on, now.  I was trying to say something ballsy.  Don't bring reality into it.Did you see the first comment? SPAM: Solaris-PAMFor the record, I do think slava has some emotional issues that is causing him to be a jerk; so techinically he is insane.

But on the other hand I don't think it is fair to say he is insane unless you also point out why he is insane. Otherwise people might think you just have a grudge, or even worse, they might start believing his illogical rantings.
Hehe, my bad. I take it back ;)And I've seen OpenBSD have a root compromise too, it doesn't mean it's the norm. The point here is the way in which a normal user is likely to use the system -- in this case, Vista.Most windows applications are made with no or little thought of security, these applications require administration privilege to install. A person on slashdot has postulated that the idea of UAC is to annoy users of badly designed application installers such that the application developers will design them to require only the necessary privileges. I find the thought plausible.

[The entire post is:](http://it.slashdot.org/comments.pl?sid=222252&amp;cid=18004416)

&gt;Everyone who complains that UAC is annoying doesn't understand that the purpose of UAC is to be annoying. UAC makes elevation a pain, in the hope that software creators will write software which doesn't need to elevate!

&gt;VMWare 6, for example, constantly elevates on Vista. What do you want to bet that VMWare 7 won't?

&gt;Well behaved programs elevate only when and where they have to. Even if 50% of Vista users turn UAC off, that's still 50% of your client base who is being constantly bombarded by elevation dialogs. The solution? Write your software so it doesn't need to elevate.

&gt;As for the article - installers pretty much have to elevate. This is true on Windows and with Linux packages (when was the last time you ran apt-get without using sudo or running as root?). Some have pointed out that you can install most packages in Linux to be specific to your user account, using special flags. This, of course, is possible in Vista as well, if MSI packages are used.

&gt;Note that I do agree that it's a problem that you can't override UAC detection. There needs to be a "don't run as administrator" option.
'Blub programmers' only testably exist when they emit assertions about their language-of-choice as compared to other languages, and not when 'in' any kind of 'action'.  If you only wanted to say 'eww, Perl &lt;sneer&gt;', then do that.  Don't try and obscure simple distaste with words you don't understand.gmail for domains doesn't even have a directory (LDAP or otherwise).  maybe you should actually try the service before making predictions about its successarticle was updated. he heard back from him.[removed][removed]The web? The web (http+html) is just a really, really limited cross-platform GUI.. accessible over the net.

At the risk of sounding trollish: Wake me when http+html has been replaced with a stateful real-time GUI. And no, AJAX is not a very good replacement for 'local' GUI's.No, I get the same behavior.  It appears to be by design: you download this person's entire wiki, and then the page uses javascript to reformat things for readability.  However, none of my browsers (lynx, links, w3) have javascript enough for this.The difference is that apt, portage,... are not executing arbitary code as root/admin, they are executing their own code, from their own repositories to install third party software. If you don't see the difference to executing an arbitary executable just because some heuristic (probably just something like "is called setup.exe") says it is an installer you really should keep out of discussions about security.&gt; Every "user interaction" is actually an HTTP request to some URL or another, so the "controller" becomes nothing more than a URL dispatcher with a funny name.

My point about AJAX was that, by reducing round-trip times and allowing a single page subcomponent to send a request, it moves web programming closer to the dispatch model of gui -- the granularity of interaction has been made finer. If you want to ditch the controller nomenclature, I am all for it. How about Model-View-Business Logic? We could also co-opt the "[Model 2](http://en.wikipedia.org/wiki/Model_2)" term from Java. I don't think Model-View-Template makes much sense, though.herein is the rub:

Most of the time *NIX software is distributed using tarballs or debs or whathaveyou, and to install you run one of the following programs:

tar
gzip
dpkg
rpm
et al

In case of Vista (if I understand correctly), arbitrary executable code which presumably sets the bit "installer = true" asks for root access or just doesn't run.

If Vista only enabled such a feature for say, .MSIs, which (in my understanding) similar to deb, rpm, et al in that they do not contain executable code but are installed by a yet another program that everyone has to trust (like tar or dpkg) then the two would be equivalent. As this article would suggest that is not the case.He can be an ass all he wants in my book as long as he remains correct while doing so. My insanity claim was based on his statements not making any sense, which I thought was obvious, which is why I didn't quote my sources. Burden of proof and all that. Oh well.# passwd *username*Regexp-based syntax highlighting does indeed also suck for Lisp, at least the one in GNU Emacs. It is especially brittle when used with escaped symbols and strings containing Lisp code.&gt; In the case of Lisp, that's a one-liner :-).

A very, very long line? :)Up.&gt; Emacs isn't a real program?

Emacs is not a Real Program.PyPy failed?  They must have missed the memo.Hush now, you infidel! Everybody knows Java is the most readable language on Earth. Gosling Himself said so.rpm, dpkg, pkg_add do not run (possibly hostile) code directly, but they happily set the setuid root bit or they create an rc file so the program will run at the next boot with system privileges.

Installing an evil package is inherently unsafe, even if you never explicitly start the executables contained within. I think there's no meaningful difference between running arbitrary code as admin/root and installing arbitrary software that will run as root/admin without your explicit permission.The original article suggests that PyPy set out to create an implementation of Python which was simpler to work on than CPython, and asserts that this is not the case and in fact PyPy is a lot more complex. So in that regard, it failed to achieve its goal.Arc has this distinctive feature that it does not exist.Despite your attitude, you're not right. Just installing an arbitrary package can compromise the security of your system, on both worlds, whether you like it or not.

There is no difference.I've worked on several 24/6 banking systems written in Java. That's reasonably mission-critical. These operations tend towards patched versions of older JDKs.

There's no way these development efforts would be able to capture the market if they were using a less mainstream language like ... well I have no idea what you think is better than Java for mission-critical stuff. Maybe Erlang - good luck finding programmers, particularly with the right business experience.

Java is the sweet spot for big business.

Also, Swing is nearly a fringe of java. I've been developing in java for five years and basically not touched it. If I was going to write a GUI tool I'd use Cocoa but whenever I sit down to do app development at home I end up writing either a command-line tool or a webapp - these days in python.
Copy-and-paste is a characteristic of lots of java code I've seen. I think this points to a weakness in the language - it's often difficult to abstract problems in the way you'd want to - no currying, functions are not first-class.

Perhaps it makes for more readable code - I find algorithmic depth to be an obstacle to reading code. Thick layers of C-macros have the same effect as meta-meta-meta functions in a high-level language.

But my feeling is that while such structures should be used in reserve, they are necessary. I've had to give up on java  in my own projects because I'm dissatisfied with it not having ways to easily use several powerful programming ideas.
24/6?[deleted]One of the problems I've found with Linux as a noob is that much of the time, I'm copy-pasting commands from websites without knowing what they are doing.  I bet that's very unsecure, as I run the risk of copying a malicious command line from a "helpful" site.C macros are pretty crap. They are not aware of C semantics, and all they do is essentially shift complexity and rename. I think abstractions such as OOP and higher order functions are different; they actually simplify code.Er - actually - yes. If you're lucky. And in one organisation I was the poor bastard who had to support that operation each week.

But the real reason for 24/6 is that that's a global trading week.

Everybody on the team takes a deep breath when Aukland starts trading on Monday morning and then tries not to exhale until well after New York has closed on Friday. You get a bit over a 'day' until the cycle restarts.

--

Update: You editing out the cool thing you'd written in your comment - please put it back - it was insightful if only by accident! :)
I would never endorse shipping code that would possibly break under some edge case. I'm just saying it's ok to ship something that WORKS although it may be ugly. For me, this is only acceptable for code that doesn't effect the overall design/aesthetic of the system you're building (it should be totally invisible to the people that will be USING the software). In other words, you should be able to go back an optimize it without having to do any major refactoring.Reminds me of the nuclear meltdown episode of the Simpsons.  Homer is working remotely from home due to weight gain, and he's typing "YES" to all prompts by the computer (increased his productivity 300% by typing "Y")

Vista security is just training people to press yes, bypass, ok, allow - without even reading.  There's got to be a better way.The "cool thing" was a quip about restarting the app server once a week, and taking all day to startup. But I edited it out because I figured 24/6 was just a typo. But no, its actually correct.No, they're not mixed up.  Do you really think that whether your function constructor is called "fun" or "defun" is deeper "magic" than whether you use an object.method syntax as opposed to a function(argument) syntax?

(edit: added a forgotten word)

(edit2: No, wait, I think I figured out what's going on.  You didn't bother to read the linked definitions of syntax and semantics in my previous post, so you missed the part where it says that semantics is about the meaning of terms, and syntax is about the structure of the language.)&gt; All languages in common use today can do most of the tasks you listed.

Turing. Complete.hey, sombody has to keep it real around here...

god damn dorks.I believe the rollover image was an example. It's pretty hard to believe that if someone is still doing rollover images with javascript that person is going to be loading over 2000 images. ;)If you were really trying to be ballsy you would have replied "No".Trick question. Emacs doesn't have a GUI.I want to try arc as much as the next lisper, but surely that wikipedia entry needs a mention of the word "vaporware" SOMEWHERE in it.It looked pretty clean for 1995; kind of reminds me of google base.

But I dont know, from that business plan; it didnt seem like a big money maker.  I wouldnt have invested.Eh, there are lots of Schemelike Lisps that don't exist...[Your loss](http://docs.yahoo.com/docs/pr/release184.html).Let X be the collection of Schemelike Lisps which do not exist, and partially order elements of X by x &lt; y if y is more expressive than x. Define functions i{xy} which map the semantics of x into y with a series of macros and functions. Then a straightforward argument shows that for any two elements x, y, there exists z with x &lt; z and y &lt; z. Arc is the direct limit (http://en.wikipedia.org/wiki/Direct_limit) of the system (X,&lt;,i{xy}).

Since direct limits are unique when they exist, this gives you a distinctive feature of Arc :-)I still stand by my decision.

No, because it is stupid code.Upload fields.It would be really obliging of you to imagine me laughing all the way to the First Federal Bank of 20/20 Hindsight right now.Let me count the WTF's for just the Point interface.

1. It has a factory class.
2. The Point interface returns other interfaces instead of implementing them. 
3. There are seperate interfaces for Polar and Cartesian cordinates.
4. There are seperate classes for Polar and Cartesian cordinates.
5. The Polar and Cartesian classes are really just wrappers over the real class, PointValue.
6. PointValue has all of the methods of the Cartesian class, but doesn't actually implement it.

It's like everything that is wrong with J2EE wrapped up in one neat little package.
They forget to put the "Func" in.

But seriously... how many such whiny articles are going to come up?  A simple Google search would pull up wonderful tutorials which go exactly how he asks.  It shocks me sometimes when people go on and on about how they can't find any information about Haskell this or that.  There is an entire website devoted organizing links and accumulating knowledge at http://www.haskell.org/, a name you might think would be obvious but apparently is not.  And even if, for some reason, "haskell.org" is not obvious, a Google search will pull it up real fast.

It's not a microformat, but you seem to be describing [OpenSearch](http://www.opensearch.org/).  Already supported by Firefox and Internet Explorer.The stupidity of this guy is almost DailyWTF material.
It's a built-in function: eval.And if I don't care, should I down-vote or leave it alone?

This unspecified behavior is bugging me.

(Note: I would like to point out that I attended PG's talk at ILC where he introduced Arc)
That's what I would expect to happen. The installer needs admin rights in order to create a directory under "Program Files".

I suppose what's needed is the ability to install software under "My Documents" so you don't need admin rights.Summary: *Haskell tutorials talks too much about abstraction, not enough about IO, files, and guis*

However, this has *long* been recognised, and now most of [the](http://cgi.cse.unsw.edu.au/~dons/blog/2006/12/16#programming-haskell-intro) [new](http://www.cse.unsw.edu.au/~dons/blog/2006/12/17#programming-haskell-part-2) [tutorials](http://cgi.cse.unsw.edu.au/~dons/blog/2006/12/18#ph-3) [coming](http://cale.yi.org/index.php/HRSS) out are about how to do these practical things.

So enjoy breaking your brain with [applicative functors](http://haskell.org/ghc/docs/latest/html/libraries/base/Control-Applicative.html) or [implicit parallelism](http://haskell.org/ghc/docs/latest/html/libraries/base/Control-Parallel.html), and also have the usual [high speed file IO](http://haskell.org/ghc/docs/latest/html/libraries/base/Data-ByteString.html), [gui programming](http://darcs.haskell.org/packages/phooey/doc/html/Graphics-UI-Phooey.html) or [XML munging](http://hackage.haskell.org/cgi-bin/hackage-scripts/package/HaXml-1.13.2) free in the mix!

If you're looking around for a modern language to learn, practically bursting with new ideas, and you want to stretch your skills, you could [do worse](http://shootout.alioth.debian.org/gp4/benchmark.php?test=all&amp;lang=ghc&amp;lang2=ruby) than Haskell.
Sorry about that.  Email me the shirt and I'll stick in the wash.

See kids?  A responsible redditor takes ownership of his mistakes!  It was my fault that malcontent ruined his nice shirt, so I'm helping him fix the problem!  Why can't you all be as responsible as me?[removed][deleted]Why did you choose to target Scheme?As someone with a purely imperative background who's been struggling to learn haskell in between other things, it's not that it's so weird that I/O is shoved somewhere in the back of the tutorial that's been difficult. I didn't have any issues with this at all, and I read through several tutorials and didn't feel this at all.

The problem was when I sat down to write code in a whole new style it was simply hard. It still is hard, although thanks to people here and in #haskell it's gotten a bit easier. Similarly, it was hard from me to move from procedural programming style to OO. There wasn't any magic tutorial that was going to solve the problem, it's just a difficult thing to do something really new and different.

If you want to help imperative programmers learn functional languages, the best thing to do is keep doing what the haskell community has already been doing so well: keep writing tutorials and blog articles, keep open forums like irc channels and mailing lists for struggling newbies, and keep answering questions for those of us who are struggling. And most importantly keep sharing your enthusiasm. We might not get it right now, but the excitement is tangible and that's the sort of thing that drives you to keep learning.The one thing has nothing to do with the other. Plenty of turing-complete languages don't have things like pointers,  first-class function, variadic functions, and the ability to create new types at runtime.

BASIC is turing-complete, FFS.here's [some context](http://en.wikipedia.org/wiki/Disruptive_technology) to my statement. I have used Google Apps, I think it's great for small orgs that are looking at the basic functionality, and that don't need the entreprise features just yet (and GApps isn't going to be static either, if adding a directory feature isn't yet part of someones 20% time it will be soon.)I always got the impression that the problems with Emacs syntax highlighting were related mainly to the cluelessness of the implementation, which in turn came from the focus on speed over completeness.  I always intend to learn how "font lock mode" works so I can go in and fix the more egregious errors.  One simple one would be to promote the handling of comments further up the hierarchy of urgency, so that things that are identified as comments don't get any other processing.*"Python is executable pseudocode. Perl is executable line noise."*

And that's how we like it, dammit!&gt; does this mean that if i want to make a method that does this to strings i have to create a subclass for string?

No, you'll notice that I have **not** subclassed the string class

&gt; I can't just have a global method and call it like `my_chomp!(s)`

Of course you can, but

1. It's definitely not the Ruby way to do that kind of stuff. Python yes, Ruby no, in Ruby the idiomatic way is to reopen the class and add your methods to it. You may also create modules to store your stuff and mix them in the existing class(es)

2. In Ruby, the "!" suffix means that you modify the object **to which you send the message**, not some random object that you happen to pass as a parameter.Which was the image with less taste?  I thought it was the last one.
GUI tutorials would be nice, but the one you show sucks.

&gt; GUIs are usually programmed in an "unnatural" style, in that implementation dependencies are inverted, relative to logical dependencies. This reversal results directly from the imperative orientation of most GUI libraries. While outputs depend on inputs from a user and semantic point of view, the imperative approach imposes an implementation dependence of inputs on outputs. 

Um, no. GUIs are generally programmed declaratively these days. I know that technically speaking we use code generators to translate it into imperitive code, but that isn't how the majority of us think about it.

&gt; As a first example, here is a simple shopping list GUI. 

This is an aweful GUI. It uses group boxes for labels, has the input textboxes marked as read-only, has the output textbox not marked as read-only, and is layed out like a poorly written dynamic form.

The tutorial also suggests that it isn't possible to do explicit layouts. Haven't we learned from Java that only offering layout managers is a bad thing?
[deleted]*Sigh*

Have a [traditional imperative programmer's GUI](http://www.haskell.org/gtk2hs/screenshots/) then. Wouldn't want to scare off programmers with funny abstractions!Perl 6 was supposed to be the Perl community's rewrite of Perl, and of the Perl community, according to Larry Wall. I'll assume you knew that already, and are just downmodding me for poor execution of an otherwise good joke. I thought it was funny, anyway.As broken as the Win32 API might appear, the thing I really like about it is its stable ABI. What would it take for Linux to achieve the same level of stability?
I can answer that, because I have to switch back and forth between them when I'm working Arc itself (the lower level stuff is written in Scheme, and the rest in Arc). I find I cringe when I have to switch into Scheme, because it is so much more verbose.  Arc is maybe 2x denser.I was amused by the fact that the article existed at all ...Err, implementing a language involves actually implementing it.  (Everything in the world is a built-in function, assuming you already have the built-in function.)Give me one reason why the style of GUI should have anything at all to do with the kind of language used to create it. There are very good reasons not to make an interface gratuitously different. They're called users.

Don't believe me? Go ahead and try to sell something that looks like the first GUI you linked o.

*edit: since I posted, the parent was edited. What now says "funny abstractions" was something along the lines of "something different". Normally I wouldn't point that out, but my post was partly in response to that bit.*Not impressed with metacircularity, eh?  Me, neither.One of the problems cited by the author is the fact that I/O is treated as an advanced problem in Haskell. I realize this is heresy, but that might not be a problem with documentation so much as an inherent weakness of pure functional languages. If common tasks are hard to do within the constraints of a particular programming paradigm, then maybe that paradigm isn't appropriate for all situations.

That, more than any documentation issues, may explain why multi-paradigm languages have been so much more successful than pure-functional, pure-OO, or pure-whatever languages outside academia.*sigh* 

It was just an example of a fun new library using an interesting [arrows](http://haskell.org/ghc/docs/latest/html/libraries/base/Control-Arrow.html)-based model that appeared in the last couple of weeks. A bit too scary, by the sounds of it. 

If you really want to get into GUI programming in Haskell, I'd start with [gtk2hs](http://www.haskell.org/gtk2hs).My challange was on two points.

1. That the tutorial author seemed to misunderstand the mindset of your typical GUI developer.
2. That the tutorial uses bad examples.

It isn't about being scared of something different, it is about crappy tutorials.
I'm not sure where this idea that IO is hard comes from. [It's not hard](http://haskell.org/haskellwiki/Simple_unix_tools).[removed]A function call to eval is a perfectly valid "implementation" of Lisp.  Good luck getting it to bootstrap, though.If I understand the examples correctly, they all operate by reading an entire file and applying some function to it. How would I do something like read a single line without disturbing any following input or requiring end-of-file to be reached?They use lazy IO. Not strict IO. So they don't read the entire file, only as much of it as you actually use.

Here's some examples:

    -- Traditional imperative style: getLine
    main = do
        x ← getLine
        putStrLn (reverse x)

    -- A bit shorter
    main = getLine &gt;&gt;= putStrLn . reverse

    -- Or just use lazy IO to only read as much as you actually use:
    main = getContents &gt;&gt;= print . reverse . head . lines

All run and produce:

    $ runhaskell A.hs
    test some input
    tupni emos tset
Yes when learning anything you have to rethink old common steps to solve a problem.  I don't have a problem leaarning new things and enjoy it but to use it in RealWorld(tm) applications it needs to have the features or power at least equal to the features and power of the last tool.  It seems to many OO and procedural coders that Haskell or functional programming is adstracting too much from the ideas of the software generation we are in which is really about input/output,user interface, user centric design.  I woud like to believe we are building hot rods and users might care what runs our systems, but in actuality they are buying a tool as well and they just want it to look good first (not distracting and usability centered) and then functional to do the task at hand.  

All NBL languages will have to AT LEAST provide tools equal to the last language.  For instance C# launched and was ENTIRELY new for many VB corporate coders, if they hadn't come from C++ it was very hard but they learned.  The tools available were more clear in C# than in classic VB. The Java libraries that made enterprise successful first were easily converted into C# NUnit, NDoc, NAnt.  Functional programming has a much tougher task in introducing things that may scare people away.  Its so simplified and abstract it leaves OO coders looking for naming to help them understand.  But when OO coders look at a functional program it is not as readable. Its gonna take tutorials with lots of comments.&gt; When you install software as root under Linux, it's A-OK.

It certainly isn't. Software that requires root is one of Unix's ugliest failings. It ensures that security on multi-user Unix systems is either a crippling obstruction (all installs at the whim of the BOFH) or a joke (everyone has root). If Windows does no better that's not good enough. At least Unix has a simple and inadequate security model (nightmares like SELinux aside); now Windows apparently has a Byzantine security model which everyone just ignores.What does NBL stand for?[deleted]Next Big Language

http://steve-yegge.blogspot.com/2007/02/next-big-language.html

It might just be a surprise Javascript 2 who knows.Comments on other predictions: please go to the [original reddit page](http://reddit.com/info/20356/comments).Looks like the person doing the erlang version of the test program did not know what they were doing.  

Next up, an erlang vs. haskell comparison coded by an erlang hacker who has never coded in haskell but was given a list of links to the various online tutorials a few hours ago...
&gt; Do you really think ["fun" versus "defun"] is deeper "magic" than [object.method syntax versus function(argument) syntax]?

Not at all.  I don't even think "fun" versus "defun" is relevant to the discussion:  it's a question of naming, not syntax or semantics.

&gt; No, wait, I think I figured out what's going on. [...]

Oh, spare me. :)

&gt; semantics is about the meaning of terms

Not just the meaning of terms, but of the language in general.  For example, Haskell and Liskell have similar (or identical) semantics, but very different syntax.

&gt; syntax is about the structure of the language.

Right, as opposed to the meaning of said structure.  For example, Liskell and Scheme have the similar syntax, but very different semantics.

In your original post, you associate "semantics" with the superficial aspects of a language, and "syntax" with its deeper design, which is pretty much exactly the wrong way around.`eval` isn't even metacircular! :)No, it's a *call to* a perfectly valid implementation of Lisp. :)

When people talk of a programming language "implementation", they generally intend something that actually runs on its own, as opposed to indirect references and metacircular definitions.&gt; They use lazy IO. Not strict IO. So they don't read the entire file, only as much of it as you actually use.

(I'm sure it's a silly question) Wouldn't reading line N require going through the file from the beginning to count the newlines?&gt;Factor doesn't attempt to do floating point with x87 at all

So no trig, logs, or exponentiation?Exactly. I completely agree.

Windows does better in some respects though, NTFS allows for natural permissions for groups with seperate allow and deny permissions. ACLs and such may be old hat, but they're still very necessary in more complex organizations.

Vista is a bit annoying at the moment, but I think either the shrinkwrap vendors will get their act together. If not, MS will probably address the annoyances in the first service pack.Yep. Though the GC will throw away the parts of the file you've read, but not used, so it'll run in constant space. E.g. find and print the 200001st line of a file:

    import qualified Data.ByteString.Lazy.Char8 as L

    main = L.getContents &gt;&gt;= L.putStrLn . head . drop 200000 . L.lines

still runs in constant space.A better way... any suggestions?

I've thought about it for a while, but there's no obvious fix. And the nagging system is still a good deal better than the XP 'allow everything no questions asked' philosophy.Roughly a year ago I wasn't paying attention when installing something (using OpenSuse's gui). The application had a 2.6 kernel as dependency (I think I had 2.4 back then, not that it matters). Needless to say, I clicked OK, OK, CONTINUE, REBOOT. 

And then the system was b0rked.

Most of the time, the package systems work magnificently. But when you get in dependency hell (and we've _all_ been there) you suddenly wish for a simple windows installer.I agree with his observations, but my conclusion is that Haskell is something like a blow-up doll for twisted computer scientists, and simply not useful for practical purposes.

All the Haskell tutorials I came across (a lot of them on reddit.com) are either incoherent babbling, or the author just throws jargons and abstractions around when trying to do something really simple and trivial, which can be done in a simple and straight-forward manner in imperative programming without all the jargons and abstractions. And then the author proclaims "it's so cool !". Well, I say you must be out of your mind.
Share and enjoy!I like his peeling potato example to demonstrate algorithm.Yeah, they're being rather hard on a project that's only a couple weeks old :)

I agree on the Gtk2Hs point. You can even use Glade rather effectively.Of course, you can't just believe everything you read. :)

* The goal/scope of PyPy is much more ambitious than a "simpler Python implementation";  it would be more accurate to say that it's an advanced framework for dynamic language translation and compilation which happens to result in a Python implementation as one side effect.  To this extent, they've delivered.
* The part of PyPy that corresponds to CPython (the interpreter, core libraries, and such) is actually simpler, and has been pretty much complete for at least a year or two now, as far as i know.  The remaining, more complex bulk of the project is concerned with things that aren't even possible or conceivable with CPython.Well, in a pedantic sense nothing runs "on its own", because everything needs to run on top of the CPU which in turn needs to run on top of the universe.  Even if you're a Platonist who says that math runs on its own, the notation still doesn't.  So, while I'm taking an extreme position as to what constitutes an interpreter, any other position requires some sort of judgement call.pkg_add has the ability to run arbitrary code.  See postfix, isc-dhcp3-server or dovecot.&gt; Writing a JIT compiler or a self hosting language is not hard.

[Partially evaluating](http://codespeak.net/pypy/dist/pypy/doc/jit.html) a JIT compiler out of a high-level interpreter takes some doing, though.I didn't know. Thanks.No, it's perfectly unambiguous, mathematically, whether an interpreter definition is well-founded or not.

Depending on orthogonal functionality like a CPU is very different to circularly depending on the same thing you're providing:  it's the difference between the equations "x = 2y" and "x = 2x".[deleted]Thanks!  I wish more people felt this way...Paul, since you're apparently reading this, any chance you can tell us why you're not releasing whatever arc interpreter/compiler you have, in addition to the essays and talk about it (which is also really interesting, but doesn't let us play with it). "release early, release often". and the word 'vaporware' is starting to appear in combination with 'arc'. Nobody's forcing you to read everyones criticisms you'll inevitably get when you release arc, what's to lose...In Scheme, you say? :)This ain't relevant to the code, but that's the ugliest fucking makeover ever. The swing people live in a different universe, aesthetically.

(Not that this wasn't already obvious from looking at the swing widgets -- but I thought maybe they just weren't trying. They were trying. They're just confused.)L is Turing-complete means you can write an interpreter for any other Turing-complete language in L. Ignoring arbitrary limits (number of user-defined variables, array length, whatever), you could write an ugly and slow but functionally equivalent Ruby evaluator in Basic.

So in a narrow but significant sense, and I think this is what Entropy is getting at, you can write pointers into a Turing-complete language that doesn’t have them. You might have to implement them as indexes of a huge sparse array representing memory, and everything using them would have to talk to that array, but it would give you the same things that built-in pointers give you. Except speed and legibility.

Presumably you’re well aware of this and reject it because it’s hideous and impractical. But Entropy has a point: Turing-complete means that the features in the language are enough to make any other reasonable feature. The fact that most common languages have roughly similar coverage *does* have something to do with the fact that most of them are Turing complete.They're taking on [MonetDB](http://monetdb.cwi.nl/)?I never meant to imply that mathematicians and engineers are stupid.  The only reason that this problem is so simple is that certain variables are made constant (uniform velocity, etc.) opening up a "short cut" solution.  For the general case, infinite series is the simplest approach.  When I first heard the problem, my immediate instinct was to use a series solution.  Perhaps one of the reasons I love this story so much is that I feel some sort of comraderie with Von Neumann (though I certainly didn't solve the problem in seconds).

I just put up another post (http://thesciencepundit.blogspot.com/2007/02/mindless-imitation.html) explaining my position further.

"Banging huey" is an expression that all my college buddies used to use to refer to a U-turn.  And yes, sexual connotations were intended.  I'm sorry if what I thought was a more widespread expression turned out to be an inside joke.

Thank you all for reading and commenting on my post.

--JavierGoogle isn't the be all end all of The Internet. Many small companies deal with Exchange (and even simple linux servers) because they maintain control over it, not some other company.

If I was a small business, I wouldn't want to just turn over everything to Google, regardless of how 'not evil' they claim to be.
Besides, not everything Google has is Mac friendly. If Apple adds a few more 'exchange like' features to 10.5 Server, it'll be a nice thing for Mac heavy shops to have.

There's nothing wrong with this at all.&gt;And it prints something different in all of them.

I would be more impressed if it printed the *same* thing in all of them.I wonder if/when he sold.  Yahoo has split 8:1 since then (assuming the Aug 1998 split was after the transaction), so that'd be close to 4 million present-day shares, presently valued at $30/share for $120 million.it would be way to difficult to go through here and reply to all the people, so i'm only going to say this once, not like it matters.


windows: installers get admin. you don't know what the installers are doing because most developers for windows are mean and bad and **know** they can get away with anything. hence the problem with UAC, one click admin privilege, people are out to do harm, and it's difficult till after the fact to call them on it


linux: package management systems need root, this is 100% true. but, the repositories that house those packages, as someone else pointed out just like windows updates, are maintained people that are out **not** to harm you, rather help you. *and* since you can see what installers are doing on linux, as the code is in front of you, you can call foul *before* the fact. this is the difference, not to mention as someone said in a properly setup environment you can install things inside your own hierarchies, leaving the rest of the system completely alone, untouched.

as a comment to the small group of people talking about permissions, you have no idea what you are talking about.
&gt; linux perms are simple and inadequate

inadequate to do what? give me one example where linux perms fails?


enough ranting... i need a drink. the only thing that i can't figure out from reading these comments was if they were worse then the comments on the page that this links too. i hate to say it but this is the first DIGG esque (juvenile) article and discussion that i have seen on reddit (not to say there aren't others, just my first). is this the end of good, knowledgeable articles with comments? do i have to go back to the book comments that are written by /. people?Not by itself. :)

People typically assume (loop (print (eval (read)))), no?  At least, I did.How were we supposed to know it was only two weeks old? Moreover, where are the tutorials of mature offerings?If it is so easy to find, why don't you post a few links on IO and GUI tutorials?[deleted]Right. But it's simply not true.

See the comments below the article for details. People have tested it and disproven the claims.No, it's C-Store.
http://en.wikipedia.org/wiki/C-Store
Keep in mind that they were getting venture funding more or less as the deal closed (I think it was one of those 'Give us a fourth of your company at X, and we'll convert it into Yahoo! stock worth 2X in a couple months' deals), and that he [ran the numbers](http://paulgraham.com/bubble.html) in mid-bubble and decided it was worth dumping.

So it's almost certainly too convoluted to tell for sure, but it *probably* beat spending the same few years cranking out code for Du Pont and Interleaf.Fortran 77 vs. Fortran 90/95/2000? 77 has less architecture than its successors.&gt; But Ruby is more dynamic than Python, so this is a good point.

I keep hearing this from people who don't substantiate it.  How, pray tell, is Ruby more dynamic than Python?&gt; I notice that most of his digs against C++/Java aren't much appreciated by the audience.

Really? His famous dig at C++ gets a fair round of applause, around [10:40](http://video.google.com/videoplay?docid=-2950949730059754521#10m40s).

Maybe you girly-men code in assembly, but a *real* he-man types out 1s and 0s.And the encouragement award goes to... you!Ah, it's the [TransRelational](http://www.c2.com/cgi/wiki?TransRelationalModel) project resurfacing.He mentions REST around 43:00.The comments are not entirely correct.  See this:

http://forums.microsoft.com/MSDN/ShowPost.aspx?PostID=1023870&amp;SiteID=1

The woman that started the thread, Corinna Vinschen , is a cygwin developer at redhat.  She's good.  Very good.

Microsoft has made a bad, bad mistake in requiring elevation for certain named executables.  What do you think the virus authors will name theirs?It's not in the browser. It's a server-side debugging system that happens to use a web-based interface. So all of the heavy lifting happens on the back end.Suggestion for Arc: At least while it still has s-expr-ish syntax, support varied brackets, with enforced matching:

(mac let [var val . body]
  [list { cons 'fn (cons [list var] body) }
        val]
)

Sure, syntax is insignifigant, but I always thought this would be cool for a lisp.He's trying to show you an example of how they managed to break up their gigantic classes on their billion-euro project into different interfaces, not saying that you should actually implement a polygon class like that.

Of course how they end up with single classes which are big enough to warrant that kind of mess in ANY project regardless of the budget is proof enough of J2EE's crapness, but hey, that's the joy of coding by UML :-)One of the many things that bugs me about Haskell programmers is that they assume that when anybody talks about functional programming, they mean programming in Haskell. Lots of languages allow functional programming, and many of the problems listed are endemic to Haskell, not to functional languages in general.Because the definition of "good code" varies critically depending on the context it will be used in.

It makes perfect sense to hack something together that barely works for a one-time conversion script, whereas the same hacking might get you fired or somebody killed if applied to your airline control system.

Examples lack context, so unless you specify one, it's hard to know whether it's truly "good" or "bad". The context people bring to code _a priori_ varies.

(Another example of something that lacks context in much the same way: "What if...?" ethical questions. "What if a gunman is forcing you to choose which of your children he will kill?" Well, am I armed? Am I close enough to just charge the gunman? Is there something at hand to throw? Why exactly am I trusting this gunman's words, again? Most, if not all, such conundrums are pointless because they lack context; any detail could be critical.)Sure. Factor has trigs, logs and exponentiation for both real and complex numbers.One of the attractions of Haskell to me is that it does not compromise the core tenets just for the sake of convenience.  This is such a curious phenomenon; after all, plenty of functional languages exist which have compromised referential transparency for the sake of I/O.  And yet Haskell stubbornly refuses to do so, and in the process, you learn something.

You learn that strict call-by-value evaluation imposes artificial limits on program structure.  

You learn that there is a well formulated generalization of certain varieties of computations which can be described and composed algebraically.  Including I/O.

You learn that a static type system can help you be more expressive.

You gain an appreciation for the simplicity of equational reasoning for understanding a program.

Then you realize that I/O is easy in Haskell, as well as many other things.  What is hard to do is abandoning your ingrained informal thinking methods which relied on assumptions about the environment which are no longer true.
And all of those languages to which you refer have compromised referential transparency and rely on strict call-by-value reduction.

If Haskell was just like Standard ML, why should it even exist?
Does this fail for the input:
...\r\r\nFrom ...
&gt; Of course how they end up with single classes which are big enough to warrant that kind of mess in ANY project regardless of the budget is proof enough of J2EE's crapness, but hey, that's the joy of coding by UML :-)

A nine digit budget and hundreds of developers makes a damn compelling use case. I'd like to do something like this (minus the interface frenzy) on a few of the several-thousand-line DB-mapped classes at work and we only have 4 devs. Business logic accretes like kudzu.[deleted][deleted]Maybe you're just thick.I have one of these, and it's great. I found it especially useful for Emacs use since the Ctrl key is hit by the thumb, which makes editing faster and easier on the hands.[deleted][deleted]I've been working from home for over a year now, and I do it in a recliner.

I do it not because it's the _most_ comfortable, but because it is the __only thing__ that is comfortable.

I'm 6'4", and _everything_ is too damn short. My knees hit every keyboard tray I've ever had. (Very uncomfortable.) The chairs on the highest settings they have (which ram me into the desk or keyboard tray or _something_) still do not allow me to sit with my knees at a 90 degree angle; it comes out less than that and my legs get uncomfortable, fast. Within an hour I'm fidgeting in my chair, which hopefully leans back, although that's still not as helpful as it could be.

Here at home, I'm on a recliner. It is, technically, also too short in the same way, but when I put the footstand up it doesn't much matter. I have a laptop, which I use as a true laptop, and it works great. My arms rest in a fairly neutral position, with my wrists straight (as they are supposed to be) and my elbows supported on the back of the arm rest. 

And I fit.

Of all the things I will miss about working at home, the fact that I can actually sit and not think about the fact I am sitting is probably going to be #1. People pay lip service to ergonomics, but how many people are actually willing to allow me to have a recliner in my office? Even if it really is the most effective thing possible?

(I'll provide the chair. You'd be surprised what you can pick up from garage sales for reasonable prices if you don't much care about color. All I need is _permission_.)

(Also, note I have a dog and other things that make sure I get up frequently enough that thrombosis is no more a concern for me than anyone else.)

Edit: I'd like to add that 6'4" isn't _that_ exceptional of a height, either, to have no furniture for it. I can't remember the last time somebody commented how tall I was when we meet, I don't have any trouble finding clothes, and meeting somebody taller than me isn't a novel occurrence. Every year I'm getting a little more and more frustrated that there isn't an "officially acceptable" ergonomic setup for me, hence the tone of this message. (I'm not actually anywhere near this pissy about it yet, but I'm probably going to be there in a year or two at a non-home job if I don't figure something officially acceptable out.)[deleted]Love mine, it reduces the stress on your wrists and hands big time. The learning curve is a bit steep at the beginning, especially if you are under a deadline.If you are thinking of switching make sure you aren't under pressure. The biggest annoyance is having the = key on the left hand side. If you write a lot of code and use vi this can trip you up when you switch between keyobards.&gt; The involuntary nature of pre-emptive multi-tasking means
&gt; that your code is non-deterministic when run in-the-wild. 

Getting sick of that "non-deterministic" idiocy.  It is about as true for threaded code as "the sequential nature of single-tasking means that you code is **divergent** when run in-the-wild".
[deleted]I mean "take on" as in "compete against".I'm not sure how the REPL is related...

I think `(define (eval expr) (eval expr))` is the closest you could get to a metacircular definition of `eval`, but it's more of a tautology, really.That application form took you an hour to write? If Arc was really so great, it would take less than an hour that to implement everything on that page.Cool bash snippet for adding all un-version controlled files in the current working directory to a subversion repository.I must admit then I entirely concur with the article. To me your example how small it looks does not sound right. I'm not saying the language is crap or anything (heck how would I know) but with my background this really does not sound intuitive. Throwing what you've learned so hard for a programmer is the hardest task of all.Summary: A [kinda-superior blog post](http://sob.apotheon.org/?p=200) about how he was right in a [minor reddit discussion](http://programming.reddit.com/info/14drw/comments/c14n7y?context=8&amp;style=nested#c14n7y) about something tangentially related to a [mailing list thread](http://mail.python.org/pipermail/python-list/2006-December/417468.html). If that wasn't incestuous enough, all his friends think he's right too, so there.

It's most curious: he's aware that [semantics](http://en.wikipedia.org/wiki/Semantics) means meaning, but he seems to think that's just things like what names to give to primitives.

Function calls and macros allow us to change the form of an operation ("this 100-line chunk of code and this single word now do the same thing"), so that we've swapped syntax while retaining semantics. The converse task, of swapping semantics while retaining the same syntax (say switching from eager to lazy evaluation), usually involves modifying a compiler or interpreter.

We now understand how to keep syntax modular and 'swappable', using function calls (the idea of RETURN, an indirect jump to a dynamic address stored in a register by the 'caller', was once an ISA novelty), macros, even operator overloading. But giving semantics the same modular hot-swappability as syntax is a [work in progress](http://lambda-the-ultimate.org/node/1472); the best we've been able to come up with so far is inheritance and AOP/advice, or lazy evaluation and pattern matching.

---

Functional programming has its semantic roots in the [rewriting-based](http://en.wikipedia.org/wiki/Denotational_semantics) lambda calculus, while imperative programming has its roots in the [operational](http://en.wikipedia.org/wiki/Operational_semantics) notion of a turing machine. The difference between these is semantics, and it's a deep difference. It was non-trivial to figure out that [they were computationally of equal power](http://www.cyberconf.org/~cynbe/muq/muf2_42.html).

I might have something wrong here; getting feedback was part of the goal in writing this comment. apotheon's wrong regardless in thinking semantics more superficial than syntax. And in mistaking [humility](http://programming.reddit.com/info/14drw/comments/c14ew5?context=5&amp;style=nested#c14ew5) and [politeness](http://programming.reddit.com/info/14drw/comments/c14n7y?context=5&amp;style=nested#c14n7y) for ignorance.What people mean with "non-deterministic", in this context, is "unsafe and unsound".

(Addendum:  Also, they mean *shared-memory* preemptive multitasking, of course.  Preemption is safe, otherwise.)
    darcs add *&gt; you could [do worse](http://shootout.alioth.debian.org/gp4/benchmark.php?test=all&amp;lang=ghc&amp;lang2=ruby) than Haskell.

Hm, I looked at this page, but I only saw some kind of graph comparing the memory and cpu use of trivial programs, organized to contrast those of a language like Ruby (using YARV, I suppose) and of a language like Haskell (using GHC, I suppose).  What do you mean for me to learn from this?  That Haskell is only good for CPU and memory efficiency (or only for these in the case of trivial programs) with GHC?  That this graph is what the Haskell community wants people to focus on, so that they don't think to ask about other kinds of aspects of languages (coloured by implementations) to make comparisons on?  And... wait... you suggest here that Ruby compares to Haskell in terms of modernity and 'practically bursting with new ideas that may stretch your skills' -- -wow-!  I'd heard good things about Ruby, but never anything as good as this!  I'll definitely pick it up and try it out, to stretch my skills and see some practical bursting of ideas.  After all, Ruby seems -much more approachable- to me, and it has -much more interesting- applications floating around -- and I think it even has very nice hosting and distribution systems (a little birdy told me that the Haskell community would prefer that you self-host and post-URLs to some kind of mailing list -- ugh!).  But I think I -will- take your comparison in hand, and avoid using Ruby for trivial programs.

Thanks a bunch!Hello,

Who I am? An experienced ( 6 years ) WEB DEVELOPER from Ukraine specialized in web application development seeking employment for the creation of web based internet service presence and/or other related services for clients. I am Interested in working with both companies and private individuals. I`m opened to a dynamic range of offers, contact me for details. 

What I can do for you? I can develop appropriate solutions at very affordable rates 
e-commerce application 
shopping cart 
ordering system, 
credit card processing, verification, payment 
transactions management 
auction 
on-line service 
events tracking 
employment directory 
dating portal 
guest book 
counter system 
rating system 
news system 
custom web development, etc... 


my portfolio is at http://www.alexit.net/portfolio.html
You can contact me via email alex77@a-teleport.com

Thanks and I look forward to working with you.
Alex
Wow ayrnieu! Bizarre contribution, even for you! Though I'm disappointed you didn't mention Mercury ;-)

By the way: [hackage](http://hackage.haskell.org/packages/archive/pkg-list.html).  You need to talk to different birdies.Most of the code in there is to differentiate between them, so your suggestion would actually be easier.Sure, other languages don't force you to program functionally all the time. They still do allow functional programming, and moreover many of them encourage it, and imperative programmers still do have trouble learning them, partly because they are best used functionally. Witness the consternation of any imperative programmer starting in on SICP. 

The point is, the problems of imperative programmers learning functional programming are unrelated to the particular problems Haskell newbies face. Haskell newbies also face these Haskell-specific problems, which is why Haskell needs better tutorials, but they're Haskell problems, not functional programming problems.I think that it's a myth. A *good* imperative programmer will learn functional programming in the time required, and will be able to exploit what he learned while programming in an imperative language. I think that what's true is that bad programmers often only know imperative languages because they are mainstream, and bad programmers don't try to explore the world of programming for something better. But now almost everybody is somewhat exposed to functional programming because now it's more and more in the news, and even languages gaining mainstream acceptance start to have functional primitives, and 'cause they  don't even master imperative programming are not good to functional programming.Vista's motto:

"I click Ok to make the boxes go away!"Is this related to functors in any way?Relevant post:

http://xkcd.com/c149.htmlLooks like code resuse through parametric polymorphism. So.. no, not directly related to functors (of either the category theoretic or ML module flavour)The post itself isn't about Functors, but it's worth noting that if the Haskell code in the post were rewritten to use `fmap` instead of `map`, any datatype implementing a Functor interface could make use of the function.For the non-C programmer wondering what's the problem with this lines, the last two parameters are inverted because the memset prototype is:

  void *memset(void *s, int c, size_t n);

The problem mostly comes from the fact that it is much more natural with this order:

  void *memset(void *s, size_t n, int c);

Edit: well... a good C compiler should issue a warning since this is trivial to spot at compile time.

Edit: valgrind is your friend (http://valgrind.org/)that is really cool. physical computation is fascinating.
 Want to to sort in constant time? cut out lengths of spagetti the same as each integer. hold the spagetti. Hit the spagetti off a table. they are now sorted by length.Great resource to have online. You just have to appreciate the intellectual rigor with which Dijkstra approaches programming, even if it does make his writings a bit less accessible. I own a copy of Dijkstra's "A Discipline of Programming", great for reference purposes, and one of the few programming books I keep coming back to.[deleted]&gt; Though I'm disappointed you didn't mention Mercury ;-)

Works of hate for the language shootout come of a higher calling, and thereby inhibit expressions of personal interest.

But for your disappointment, I'll go now to submit a Haskell URL and then mention Mercury in a comment :-)

&gt; By the way: hackage.

Good!Strictly speaking, `map` is a part of the list functor, but i don't think this is what the author meant.[deleted]`hg add`[deleted]I've been wanting one of these for a while now, the only thing still holding me back is the price really. Interesting factoid about the refurbished models though.so what did your mother call you?

no on second thoughts, I don't want to know.I've been toying around a bit with emacs lately and one thing that I'm having a bit of trouble with it that I can't figure out how:

a) to enable a mode that gives me a file / class overview

or

b) if the above is not available, how people actively developing with emacs get around this.

Old hands probably don't realize how helpful documents like this are: even when the error tells an experienced reader -exactly- what the problem is and also suggests to the experience writer -exactly- what sort of action the mistake requires, people new to the language will read past important information and generally frustrate themselves (with ineffectual changes to their program, shotgun solutions, &amp;c).  Even in IRC channels dedicated to C, you'll have 'why do I get this error message?' questions and then three answers that repeat the error message back.  In C again, you'll find college students who turn in C programs that fail due to some obvious deficiency that -their C compiler warned them about-, but that they didn't fix because they'd gotten used to simply not understanding what the compiler says.  Languages with new and strange features will have new and strange (and therefore incomprehensible at first) errors, even for an experienced programmer.

Related to this, [an index of my posts about errors that Mercury gave me](http://community.livejournal.com/mercury_blog/2007/02/).  Some of those are pretty trivial; some of them I wrote because I came to like the idea of that list; some of them frustrated me for hours upon hours; some of them frustrated me until a more experienced human could point out the solution.  [Discussion about that page.](http://news.gmane.org/find-root.php?message_id=%3ce1e139960702120327i704d6525rcfe3103b288f7262%40mail.gmail.com)[removed]You win :-) Thanks!Preferably in synch with the clock so you don't have to have an ENTER button.[removed]GB and UK aren't the same thing anyway because GB doesn't include Northern Ireland but UK does.  UK is really short for "The United Kingdom of Great Britain and Northern Ireland".What the hey?  Seriously:  direct yourself to the nearest literature, and get your definitions right.  Wikipedia's _[Programming language](http://en.wikipedia.org/wiki/Programming_language)_ and _[Formal semantics of programming languages](http://en.wikipedia.org/wiki/Formal_semantics_of_programming_languages)_ articles should provide enough references.Man.  Talking about [Dan Piponi](http://www.google.com/search?q=Dan%20Piponi) (sigfpe) as a "display of ignorance" is just... mind-bogglingly unwitting.You're spot on. But I don't think this guy can be persuaded by either argument or fact. For he _knows_ that semantics (the _meaning_) is superficial and unimportant compared to syntax.It probably is, with particular reference to [execution in the kingdom of nouns](http://steve-yegge.blogspot.com/2006/03/execution-in-kingdom-of-nouns.html)Wow! Haskell on the JVM. I know that a lot of the GHC folks work for Microsoft but any chance of integrating this when it gets completed?There was an article before on reddit on using regexes as a poor man's static analysis. I wish there was a collection of regexes for common programming errors like these.That isn't the point.  They are in widespread use and are a significant improvement on what went before.None of the tutorials linked from [here](http://www.haskell.org/haskellwiki/Books_and_tutorials#Tutorials) follow the form I was thinking about; i.e, they start with Hello World, cover how to do console input, show you how to define functions without lots of theory, and then move on to more abstract topics.  Even though tutorials exist for more concrete topics, like file IO and GUIs, they can be incomplete or difficult to track down.  

If its proponents want Haskell to see more wide-spread adoption, they have to make it easier to get into; IO is a standard part of any C or Java tutorial.How quaint...

"2. Secure server software ($5000). This does not seem to be an absolute necessity; there are a lot of sites on the web where you can send your credit card number unencrypted, and to date there have been no reports of the numbers being stolen. But catalog companies may *believe* that a secure link is necessary, and spending this $5000 would give Webgen a much more professional look."
I don't agree with his personal jibes, or consider it helpful.

However, I agree that the conversation was odd for having a weird bias against syntax, for being "superficial." I couldn't care less about subjective terms like depth or superficiality; I don't look to programming language design for philosophical depth. However, I do look to languages for power. And I'll happily accept (what some consider) a not-so-profound feature if it gives me significant power.

In the design of other programs, like OSes and word processors, depth vs. superficiality tends to be brought up to justify missing features, by users who are obviously partisan. (Personally, I use Common Lisp, and many have observed some Common Lisp users using these rhetorical devices instead of serious arguments.)
Indeed it is useful but I think for C there are far better lint-alike tools available for static analysis. I tried a couple of this in the past but can't remember their names since now I switched to unit tests and valgrind.If you're serious about combatting RSI, then skip half way house solutions like the Kinesis and go straight to a real solution, like the Maltron keyboard. You can switch between using a QWERTY layout, and a Maltron layout specifically designed to help those suffering from RSI.

http://www.maltron.comyou're right, it was neither simple nor even a question.

somebody just highlighted their totally incompetent hiring policy.
[removed]great thanks, i get it.

This means i can have my own customized versions of string and array and hash that do all the little things i wish were built in. brilliant.That's pretty impressive and out of the blue. I think he left-sided many people with this :)There are some important facts missing in your statement.

Perl6 was planned as and still is a community project. That doesn't mean it has to be a "design by comittee". Larry is making the final decisions, that's correct, but not after long discussions on the Perl6 mailing list.

Dan Sugalski has moved on, but Leo Toetsch has taken his place in the process and Parrot is under active development.

Audrey Tang made a huge contribution in the form of Pugs, an experimental Perl6 compiler written in Haskell. But she too must make a living from time to time. She continues working on Pugs as time permits.

With the advent of Pugs, two important things happened in the process of developing Perl6:

1st, there was finally a possibility to actually play with Perl6, and to see how it felt like to write things in Perl6. That was always an important thing in Perl: how do things _feel_. Do they feel _right_? Or do they get in the way of the progammer when he wants to express things in an intuitive way?

2nd, Pugs brang a new influence to the design process, simply because it was written in Haskell. Early Perl6 was partly influenced by Ruby, and today Perl6 has (in my opinion, of course) the best design of OOP features of any language out there. Since Pugs, functional programming techniques (such as laziness) are given more thorough consideration, which of course appeared to slow down the overall design process.

The fact is: this community wants to do things _right_. There's no one in the background wailing: "We're losing customers! Get the product out! We're going to add the fancy stuff later." We already had enough of these.[removed]DAMN, what's up with those annoying-ass boxes that pop up on link mouseover?We're talking about a list of methods/members in a class, right?

One of the best choices for people coming from IDEs would be the [Emacs Code Browser](http://ecb.sourceforge.net/), which adds a few windows to the main Emacs frame that display some of the information you want.

The 'traditional' approach would be something like ctags, i.e. a program that extracts the neccesary information and stores it in a TAGS file. [Exuberant Ctags](http://ctags.sourceforge.net/) supports over 30 languages, so this should be fine. Emacs has a nice integration with tools like these.

CScope or simply grep are other options to navigate within files. Several programming language modes have some tidbits of their own to offer.This article has a neat Euler diagram:

http://en.wikipedia.org/wiki/British_Isles_(terminology)Wow is right. The author should announce this to the community and gather some support for further open source contributions. Success with this kind of project is a social process: he needs community backing.

Let's hope the author reads reddit :-)I think Larry's greatest contribution to the programmer brainpool is the idea that linguistic theory and Huffman coding can both be applied to programming language design.  Perl's strengths are tied most closely to that.

Its weaknesses, on the other hand, are tied to the fact that Larry is stark raving bonkers.  Hey ho.Next: Binary computation by duct-taping cats together.This one my favorite -- it's in a test of memset (and it's ok) -- the fact that this works (and is tested) means that all of the others are just null ops.

          my_strcpy(x, "XXXXX ");
          ret = memset(x, 'Y', 0);
          if ((my_strcmp(x, "XXXXX ") == 0) &amp;&amp; (ret == x))A better way... 

Ok, first of all, don't ask for confirmation on reversible actions. Never. Sending a file to the recycle bin should not require a confirmation, as I can easily revert that action. Oh, and for those applications that hate you closing them (games tend to do that), closing an application is not a destructive action unless I have unsaved work. For non-reversible actions, only those that could conceivably be activated by mistake should ask confirmation. Emptying the recycle bin, for example, requires a moderately complex sequence of actions (right click, then select the specific action from a menu) and would make a change so unimportant (deleting files you already decided you didn't want) that a confirmation there is ridiculous.

For actions that definitely require a confirmation, make that just one confirmation. If I am deleting a bunch of files, and I say "yes to all", I want to be able to go to the bathroom while the deletion is done; I don't want to be prompted again when the first "read only" file is found, and again when a "system" file is found, and again when an executable file is found.

That is just some ideas. The aim is to make the occurrence of a popup message uncommon enough that when you see one you know that something important is going on, and you wouldn't just click ok to make it go away.[deleted]lol, enterprise.This is an incredibly appropriate response to the [Why it’s so Hard for Imperative Programmers to Learn Funtional Languages](http://programming.reddit.com/info/14m2o/comments) posting.[removed]&gt; However, I agree that the conversation was odd for having a weird bias against syntax, for being "superficial." I couldn't care less about subjective terms like depth or superficiality;

Bias?  Subjective?  I think you're reading too much into the word.  In this context, it means "surface-related" (the way your skin is superficial), not "insignificant" or something.

It should be no surprise that syntax is so important *because* it's superficial:  it's what you're in contact with day in and day out.  Ask any Python programmer. :)[deleted]Well, it appears that the context of the discussion was laid down in [sigfpe's post](http://programming.reddit.com/info/14drw/comments/c14ew5): "If the final decider of elegance and simplicity is a surface detail like where you place your operators then we're clearly not talking about a very deep analysis. Or is there some subtle and deep property of the placement of operators that I'm missing?"

I understand "surface detail" in this context as a kind of antonym of subtlety and depth.

(Again, I should make clear I found nothing wrong with sigfpe asking an honest question. But I expect there are assumptions underlying it that I'd disagree with.). . . and if there was any likelihood that was what you actually meant by "superficial", the conversation would likely have gone very differently.Previously: mecano tic-tac-toe
http://www.rci.rutgers.edu/~cfs/472_html/Intro/TinkertoyComputer/TinkerToy.html
More previously: solving non-homogeneous differential equations http://en.wikipedia.org/wiki/Water_integrator
More previously still: using beads and twigs
http://en.wikipedia.org/wiki/AbacusReally?  Ya think?

Tell ya what.  Make a statement about how semantics has "depth" the way *akkartik* did (except for the bit about flying off on a "this guy's a moron" jag), and maybe I'd consider the value of your words.  Make wild-ass statements about "No, you have that backwards, 'cause I say so," the way the people in the previous discussion did, and I'll continue to look at you like you just grew a third arm from your forehead.

When people try to contradict what I'm saying by telling me I'm wrong, and seem entirely incapable of putting together a cogent argument, I tend to consider their statements summarily discardable, obviously useless, and generally lame.  When they put together a salient argument with some kind of support, I consider it on its merits.

Thanks for assuming bad faith at first glance, though, just because *pjdelport* can't construct an argument.Arguments from authority aren't really helping your credibility any -- at least, not with anyone that has his/her head firmly attached to some higher-elevation region of the body.That's Python (syntax highlighter), not (Python syntax) highlighter.Alternatively, you might like [Quick and dirty code folding](http://emacs.wordpress.com/2007/01/16/quick-and-dirty-code-folding/).

ECB is pretty much what I was looking for, thanks !Or [KX](http://kx.com)?I don't get it, whats the fuzz about, what am i missing?  In python i would just do this:

def addTenAndConvert(l):
      return ["***"+str(x+10) for x in l]&gt;The installer needs admin rights in order to create a directory under "Program Files".

False. You will often get through by simply assigning write rights to "Program files" and HKLM to JoeUser.

edit: and HKCR.Splint (open source) - www.splint.org

Popular commercial tools are PC-Lint, Coverity, Polyspace...
I know EXACTLY what this guy is talking about. I have been to similar places in India. the so called 'Computer Gully' in Mumbai, centain areas of Campus and Deccan in Pune. The atmosphere, the electricity (sic) is exactly like this article.&gt; pjdelport gets stuck on the "form vs meaning" distinction. s/he doesn't realize that the form is what creates the expressive power of a programming language, and the meaning you express with it is secondary.

See, pjdelport, that's your problem right there! You think the *meaning* of your code is important.I've been using functional languages for about 16 months and I still feel like I'm nowhere close to mastering functional programming.

Does that mean I'm dumb, Slava? Are we not good enough for you? :([removed]I tried to learn Scheme from SICP years ago and I gave up initially when I learned that Scheme has no for loops.

Then again, I was like 16 years old.And then... in 5 or 10 years, you use whatever *practical* language took some of those concepts and became popular?

All the while grumbling about how "Haskell did that 10 years ago"?

Time will tell...Great script, I've been looking for something like this!&gt;Properly written Windows applications don't require elevated privileges either.

Apps maybe, but an installer? The second it tries to simply register a file type, it needs write access to HKCR. By default, for example, in XP, you need to be either System or in Administrators group to do that. So, to install this very simple feature (file association), you need Admin rights.

I am sure one can come up with many other examples.&gt; I understand "surface detail" in this context as a kind of antonym of subtlety and depth.

Right, and both the surface details (syntax) and their underlying meaning (semantics) are crucial in a language analysis or comparison.  As sigfpe was saying, you can't just stop at the former.fyi, CAL - http://labs.businessobjects.com/cal/ - is similar to haskell and runs on the jvm.It's called a rhetorical question. And indeed, if the main differenciator between the elegance and simplicity of two PLs is the parser, then I think we've got a problem. Why are we working on type systems, different paradigms, etc., when we could just take assembler and change the order of arguments to opcodes until we got it just right? Surely we'd have exhausted the design space by now if we'd been doing that since the 70's.I wasn't arguing anything;  i was expressing my disbelief at the sheer irony of the situation. :)...and has additional strictness annotations - in particular, function argument strictness. However, it favours semi-colons over the significance of whitespace, which might be a put-off for some purists.Ditto.  

Let's see, in the last 10 years, Paul founded Viaweb, has written dozens of thought-provoking essays, evangelized Lisp to a new generation of hackers, started Y-Combinator and mentored a generation of new founders through Y-Combinator and his writings.

And the Trolls are getting pissy becuase he hasn't put out his first public beta of ARC yet?

Relax people.  It'll be done when it's done.So I should stop using memset(rand(), rand(), rand())?Jeez, let go already. There's no shame in being mistaken, you know? But if you go on a blog-rant about something it helps to check the facts first. 

The subject at hand is not subjective. It's based on definitions, clearly defined ones at that. I'm not going to debate the meaning of definitions with you.I might say you weren't arguing anything anyway, even if you were trying -- even in the other discussion.  That's really the problem.Wonderful history and a great way to explain macros to novices and non lisp programmers&gt; It's called a rhetorical question. 

That's not a rhetorical question. A rhetorical question is one where you don't expect an answer.

&gt; . And indeed, if the main differenciator between the elegance and simplicity of two PLs is the parser, then I think we've got a problem. [..]

Non-sequiteur. This sub-thread is talking about what the meaning of "superficial" was. You may want to move that argument somewhere it makes sense.My thoughts went immediately to the Crunchly saga and its "Advanced Dynamic Hydraulically-Operated Computer". Wonder if that's where the inspiration came from?If it was truly a merely rhetorical question, then it was a stupid one.  I preferred to assume the poster wasn't an idiot or malicious -- which are the two options for characterizing the statement about prefix vs. infix notation the way sigfpe did if the question wasn't an honest one.More talk and no code. So I'm a C programmer and you tell me IO doesn't matter. Then, without even knowing Haskell, I can reduce all your programs to the following elegant C program:

    int main(int argc, char** argv) {
        return 0;
    }

It should be noted that this is even referentially transparent. Now, I have to admit C is not very concise, so for the ones that prefer other imperative languages, here is the same program in Perl, Ruby, Python, Lisp, Scheme, bash, PHP, OCaml, Io and Lua:All I/O involves "side effects." The operations are fundamentally procedural.

IMHO, no one has yet found a way to bolt procedural operations onto a functional language without creating massive ugliness. The problem resembles the search for a "grand unified theory" in physics.That's true.

But I think a seperate program should be spawned for changing the file association. So you're running the installer, then the 'association-changer' is launched, an elevation dialog pops up, and no matter what you option pick (allow/deny), the installation will go through.

It's ugly and perhaps inconvenient, but switching to admin at the last possible moment seems proper to me. I think that code that requires different levels of privilege should be contained in seperate binaries anyway. Or at least kept seperate by some means.The frequency isn't surprising.  The offending projects are.I never said I wasn't mistaken.  I suggested that just proclaiming it so without providing some kind of intelligent argument or analysis, and using that statement as a bludgeon with which to beat on someone's reputation, doesn't accomplish anything worthwhile.

re: blog-rant
Have you considered that maybe claiming I'm some kind of idiot who can't be "persuaded by either argument or fact" is over-hasty, counterproductive (if you're right), and at least as stupid as you'd like to think I am?  Has it occurred to you that my comments to the effect that *akkartik*'s arguments were better than all the rest put together might indicate that I'm open to being convinced, if someone bothers to say something intelligent?  This, despite the fact *akkartik* clearly set out to insult me, as opposed to setting out to educate me.

That's okay, though.  You're welcome to your little world of snap-judgment and closed-minded assumption.  Whatever you do, don't read the next (chronological) post in my weblog, where I might have said something that dashes your expectations.&gt; I wanted to know how to go about reading from stdin, applying a function and writing to stdout. I was told “well, there’s a lot you need to know before doing that”.

Right.  The experienced C programmers told him that "Hello, world!" was too advanced to hear about first.  Somehow I doubt this story.So you use ad-hominem to support your position, after mistakenly applying applying argument from authority fallacy to his statement. Well done.I've had ideas for electronics projects but never really got into it as a hobby b/c by the time I order the parts I have lost interest (let alone forgetting to buy a particular part).  A market like this would have been awesome.  There are a few electronics stores in the US that do have a flea market feel (e.g., Halted in Santa Clara) but they are few and far between.

This bodes well for the future Chinese tinkerers.
That wasn't an argumentum ad hominem fallacy.  That would require I attacked his/her character or something like that.  I just pointed out that (s)he hasn't made a cogent argument (yet).

. . . and I doubt the veracity of *pjdelport*'s objection to the fallacy identification.He doesn't like the term *refactoring* because it "denigrates the standard of the software industry," so he creates a term that means the same thing.  Oh well.Yes.

You have to be careful about it though, since others will do the same you may get collisions (Rails adds a lot of stuff to the base classes for example), but other than that that's basically it: the built-in types are as open as your own.glibc, erlang, mozilla, kerberos, parrot and picasa just on the first two pages.

(edit: followed by tor, ruby, python, and gimp on the next page.)A related goof I've seen too many times is forgetting to multiply the number of elements by sizeof(the_type) when passing the size to memset/memcpy.[deleted]Why should _we_ go out of our way to convince you, when you can simply do the research yourself?

You're all about "indications that you're open to be being convinced", as if it's our obligation to provide you with arguments to show you 'the light'.

This isn't a debate. Somebody just pointed out in the original thread that you mixed up the meaning of two words. That's not a big deal. Don't turn it into one.That's just suicidal.As much as I like it, RoR isn't a panacea, especially if you're not familiar with it.  With real deadlines, and no "do over" time, you really are better off using what you know.  Now, if you have the time to (potentially) throw away what you've written, attempting to write a Real Web App as a learning exercise is great.That computer was developed and utilized quite some time ago by "Aqua Teen Hunger Force"Let me try to enlighten you with an analogy into natural languages. The syntax defines the words and sentence structure of the language, while semantics is about what the words and structure mean.

Let's look at your example:

&gt; "Semantics" refers basically to the superficial aspects of language design, like "Are we going to call our function constructor fun or defun?"

That's incorrect. If 'fun' or 'defun' *means* the same thing, it's not a question of semantics, but syntax.

&gt; "Syntax" refers to the form the language takes, the kind of thinking the (successful) programmer will do, and how programs fit together. That's the deep magic of a given programming language, as manifest in its design. The choice of prefix or infix operators is a part of that.

Partially correct. Syntax is not *only* the form the language takes, but also what words are used. For example, *this* from C++ and Java and *self* from Smalltalk refers to the same concept, but with different syntax. The choice of prefix or infix operators is part of the syntax, as you say.Function argument strictness, i.e. bang patterns?

    f !x = x + 2
He didn't say that I/O doesn't matter.  He said it not the most important part of programming.
And there was code, but maybe it was too short for you to notice. :)Actually, since IO doesn't matter, you should remove the "int argc, char** argv", no?You imply he's incapable of argument. How is that not an attack on his (I assume) character?

If he was making an argument from authority, there better be an argument from that authority. There is nothing but a simple question from sigfpe.To be strictly accurate, this probably isn't the final form of the language that they'll be using, and probably not even the language they'll be using for most of the code -- just lower-level stuff. Still, it's very exciting to see. I hope this grows into a really nice general VM-construction kit.&gt; He didn't say that I/O doesn't matter

(hint: read the title)

&gt; not the most important part of programming

Dunno what the "most" important part is. For my part, I have no use for programs that do no IO.

&gt; And there was code

Ironically, it was IO code.It seems clear you have not much experience with Unix. I almost do not install software under root users and guess what? It works.You are right, of course. Somehow I assumed it is required for standard C programs.the SALIENT thing (IMO) about this article is you CAN'T buy (how much of it?) stuff like this in Amerika?

Why? because the corporate stranglehold dicates that the only 'innovation' comes from THEIR 'labs'...not someone's garage.No, more like, he needs to know how to write a growable buffer list with heap allocated buffers, how to read into it without overrunning a buffer, how to print from it without falling off the end, and how to free it only once.With FreeBSD (Unixy enough) the default way to install packages is with pkg_add (port binary) or by compiling from the ports tree. This requires root access.

The _default_ way to install packages requires root access.

You can install Winamp and Filezilla under Windows by extracting a zip file - that doesn't require admin access either. But it's not the default way of doing things, which means it doesn't have an impact on the grand scheme of things.

When you install a package as root under unix, you risk your entire system being compromised. The same thing applies to installing an application under windows as administrator. Comparing apples to oranges (admin under windows, nonprivileged user under Unix) is unfair.Seeing that huge part of all memsets is to 0, one should prefer ZeroMemory (or an equivalent thereof on your system).

Just run a quick scan on our biggest project and... Whew! we don't have such bugs.But what if you have to handle a billion requests a day? What if you're Google? What if a bunch of other stuff that never happens for 99.99% of all projects becomes true overnight? If you don't have that available today, then your toolchain/language/platform is obviously useless for small to medium sized projects.You are glossing over some details, but in any case this is linear time, not constant time. You hit the spaghetti against the table to align their bottoms, but then you have to repeatedly remove the longest straw from the remaining bunch and add it to the sorted list. It's basically selection sort with a constant time select-max (rather than the usual linear time).

But yeah, this is a really cool idea. I always enjoyed these simple but effective physical hacks. Another hack, which was shown to me by my high school physics professor, was to approximate a definite integral of a curve by cutting that curve out of a piece of paper (with scissors) and then weighing the paper on the kind of high-precision scale you often find in labs.(hint: the most important part is between the I and the O)int a;

int add()
{
a=a+1
return a;
}

main() {

new Thread() {
public void run() {
x=add(); 
}}.start();

new Thread() {
public void run() {
y=add(); 
}}.start();

System.out.println("x: "+x);
}

What is the output? You can't say. It's non-deterministic. [Not quite correct: The correct answer is "a compiler error."]Heh.  Well, Haskell actually *did* do it ten years ago.  The monadic I/O model has been around since the Haskell 98 standard.

Do I get my official Smug Haskell Weenie badge now?Some credit is due.

I took this from a dailywtf comment, which in turn got it from here:
http://www.kottke.org/06/10/google-code-search

Some other interesting searches there.Apologies you are correct. You also have to use linear amounts of memory (spagetti). 
An interesting read is this http://www.scottaaronson.com/papers/npcomplete.pdf
on the subject on how physical computations can carry out calculations[removed]Cute, but not as cute as [MONIAC](http://en.wikipedia.org/wiki/MONIAC_Computer) ([reddit](http://programming.reddit.com/info/14qg6/comments)): a hydraulic simulation of the UK economy.I love the last line:

&gt; Spending some money on advertising might also be a good idea.&gt; With FreeBSD (Unixy enough) the default way to install packages is with pkg_add (port binary) or by compiling from the ports tree. This requires root access.

If your admin trust you enough. Some installs don't trust you to become root (universities, business environments, etc), thus if you need an specific app you should provide it yourself, in your own environment.

In FreeBSD it's the default method, because it's aimed to admins. *Edit*: thinking about it, there's no default method of installation of software in Unix; which is a great advantage, if you ask me.

&gt; When you install a package as root under unix, you risk your entire system being compromised.

Unless you're using applications that employ privilege separation. Or do not install packages as root, if you don't need them there at all, or feel constrained.The slash? :PFor the blogger. The user doesn't care what's between, as long as input and output are OK (including good user interfaces). We as programmers should satisfy the needs of our users, right?

Some say Haskell is the best imperative programming language and maybe they are right, but this blogpost addresses none of the issues raised in the qftblog. Maybe it assures Haskellers that they got it right already. For someone that isn't already convinced, the blogpost is content-free.This was presented as a way to "demystify computing"...but imagine the confusion of the lay people when you bring along a huge board of hoses and piping and tell them that you can turn some valves to instruct the machine to multiply two numbers. Then their astonishment when you turn on the water and the patterns of water/not water in the exit tubes provide the answer to the multiplication. I imagine that the man in the street would be _more_ confused at how computing works after seeing this than by using a magic calculator. And to top it off, the experiment would just add fuel to the side of those who claim that the internet is a series of tubes.By this argument, you're including every part of programming in IO.rails.vim is an excellent utility for, well, working with rails in vim.Not purely IO code though. He wrote essentially

    main = interact f

f here is just taken to be a pure function from strings to strings that describes how the program is going to transform its input into its output. It is permitted to act lazily, so that if it can produce some of its output before it has all of its input, that output will occur as soon as possible, and the memory associated with it freed. Doing that sort of coroutine-like interaction properly in C is more difficult.

While most standalone real world programs need a bit more control over I/O than this, there is a fairly large set of problems that can be handled with even such a simple user interface. The Unix philosophy of shell programming is essentially chaining together many small programs of this type. 

Taken together with the REPL, this is sufficient to make Haskell useful enough at least until one has the right frame of reference to learn the rest about IO properly.Rats' nests of wires don't scale :(err the presentation of the "water computer" would be with an explanation?
Edit: I am sorry, didnt get the joke.Maybe WebSphere "next" will be able to pick up the full Jython 2.2 build? (Current WebSphere (6.1) includes Jython 2.1).I'm not even a hardware guy, and I'm exited.
Great story.No, but that *is* what you need to know to "read from stdin, apply a function and write to stdout" in C, safely, in the general case.

That is, unless you're going to try to get away with unrepresentative corner cases, or worse, "C programmer's disease" (pre-allocated buffers and no overrun checks). But then I would point to the "interact" function in Haskell as an equally cheating answer.IO is NOT a part of C.  There is no provision for IO in C.   Everything else is just a function call.  It just happens that someone wrote functions for IO, and they are part of the standard library, but not part of the lanugage itself.

When I learned C, the books all stated that IO was not a part of C, but you can use printf (and soon scanf and a bunch of other functions) for now.   It was years before I understood the distiction.&gt; Now go and try to have some fun, damnit.

That's the precise problem if you want to have fun by performing IO tasks, isn't it?Hm. Here up north, you could run one of these in the same box with your PC; the former would keep the latter cooled, and the latter would prevent the former from freezing. :-)They are very much functional programming problems.  It's functional programming when you don't cheat and break referential transparency just to cater to some set of assumptions about the world which don't have to be the case.

If Haskell broke down and just accepted being call-by-value with undisciplined use of side-effects, it would no longer be an interesting language to learn and research.  Just another ML.  And then people would start writing lazy functional languages on their own again; the whole reason for Haskell to exist is to focus those efforts into one place.
So start off with

    main = interact myFunction
    myFunction = ...

Much easier than C.
The best thing so far to get me really thinking functionally was the article on "writing as if the function to do the X that you need to do already exists". This actually results in decent functional code, when doing it in an imperative language tends to result in a nasty mess that needs refactored every other hour.Syntax (the lexical and grammatical structure of a given language) is the easy part. Easy to define, easy to learn.
Many languages provide multiple, separate syntaxes, that is, multiple notations (for example, OCaml). The particular notation doesn't affect what a program does, hence it is superficial. Syntax is superficial.

The semantics of the language, however, is the hard part. Pretty much every language has a formal *syntactic* definition (a formal, or semi-formal grammar), but very, very few* have a formal *semantics*, either static or dynamic.

So I can only imagine that [when you say](http://programming.reddit.com/info/14drw/comments/c14i43):

&gt; "Semantics" refers basically to the superficial aspects of language design, like "Are we going to call our function constructor fun or defun?"

And then:

&gt; "Syntax" refers to the form the language takes, the kind of thinking the (successful) programmer will do ... deep magic of a given programming language,

that you must have made a typo. That would explain it, if it wasn't for your embarassingly public attempts to defend the original statement. 

Then again, maybe my problem is that I think that what a program does is what matters most. Not what notation I use.

----

1. Milner, R.; M. Tofte, R. Harper and D. MacQueen. (1997). The Definition of Standard ML (Revised). MIT Press. ISBN 0-262-63181-4.That's exactly what I thought too.

I will say that the automatic currying aspect of his code is nice - yielding:

    let addTenAndConvert = map (("***" ++) . show . (+10))
    addTenAndConvert [1,2,3]

UPDATE: Ok, I actually tried to compile this shit, and realized why Haskell is pretty but sucks my left nut. Fucking bondage and discipline.

I loaded up the Hello, World program in Visual Haskell, and set to work modifying it. First thing I did was add the addTenAndConvert function, yielding:

    addTenAndConvert = map (("***" ++) . show . (+10))

    main = do
        addTenAndConvert [1,2,3]

which gives an incomprehensible error: 
    Couldn't match expected type `IO a'
\t   against inferred type `[[Char]]'
    In the first argument of `GHC.TopHandler.runMainIO', namely `main'
    When checking the type of the main function `main'

Grr, fine. the main function wants to return an IO object (why?) but is getting a [[Char]] instead (why?).

So, the author used a 'let'. Let's try putting that in the main function:

    main = do
     let addTenAndConvert = map (("***" ++) . show . (+10))
     addTenAndConvert [1,2,3]

Same Farging error! What the crap does it mean? How about before the 'do'?

    main = 
       let addTenAndConvert = map (("***" ++) . show . (+10))
       do
           addTenAndConvert [1,2,3]

Nope! it's a parse error, no further information given.

Well, I know putStrLn returns the right type for main, so let's modify things to try and use that:

    addTenAndConvert = map (("***" ++) . show . (+10))

    doAddTen = addTenAndConvert [1,2,3]

    main = do
        putStrLn doAddTen

Now we "Couldn't match expected type 'Char' against inferred type '[Char]'. At this point, the compiler is just f%$#ing with me.

Now I give up. I'm sure that the answer is easy, and this is probably all my fault, but it shouldn't take me half a damn hour to figure out (unsuccessfully) how to run a function because the f$*%ing type system is so f%&amp;$ing anal. 

What's worse, I don't see any way to find the knowledge that I need short of 1) asking in public, or 2) going back to the beginning and randomly learning things about Haskell until, someday, I might know enough magic to be able to make a random function I thought of match the type expectations of the compiler.His point was that `interact` allows you not to worry about IO: it takes a function of type `(String -&gt; String)` and does all the IO stuff, you just provide the computations.It's not really an unrepresentative corner case to use fixed size buffers, especially for a newbie. I would have told him to allocate a buffer of a suitable size and use fgets to fill it up. Certainly easy to grasp the implications of a fixed size buffer, and there is no need for overrun checks when you use fgets.Was the introduction ever finished? Two sections whose headlines were particularly appetizing (Variables and relations between their values, Programs corresponding to recurrence relations) were listed as "in progress".Recommended reading: Paul Graham's recent essay on Smart vs. Wise. An incredibly *smart* imperative programmer might not have the broad *wisdom* to be sufficiently *good* at functional programming. This doesn't make them any less *good* at impertative programming (by necessity). In fact it might tend towards allowing them to be suitably *genius* as an imperative programmer. (It might not.)Milton Friedman is smiling upon you.Yeah, I just hate how little innovation has come from American startups.If the United states doesn't start building things like this, China is going to crush it in a few years.

Combine cheap knowledge and cheap materials with an economy that's getting more market-driven by the day, and you'll eventually get massive innovations.

It may be time to start learning Mandarin.I can't speak for Visual Haskell, but you should be able to take any bit of code and load it up.

So start small and make it compile, then add more.  You do not need the main function right away, either.  This is not C.

So you put

    addTenAndConvert = map (("***" ++) . show . (+10))

in a file and load it into the interactive prompt.  Then you can try it out:

    Prelude&gt; :load MyFile.hs
    Prelude&gt; addTenAndConvert [1,2,3]                                                                                           
    ["***11","***12","***13"]                                                                                                   

Great.  How about the type?

    Prelude&gt; :type addTenAndConvert                                                                                             
    addTenAndConvert :: [Integer] -&gt; [[Char]]

List of integers to list of [Char], but remember, String is a synonym for [Char].  So it's really [Integer] -&gt; [String].
And to keep that in mind, let's add that to our file

    addTenAndConvert :: [Integer] -&gt; [String]  
    addTenAndConvert = map (("***" ++) . show . (+10))

Let's start off with a simple main function

    main :: IO ()
    main = putStrLn "Hello World"

What's this say?  main expects to be returning some action which does IO and has no value wrapped up.

We know that putStrLn is String -&gt; IO (), and you can check with :type if you are uncertain.  We need to get from \[String\] to String.  If you can't think of a good function to do this, try plugging \[String\] -&gt; String into [Hoogle](http://haskell.org/hoogle/).  It suggests unlines or unwords.  Let's try it at the prompt.

    Prelude&gt; unlines ["a","b","c"]                                                                                              
    "a\nb\nc\n"                                                                                                                 
    Prelude&gt; unwords ["a","b","c"]                                                                                              
    "a b c"

I like unwords better, so let's stick it in

    main = putStrLn (unwords (addTenAndConvert [1,2,3]))

or even just try it out at the prompt

    Prelude&gt; putStrLn (unwords (addTenAndConvert [1,2,3]))                                                                      
    ***11 ***12 ***13                                                                                                           

The type system is not being "anal" when it tells you that a list of strings is not a string.  It's being a strong type-system.
I quite like "At present Webgen has only a 28.8kb connection."[deleted]Also, here's a [diagram](http://www.cs.man.ac.uk.nyud.net:8080/CCS/res/images/res12b.gif) of the MONIAC.  pretty darn neat."there has to be a Jobs and Wozniak here, quietly building the next revolution"  
um... another product 94% of the world doesn't want to use?Shhh, don't say that. Since MIT is in Boston, I'm surprised the mayor hasn't arrested this guy as a terrorist.
That's f'in sweet manA [diagram](http://www.cs.man.ac.uk.nyud.net:8080/CCS/res/images/res12b.gif) of the beast, courtesy of [jczerg68](http://programming.reddit.com/info/14p7r/comments/c14qjj).[Mechanical Pong!](http://www.cyberniklas.de/pongmechanik/indexen.html)they're graphical previews of the pages linked to.  I like the concept if not the implementation.There are plenty of languages easier than C. The point is not really there is it?on a similar note: http://jaskell.codehaus.org/Well you know that your one string is at least much clearer than the dabbling of that article :/

As someone else says, people who write tutorial for programmers coming from a non-functionnal background should not say "forget everything" but instead "let's try to map things so that you can smoothly join the fun".

I cannot put into words the magnitude of my astonishment.

&lt;sarcasm&gt;The peak of human achievement in OS design runs an applications as root, if its file name contains certain substrings! Awesome! &lt;/sarcasm&gt;Thank you for responding to my frustrated comment with information and with speed.

That said, do you see why it was frustrating to me? I'm used to dynamic languages, where "print [1,2,3]" yields "[1,2,3]", not "Couldn't match expected type `IO a' against inferred type `[[Char]]'".

Instead of programming by example (try x, see if it works, runtime error, trace x, modify x, see if it works, etc), I end up having to try and trace the type system (try x, got a compiler error, let's look in detail at the types yielded by the functions I'm using, add some type annotations, modify x, try x). It's easier for me to debug a running (but failing) system than it is for me to try and build a perfect mental model of what's going on in my program so that I can make the compiler happy.

Once you're used to it, perhaps it yields better, more correct programs. It certainly seems to me to be beginner unfriendly, though.

Perhaps there should be some sort of "newbie switch" where the compiler will give you more information on the errors it found? Maybe even drop you into an interpreter with the correct functions loaded so you can check their types and play around with possibilities?Hmm...interesting, but the lack of clicking seems to be replaced with randomly opening links you don't want to.Isn't C a pretty poor language to compare Haskell with? I don't personally know any programmers who work with C, and somehow I don't think C and Haskell are really meant for the same problems. What about IO in Ruby or Python? Its pretty easy with them.

The issue I have is I just don't see what Haskell solves for me. Everything I do is IO, and its not just command-line type IO; it is a web app with multiple forms or a GUI application with several frames/windows/inputs, etc. If I want to do a GUI, I've got a ton of options in different languages, more mature options at that, and I don't need to spend the 6 months required to twist my brain into a shape that will understand Haskell. And if I want to do a web app its the same thing.

Not to mention if the project I am working on will ever be maintained by others. Should I expect them to learn Haskell as well?

Haskell needs to offer a superior solution to a problem a lot of programmers have. Since most of us are working with rather complex IO all the time, we need something that does IO well. This is precisely why you are seeing interest in Smalltalk because doing IO correctly on the web is hard, and Seaside promises to make that somewhat easier.

Most of us aren't writing file converters or language parsers. If we are using something like that, we are interacting an outside library.

Anyway, after that long comment, my main question is: what does Haskell do for me, the everyday programmer who is constantly dealing with IO? What does it do better for web apps, for GUIs, for the common tasks that we all deal with?

And I apologize if I am ranting a lot lately. I just don't like all the marketing that we get from every programming language camp. And yes, I have given Haskell a chance, but like with every programming language, if I don't see what it can immediately help me with, I lose interest in it, no matter how cool theoretically pure functional programming may be. I'm not sure if that is naive or just pragmatic on my part.[removed]Right, and I come from Common Lisp where programming by example is king.  So, I spend a lot of time mucking around in the GHCi prompt, just trying things out.  It's no [SLIME](http://common-lisp.net/project/slime/) but it can be used quite effectively nonetheless.  I came back to note that if you do want to get the effect of "print [1,2,3]" such a function does exist in Haskell.

One of the nice things about its type-system is that with type-classes you can express a function which can convert any value to a string (given a suitable instance definition).  Well, all of the types we're dealing with have built-in Show instances.  So in fact,

    main = print (addTenAndConvert [1,2,3])

works just fine.

    Prelude&gt; print (addTenAndConvert [1,2,3])                                                                                   
    ["***11","***12","***13"]                                                                                                   

I don't personally use print a lot so I neglected to mention it.  print is a standard function defined

    print = putStrLn . show

using the very same show function which is used in addTenAndConvert.

    Prelude&gt; :type show                                                                                                            
    show :: (Show a) =&gt; a -&gt; String                                                                                             

this says that anything which is an instance of Show can be turned into a string.

    Prelude&gt; :info Show                                                                                                         

spits out a lot of information but let's take a peek through it

    class Show a where                                                                                                          
        showsPrec :: Int -&gt; a -&gt; ShowS                                                                                            
        show :: a -&gt; String                                                                                                       
        showList :: [a] -&gt; ShowS                                                                                                  
        -- Defined in GHC.Show                                                                                              

the class defn which we can ignore for now, other than to note that it defines the function show.

    instance Show Double -- Defined in GHC.Float                                                                                
    instance Show Float -- Defined in GHC.Float                                                                                 
    instance Show Integer -- Defined in GHC.Num                                                                                 

Now we know that these are all Show-able.
Later on...

    instance (Show a) =&gt; Show [a] -- Defined in GHC.Show                                                                        

This says "Anything that is Show-able is Show-able in a list too"

With that (probably excessive amount of) information, you can find out for yourself what I said above: show can be used on many different types to produce a String representation.
&gt; I'm used to dynamic languages, where "print [1,2,3]" yields "[1,2,3]", not [...]

Well, did you actually try that in Haskell? :)

Saying `main = print (addTenAndConvert [1,2,3])` in your original example works just fine.So why didn't you try what you suggest?

main = print (addTenAndConvert [1,2,3])

It works like a charm.  :)

On a similar note, I don't think this does things like code-folding or other more complex parsing of syntax, is there anything out there that does which is open-source and either Python or C/C++ with a Python binding?

BTW, Pygments is really cool. I'm constantly amazed at how many cool projects get put out by the Python community, and how little Python gets talked about in turn.

If other languages are the hare, Python is the turtle. Slow and steady, not flashy, not a lot of buzzwords, but it'll get the job done. You might say Python is a blue-collar worker."non-deterministic" has well-established meaning. If people mean "unsafe and unsound" then, please, let them say "unsafe and unsound".

Even if they did, it would be an utterly stupid overgeneralization.  "Error-prone" would be infinitely more appropriate.Its not randome, it just opens the links you move your mouse overWhat is the state of Squeak?

I've been perusing a lot of Seaside/Smalltalk blogs lately because I'm very interested in what both offer, but I've heard different grumblings about Squeak.

Is it a good stable platform to develop in with access to a good amount of libraries? Is it something that can serve a production web app (is dabbleDB running on Squeak)?

Hmm, lots of questions. Hopefully someone likes answering. :)There's been [pressure](http://www.nytimes.com/2005/04/02/technology/02darpa.html?ex=1270098000&amp;en=e081c19247a119ed&amp;ei=5090&amp;partner=rssuserland) for the politicians to restore funding to the fundamental federal research which fuels the US startup system. (As I understand, Japan has the advantage that it can just fund high-tech directly, without having to go through military or whatever.)Is your program representative for every multi-threaded program out there ?
signed!
lol, Enterprise, lol.Haskell will never become mainstream IMHO. I am mainly playing with Haskell (night programming) due to some interesting concepts which I like &amp; which will eventually be sucked into our day-time programming (c# 3.0, Boost, ...).Well, to the extent that adding preemptive threading turns previously deterministic (correct) shared-memory code nondeterministic (incorrect), it's accurate to describe the nondeterminism as unsafe/unsound.The problem that I'm currently running into (as I'm trying to learn haskell in my spare time from working, learning django, reading the thousand books I just can't stop buying and playing Ouendan on my DS) is, in fact, that things don't map well between imperative languages and FP (Haskell really). It's probably just easier to crush your brain into pieces, make the pieces explode, and then rebuild your brain from the leftover fragments.

Brain explosion is one of the favorite pastimes for haskell newbies anyway.That actually seems like the recipe for recursion (the leap of faith), which obviously has a lot to do with functional programming. :D

"To understand recursion, you must first understand recursion."No;  most real-world code isn't that simple and manageable. :)Ah yes, but if you leave away the I and the O then nobody will notice if you leave away the important part too.

L.



there are already many bugzilla/trac/subversion hosting companies. just google for 'subversion hosting'

but he's right about the trust. we needed a secure internet accessible code repository, and we couldn't get a warm fuzzy about having that farmed out to a 3rd party, so we spent a lot more money doing it in house (certainly at the risk that our in-house implementation is less secure).I'll be sure to throw "rethinking" charges on the next invoice. The customer will love it.[Haskell's HOPL paper](http://research.microsoft.com/~simonpj/papers/history-of-haskell/index.htm) offered interesting counterpoints. For example:

&gt; The phrase “syntax is not important” is often heard in discussions about programming languages. In fact, in the 1980’s this phrase was heard more often than it is today, partly because there was so much interest at the time in developing the theory behind, and emphasising the importance of, the formal semantics of programming languages, which was a relatively new field in itself. Many programming language researchers considered syntax to be the trivial part of language design, and semantics to be “where the action was.”
&gt;
&gt; Despite this, the Haskell Committee worked very hard—meaning it spent endless hours—on designing (and arguing about) the syntax of Haskell. It wasn’t so much that we were boldly bucking the trend, or that the phrase “syntax is important” was a new retro-phrase that became part of our discourse, but rather that, for better or worse, we found that syntax design could be not only fun, but an obsession. We also found that syntax, being the user interface of a language, could become very personal. There is no doubt that some of our most heated debates were over syntax, not semantics. 


In fact, Richard Bird was adamant about a syntax I think is wonderful:

&gt; We are urged to return to the mind-numbing syntax of Lisp (a language that held back the pursuit of functional programming for over a decade). 

Personally, when programming languages one day get syntax right, then I'll consider it a simple matter. ;) I suspect opinions of profundity vs. superficiality on this topic reflect personal commitments more than anything objective...interesting yes, workable no. It's way to slow to be really usable.Kind of old. If nothing else, that site shows how bad it can be to break a user's fundamental assumptions about the UI. Even knowing that you're not supposed to click, it's awfully hard to resist.Nope, I assumed haskell putStrLn &lt;=&gt; python print, based on Hello, World. I guess I know what happens when I assume.Sorry, when I said random I meant accidently.[removed]Note that the text-based search can mis-identify expressions where a non-offending memset call is nested within other parentheses. There are also some autconf-like tests that simply try to compile a memset with nonsense args to see if the routine exists in a library, but not to call it.You seem to deliberately misunderstand about what properties of code I talk about.&gt; What is the output?

What is your desired output?

&gt; You can't say.

Does it matter? Replies to the effect of "but...but...it's *non-deterministic!*" will be ignored.

edit: Actually, I can say. Assuming a is initialized to 0, the output will be 1 or 2. Though non-deterministic, my answer is completely logical.at least it's not a product that the world is forced into using despite serious security issues, plethora of viruses, monopoly business practices, user interface holes, draconian licensing agreements, heavy-handed upgrade support agreements, and a generally crappy product that hasn't had feature improvements until competitors started knocking on customers' doors.Yeah, China is totally going to crush the United States... except of course that their resource use is substantially less efficient than in the US and that eventually that will all catch up with them in the form of enormous environmental damage and other unforeseen consequences.

Remember that whole thing with their anti-satellite missile? Maybe they meant to do that!Yup, and some of the results are unit tests to make sure the call fails; I didn't include gcc in the list for that reason, but I didn't look very hard at the context.[deleted]A fascinating read.  With cheap and easy access to education and basic infrastructural building blocks, technological innovation in the developing world will be rapid.  

The maltron looks quite similar - what's the explicit difference, other than the numpad?I think the best part is the IE tooltip that says "Click to activate and use this control"Key point on this article: homoiconic programming language

http://en.wikipedia.org/wiki/Homoiconic

It seems obvious now, but I didn't really know about the concept explicitly.  And THAT would be what really makes Lisp, TCL, etc. special compared to languages like C#, Java, etc.  

Put another way, I know I can generate and execute code at runtime in C#.  But, I also know I will do everything I can to avoid doing so because of how difficult the CLR and .NET API makes it.

Short and sweet and still enlightening.  Good stuff...
"Adding preemptive threading" does not have well-defined sematics.

The implication

for every X. multi-threaded(X) =&gt; non-deterministic(X)

where "non-deterministic" is with regard to program inputs and outputs,  does not hold.  This is easily proven by a counterexample.

There exist other forms of non-determinism, which are neither unsafe nor unsound.  For example, the expression
a + b + c + d, in some languages does not have a predefined order of evaluation, thus a compiler, which selects one of the possible valid orders, based on a true random number generator exibits non-deterministic behavior, which is neither unsafe nor unsound.  Furthermore the execution of a machine code, generated for that expression may possibly utilize multiple arithmetic units in parallel or may perform sequentially, or may use different arithmetic units in different points in time of the execution of the program, thus being non-deterministic in that sense, but still not being unsafe or unsound.

Evaluation of "map (*2) [1..1000]" also exibits non-determinism (or opportunity for one), without being unsafe or unsound.

There's a lot of non-determinism even in threads without shared mutable state ("processes"), which comminucate with exchanging messages.

At most one can say that non-determinism is characteristic for parallelism.  But it would be going too far to say paralellism is unsafe or unsound.`find / -user dons | xargs mod +1`

Hmm, `xargs(1)` is a good analogy for teaching currying and sectioned operators.&gt; As I understand, Japan has the advantage that it can just fund high-tech directly, without having to go through military or whatever.

Japan's record at government funded IT research has not been [smashing](http://en.wikipedia.org/wiki/Fifth_generation_computer).[deleted]What is your desired output?

1 (one)

Does it matter? 

Yes.Yeah, but an expensive product that 6% of the world does want to use.The latter was a pretty good screencast, or more like a video blog or something.

But I don't think Seaside's advantage on Rails was properly demonstrated. He *said* Seaside "clobbers" Rails, but did not show even once how it did so. I'm sure it has advantages, I just don't know what they are.

And sure, Goto is kind of like Href, but the major difference is that the web is all about *Location*, while code is *not*. In that sense, Href is not harmful, much unlike Goto.You mean smashing [like this](http://en.wikipedia.org/wiki/Thinking_Machines)? The US went through the so-called ["AI winter"](http://en.wikipedia.org/wiki/AI_Winter) as well, where work on massively parallel machines and AI was sharply curtailed.

Looking further at Wikipedia, I'm again struck at how robust [Japan's economy](http://en.wikipedia.org/wiki/Japan#Economy) is, despite "very limited natural resources":

&gt; Close government-industry cooperation, a strong work ethic, mastery of high technology, and a comparatively small defense allocation have helped Japan become the second largest economy in the world, after the United States, at around US$4.5 trillion in terms of nominal GDP and third after the United States and China if purchasing power parity is used.
a = 0;
    a++;
    print a;

There, I fixed your code. You asked the wrong question, so you got the wrong answer.Hey, I'm genuinely sorry I seemed to be insulting you. No such insult was intended. I tried to restrain myself to arguing the core point in this now-extended thread: that semantics isn't just naming*.

My criticism (first and last sentence) was targeted purely at your tone. If you recall, I wasn't part of the original discussion. As I traced the evolution of that thread and your blog post, it seemed to me the only one being a little personal was you, first by making hypotheses about where others may have gone wrong, and then by passing judgement on your blog. It's irrelevant who was right or wrong, or who was learning from whom; you were calling ignorant people whose comments I have come to value in this forum.

---

That then is my version of events. But I don't want to help the conversation descend into 'he said, she said'. It is just my version, my subjective account of how things seemed at a certain point in time, and we can do better things with our time than try to argue why things should or should not have seemed that way.

1. Making hypotheses about where others may have gone wrong is not a bad idea in general, but in my experience it seems to only distract an internet conversation. A much better use of one's time is to restate your point for greater clarity using that hypothesis.

2. Nobody called you names in the previous thread. All their comments were directed on *your actions*. It is a good habit to make that distinction and focus on the latter, if I may make the recommendation. If one isn't in full control of what one says online then one is surprised when others feel rubbed the wrong way and reply in kind. My personal heuristic is to treat the word ['you'](http://programming.reddit.com/info/14drw/comments/c14mfo?context=5&amp;style=nested#c14mfo) as if it has a tax imposed on its use. If we start there we have plenty of cushion between ourselves and the precipice of name-calling.

3. The only *real* idiocy lies in being sure one is right. I don't think anybody was doing that in this thread, it was simply a misunderstanding.

4. I (and, I'm sure, others) have no objection to answering questions. 'Educating you' sounds too presumptuous, but my subconscious keeps surprising me by its presumption, so I've resigned myself :) Besides, this thread needs to discuss the intriguing topic itself -- syntax vs semantics -- rather than silly sidelines like who was right when and who is close-minded. Hopefully *then* this thread will be full of useful information, new and interesting. So if you would care to reply to the meat of my post, we will say both (all?) of us unintentionally insulted the other without due cause and move on. How about that?

\-----------------------------------------------------------------------

'*' - Naming isn't as superficial either as most of us assume, but that's another story.

As a gesture of good intent, I offer up for perusal a thread I am not proud of: [my first flame war](http://programming.reddit.com/info/12grg/comments/c12k6b?context=5&amp;style=nested#c12k6b). So you see, I'm way over-budget on my 'you's :)&gt; a good C compiler should issue a warning since this is trivial to spot at compile time.

Well, let's see:

    $ cat &gt; memset_test.c
    #include &lt;stdlib.h&gt;
    #include &lt;string.h&gt;
    
    int
    main(void)
    {
        char foo[42];
        memset(foo, sizeof(foo), '\0'); // Will gcc spot this?
        exit(EXIT_SUCCESS);
    }
    $ gcc --version
    gcc (GCC) 4.0.3 (Debian 4.0.3-1)
    $ gcc -Wall -Wextra -std=c99 -o memset_test memset_test.c
    $
    $ splint memset_test.c
    Splint 3.1.1 --- 23 Apr 2004
    
    memset_test.c: (in function main)
    memset_test.c:8:23: Function memset expects arg 2 to be int gets size_t:
                           sizeof((foo))
      To allow arbitrary integral types to match any integral type, use +matchanyintegral.
    memset_test.c:8:30: Function memset expects arg 3 to be size_t gets char: '\0'
      Types are incompatible. (Use -type to inhibit warning)
    
    Finished checking --- 2 code warnings
    $

Summary: gcc won't see the problem (which is logical, because it is perfectly legal C code; size_t is nothing but a typedef) and Splint will.

Valgrind won't see the problem either because it's not a run-time error (it's a misuse of a library function with no side effect).
No, but hearing all these constant complaints about monadic I/O is getting kind of tiring. People who complain about it haven't looked at a single Haskell tutorial, they just hear the word "monad" and read that Haskell is pure so assume I/O must be hard. Then they go and post dozens of blog entries about it instead of learning.&gt;And sure, Goto is kind of like Href, but the major difference is that the web is all about Location, while code is not. In that sense, Href is not harmful, much unlike Goto.

I agree _if_ we're talking about the web as a set of resources that you just go and view.  However, when talking about web applications where the actual location of, say, the checkout step 2 page isn't so much the issue as where it fits into the flow of the web application, and it's in those cases that (from my own rather limited understanding) the Seaside folk think Href is harmful.Nice use of legoThe comparison between Rails and Seaside seemed to assume you already knew what the important differences of the two were and which one was better...

You have to actually back your metaphors up... You can't just say "Rails is a speed limit" and move on.

Similarly, you can't simply state "Abelton Live is awesome because it doesn't have to track session, just like Seaside"... and then move on and expect anyone to care.In addition to that, I consider shared state to be a pro rather than a con. The key is in not sharing by default (much in the same way as you do not access an object's private members) and building higher level synchronization abstractions out of the fiddly lower level ones.

I'm partial to object queues myself; thread A finishes with the object, sticks it on the queue and throws away its reference. Thread B, C, or D pick up the object from the other end and do whatever with it. All synchronization is handled in the reusable queue class. Easy.Keep in mind that 99% of China is dirt poor and nearly pre-industrial in technological terms.  Add to that the refreshing flavor of communism which tears through anything academic like agent orange through a forest, and I don't think China is in much of a position to become an intellectual player.  Manufacturing, sure.  But invention and innovation?  Not any time soon.mynameishere's example is nonsensical. You don't stick order-dependent actions in the non-determinism monad and expect to get the correct answer back. Such actions cannot be parallelized. My object queue example elsewhere in the thread ensures correct ordering while allowing for parallelism among multiple invocations. Really, it's just serves as a sort of monad bind.You're not serious I hope. The code sample I made was a _sample_. In terms of real-world use, the add() method would perhaps be a useful dummy to test a thread framework, but nothing else. The real solution is a mutex, since part of my (upspoken) requirement is that the process is threaded.

...I'm trying to make an _obvious_ race condition without writing any real world code--which would, perhaps, fetch a resource from high-latency input like the keyboard, or the mouse, or a NIC.[deleted]This concept has been around since the 1960s. I remember seeing pictures of these same sorts of fluidic logic gates in a Popular Science article back in the sixties. This MIT student's designs are unoriginal and crude.MIT is not in Boston.
You've got a point there, but wizard-style multi-step actions are not exactly the norm on the web. And even there, the Href isn't as detrimental to the flow of a program as the Goto statement is. Using Goto a lot turns the program in to a fucking mess, without exceptions, using Href does not. Not even close. Saying it's *harmful* is stretching it, that's all I'm saying.See [my other comment](http://programming.reddit.com/info/14ofl/comments/c14rhj) on why your example is stupid, wrong.Obviously, the solution to the problem of "point and grunt" interfaces not being rich enough to capture the way people want to interact with their computers is to revert to an interface where you can only point at things.

Brilliant.

Next, let's take the keyboard away. It's so complicated, with all those buttons and stuff.&gt;He didn't say that I/O doesn't matter. He said it not the most important part of programming.

Actually, IO is the most important part of programming.  Just picture a program that does no IO; it effectively does nothing.  It could have the world's most beautiful algorithms, but it would all be for naught.  Even a program that only printed "Hello, world!" on the screen would by definition be more useful than a program that did nothing.You could also just call printf().Curious, RSI doesn't exist in this universe.&gt;Keep in mind that 99% of China is dirt poor and nearly pre-industrial in technological terms.

These days it's more like 75%. The remaining 25% is a population almost the size of the US. The east coast of China is an industrial machine that has no parallel anywhere in the world.in most every discussion of the dangers of buffer overruns in C/C++, you get the elitists who claim its not a problem because only stupid programmers make those errors.My example demostrates a race condition for the sake of demonstrating a race condition. That is absolutely clear, and if you are too obtuse to realize that, it isn't my problem.ok, you're stalking me now. Go away.PC-lint is the s**t. it finds all kinds of things and gimpel is constantly adding new stuff. no i don't work for them. i use it though.I was under the impression that NDAs would be used to surpress technical information prior to the launch of a product, so that support in the Linux kernel could be available on the day of the release.

The prediction of pseudo blobs appearing is a bit steep, as they are hardly likely to pass any sort of kernel Q-A check, let alone a moral one.But can you name a place where I can get it within 1 hour of whereever I happen to be?   I know I can get that stuff, but I can't get it today no matter how much I want it.   If it is a weekday I can get it tommorow - at extra cost - but if this is my hobby odds are I want on a saterday, and that means my project is put off until next weekend, all for want of a $.50 part.     A few incidences of that and I find a new hobby where I can get instant gratification.For an alternative, lighter approach, see here:

http://technomancy.us/article/emacsrails&gt;It isn't about being scared of something different, it is about crappy tutorials.

Soooo true.  I've was beating my head trying to wade through several Scala tutorials and couldn't figure out why I was having such a hard time with it.  It then dawned on me that their tutorials royally sucked.  To start with, why would you start a tutorial with a red-black tree or [quicksort](http://www.scala-lang.org/docu/files/ScalaByExample.pdf)?

Crappy tutorials lead to low adoption.  I struggled through learning Scala but it shouldn't have been a struggle.Is the video downloadable? Failed to work for me in vlc and mplayer.Your example demonstrates so many problems on so many levels that it's hard to keep track. It even fails to be an example of a race condition considering that your assertion of printing a value of 1 was not codified -- nondeterministic behavior is not wrong in and of itself. Basing any sort of argument off that code is stupid.[removed]I'm no squeaker, but my impression of squeak is that while it has a lot of functionality and most of it is stable, there are many rough edges, and the documentation is thin. With some experience one learns to avoid the rough edges. DabbleDB does run on Squeak, I believe. It can be a stable production platform if you know what you're doing.It was a joke. Of course, with a proper explanation this project could certainly give normal people some insight into the workings of computers.used to be you could roam around junkyards to your heart's content. i can't find one that allows that anymore (insurance is a common excuse..as is theft)...course that could be due to the interweb making circulation of 'junk' parts more lucrative, therefore more valuable and in need of protection.

neat resource. thanks.

personally i love having a reference to go look up things like this i run across in the 'real' world. since i don't (can't) keep that sort of intimate detail in my head. so i see reference to "fermet's..." and i'd like a ready reference to look it up before i forget. i use google and wiki mostly..but this will be a nice addition.none of that applies at all. the haskell designers "found that syntax design could not be not only fun, but an obsession". they also found that "syntax...could become very personal". nowhere do they claim that it's more important than the semantics (which, in the case of haskell, is supremely important but extremely well defined as a result of it's type structure and insistence on purity).Not that I know of the original link is here http://www.stanford.edu/class/ee380/ and it's copyrighted by Stanford.  SorryYA! MIT is in New Jersey.Cambridge ≈ BostonWhen did I claim syntax was more important than semantics? If I think that some have a "weird bias against syntax" (as I mentioned earlier), then it would be rather hypocritical for me to have one in the other direction.

As for the HOPL Haskell paper, I think it offers quite some evidence as to the real impacts of syntax, regardless of the desire of some to sweep the issue under the rug.This will make good bedtime reading! TYthread.destroy()I prefer refuctoring, myself.  There was a great [presentation](http://waterfall2006.com/Refuctoring.pdf) on it at [Waterfall 2006](http://waterfall2006.com/).Refunctoring means 'write software using functional programming techniques using your own reasoning, without the need to *change* your code all the time (instead, just produce the end result) without a functional language and with or without knowing or understanding why'.

Refactoring means 'change your code this way because the book/bible said it is good and you subscribe to the ideology typically, without knowing or understanding why'. Further, 'do not reflect on the fact that you're emulating functional programming'.If *everything* you do is IO, even in an  web-app, your work is pretty boring, my friend. Go use Rails, peasant, it cuts your work in half.[deleted]stupid. If you want to discourage spam, that's your right. Nobody has to respect the nofollow tag.. but they do... I wonder why?What practical languages did that? In fact, a key question becomes 'what makes them practical if they are fundamentally equivalent?'. Further, 'what makes others impractical?'.

The reason I ask is because I believe that by answering these questions, the term 'practical' becomes superficial and if anything, the inverse is found to be true. e.g. discovery that Java is *impractical*.[deleted]G'day Don, yeah pretty much though the documentation uses the term 'plinging'. You (and other Haskellers) might be interested in http://resources.businessobjects.com/labs/cal/cal_for_haskell_programmers.pdf[deleted][Yet Another Haskell Tutorial](http://en.wikibooks.org/wiki/Haskell/YAHT) and the [Haskell Wikibook](http://en.wikibooks.org/wiki/Haskell) are two good places to start.[deleted]At a basic level, what's wrong with thinking about the Internet as a series of tubes? It seems like quite a useful analogy in a few basic ways.

Edit: Oh sorry, if you were joking I didn't get it.It actually takes quite a while before you can start applying your knowledge of how to code in an imperative language. You sort of have to be comfortable with the entire way that computation is expressed first. (But it does happen, I'll give you that.).asx, such a pity :&lt;These dam scientists.. think they have the solution to everything.Nice idea, assuming you still want your right hand on the mouse.  

As for me, I'd love to see an entirely mouse-free GUI.They are not procedural or 'inherent state'. This distinction between the physical world and the abstract world is erroneous. You *can* pass a file system as a function argument and/or receive one as a return value. Laziness makes it perform well and the fact that *all* software solutions exist in the abstract world make it possible. You do not for example put a physical file system into a physical function. You put a *representation* of that file system into a representation of a function.

Let's look at a toned down version:
f(2)
Did you just pass a 2 to a function f? No, you passed a representation of the concept of 2, since 2 does not exist in the physical world - and, neither does f. Mathematicians with a philosophical bent refer to 'twoness' as opposed to two as a physical entity or some 'thing'. I think it is about time that we software developers did the same, or created a vocabulary where the distinction was implicitly understood.

The critical question is, *what's the difference between f(2) and g(fileSystem)?* Absolutely nothing, that's what. If there is absolutely no difference, then why is one considered 'state' or 'inherently procedural' (or whatever the term of the day is) while the other isn't?

A char* is *not* a pointer - it is a representation of a pointer, which will probably manifest in the physical world in some way as electrical currents or whatever (outside my domain).
&gt; Alex is a web developer and IT guru located in Montreal

He's a guru writing web sites in PHP? This joke is not funny.[deleted]Thank you for saving me from writing that, and commendations on extrapolating what was in my head from my terse reply. I'd sum it up thusly: when everything is functionally equivalent, our watchwords are style and speed.Wouldn't you love to know what some of the people who buy a tray of chips from a place like this are doing?

My imagination is going overdrive imagining some back street guys with reconditioned PCB machines cooking up something we're only dreaming about. &lt;/gibson&gt;
so, let me get this straight here.... the argument is that Massachusetts Institute of Technology is in New Jersey?   

You guys must be using these water computers.Pneumatic "computers" were used in the 60s by Nasa and others. Hydraulic/Fluidic ones as well. 

Old military article
http://stinet.dtic.mil/oai/oai?&amp;verb=getRecord&amp;metadataPrefix=html&amp;identifier=AD0610320
http://stinet.dtic.mil/oai/oai?&amp;verb=getRecord&amp;metadataPrefix=html&amp;identifier=AD0605509

These have been used in the past for performing simple operations with Pneumaticly powered robots, such as controlling arms. The Military interest in "Pneumatic" logic gates is that they are impervious to high radiation, and so could be used to help control robotic arms in such a environment. 

Hydralic computers have been built before as well, and used for controlling hydralic systems. It's not exactly something "new". But it is neat to see a hobbyist do it. I've seen pneumatic and hydralic logic gates before in old scientific encyclopedias. But I can't remember exactly where.

Hmmmm...

They're still being sold, researched and used:
http://www.elecdesign.com/Articles/Index.cfm?AD=1&amp;ArticleID=3954
http://en.wikipedia.org/wiki/Pneumatic
http://www.trnmag.com/Photos/2006/050806/Pneumatic%20logic%20Image.html
http://adsabs.harvard.edu/abs/1979PhDT........47KYeah, especially when FruityLoops is ten times more awesomeI get "object errors" in Adobe Reader 5. :(Note: ghc 6.6 will require the option -fallow-undecidable-instances in addition to -fglasgow-exts to allow this to compile.

I am wondering, perhaps we can come up with a unit of measurement for rate of type-system wizardry over time.  Assuming those 3 hours he spent were due to this, then this project took an average of 1/3 Olegs per hour.
not to mention disabled access is nil (such as blind users).I *have* done research.  It's not as clear-cut as you seem to think.

I never said you have a duty to provide an argument.  I'm just trying to point out that you shouldn't be surprised if your declarations are ignored when they consist of nothing more than "you're wrong".&gt; You imply he's incapable of argument. How is that not an attack on his (I assume) character?

1. You infer meaning.  If and when I want to get the impression across that someone is incapable of something, I'm more clear about it.  Assume good faith, or prepare to be told you jumped to a conclusion.

2. That would be an attack on competence, not character.

&gt; If he was making an argument from authority, there better be an argument from that authority. There is nothing but a simple question from sigfpe.

What does what *sigfpe* said have to do with whether or not *pjdelport*'s comment fits the description of an argument from authority fallacy?  He implies that by my reference to *sigfpe* I undermine my argument, because *sigfpe* is an authority.  The obvious interpretation is simple: by making statements with which *pjdelport* disagrees that question the perspective of *sigfpe*, I mark myself as "wrong".

An argument from authority fallacy is not an argument made by the authority -- it's an argument substituting reference to authority for actual logic.*Python programmers at Google must follow a strict style guideline (based on PEP8 with 2 spaced indenting)*

Not sure about the two space comment for indents since PEP8 says 4 spaces. :P

*Someone asked why Google even used Java at all? Greg appeared to bite his tongue and then said that there are a lot of good java programmers out there and Google hires a few of them.*

Come now, Java isn't THAT bad. For what it is, a general use language that protects the user from themselves, it is okay. Hell, throw out the J2EE stuff and other ridiculous design patterns and it can be quite useable.
&gt;Combine cheap knowledge and cheap materials

It's cheap labor and material forced down in value (pegged unrealistically low on the U.S. dollar).[deleted]I guess Valgrind might catch it as a read of uninitialised data or a write to unallocated memory, if you're lucky.I did read your second blog post on the subject. I understand how _you_ define syntax and semantics. I acknowledge that given those definitions your original post is logical. But by using unconventional definitions (without warning us) you create unnecessary confusion.

You're free to ignore whatever I write, like everybody else. I for one appreciate it when people tell me I'm wrong, even if the message is just 'No.', or 'You're wrong'. For me, that's enough incentive to do some more research or ask for elaboration if I can't figure it out myself.I saw a (sizeof(type)*n+1) last week.......and after that, quantum computation by duct-taping Schrödinger's cats together.Can the people who never took the time to learn more than one language stop commenting on 'practicality' of things they don't know about?Thanks for bringing that up.

I think the statement about the "very personal" nature of syntax could be understood in two different ways, however.  On one hand, someone (like *querulous*) could see it as meaning that it's subjective and, by some form of extension, superficial.  On the other hand, someone (like me) could see it as an indication that it has a very personal effect on the programmer: it affects the way we think about programming (which has been my point all along).  Syntax is (I believe, though I haven't really played with Haskell so I'm just generalizing from what I know of other languages) probably part of what makes Haskell "mind-bending", as I've heard it described so many times.  The fact the Haskell designers have been so painstaking and attentive to the task of designing the language's syntax seems to suit that impression, and reinforce my beliefs about its importance.

I am, however, softening someone on my position of how it relates to semantics, mostly for reasons unrelated to the comments made by others here on reddit (though there have been a couple that said something interesting on the subject and got me thinking about the opposite perspective).  In other words, I think I'm coming around to your point of view -- that both are pretty much on equivalent footing, though very different animals.Precisely.

Not understanding your Linux system is what is making it "insecure."You said:
&gt; The particular notation doesn't affect what a program does, hence it is superficial. Syntax is superficial.

*tayssir* quoted:
&gt; Many programming language researchers considered syntax to be the trivial part of language design, and semantics to be “where the action was.”

That seems pretty relevant -- not like it doesn't apply at all, as you suggest.I kinda expected a Goatse guy...I still don't get it.

&gt; IO doesn't matter

That's about as meaningful as:

"Life doesn't matter."

"Why should I quit smoking, I'll die anyway."

"Nobody uses my programs, so why should I care about IO?"

Perhaps it's time for a Nihilism subreddit, so we don't get such submissions on the programming subreddit? Is Haskell Java for academics, because every problem that is easy to solve in imperative languages provides plenty of material for research papers?

Regarding newbies, `interact` works until you want to read a file, write to stderr, connect to a database, show a dialog box, write to a log file, kick off another program, ... Proposing `interact` as a solution to a Haskell newbie who has problems with IO is a polite way of saying "Fuck you".

Perhaps it's time for a reality check?

Apparently many Redditers upvoted the article. Would be interesting to know why. To expose the stupidity? Or is Reddit dominated by ignorant academics? Nihilists? Upvote reflex of Haskell advocates?

Note: This is not a Haskell-the-language rant, this is a Haskell-advocacy rant.

_Edit_: Just one more thought :-)

&gt; Haskell weltanschauung

Is Haskell a religion and "IO doesn't matter" a dogma? Hmm, that would explain a lot.Upvoted for making something approaching a salient point, even if you couldn't resist trying to insult me with comments like "embarassingly[sic]".

OCaml is an interesting example.  What I find, though, is that its varying syntaxes are not as different, from one to the next, as you seem to imply.  Just as making minor alterations to a language's semantics is not as difficult as fundamentally redefining it, so to is it easier to make minor, superficial changes to a syntax than to restructure it.

I also find OCaml to be an interesting choice, considering it has no formal language specification at all.  It seems odd to use that as your example while talking about how "pretty much every language has a formal syntactic definition".  I don't disagree with the statement -- but then, I haven't actually studied the *actual* language specifications of too many languages, so I really don't know what "pretty much every language" has.Your example sounds like message-passing to me.

Also, if your queue is shared by B,C, and D, then you have to deal with locking it while extracting objects or you get the same behaviour mynameishere is explicitly showing.

I've seen plenty of 'real' code that is similar to this example, just not as simplified.  
OK, that was just awful.

It's as if I were on a website just endlessly clicking the mouse button.  If I should accidentally move my mouse over something on the way to the intended target, I'm screwed.

Imagine how tricky it would be to get all the way up to a menu or down a blog page trying to avoid every possible thing you could click.

Right now, I just noticed my mouse is sitting over the "help" button below this box.  Thank god it's not set for onMouseOversuch as eye controled? they did something like that in a couple of ways. surprised apple hasn't yet tried to implement something of that sort
hehe...no right mouse button - "Blink different"Actually, the program would still be an implementation of Unix's `true` command. Try *not* writing something useful in C!&gt; The implication
&gt; 
&gt; for every X. multi-threaded(X) =&gt; non-deterministic(X)
&gt; 
&gt; where "non-deterministic" is with regard to program inputs and outputs, does not hold. 

Right.  That's why i specified "to the extent", and "shared-memory code".

&gt; There exist other forms of non-determinism, which are neither unsafe nor unsound.

Right, as i pointed out in my first response.

&gt; [...remaining examples...]

Absolutely.  We appear to be in violent agreement. :)Interesting, yes.  
Useful, no.

When i saw the convoluted "button activation through mouse gestures" tutorial, i laughed and closed the page.It's a neat toy example, but keep in mind that they did this same toy example in Lisp in the 60's.  No joke.Wow.  Someone's cleaned it up a lot since I was last there...&gt; such as eye controled? 

How 'bout good-ol' "keyboard-controlled"?Does this mean that Macs are going to go from 1-button to no-button?The essence of "randomness" is "unpredictability".

"Random" strikes me as pretty applicable to the _human_ experience with this stuff, even though the code is, of course, completely deterministic and predictable to the computer. A few pixels off, and _wham_, something "random" is happening.I define syntax and semantics pretty much the way the dictionary does.  I think you're missing the point of the follow-up post, if all you see is a difference in definition.

The definition isn't the reason for the different perspective.  In fact, in the previous reddit discussion, I very specifically linked to definitions in a dictionary.  It doesn't really get any more standardized than that.  The difference in perspective (which I thought was pretty clear, but apparently not -- at least to someone who starts out disagreeing with me, and is thus inclined to see things I say in a disagreeable light) might be more fully explained by making a further statement about how syntax and semantics affect the way we program:

*The semantics of a language* can limit what we can do.  This can lead to interesting effects, as programmers attempt to hack their way around the language's limitations.  One might, conversely, say that a language increases or changes what a programmer can do by providing avenues of achieving a given programming goal that do not exist in another language.  Once one becomes accustomed to these semantic differences, however, the increase in power a programmer felt moving to the new language begins to feel like a limitation when using the old language again -- so I think the characterization of the semantics of a language as limiting the programmer is more appropriate, assuming equal knowledge of all languages.  Thus, the most semantically "powerful" language is the one whose semantics limits the programmer least.

*The syntax of a language* changes the way you use the language's semantics.  It affects the very structure of your thoughts about what you do with the language's semantics to achieve your programming goals.  It also, to a fair degree, enforces certain semantic effects -- such as operator precedence.  With a prefix notation, for an example that brings this full circle, the operator precedence of infix notation is thrown out the window: it is obsolete.  Operator precedence is semantic, and its necessity is predicated upon notational concerns such as whether you use prefix or infix notation.  So, too, is the (semantic) necessity of using a grouping mechanism (typically parentheses) to alter precedence in an operation largely a function of which (syntactic) notation you use.  Ultimately, whether you use prefix or infix notation answers the question of which semantic convention of nesting or chaining operations you end up using.  This one "superficial" alteration completely changes the way a programmer thinks about composing a complex arithmetic statement.

I hope the example of prefix notation, in this light, helps to explain whence my perspective came -- and that one need not alter the definitions of syntax and semantics from those to which you seem to subscribe to arrive at that different perspective.  Keep in mind that "superficial" is a description, not a definition, and is thus part of the perspective rather than the definition.

&gt; I for one appreciate it when people tell me I'm wrong, even if the message is just 'No.', or 'You're wrong'.

I don't find mere mechanical contradiction very valuable, particularly when my opinion is not just magicked out of the air -- but is, instead, actually based on the sort of research you seem to think I needed to do to come around to the opposing point of view.  In any case, I did hunt around for more information on the subject, and what I found did not invalidate my position at all.

A couple of the less facile comments made in these discussions have prompted me to see another perspective, however.My preferred method of navigation is to right-click a link and open it in an adjoining browser tab.  I can't do that with this site's navigational model.

Anticipating the next question, no, I don't want to configure my browser to automatically open all links in a new tab.  Although I less often open links in an existing tab, I do so often enough that I don't want "open in a new tab" to be my default.

In other words, clicking isn't a "problem" I need to solve, but rather a way of retaining control over my user experience.This is what I expect from Web 3.0: build my webapps just like I build my desktop apps. It would only be a matter of selecting what target I want to compile: "Build for Win32", "Build for Mac OS X", "Build for Linux", "Build for web (Rich Internet Application)"

More here: http://www.elpauer.org/index.php?p=212
The funny thing is that the guy adds nofollow on his blog. Go to http://blogs.concedere.net:8080/blog/discipline/links/?permalink=Links-01-24-2007.html&amp;page=comments and view the source ;)Middle-click baby, middle-click...&gt; Refactoring means 'change your code this way because the book/bible said it is good

I'm reading the so called "Refactoring Bible" as you call it and I see a lot of good ideas that any middle-level coder can understand. Either you're really too stupid for computers, or you haven't read the book.Great -- tell me, please, how one would middle click something on this post's clickless page?

The very point (no pun intended) is that one no longer has any such choices.  It's all mouseovers...[removed]&gt; Refunctoring means 'write software using functional programming techniques using your own reasoning, without the need to change your code all the time (instead, just produce the end result) 

Okeh..  you must be in high demand.

what kind of games does it haveI enjoyed the fact that I had to click to activate the embedded Flash object which contains the whole site. (Thanks to NoScript.)

Not that the author could do anything about it, of course.[deleted]don't use C++Right.  For what it's worth, what you're suggesting is generally [classified](http://en.wikipedia.org/wiki/Concurrent_computing#Concurrent_interaction_and_communication) as message passing, though, not shared memory/state.I've been to SEG.  These markets of comparable sizes are dime a dozen in every major Chinese city, and they're all short bike trips away if you live in the urban area; There are three such buildings right next to each other outside the university where I stayed.


Retail consumer components are only slightly cheaper than in the west, but the domestic made raw components are dirt cheap.  The best thing is that the selection is huge, and specialized sellers are located next to each other to compete for customers.


It's not surprising that these places exist in a culture where geeks have more sexual appeal than jocks.read the last post to the article. the author recants most of his analysis.Yes, I know Haskell, OCaml or Scheme are real languages for functional programming, but sometimes it is just not an option to use an esoteric language.

C++ is multi-paradigm, supports some generic programming and there are template metaprogramming libraries that claim to make functional programming possible. Two examples are [Boost.Lambda](http://www.boost.org/libs/lambda/) and [FC++](http://www-static.cc.gatech.edu/~yannis/fc++/).

Is anyone using C++ in this way, and does it work?Right, in addition to the Lisp-like Coke (a.k.a. Jolt), they also have the Smalltalk-like Pepsi, and a baby version of Javascript.  Plus a cut-down but complete version of Squeak.  And it looks like Alan Kay is leaning toward a graphical programming language using tiles.

Go [up a level](http://piumarta.com/pepsi/) to see all their goodies, and also have a look at [the papers](http://piumarta.com/papers/).&gt; What practical languages did that?

I was thinking of things like Java, Python, Ruby, (attained greater popularity later and are not 'pure') and so on compared with Lisp and Smalltalk (been around for quite some time, very elegant and pure).

I'm curious if the same thing will happen with functional languages.There's probably a heap of very clever and completely unreadable hacks involving templates and obscure semantics rules for this!
&gt; I tried to restrain myself to arguing the core point in this now-extended thread: that semantics isn't just naming*.

You did manage to get some of your point across, which got me thinking, so even your divergence from the core point didn't really get in the way much.  By the way, I agree that semantics isn't just naming -- I think I may have employed a little more implied hyperbole in previous statements than was really necessary, and as such may have given an incorrect impression of my thoughts on what constitutes the sum total of semantic characteristics of programming languages.

&gt; making hypotheses about where others may have gone wrong

I like to think about the way people think.  I don't see anything wrong with that.

&gt; It's irrelevant who was right or wrong, or who was learning from whom; you were calling ignorant people whose comments I have come to value in this forum.

The term "ignorance", as I've already mentioned, was not (the one time I used it) intended in a pejorative sense.  In retrospect, it may have been a hasty conclusion, however.  The problem may in at least some cases have been that people were utterly failing to display their knowledge, not that they were displaying ignorance.  From the outside, however, the two are almost indistinguishable -- and the latter is a far more common phenomenon, so that's what naturally arose as my impression.  If you knew nothing of the subject matter at hand, you might be more inclined to agree with my early assessement, because I was explaining my position and everyone else just seemed to be unwilling or unable to actually address the points of those explanations with rebuttals more substantive than "nuh-uh".

&gt; That then is my version of events.

Thanks.  It's pretty clear at this point, and it makes sense from your perspective.  Hopefully, you see mine as well.

&gt; Making hypotheses about where others may have gone wrong is not a bad idea in general, but in my experience it seems to only distract an internet conversation.

That's why I didn't fling those statements about as accusations here at reddit.  In fact, I was quite surprised to find out that someone had submitted my weblog entry at reddit at all.  I meant to avoid the distraction in the actual discussion because I kept hoping that someone would chime in with something substantive, or that people who had nothing substantive to say would stop talking about it.

&gt; Nobody called you names in the previous thread.

Did I say someone called me names?  I honestly don't recall making that statement.  If you point me at my statement to that effect, I'm sure I can explain why I said so (even if the explanation is "I hit Reply before I thought about what I was saying").

&gt; The only real idiocy lies in being sure one is right. I don't think anybody was doing that in this thread, it was simply a misunderstanding.

I, too, am increasingly of that opinion.  It's an interesting challenge to see others' points of view when they say nothing other than that something must be so, however, so it has been "fun" trying to see opposing perspectives.  I think I actually, in the long run, did a fantastic job of seeing other perspectives, considering nobody was really willing to offer any hints on what those perspectives really were for a significant chunk of the discussion.

&gt; 'Educating you' sounds too presumptuous, but my subconscious keeps surprising me by its presumption, so I've resigned myself :)

I guess it depends on what connotative meaning you assign to use of the term "educating".  I think many of us at reddit have a lot to teach each other, and I'm more than willing to be a part of that.

&gt; So if you would care to reply to the meat of my post, we will say both (all?) of us unintentionally insulted the other without due cause and move on. How about that?

That sounds excellent.  I'll give your previous statement another look to help organize my thoughts, and see what specifics I have to offer beyond what I already said at [SOB](http://sob.apotheon.org).

&gt; Naming isn't as superficial either as most of us assume, but that's another story.

I'm curious what you mean by that.  It strikes me as very superficial, in and of itself -- though concerns such as etymology and implied networks of relationship between terms do add depth.  I'm absolutely of the opinion that a good, intelligent naming convention is very important, but not that this importance makes it any less "superficial".To expand on this a little, Pygments highlights:

Boo, Befunge, C, C++, C#, Delphi, Dylan, Java, JavaScript, Lua, OCaml (*I wrote this myself last week*), PHP, Perl, Python (incl. console sessions and tracebacks), Ruby (incl. irb sessions), Scheme, Visual Basic.NET, Django/Jinja templates, ERB (Ruby templating), Genshi (the Trac template language), Myghty (the HTML::Mason based framework), Mako (the Myghty successor), Smarty templates (PHP templating), JSP (Java Server Pages), Apache config files, Bash shell scripts, BBCode, CSS, Diff files, Groff markup, HTML, INI-style config files, IRC logs (irssi style), Makefiles, MoinMoin/Trac Wiki markup, ReST, SQL, TeX, Windows batch files, and XML.

(http://pygments.org/languages/)My worry is that advertisers while start opening pages when you roll over a banner ad.A single [buttered cat](http://en.wikipedia.org/wiki/Buttered_cat_paradox) seems a more efficient use of cats...albeit a less efficient use of buttery toast. Maybe some sort of atomic buttery toasted cat...I appreciate you took the time to explain your position. Unfortunately, your definitions still seem a bit off.

The semantics of a language start when walking the Abstract Syntax Tree. Everything you do in order to construct the AST is nothing more than syntax. You can speak of the semantics of a programming language without any formal syntax whatsoever. Ambiguous code? No problem. Lack of braces/brackets? Doesn't matter.

Operator precedence is a hack used to construct a tree from code without forcing the programmer to use a lot of braces. This has nothing to do with meaning, the semantics. For exactly the same reason that 3 + 4 and ((3) + 4) are semantically the same (asuming ordinary math): the same code tree is formed. By abstracting over syntactic matters you can get to the meat of the program: what it does.

&gt; Two programs are semantically equivalent if and only if
&gt; they produce the same output for the same input, for any input.
(Emphasis)

The formal semantics allow one to prove this for any program. They allow you to prove that {x = a} x += 10; x++; {x = a + 11} holds, if formal semantics for addition is specified.

* Operator precedence is an artifact of syntax. A program with sufficient brackets makes operator precedence redundant without affecting the semantics of the program.


&gt; "Semantics" refers basically to the superficial aspects of
&gt; language design, like "Are we going to call our function 
&gt; constructor fun or defun?"

So this is syntax, even if fun and defun are keywords. The concept of function construction has semantics, but those semantics are invariant over the name of the keyword. It does not affect the functioning of the program (for any program, for any input), therefore it cannot possibly have semantical meaning.

&gt; "Syntax" refers to the form the language takes, the kind
&gt; of thinking the (successful) programmer will do, ...

Yes, but the semantics are at least as important. E.g. what is the scope of this variable? Or, can I swap these two statements without messing up the code? Refactoring is all about maintaining semantics while improving the syntax (how it looks). If refactoring involves making extra abstractions, it can have both semantic and syntactic meaning.

&gt; ... and how programs fit together.

How programs fit together is a purely semantical issue. Programs written in any two languages can be made to fit together, even though the syntax may be completely orthogonal.

Still contradicting, but less mechanically this time.* You can add/change methods to built-in objects on the fly.

* You can add/methods to the base object on the fly.

* Anonymous code blocks/closures are practically first class objects.

* Creating a Domain Specific Language (DSL) inside Ruby is an sanctioned and frequently used technique.

Ruby programmers relish these dynamisms.  Pythonistas do not.  (I am a Pythonista)

http://www.treelight.com/software/ruby/RubyRocks.html

You really had a problem with people claiming Ruby was more dynamic, but not giving specifics?  It is difficult to keep Ruby programmers quiet about Ruby's more dynamic nature.I don't know what the downvote is for.  I'm not making this stuff up.  They even build a symbolic differentiator in [SICP](http://en.wikipedia.org/wiki/Structure_and_Interpretation_of_Computer_Programs), in chapter 2!  And that book was written in 1984 for intro-level CS.  Like I said, it's neat to see it in Haskell, but we aren't talking about rocket surgery here.A pointless experiment that far too much effort has been ploughed into. Absolutely appalling for anyone that multitasks. As someone that routinely has several windows open across multiple monitors, all doing different tasks, the idea of losing control of exactly when the interface responds to my direction is frankly insane. 

The fact that the interface reacts as you are moving between points of interest discourages user interaction and makes it effectively useless. I can't, for example, relocate the window and resize it without losing the content I was trying to read.With a reliable supply of alcohol.So, what he should do now is build a whole processor out of this, and then port a webserver to it.  Then, when it gets posted to reddit/slashdot/digg, the water will boil and the whole computer will (at least somewhat literally) go up in smoke!I like the message passing abstraction, but I don't like the loss of flexibility, hence the combination. Also, the object passing is pass-by-reference since memory is shared.&gt; sometimes it is just not an option to use an esoteric language.

Don't give up hope too soon:  there's [Unlikely Scheme](http://marijn.haverbeke.nl/unlikely/), at least.Your discussion of "swappable" syntax with a (mostly) static semantics is a significant chunk of the reason I started rethinking my position -- thanks.  I don't think it really stands up as an argument that semantics is less "superficial" than syntax, however -- just that it appears to put semantics on equal footing with syntax.  After all, similar syntax with different semantics is just as possible as similar semantics with different syntax.

One might make the argument that replacing the semantics while carrying the general form of the syntax through creates a new language, but I'd say that the opposite is true, too: changing the syntax while mostly keeping the semantics the same also produces a new language.  A language wherein you can use two different, equivalent syntaxes for the same operation does not in fact change its syntax -- it has a single syntax that encompasses both.

The idea of a "hot-swappable" syntax vs. a "hot-swappable" semantics, on the other hand, requires a different approach to viewing its relevance to the matter at hand.  Frankly, I haven't seen anything indicating that the greater difficulty, at present, in a "hot-swappable" semantics is other than one of the following two problems:

1. an implementation detail -- it's possible that semantics is more difficult to "hot-swap" merely because language implementations are not architecturally composed in a manner that facilitates it

2. a matter of priorities -- more has been achieved in "hot-swappable" syntax because it is (or at least has been regarded as) more useful than a "hot-swappable" semantics

Even if neither holds true, and semantics really is just a more intrinsically difficult thing to "hot-swap", that doesn't necessarily make the point that I was wrong (even if it makes me think that the truth is closer to a middle ground than I'd initially guessed).  Whether or not it's easy to alter the language (or language implementation) in that manner is not necessarily relevant at all to the question of which -- semantics or syntax -- has the more profound effect on the way we write, and think about, code in a given language.

&gt; Functional programming has its semantic roots in the rewriting-based lambda calculus, while imperative programming has its roots in the operational notion of a turing machine. The difference between these is semantics, and it's a deep difference.

There are other concerns of programming linguistics than semantics and syntax, and from where I'm sitting it looks like these computational models lie outside of both semantics and syntax.  Semantics and syntax just manifest the formalized manner of defining something according to the computational model's principles; neither encompasses the computational model within its definition.&gt; Naming isn't as superficial either as most of us assume, but that's another story.

I think he refers to binding (e.g. lexical, dynamic). A purely semantical issue that has fascinating implications for naming (bindings of variables)."The idea of the project was to build some devices that could do computation without eletrons."

But water contains electrons! ;-)[removed]The comment by wtfol is also relevant because it indicates that all this "RPython" and "Python implemented in Python" stuff may be a couple orders of magnitude faster if implemented in Lisp.  In other words, Lisp compilers give many if not all of the features of a stripped down Python interpreter, so all religious discussions (i.e. meaningless discussions) of syntax aside, if "Python speed" is the goal it may just be easiest to implement some macros to parse the Python syntax into Lisp.  Of course all the existing PyPy code could serve as a "library" extending the core language and critical functions translated by macros.No, you can't remove the keyboard, its not simple enough to use. We only need to simplify using things that are already at their simplest, that way it can become more complicated during the simplification proccess.

At least it shows that one would not want to get rid of the mouse button. (And windows users complained about mac having only one button, ha)[deleted]I think we agree on what the terms mean but in your comment, you said..

&gt; "Semantics" refers basically to the superficial aspects of 
&gt; language design

and 

&gt; "Syntax" refers to the form the language takes, the kind 
&gt; of thinking the (successful) programmer will do, and how 
&gt; programs fit together.

Isn't this just a typo?
"Find the leak" and "mop the floor".there are [ways around it](http://www.amarasoftware.com/flash-problem.htm)Of course it needs locking. The point is that you implement a higher level primitive like this once, then use it instead of having to deal with mutexes all over the place.You seem to have no compuction about commenting on people that you don't know about.Make sure you compare apples to apples.  Making a symbolic differentiator of data is easy.  Making one for compiled code requires trickery. :)
The ones I have seen in LISP have all been for data, but with dynamic binding of the arithmetic operators I guess you might be able to do something for compiled code, unless you have open-coded the primitives.Thanks, yes your perspective makes a lot of sense as well.

You're right that others weren't giving reasons for disagreeing, but I think that was just because it seemed minor. They weren't violently disagreeing with you, so perhaps they didn't see a need for explanation. I can totally see how they would assume it was a typo. Pursuing the source of disagreement to try to understand it, rather than letting it go, was a good thing. Next time just ask them why they disagree :)

As for the name calling, that was mostly the word 'ignorance'. I think that's a pretty pejorative word, but it's cool if you didn't intend it that way.The more you repeat "IO is hard in Haskell" the more true it seems to become.
Never in my life have I so desperately wanted to click. I had to scurry back to Reddit just so I could relieve my tension by clicking on the upmod button.very, very well done site; really. But all in all: pointless.The biggest problem with this sort of interface is that you have to be very careful about the route you take to reach a link with the cursor, so you don't accidentally end up opening a hundred things you didn't want. Naturally, any sort of interface requiring you to actually *map out* your movements is going to be really counterproductive.What an amazing wad of suck.

Why do people so often try to reinvent the wheel to show how clever they are? And why do they then make it a hexagon this time to be original? 

Truly smart people realize that solved problems are solved. Clicking works. There's no problem with it. Use your time and energy on problems that aren't solved. Like search ranking. Or unified security credentials management. Or loops in reference-counting garbage collection. 

Oh, I'm sorry, those are real problems, and you're just some jerk who knows flash. Carry on, then.Why is Lisp any more 'pure' than Ruby? Why is Python less 'pure' than Smalltalk? What does 'pure' mean? Do you even know half the languages you talk about?Because I'm getting sick of people who think that reading a blog article about some programming language makes them an expert. Then they write more blog articles, and more idiots read these blogs and continue spreading misinformation, in the meantime, none of the people involved actually bothered to check their facts.Ruby on Rails is not a programming language. I keep seeing it being labeled as one, and it's kind of annoying.Actually, you know that spam showVal returns a function because otherwise it wouldn't type-check with the (.) operator.

    (unwords . spam showVal)

You know that the rhs has to be a function.  If it wasn't, it would have to be written

    (unwords . spam) showVal

or more idiomatically

    unwords . spam $ showVal

The obsession with currying demonstrates a complete lack of understanding of where the concept comes from.  Currying isn't something that is added to the language -- it is a natural consequence that arises from the language.  Functions in Haskell only accept a single parameter,  period.  The illusion of multi-parameter functions can be constructed via either 1. tuples or 2. higher-order functions.  The latter is often termed "currying" when combined with syntax which makes function application left-associative.


Interesting is just another word for dumb. 

Cheap tricks galore, but not functional for a real web site.

I felt cheated about the poll.  I was trying out the interface for maybe 30 seconds when it interrupted me with the "do I miss clicking?" question, and at the time, I thought, "well, this isn't horrible yet, and I don't feel the urge to click yet," so I voted "no."

About 30 seconds later, I hated the interface and wanted clicks back and a way to change my vote.

As others are hinting/saying... the "freedom" of not having to click means tight restrictions on how you move the mouse.  Usually, after a click, I move the mouse slightly away, either to read something or to anticipate another click.  This interface forced me to not move after activating something.STOP. You don't "throw" anything out when you learn a new language; you simply learn new skills which complement existing ones."Known bug: Dividing two very big numbers causes the interpreter to hang on a Mac."

Heh. Perhaps they should just use GMP or the Scheme48 bignum implementation.Judging by what you've said here, I think the real point of dispute regarding definitions is in definitions of terms like "superficial", "fundamental", and so on.  This, again, hearkens back to the comments I made about perspective: you're discussing language design when using these terms, in and of itself -- I was talking about language design as it relates to the way it affects the "art" of programming.

For instance, whether a programmer defined function can yield side effects is more a matter of semantics than syntax (though both are involved, semantics is far more directly involved than syntax, I think).  In terms of thinking about programming, however, this is a very easy concept to grasp, and (for me at least) a very easy adjustment to make -- when I started playing with functional languages for the first time, I read about the notion of avoiding persistent state via variables and interconnected program logic via side effects, and it just clicked.  I had already written functions and larger routines that had this behavior, often as part of programs that also used variables and side effects: all I had to do is change my approach to programming enough to expand that part of the approach to programming I already used.

That's not trivial, of course -- not by a long shot.  On the other hand, it didn't have as much of an effect on the way I thought about programming as the syntactic requirements of prefix notation functional languages.  The ability (though some might advise against it) to use nested functions via prefix notation as a form of computational scoping to write an entire program of arbitrary size and complexity as a single complex operation had by far the more interesting effect on my thinking as a programmer -- even when doing non-functional programming.  My Perl programs with side effects are cleaner and more elegant as a result of the thinking that fostered, for instance.

Just as a lack of side effects draws at least in part on syntax to implement in a language, however, so too does the "nested scope" effect of prefix notation draw at least in part on semantics.  The point of delineation between semantics and syntax is pretty fuzzy in attempting to measure the effects of either on how we think about programming, and getting fuzzier the more I think about it.

&gt; A program with sufficient brackets makes operator precedence redundant without affecting the semantics of the program.

On the other hand, judicious use of parentheses in an infix notation complex operation effectively alters the semantics of the operators.  The parenthetical point about infix operators can go either way.

&gt; So this is syntax, even if fun and defun are keywords.

Not necessarily.  You get a syntax error if you use the wrong one of "fun" and "defun" (unless the language has both), but it's not because the choice of term is a syntactic concern.  You get the error because the programming language implementation expects a particular syntactic element at a given point of the program: if it finds something else there instead, it pukes.  The same is true of putting a plus sign where "fun" is required, or "printf" where "defun" is required.

The choice of "fun" or "defun" is, as far as I can see, either semantics, or neither semantics nor syntax, depending on whether you are willing to regard a reversal of the direction of semantic definition as also part of semantics.  I have looked at it this way:

If you use "fun" in language foo, you are telling the parser that you want the collection of effects that define "fun" to occur.  That is semantics.  Similarly, if you want that collection of effects to occur, you use the term "fun" (to approach from the opposite direction).  That, too, is semantics.  It's just a difference of perspective on the process of connecting the term with the meaning -- one in terms of how you decide to design your language's semantics, and the other in terms of what term (or collection of terms) the programmer needs to use to achieve the desired program behavior.  In the former case, you're thinking about how you want a given program written in language foo to behave when you design the language.  In the latter case, you're looking up terms to achieve behavior.  Both are related to semantics.

By contrast, a flowchart for a program's architecture (if anyone even uses those any longer) may be entirely non-portable from language foo to language bar for "merely" syntactic reasons.

&gt; Yes, but the semantics are at least as important.

I'd say that semantics *may be&amp; as important, not that it is *at least* as important.  Clearly, however, we've been considering the matter with different perspectives on "importance".

&gt; what is the scope of this variable?

Both semantics and syntax come into play here.  Your means of syntactically representing scope determine the scope of a given variable, from the perspective of the programmer.  From the perspective of the language designer, a choice should be made about how many different types of scope that should be available to variables, and what those types should be -- which, again, looks to the programmer like the syntactic matter of how one represents variable scope, thus shaping the way the programmer thinks about programming in that language.  Ultimately, that's what the primary concern of a language designer (performance concerns aside) should be: how the programmer using his/her language will think about programming when using that language.

&gt; Refactoring is all about maintaining semantics while improving the syntax (how it looks).

I think you may want to think about your own statement here a bit more, and perhaps realize that refactoring is really pretty deep when it comes to how programs are written.  Assuming the same outward behavior of a program -- how a user interacts with it, and what it allows the user to do -- and assuming all else (like performance) is equal, the important part of language choice is the kind of programming it allows, and encourages, you to do.  Some of that is semantics, to be sure -- but a lot of it is syntax as well, and refactoring is about changing how you've written your program, about that very same important factor.  As such, your statement about refactoring (aside from the dismissive implication of the "how it looks" part) reads to me more like an endorsement of my position than a rebuttal.

&gt; How programs fit together is a purely semantical issue. Programs written in any two languages can be made to fit together, even though the syntax may be completely orthogonal.

Um . . . I was speaking of internally fitting together, not externally.  In specific, I was talking about how *a program* fits together, not how you could make two separate programs work together.To read from stdin? That's a feature of printf I haven't heard about.I almost never use the word "ignorance" pejoratively, and I think it's a shame that it has taken on connotative pejorative meaning in the minds of many.  There just isn't another word that really replaces its non-pejorative purpose, so I'm stuck with that and the potential misunderstandings that arise as a result.  I often forget to make disclaimers when appropriate about the intended meaning of ignorance -- but I also tend to feel like I'm going out of my way to accommodate the prejudices of others when I do so.

&gt; Pursuing the source of disagreement to try to understand it, rather than letting it go, was a good thing. Next time just ask them why they disagree :)

I was kinda doing so.  Kinda.  I guess I should have specifically asked in addition to trying to expand upon my own statements, however.  I just figured initially that there was a misunderstanding (I was right about that, after all) and tried to defuse it by presenting a more complete picture of what I meant.  I hoped others would reciprocate, if that didn't solve the problem.I don't think that's *all* (s)he meant, even if (s)he did (also) mean that.  Feel free to expand on that, though, as I'm not entirely sure I know what you're trying to say there.  At first, I thought I did, but then I realized that your statement could be variously interpreted -- so I decided to ask (in part on *akkartik*'s advice).It means it is based on PEP 8, and the only change is that it uses 2 space indent.  What other meaning could you take?&gt; Isn't this just a typo?

No.  See [this linked discussion](http://programming.reddit.com/info/14oi9/comments) for details -- particularly as regards statements by me, *akkartik*, and *fry*, in descending order of importance for understanding my point.Sounds like Google reinvents the wheel a lot (e.g. python build system, custom binary RPC).  I guess you can't blame them, because reinventing the search wheel certainly paid off.Nope. The Flash object was not deactivated by IE, but by NoScript (just as I want it to be).But it's death to the brainstorming/tinkering process that can lead to innovation.I don't think so. What about when you need to go further than what you get from `interact`? In other languages, the complexity doesn't suddenly jump when you go further than hello world. Perhaps it's easy to do non-trivial IO in Haskell, but the article sure doesn't say how.No matter what arguments I come up with, you can always come up with elaborate reasons to reinforce your beliefs.

Your definitions make the whole distinction between syntax and semantics meaningless because it's 'always a bit of both'. I'm going to stick with the offical definitions, the ones used in all the books written about the semantics of programming languages. (Yes, authority)

I'm not going to participate in this any longer: it will only aggravate me further. This post of yours shows the same misunderstanding as one before. It's probably my own fault, because I'm not able to succinctly explain where you go wrong and why, but clearly we're not getting anywhere.Oh, I'm perfectly capable of pretending it makes sense. I spent years in a [monkey cage](http://www.ibm.com/) doing exactly that.My remark was just clumsy. Please don't seek any meaning in it.[removed]Care to take up a challenge on that? Find me a so-called 'good idea'. I'll demonstrate it to be a bunch of bollocks with an appealing facade.

Either you're clever enough to figure out that you're wrong, or you're just the one of the predictable responses that is expected when the establishment is challenged.What about your left hand?

Sure, I'm right handed, but I mouse left handed.I know what they are.  The problem is that functionality like that belongs standardized in the browser, rather than as a feature of a single site that is forced on you.  The feature in no way relates to the core functions of the site, and is merely "bling".Is it cooled with electricity?I've already switched over to just making all of my GUI's web-based.Maybe we write different kinds of programs.  In the code I write, the IO part is perhaps 2% of the code.  So I'd say "I/O doesn't matter".  It's not the I/O that has the interesting code.  Of course, it has to do I/O, but it's a thin layer on top of what solves the problem.
Now, I happily admit that not all programs are like that.  Some of them are just about moving data.  Personally, I find them boring.Well, heck. I think i actually see the point and potential of GADT's now. That's impressive.

Still not sure where existential types come in, but one major new concept a day is probably a reasonable limit.[System.IO](http://haskell.org/ghc/docs/latest/html/libraries/base/System-IO.html)

And just for laughs, an example. Print in reverse the 11th line of a file:

    main = do
        s ← getContents
        putStrLn . reverse . head . drop 10 . lines $ s

The above docs explain the rest of the basic IO api. Its pretty much as you'd expect.I think the point is regarding navigating in general as opposed to this particular site. In firefox, middle-clicking a link automatically opens it in a new tab, which is simpler than your right-click-and-select method.Man, i bet he'd be steamed if that happened.main() {
        return *0;
    }&gt;Aside from the fact that you're left wondering which has higher precedence, "$" or "!!", this is an improvement. 

Um, no you are not. $ has lowest possible precedence.
This example of yours as others, commented already here, show that you DO NOT KNOW haskell. Not that haskell is confusing.
Haskell code is actually very clear and unambiguous, and is a pleasure to read, unlike code in java.

ok, you can stop filling my mailbox with this stalker shit now.Scary. I like my web consisting of documents not applications.Nvidia says Open Source developers are too stupid to understand 3D drivers, so the NDA-way won't convince them.

There may be specs, which can't be disclosed, because of third-party intellectual property. A NDA could be an elegant way around this.Since regular GHC does not support cross-compiling right now (correct if wrong), this is quite interesting for me.

Anyone found out how it interacts with regular Java-libraries?

I found [Scala](http://www.scala-lang.org/index.html) to be the most promising alternative language for the JVM, but this project could very well overtake Scala.Fair 'nuff.[deleted]&gt; No matter what arguments I come up with, you can always come up with elaborate reasons to reinforce your beliefs.

So . . . your argument is basically that I'm wrong because I keep coming up with reasons that I'm not wrong.  I'll have to remember that debate tactic next time I run for office.Don't be silly.What advantages did C++ bring that were not already in other languages?

What advantage did Java bring that was not already in another language?

I see no advantage that Java or C++ brought to the table that were not already there in another language.

Alan Kay is looking at the big picture.  Computers are not delivering as much as they could.  Sure they've brought tremendous advances but only a small part of what they could.  Alan wants computing utopia not the mess we have now.

Personally, I'm not sure utopia is what we need but a move towards it would be welcome.

IMHO, we have already had two environments that were better than the current status quo, the first was Lisp the second SmallTalk.

I think Alan will have another glorious failure, like most of his projects.  The reason being Alan himself, look what is going on with Croquet.

FWIW, I think the coming transition from Von Neuman machines will be the impetus that finally makes brain dead programmers finally realize what computing is about.  It's not about writing code but expressing thought.

Impressive, however, I'm pretty sure the work they are showing off there is really Ageia and Meqon's (later aquired by Ageia).  The wood breakage stuff in particular is ageia magic, and meqon is responsible for the character dynamics.

In a demo released by meqon, there were 'zombies' with procedural, physical animation.  You could chuck balls of various sizes and speeds at them, knocking them over, etc.  They'd try to block with their hands.  You could also lift them by a joint and they'd struggle about.  There was another portion of the demo with a biped dynamically attempting to break its fall.

I think his description of the AI is a bit over-glorified... Most of the work is likely done by the physics engine - fakey Inverse Kinematics using the physics.  With constraints this gives somewhat of an appearance of having 'muscles'.  I bet they aren't really controlled by the 'spinal cord' reffered to, unless by the spinal cord they mean reflexive action driver...

You'll notice that the stormtroopers are very weak.  They have surprisingly strong grip, but they can't seem manage to pull themselves up.  This is because they don't really have a brain, or any real urge of self preservation.  They just have the preprogrammed routine of grabbing onto stuff (using fakey IK which allows superhuman strength sometimes, and baby strength otherwise) when the fall is far.

Not saying it's not cool, but I don't like sensationalism in such matters.  And not giving credit where it's due "You'll only see this at Lucas Arts" - yeah, right.  I've already seen it in physics demos...Well I think it IS hard, although not because it requires category theory, just because of the pain I get from having to  handle the IO type...I guess it comes from a lack of understanding, but I still find it a pain.&gt;Function application
&gt;
&gt;I think "f(a, b)" more clearly portrays function application than "f a b" does.

Sounds like this joker's never used the bash shell.Pretty much the [same thing](http://sleepingsquirrel.org/lisp/sym_diff.lisp) in Common Lisp.ah. missed that part of your comment. it does work for IE, though... 

Anyway, what's so bad about flash that you want to block it by default? If it's flash ads, I'm sure you've got adblock...Java is much easier, right?

    try {
            java.io.BufferedReader in = new java.io.BufferedReader(new java.io.FileReader("infilename"));
            // do some Java-monkey-shit
            in.close();
    } catch (java.io.IOException e) {
    }


Imagine you'd have to use something like Python's unintuitive

    f = open("file", "r")
    # do some smart-assed Python-magic
    f.close()

I had problems when I started out, but once you realise that it's a monad, and you can't perform the monadic actions of one monad within another monad without a monad transformer (those monads that end in a capital T usually), it makes perfect sense.

And no, it didn't require me knowing anything about category theory to get it. In fact, I still don't really know any category theory, however I have read some basic introductions and lecture notes.How about every webpage that decides to play music automatically, jarring with my own music selection in one of the dozen tabs I might have open? I don't really have time to adblock every single ad that talks at me, nor do I want to disable those things all the time. FlashBlock is my new god.You can't open up anything in a flash movie in a new tab, anyway.Java is not a good model of simplicity or clarity, for I/O or otherwise.

In that Python example, is 'f' closed if the smart-assed magic throws an error? What if you're using IronPython or Jython?Probably not in any sort of informed manner.  Imagine how many people would have to learn it first.Mmm! That python example raises an interesting point.

Lazy IO is quite cute for a number of problems, like this, since it obviates the need for the explicit managing of the file handle resource:

    do f ← readFile "file"   -- lazily read the file
       smart-assed lambda-magic $ f

and the file handle is closed once the file's contents have been demanded by the function *smart-assed*. Lazy IO is like GC for file handles in some sense, in that it removes the need for manual management of that resource.

It would make sense to implement a lazy IO library for languages like python, where there's often a lot of text/IO, since it makes IO easier for many tasks.&gt;&gt; I wanted to know how to go about reading from stdin, applying a function and writing to stdout. I was told “well, there’s a lot you need to know before doing that”.

&gt; No, more like, he needs to know how to [...]

\t#include &lt;stdio.h&gt;
\t
\tint function(int n) { return 2 * n + 1; }
\t
\tint main(void) {
\t  int num;
\t  while (scanf("%d", &amp;num) == 1)
\t\tprintf("%d\n", function(num));
\t  return 0;
\t}

*Edit:* I include the original quote, since I intended the program as something of a punchline to it, and it seems to me it's being missed.At this stage in the farce, simply hearing the words "semantic web" is enough to make me snork.Exactly, if your goal with memset was to reset a data structure because actually you expect to read zero before to initialize it valgrind should be able to spot the read to garbage.[deleted]http://dspace.mit.edu/handle/1721.1/6111So it's like a statically-typed ducktyping? That makes sense. A few more hoops to jump through than in a dynamically-typed language, but that's expected.
Is this a joke?&gt;How about every webpage that decides to play music automatically, jarring with my own music selection in one of the dozen tabs I might have open?

Happens so rarely, it's not worth it. More often than not, whatever it's blocking would be whatever I'm trying to get to. I'd get annoyed at having to do that extra click all the time.

&gt;I don't really have time to adblock every single ad that talks at me

Uh... I haven't had to manually adblock anything since they added subscriptions to filter lists, so that's also a non-issue.

It just makes more sense, to me anyway, to have to disable something every once in a while when it's misused than to constantly have to enable it for legitimate purposes.

&gt;FlashBlock is my new god.

Seriously...Uh, my point was that Cambridge != Boston

I don't know what prompted the Jersey comment...Well, Chapter 11 is totally broken.

http://dbaron.org/css/1999/09/links

Is how it is supposed to be done.Beijing has a pretty good place like this do, called the Computer District. You can spend several days just wandering around bargaining and getting cheap, neat, little parts and things there. It's an enormous store, with lots of little stands like in the pictures, and several stories. Crazy stuff.[removed]I have generally enjoyed reading your submissions and comments in the past, so you surprised me with your (IMHO) very judgmental and self-righteous post. Maybe you didn't mean to, but you certainly came across as very condescending.

My previous experience with pjdelport is that he has a good grasp of programming language definitions. He responded to you politely and correctly, and you treat him like a moron.[deleted]&gt; So, the first step for learning Haskell is to start learning the algebra of programming.

That doesn't seem right to me. What is so wrong with starting with the basics like "This is how you ask the user for two numbers and display the total."?
The GPL and BSD guys clearly have different definitions of open and free. It's tough to have a discussion about this when you are speaking a different language. Its no surprise that it always degrades to personal insults and nobody changes their mind.

Reminds me of discussions between atheists and theists.Nice error handling. I guess you don't care if your program bombs, though I suspect your users might.That was so funny, I wet myself.[removed][deleted]you had me at 'point and grunt interface'.[removed][deleted]&gt; For instance, whether a programmer defined function can yield side effects is more a matter of semantics than syntax (though both are involved, semantics is far more directly involved than syntax, I think).

*Whether* a function can have side-effects or not is *entirely* within the semantics of the language, and has nothing to do with syntax. A concrete example of this is call-by-reference and call-by-value in C and C++. There is no call-by-reference in C; the semantics of the language does not support it. Hence, obviously also no syntax for expressing it. It's possible in C++, so you need a way to express it. That is the syntax. This does not imply that whether call-by-reference is supported is both a matter of syntax and semantics. 

Semantics does not exist without syntax, just like the taste of a grape does not exist without the grape itself. But they are clearly distinct concepts.

&gt; The choice of "fun" or "defun" is, as far as I can see, either semantics, or neither semantics nor syntax,

Well, you may make a case that it's naming and not syntax, but it's clearly not semantics. If you want to discuss this, please reply to [my other reply](http://programming.reddit.com/info/14oi9/comments/c14pvu).

&gt; &gt; what is the scope of this variable?

&gt; Both semantics and syntax come into play here.

No, *what the scope of the variable is* is clearly within the realm of semantics. You do define that scope through the syntax, but these concepts are distinct.guys, seriously.

I/O in Haskell is seriously NOT worth the effort.



It took me less time to write an app in java to make calls to my prover binary (halp on the haskell wiki) than it would have to actually learn how to handle monads and I/O.
They've since fixed that. Well, not really fixed. But there's a workaround. It's kind of complicated. But the bottom line is it no longer hurts you.[deleted]Shoot, I'll take all the help I can get.  I think its great there's so much interest in Haskell lately!My favorite comes with its own assurance:

    310:   /* no header found, fake some 00's (this works, believe me) */
           memset( p_sys-&gt;pes_buffer, 4, 0 );&gt; the leap of faith

I like to conceptualize it more as a *contract* that assures you that, should you find a turtle supporting your weight, there will be turtles down to a suitable depth so as to support the whole structure. In other words, it's not *faith* but *trust*.

But that's the heathen me.It's not *hard*, it's just that it's subtly different in some ways that are simpler to explain once the user knows some more things about types rather than right at the start of the tutorial.

You have a distinction between *IO actions*, and the rest of the program. IO actions are basically inert under evaluation, but when executed produce side effects, and a return value. The only action which ever really runs is main, and it's typically built up from other IO actions and pure functions, using do-notation or various monadic combining functions to glue IO actions together. The details of how it all works are much clearer once you understand types, because you can look at the type signature for readFile, which is String -&gt; IO String, and see that it's a plain old function which takes a String for the filename, and produces an IO action which *if executed*, produces another String. 

This is not to be confused with String -&gt; String, which is a function that takes strings to other strings.

The most common way to ensure that an action is executed is to use do-notation to build another action and place it on the right side of an "←" (really a less than sign followed by a hyphen -- I'm only using unicode because of reddit's draconian parsing), or if you don't care about the result, on a line of its own in the do-block.

For instance, we might write the following:

    main = do contents ← readFile "myFile.txt"
              putStrLn (reverse contents)

(readFile "myFile.txt") and (putStrLn (reverse contents)) are both IO actions. The variable "contents" is bound to a String, the actual contents of the file.

What we could *not* write is the following:

    main = putStrLn (reverse (readFile "myFile.txt"))

(Not even if we included the do, which is only really necessary to join multiple actions together.)

Why is this? Well, readFile "myFile.txt" :: IO String -- it's an *action* which will produce a string when run, which is different from it just being a String -- this means that we can't apply reverse to it, because reverse expects a string or a list of some sort.

At first that seems inconvenient doesn't it? Don't despair, it's being used to buy you something very valuable. You can pass IO actions around as first class values with no danger of causing them to be executed inadvertently. What this means is that you can write your own control structures. Haskell doesn't have a built-in foreach. That doesn't mean we don't have one, it's just an ordinary function in the Monad library.

    forM []     f = return []
    forM (x:xs) f = do v ← f x
                       vs ← forM f xs
                       return (v:vs)

There you go, an implementation of foreach right there, in terms of plain old recursion. It even nicely returns a list of the results of each iteration. In reality, it's actually implemented in terms of a function which constructs a loop from a simple list of actions, but this is equivalent and close enough for discussion.

IO actions are first class values. You can put them in data structures, you can build them with functions of your own, you can write functions which take IO actions as parameters and produce others by sewing them together somehow without risk of missiles going off inadvertently during the surgery.

So that's the difference. It usually helps to have a discussion about types first, and naturally leads to questions like "so what are monads anyway?", a discussion for which it's even more important you know how types work, but if treated properly is really no more difficult.

(Hint: the "IO" symbol you're seeing in the types I gave for the actions is one example of a monad, due to the fact that it comes along with some definitions for some functions for which the do-syntax is sugar. The M in forM is an allusion to the fact that this is a for-each loop not just for IO, but in any monad.)

Hope this helps :)It's like this bizarre mindset that says, "It's too (easy|simple|elegant) for it to *really* work. I know, because if I wanted to do the same in (C|C++|Java|C#) I would have to do *real* work (i.e., write lines and lines of code)."

And then, "yeah, but what about the runtime? C programs are *really* small. Betcha you can't match it, huh?" (nevermind Java's RT, or C#'s, or Perl's, or...).This would be nice mixed in with the ideas of that large multi-touch screen interface...

very futuristic, if you imagine this replacing the top of a desk...I don't think anything he said was racist or semi-racist.  He merely stated that a large percentage of chinese people are poor.  This a true statement and does not imply anything negative about them.  He also states the communism has a negative effect on innovation -- which it certainly does.I don't read reddit but someone was kind enough to point out this post.

I think what I need more than community backing is the community to kick me in the ass and get me working on it again (which has successfully been accomplished). I've been busy with other stuff the last month or two.

Other than that I think it is too soon for anyone else to start working on it (except maybe library support). Things are still too much in flux.Yes, but this site isn't a "movie" per se, it's a putative new navigation scheme.

A navigation scheme that places possibly onerous restrictions on how a given user navigates, no less.Tried it some number of months ago, when my mouse still had a middle button.

But now it doesn't -- I have a Marble Mouse trackball and the middle-button simulation doesn't work.  So alt-clicking is where how I go...&gt; You learn that strict call-by-value evaluation imposes artificial limits on program structure.

I don't agree with this assertion, unless you care to enlighten me with suitable substantiation. What I'd say is that CBV imposes a different program structure, a form of expression, a *style* that, practice shows, is more removed from the purely algebraic synthesis you achieve with CBN. It's --to me-- akin to saying, "prose imposes artificial limits on expression (w.r.t. poetry)", because you're expected (?) to say what you mean.How do you export haskell functions ?

The method described everywhere is not elegant.

http://www.haskell.org/hawiki/FfiTutorial

For some reason it requires additional steps to just declaring functions as exported and compiling.

Or maybe I did not understand it.
I must have mis-read that, you are right.

Curious that they use 2 spaced indenting, does anybody know what they use for C++ and Java? 

I was assuming that since GVR works there and he authored that PEP they would use 4 spaced indenting, then again he only joined them in '05.You're correct, GHC does not support cross-compiling. When you build LambdaVM you're basically building a GHC targeted for your native arch with a JVM code generator in addition to the normal one (the Cmm backend). Most of the target specific stuff is only in the Cmm code generator so the target you build for doesn't really matter for the JVM stuff.

Unfortunately there still is leakage of word sizes, etc up to the higher levels of the compiler. I think I've hacked around most of them but I suspect I've missed a few. I haven't tried LambdaVM with a 64-bit build of GHC yet.Well, the analogy breaks down when can send an "internet" last Friday and have it take several days to arrive.As for Java interaction. You can use standard FFI style foreign imports.

foreign import jvm "length" string_length :: JString -&gt; Int

Foreign exports aren't working yet but they should be pretty easy to get going.&gt; And all of those languages to which you refer have compromised referential transparency 

This is not, you know, a purity pledge or a way to avoid disgrace from your church community. As long as you *can* reason about whatever referentially transparent core, kernel or nucleus of pure code you need to, if the rest is impure, so what? Maybe all this discussion is not about Haskell, how difficult it is to learn and use it, or the organizational quality of the tutorials, but about the *pragmatics* (individual *and* social) of learning and using Haskell (or any other "pure" functional language, scary quotes because there is more than one definition of "purity", cf. Clean's philosophical stance).I'm hoping to get it integrated into GHC eventually once I get everything stabilized a little more. Trying to maintain it upstream will just slow me down for now.

The only trouble might be some of the changes I had to make outside the JVM backend. (One of which was outright dismissed by SPJ, but I didn't try very hard to convince him.)Yeah, though not too much:

    data T = forall a . I a =&gt; T a

statically guarantees that something of type T will implement interface I, and the actual representation type, 'a', you can never see.

Existential types are the essence of encapsulation: the concrete type is statically guaranteed to not be visible outside the interface.I believe this paper addresses your question more thoroughly than I could do.

http://www.cs.chalmers.se/~rjmh/Papers/whyfp.html&gt; If Haskell **broke down** and **just accepted** being call-by-value with **undisciplined use** of side-effects, it would no longer be **an interesting language** to learn and research. **Just another ML**. And then people would start writing lazy functional languages on their own again; the whole **reason for Haskell to exist** is to focus those efforts **into one place**.

The way you say it, it sounds to me as if it were a moral choice rather than a technical one (emphases mine). It's **just** a language, you know, not a religion nor a Messiah.How do you export functions to foreign languages?

You mean:

    add2 x = x + 2 -- my haskell function

    foreign export ccall "add2" :: CInt -&gt; CInt

Doesn't seem terribly hard?Without the purity, lazy evaluation becomes seriously difficult to reason about, if even possible at all.
But the cleanup is still done by the GC in your example, isn't it? And what about error handling?

So I don't see the real difference to the Java example: Handling the content as a list or as a stream is relativly unimportant in most use-cases.

BTW: The Java example is bad-style because if there's an error in the Java-monkey-shit-part, the file is only closed by the GC. And yes: Error handling in Java is urgh. C# has done it better with its 'using'.
Very interesting. You have all the reason.I know that part - but continue :))
Exporting ends when I actually have a dll that I can use from other languages
What next ?
&gt; the sequential nature of single-tasking means that you[r] code is **divergent** when run in-the-wild

Divergence is nice and easy, very tractable (just compactify your domain). Non-determinacy is neither.Polynomial time is in NP too, it's just not NP-Hard ;-)

However, Sokudo is NP-Complete as is Independent Set.  They demo both.  On the other hand, I can solve fairly large Soduko puzzles in conventional hardware using modern SAT solvers.this tutorial is pretty cool. just good set of basics to get you going.You used Java IO instead of native Haskell IO?!
That's a first I think..

Really, what did you find hard about [System.IO](http://haskell.org/ghc/docs/latest/html/libraries/base/System-IO.html)?

*/me boggles*You can think of it as a social choice.  Either the functional programming community fragments into hundreds of very similar research languages like Lisp once did, or they combine efforts and produce a common language.  There is a wonderful recent paper about the history of Haskell which delves into this more.

http://research.microsoft.com/~simonpj/papers/history-of-haskell/index.htm

I don't appreciate the insinuation of religion.  I am trying to explain briefly why the people who designed Haskell chose to do things the way they did, and bringing religion into is a non-sequitur.

There are some very sound technical reasons why Haskell exists the way it does, and if you're interested in learning why, that paper is a good starting point.
That's it though. When you compile this, you get a C object you can use. On the C side you #include the header generated by GHC, and link in the .o file.

Regarding DLLs, well, check the docs for your platform.Frankly, it sounds like you didn't even try.
The issue is Microsoft CAN'T copy the interface anymore due to the strange patent world we live in.  So they have to do something different, or risk being sued.&gt; (scanf("%d", &amp;num)

eaugh, no!  You can't possibly provide a sane user interface with scanf().  Please teach people to use fgets (and strtol(), and even sscanf()) up-front.Writes the man, *into* a web application.took too long to load... so i left.

peace out dontclick...dittoWindows. How do you create dlls that could be used from other (any not just c) language ?

Is there a tutorial ? or a manual ? The only thing I found about dlls was that you can use them only from haskell :))

That's not pretty much useful.
[deleted]But you must be using the x87 for those, since SSE/2/3 doesn't  have [instructions](http://docs.sun.com/app/docs/doc/817-5477/6mkuavhrn?a=view) for sin,cos,exp,log, etc.  Right?Thanks, both of you. I think i was blocked by the fancy nomenclature -- my philosophy classes were great, but gave me a rather specific notion of "existential".Give me Wolfenstein in 5K lines, rather:
'
http://www.wolf5k.com/[removed]Haskell certainly has -harder- IO, in terms of learning how it comes about, but it has -easier- IO (especially as compared to C -- it astonishes me that -C- has somehow been the language of choice for comparisons, recently) in terms of writing programs, after you get to the point of using all of its neat facilities instead of figuring out how they work.  Mercury has a much easier to understand IO mechanism, which cuts through Haskell's layers of cuteness in monads by way of making the IO state explicit, but it is easy to foresee an "Ask Reddit: Can we all agree that I/O in Mercury is not that verbose and, as a community, move on?"Cool, in the future you should label stuff as being a pdf though. :)Maybe he's done more with mathematical functions than with commands in a unix shell.  Maybe he has a silly preference.  So what?  What do you think about his other complaints?Wow, words cannot describe the void of reason that gave birth to this site.

At least it's keeping the authors occupied so that they can't do any damage if they ever tried tackling something important.&gt; How do you create dlls that could be used from other (any not just c) language ?

Well, usually other languages need their own wrappers in order to call C functions.  Java needs JNI headers, Python and Ruby need their extension wrappers, etc.  Nothing specific to Haskell there.

Has anyone tried SWIGging Haskell foreign C calls?  I've never used the Haskell FFI, but if I can just do a "foreign export ccall "myfunc" :: CString -&gt; CString" and call it from Python, I'm suddenly very interested.  Then I could let Python handle the I/O and libraries and only use Haskell for the complex algorithmic stuff.If I understand the Haskell code correctly, it's cleaned up by  virtue of being in a monad.  Once the monad goes out of scope, the whole computation is finished and any resources can be reclaimed.  It's very much like RAII in C++.Let's not be too hard on him, he's just starting out. :) I wrote some long responses to the questions he brings up in a comment on his blog.hot damn that's so cool !!!!!!!!!!i study at a design school(product design).. and for one class we looked at this website model.. process works on all levels of design.
anyhow, through a lot of testing and research.. a common answer we found was that the website had a 'soothing/calming' effect on users. dont have proof, but those are just some of the responses.
  so am not saying the whole website is the way to go, but elements of usabilty and navigation are quite innovative.care to explain why its not 'functional' ?
the whole website works dont it?Because (for reasons discussed at length recently here on reddit) it's easier to learn how IO is done in Haskell once you have some of the basics down. Not that it's all that hard, but it's just easier to start from a more "mathematical" viewpoint of learning how to program in Haskell.

Besides, you have the REPL, and that's sort of your default user interface to start out with. To add two numbers, you type them in, separated by +, and then press enter. ;)Funny how the diagramming software complained about the use of the [greengrocers' apostrophe](http://en.wikipedia.org/wiki/Apostrophe#Greengrocers.E2.80.99_apostrophes)
in "PC's" (bottom left corner), but the author chose to ignore it.[removed]&gt; In the code I write, the IO part is perhaps 2% of the code.

I think you might have lost the context here.  We're talking about newbies trying to learn Haskell, not people who already know Haskell writing production software.  The code you write is not pertinent unless you are a newbie learning Haskell.

When it comes to a newbie learning the language, I/O is a *vital* part of the code.  I mean, the canonical first program everybody writes, Hello World, is *only* I/O and nothing else.  It's one of the most basic yet functional operations possible.  If it's in an advanced section of a tutorial, then either the tutorial or the language is severely broken.
You've taken your online porn to a whole new level.&gt; There is no provision for IO in C.

Read any of the C standards that define C.  They include the standard library.

&gt; Everything else is just a function call.

I/O is not part of the syntax, but it is still part of C.
That's what I want also. No intermediary с files and compilation between haskell and other langauges.
&gt; No intermediary s files and compilation between haskell and other langauges.

eh?  You'll necessarily have a compilation step with Haskell in any case&lt;0&gt;.  Do you actually mean that you want a build system that makes Haskell+other-languages as easy as Haskell alone?  I expect that GHC already offers this.

0&gt; unless you mean Hugs -- but nobody ever means Hugs.This would be a lot more interesting to watch if he added dye so you could see where the water was going.Who wants to work somewhere that doesn't do code review?And that badass plastic marvel: [Digi-Comp](http://en.wikipedia.org/wiki/Digi-Comp_I)This does not have code.  Wikipedia, however, -does- have code (or easily comes to have code) -when- it covers the subject at all.  If any of you out there want to mine reality for glory points, or however your post-gaming worldview terms it, then please accept this meta-quest:

1. Walk this nist.gov dictionary, picking a new algorithm or data-structure.

2. If wikipedia does not have an entry on this, create one with possibly the nist.gov entry as a reference.

3. If the wikipedia entry on this does not have a code example, add one: by tracking one down, by understanding the subject and creating your own, by asking a likely programming community for an example.

4. Collect glory.  Recur.Is this an internal document of Andersen Consulting ?
That's the single most useful new word I've learned this month.Sigh. I never thought such a simple thing would require so much explanation.

You write a library in с - You compile it in с - you get a dll, that is usable from any other language.

You write a library in delphi, you compile it in delphi - you get a dll, that is usable from any other language.

You write a library in VB, you compile it in VB - you get a dll, that is usable from any other language.

What is so hard to understand?

You write a library in haskell, compile it in haskell - and  get a dll, that is usable from any other language. No с headers, no object files, just a f*king dll. How hard is that ?

I don't know much about doing stuff on Windows, particularly DLLs or whatever they call'em.

But I took 2 seconds to open the GHC user's manual available from the GHC web site, and I scrolled down to the chapter labeled "Creating a DLL" and voila

http://www.haskell.org/ghc/docs/latest/html/users_guide/win32-dlls.html
Hmm...You're not hanging out in the right circles.  I'm seeing guys writing AJAX apps that are complete, responsive, and web-enabled versions of desktop applications.  In a year, or less, there will be a suitable web-based replacement for every major desktop application for the office, and if you've got a low-latency network, it'll be as responsive as the traditional "stateful real-time" desktop GUI you're used to.  (Speaking of which, and to bring this back full circle, Paul Graham's Y Combinator happens to be funding a couple of said developers in the current Winter Founders program.)

The end of the desktop OS, as a GUI target in most application development, is far closer than you can apparently imagine.visionaryI don't see why people continue to think that unmaintainable code == job security.  I'm familiar with several companies that would fire you immediately if they discovered this kind of behavior, in order to minimize any damage you could do.  Where I work everything is backed up (and occasionally reviewed), so they'd just restore everything from backup.I stopped being a lurker just to upmod this. Needless to say it  was a long day of looking at refuctored code.My only question is: which of my co-workers wrote this?  And how did s/he get time, in between debugging?No, he's saying (quite politely) that your reasons are little more than elaborate statements of how your definitions of the words differ from the standard ones used by programming language designers and users, and that you seem to be ignoring the references and explanations to the contrary that you've been given.I'm exactly the same way about tabs. I'm actually working through some ideas for how to reform the tab mechanism in FF to make it more friendly toward this kind of behaviour. (Smart loading and unloading, treating tab order as a queue, blurring the line between tabs and browser history, etc.)MSVC does that(C4318).[deleted]It is also less portable: bzero first appeared in 4.3 BSD (although it has been implemented on a bunch of platforms), whereas memset is C89.

"bzero should not be used in new programs", indeed.For now I just call certain C math functions with FFI:

http://factorcode.org/repos/Factor/core/math/libm.factor

The words that you are supposed to use implement complex-valued elementary functions. Just like CL, support for complex numbers is built in to the language.

See http://factorcode.org/responder/help/show-help?topic=math%2dfunctionsThe problem with Lint is the false positive rate. I remember trying a lint with a hello world program. First it complainted that my program was void main() instead of int main(). Fair enough. Then it complained that the main function never returned a value. Ok, put a return 0; at the end. Then it complained about the program returning from main. OK! I put a while (true) ; after the printf. Can you guess what it complained about?

Yup, unreachable code (the return statement!).

Regexes aren't that smart, good and bad!Too bad you have to drop down to the new assembly if you have to do anything novel and speed-essential in Ruby. Doh!&gt; 10 years ago a programmer would be more likely to "switch to assembly" for a much-needed performance boost.

No, the situation with dropping-down-to-C has little-changed from ten years ago.  Your sudden decision to 'hop on the train' does not mean that the train just started rolling.

(Unrelatedly: farther back than ten years ago, C apologists would note that you could write the numeric-intensive parts of your program in Fortran.)As I recall, Ruby at least makes this easier than in Python or Perl&lt;0&gt;, in that it doesn't require that you deal with reference counting.

0&gt; [Inline::C](http://search.cpan.org/author/INGY/Inline-0.44/C/C.pod) probably changed the balance a bit.&gt; My example demostrates a race condition for the sake of
&gt; demonstrating a race condition. 

Yes. It demonstrates a race conditions.  And that's it.  It does not support, nor object the implication that all multi-threaded programs contain race conditions.  Thus it's at best offtopic.  Or simply fallacious.
Why did you feel compelled to share this in a form of a reply to my comment ?

It seems you didn't understand that I don't compare the relative "niceness", "easyness" or "tractable-ness" of sequential and parallel programming.

It's just that claiming that all multi-threaded programs are non-deterministic is a COMPLETE AND UTTER BULLSHIT.  Period.

(End of thread.  It's impossible to argue with people with prejudice against threads, who will present  many arguments why they don't like threads, no matter what was the context threads were mentioned and whether their comments has any other connections with the context, except the word "thread").This is only true once you get used to the language. When two languages are so different you, in the facts, start from scratch. Damn people are on the defensive side this week. Wow.Dang, I had a similar idea just last week.  I called it a "streak" interface.  Not that it would convince anyone, but I sent myself an email last Friday, the 9th.  I see this non-clicking interface as being a useful *mode* or *adjunct* to the normal interaction style we're all used to.  Here, for what it's worth, is most of my email from last week:

For usenet, reddit, email these days, or any other firehose of info where you want to reject most of the items, you want to "handle", say, fifty at a time, do a batch of selection followed by later reading.

Each item has three states -- "not seen", "rejected", and "marked to read"

There's a "start streak" button, and a ("pre-reject") button that sets the initial state of everything on the page.

     On the right hand side of the world, each item has some boxy areas:


                      mark   neutral   reject

The main idea is that as you move your mouse down through the thing, the current item is subtly selected, you move down through the reject or the neutral column.  Maybe have a fading red trail like you can do with mouse gestures in ffox.  For articles you want *not* to reject, make your mouse path catch the RHS of that article's "mark" area, and it gets marked.  Once you hit the bottom of the page, you can slide right down into either the "DO IT" box, or the "NEXT PAGE OF HEADERS" box, or the "SWITCH TO READING MODE" box.

Issue:  auto-scrolling should be done, smoothly, when the nr of items exceeds the page size.

Wow, I'd really like to have this, NOW.&gt; Everything I do is IO, and its not just command-line type IO; it is a web app with multiple forms or a GUI application with several frames/windows/inputs, etc.

Actually, you'll find that most of that code doesn't do I/O;  it transforms data, calculates things, and so on.  The actual I/O is mostly just the shiny wrapper around the outside.

&gt; my main question is: what does Haskell do for me, the everyday programmer who is constantly dealing with IO

Haskell lets you encapsulate your I/O, keeping the bulk of your code pure.  Another way of saying this is that Haskell allows greater abstraction, because it has first-class support for side effects and I/O.&gt; that the BSD's are now dying

eh?  A little less dancing on imagined graves, please.

&gt; GPL turned out to be vastly more useful

Oh my, -vastly- more useful?  The commercial applications of the GPL are -vastly- more useful than the commercial applications of BSD licensing, are they?

Can you turn this -vastness- into something tangible?  AFAICG, you think it -vast-, the visibility of commercial entities releasing GPL-licensed code, and the invisibility of commercial entities using BSD-licensed code.

Does this -vastness- of yours translate to a sneer at X?  X is pretty much BSD without the advertising clause, from what I remember about the FSF's objections to it.  Oh, I can't even imagine how -vastly- superior a GPL'd X would have been.  Although, fortunately, the X license allows you to take the whole system and apply the GPL to it, the way it allowed that poor XFree86 person to obviate himself.
Yeah, but those links move around. You move over something and EVERYTHING, including other links, moves around the page.

It's like going to close a Yes/No/Cancel dialogue, and when you  hover over a button the other two move outside the window and to random places on your screen.Oleg is unbelievable!

(I think I even spelled it right this time)My friend, you just said 'Micrsoft fears the risk of being sued' and '$X fears patents on interface' in the same breath.  Please work to correct the errors in your logic or knowledge that led to these bizarre assertions.The thing is that china is not really much of a communist country..... 

It is a dictature, I'll concede that to you, but it certainly is not a communist country anymore.....For the record, you'd write that Python code more idiomatically as:

    with file(...) as f:
        # smart-assed snake-magic

instead of calling `close` manually.

&gt; It would make sense to implement a lazy IO library for languages like python

Python's I/O is lazy in obvious places, at least, such as line-wise iteration:

    for line in file:
        ...The electronic revolution was years ago... of course, electronics are important but not so much.....

Now innovation comes from new innovative software, not from hardware anymore.... This is part of the reason why japan is slowly loosing groundMONIAC is a nice idea, but it didn't handle rational expectations any better than Phillips' [other invention](http://en.wikipedia.org/wiki/Phillips_curve#The_Phillips_curve.2C_NAIRU_and_rational_expectations).  In fact, he used MONIAC to explain the Phillips Curve relationship, so to some extent that bizarre water simulator was directly responsible for stagflation. History is weird.In smaller non-IT shops, where people have their own staffs over internal products, this is more common. 

Some have suggested that Perl has been used by system administrators from day one for this exact reason.it looks to me like you are right. plus you added simplification rules as well. did you come up with this as a reply to this post? either way, hats off to you sir.Paul Graham's article was very interesting, but you can also be a genius functional programmer without being wise. I cannot believe that is the barometer for being wise.You are reading too fast. Slow down. First sentence on that page:

&gt;Making Haskell libraries into DLLs doesn't work on Windows at the moment; 

:))
&gt; Python's I/O is lazy in obvious places, at least, such as line-wise iteration

Aye, and for a 'language like python', Perl has the brilliant [Tie::File](http://search.cpan.org/author/MJD/Tie-File-0.96/lib/Tie/File.pm).  I think that Haskellers mostly focus on enticing people away from easy targets like C, and either have given up on, or simply don't understand, the complexities about enticing people away from dynmaic languages.  Well, not directly: 'Wow!  Haskell is cool!' does this to an extent.  'shootout' results can make a Perl programmer aware that -GHC- does not suck, for instance, but it doesn't do anything to convince that person to -use- Haskell.  Communities like Haskell need to show how their language scales in terms of -programmer- speed, to show off how well a Haskell programmer can put something on Hackage and how another can use that.  They might blogs about how typeclasses let a user easily build an agent for a spidering library, or about what it takes to dive into a core library to make an important change and then submit a patch.  Haskell -should- be better at that last example, as GHC is Haskell 'all the way down', but I wonder how much trouble you have in practice with needing to recompile everything.The canonical structure for a web app (like Reddit!) is CRUD: Create, Retrieve, Update, Delete. Those are all state changing operations. The programming logic is the glue between the "shiny" HTTP/HTML I/O and the database. This style of application is continually increasing in popularity because we are less and less using our computers to compute and more and more using them to communicate. The network is the computer and so forth.
Really pointless idea. I do not want things to pop up before I click them - in fact, I want to be able to move my mouse freely, not to be afraid what happens if it goes over some trigger area.

No, thank you. One of the worst navigation ideas I've seen.I actually LOLed when I read your comment.

He called them *dirt* poor, referenced agent orange, and then said they weren't capable of invention or innovation.Holy crap:   "Water analog computers were used in USSR until the 1980s for large scale modelling."

And some people still think Reagan won the cold war...Each and every computer user has at least once suffered from the action of a virus. The damage such a malicious little thing causes can be very serious, leading up to the theft of your data. Your programs will not work any more and you will end up losing a large amount of time trying to fix them. The worse thing that can happen is your real money being stolen! Could you say that this is a risk worth facing? The answer will most certainly be negative!

Another thing to keep away from is a virus hoax. It will arrive in your email and warn you of a brand new threat to your computer. Once the email outlines how the new virus works it asks you to forward the mail to others in your email address book. In fact this chain letter can cause a great deal of damage.

Just think of how much these virus hoaxes can travel! Should each person who gets it send it to ten other, and then each of these ten people send it to ten more people, the outcome can be very serious. It can get up to millions of false emails that have no purpose and are using up space on networks. Therefore, the routers and servers will probably crash or at least slow down.

You can be sure that you�ve got a hoax when a lot of technical words are being used.

Some hoaxes will contain a reference to an organization that really exists such as a legitimate company that sells antivirus software. You can easily verify this information. You must have seen references of it somewhere, such as on television, as well as the internet! Another way to protect yourself from getting a virus hoax is making sure that you never buy anything from someone who claims to be able to fix any virus infection that your computer might have. This cannot possibly be true!

A hoax can indeed be very harmful. Some of them will warn you to delete files on your computer that might have the virus. Should you follow their directions, you might not notice the effect immediately. However, when you will reboot your system, it will not start anymore. The thing that the hoaxes are counting on is the fact that these files are often unknown to most users, even if they are very important to the proper running of your computer.

Each and every computer user has at least once suffered from the action of a virus. The damage such a malicious little thing causes can be very serious, leading up to the theft of your data....or do it the most elegant way with Maxthon - just slightly drag a link to open it in a new tab.The reddit people have:

0&gt; An eminently copiable design, and no history of interest except for
the 'related' and 'browse' functionality, which I, at least, have never
found very interesting.

1&gt; A system written in Javascript and Python, which everyone knows or
can know on demand with the amazing hacker ability to lazily learn simple
languages like this.

2&gt; A user-base that includes a significant amount of programmers.

3&gt; A user-base that provides -all of the value in the system-.  If
people stopped posting to programmers.reddit.com, the value of
this subreddit would disappear with not even some kind of fanciful
poof of smoke.

4&gt; A bunch of problems: their 'number of comments in this reddit'
regularly indicates one more than the -actual- number of comments.
You can't post a link with a broken title, delete that link, and
re-submit it.  You can't have OMFG LESS-THAN SIGNS-IN MARKDOWN
CODE.  ahem.  People regularly ask for additional subreddits, or
tags, or tags instead of subreddits, and such things, and the
reddit people for whatever reason cannot respond to this requests,
or try them out, or even appear to have any progress with adding
value or fixing bugs.  In fact, I've mainly ever noticed phantom
breaking of the markdown system, or flashy new bugs like the
'number of comments' one introduced recently.

5&gt; A closed-source design with no community input.  WHAT THE FUCK?
WHAT IS WRONG WITH THESE PEOPLE?  Post a subversion link, have dual
mailing lists for updates and discussion, and focus on important 
issues like saying 'woah, thanks, looks like this fixes that bug'
and 'no, that looks interesting, but it changes the flavor of
reddit too much.  Or something.' and 'OK, here is another blog
post about culture or community or behind-the-scenes server and DB
work.'  They obviously have nothing at all to fear from opening
the source (see points 0-3) and obviously have a lot to gain
from opening the source (see point 4, and their own admissions about workload and -- and see point 4 again).

Seriously, I don't get it.  I've wished in the past that someone would
simply put up 'betterreddit.com' and blow these fools away, but I'd be
even more happy if they'd just get a clue.  I mean, imagine it: tomorrow
they start the lists and post the subversion link, and then the day after
tomorrow the most egregarious bugs simply disappear, and then things
more or less move on as happily as they do now, but without the bugs.
Worst case, you'll have the occassional angst about the reddit people
not accepting a patch -- the kind everybody has.  Horrors.

ADDENDUM: there aren't even any interesting licensing issues to consider: if they started opening things tomorrow, and said "sorry, we are powerless or paranoid and so need an old-style Mozilla license.", the egregarious bugs would -still- go away.[C--](http://www.cminusminus.org/) is the new assembly. Compiling tail calls to C is unpleasant, and it hogs the registers.these are still on sale
http://www.mindsontoys.com/kits.htm?dc1_main.htm
I want to put one in my office in a glass case with a sign "in case of power cut break glass"&gt; C-- is the new assembly.

Even so, what uses it that way?  GHC compiles through GCC.  Mercury compiles to high level C, or low level C, or as a core frontend to GCC, among its backends.  Chicken Scheme, and probably many other compiling schemes, compile down to GCC-C or C.  Squeak generates C to host itself.  [Slate](http://slate.tunes.org/) compiles to GCC-C C in the same manner.

That doesn't even consider the vastitude of systems that use C in the sense of this article, as a 'drop down' language for efficient mangling of memory (or, less interesting for 'assembly' senses, the use of C libraries).  You don't need tail calls in a 'drop down' C; you only need a [simple enough, and cheap enough](http://www.cs.mu.oz.au/research/mercury/information/doc-release/mercury_ref/Language-specific-bindings.html) interface to not get in the way.Yep, dropping down into the new assembly may be quite a bit easier in Ruby than Python or Perl. But you're still giving up garbage collection.Keep in mind that Mac OS X is a BSD. The BSD license has been pretty useful for Apple's commercial purposes.&gt; But you're still giving up garbage collection.

... ugh.  I intended to have a simple "no you aren't" that pointed to Ruby documentation on the subject, but I'd forgotten that post-pickaxe Ruby doesn't have documentation anymore.

Horror-stricken all over again.

Uh, anyway, no: you probably just need to use extra methods to allocate memory or to tag allocated memory, so that Ruby's mark-and-sweep or whatever-it-is-now can process things.  It may be the case that Ruby only GC's the ruby objects you create.
&gt; You can't have OMFG LESS-THAN SIGNS-IN MARKDOWN CODE.

&lt; oh really? \&gt;

I don't care, as long as I can have the Kirby Dance...

&lt;(' ')\&gt; &lt;(' '&lt;) &lt;(' ')\&gt; (\&gt;' ')&gt;

... I'm happymain = do
        x &lt;- getChar
        putChar (toUpper x)

Considering the amount of code posted in the programming subreddit, its suprising that a) there's no syntax highlighting, b) this &lt; bug has been open for months.

Why not leverage the community here?No, it's not just that. I've worked with ML a long time ago, and did my thesis in Lisp, so while I make no claim to being a functional guru, I'm pretty comfortable with it. (I've only developed professionally in imperative languages though).

I even found an okay intro to monads once, which yielded the presumably good reaction of "okay, that's no big deal". On the other hand, following one of the links on this page (don't remember, I've had a nasty stomach flu) the author basically said, "It's bad that some people are trying to teach Haskell without category theory. Here's some category theory..." 

This turned me (and I actually like math) right off. First it's just bad teaching style -- generally you go from the concrete (to develop the intuition) to the abstract (to lock in the understanding). It takes you on a tangent, far away from what you want to do. It's annoying, and it's exactly what a number of people have complained about, and it seems a specifically Haskell problem.

I may, in the end, learn it. Partly because so many people say it's great, partly because the main STM implementation seems to be with it, partly because a lot of interesting compiler research seems to be happening with it.

But probably not, since I'm just getting started with Python, which seems much more mature and *useful* and Erlang is also higher on the list, since it seems to 'solve' a specific problem very well, and be a nice swing in functional land.It's all about the community and not about quality. See MySpace for a more extreme example.[deleted]Well yeah that one's a big issue, as is the lack of syntax highlighting, especially since Python has at least 2 or 3 general-purpose syntax highlighting packages (one of which was linked here this week)[deleted]Yes-- this is why it is so safe for them to open up things a bit.  MySpace, too :-)[deleted]&gt; In that Python example, is 'f' closed if the smart-assed magic throws an error?

The handle will be closed by `f`'s finalizer, but that gets delayed by exception's traceback keeping `f` alive.  A better way to ensure it gets closed is using a `with` block.Why would you want to use reference counting garbage collection? Isn't that like a hexagonal wheel?As inane as it sounds, fear of lawsuits is in fact the primary driving force in Microsoft nowadays.  That's the reason their names for things are either gibberish (what the frack is Zune, anyhow?) or else long-winded and legalese ("Microsoft[TM] Windows [TM] Vista [TM] Extended Enterprise Professional Multiserver License Edition [TM]", instead of, say, "Leopard").Did anyone else find that a number of these (treasure hunt, state the bleeding obvious) seem to be standard, if not recommended practices in Java?[removed]I can only guess you haven't actually read any of the stuff about varied perspectives, and so on.  Y'know, almost everyone else that has continued to participate in this discussion has gotten off his/her high horse and stopped being a stubborn mule about it (myself included).  You might like to try doing the same.

It looks like *fry*'s comment is more applicable to you than to me.Yeah, I'd add that the recommended page is pretty useless, and the main page quality has taken a nose-dive.

My guess is they're not hungry any more -- Conde nast bought them out, right? So why bother?&gt; That's incorrect. If 'fun' or 'defun' means the same thing, it's not a question of semantics, but syntax.

Two identical terms have the same *meaning*.  If that's not a semantic matter, it certainly isn't syntactic either.

Your enlightenment is a bulb short of actually (credibly) disputing anything I said.&gt; has nothing to do with syntax.

&gt; obviously also no syntax for expressing it

Which one would you like to discard?  After all, without syntax for expressing it, it doesn't happen.  I suppose you might just choose to be self-contradictory, but that wouldn't be my choice.

I did make a point about syntax being almost uninvolved, but you decided to make an absolutist statement about it being entirely uninvolved without bothering to make sure you didn't imply it *is* involved.

&gt; Semantics does not exist without syntax, just like the taste of a grape does not exist without the grape itself. But they are clearly distinct concepts.

I don't think this has come up as a point of disagreement for anyone here, and it doesn't rebut anything anyone has said in this discussion.  What is its relevance?

&gt; Well, you may make a case that it's naming and not syntax, but it's clearly not semantics.

Nice refutation of my explanation following what you quoted.  When will it include something substantive?

&gt; No, what the scope of the variable is is clearly within the realm of semantics.

I'm afraid I must inform you that saying it doesn't make it so.

&gt; You do define that scope through the syntax, but these concepts are distinct.

Yes . . . which doesn't dispute anything I said at all.Yeah, I should have been more careful with the terminology. ;)

Are they claiming a non-linear QC?Syntax highlighting would be really easy to do, too!

[A simple generic tokeniser is enough for most common cases](http://hpaste.org/512).&gt; I have generally enjoyed reading your submissions and comments in the past, so you surprised me with your (IMHO) very judgmental and self-righteous post. Maybe you didn't mean to, but you certainly came across as very condescending.

My tone was perhaps a little much, and it's a shame I let it get away from me.  I was not, however, intending judgmentalism or self-righteousness.  The condescension was, unfortunately, honest -- but largely accidental.  I should have kept a lid on it.

I have a very difficult time avoiding a condescending tone when someone disputes me without actually saying anything meaningful or logically valid as a refutation.

&gt; My previous experience with pjdelport is that he has a good grasp of programming language definitions.

Maybe he should have shared his knowledge, then, rather than declaring me obviously wrong without substantive comments and otherwise being kind of a bad citizen about it.&gt; they're not hungry any more

which is exactly why they should open the source up and let the community help fix things!I don't like this interface, there is less control, but sometimes at night while surfing the web I wish I could click in firefox just drawing a circle with the mouse. Unfortunately all the mouse gestures plugins I downloaded all are in the style "click and do the gesture". That sucks...html + javascript is the new assemblyFrom a usability point of view, this is exactly wrong and a fine method to scare away 9 out of 10 visitors.

There are two things wrong about it.

1) It tries to innovate and be unique on navigation. Nice. But give visitors a learning curve, and you lose, and your company goes broke. Not so nice.

2) You know, those pulldown menus you see a lot, are all wrong. It hides navigation and makes the user perform an action before he can navigate or even see links, where the same user can passively be motivated to click a link in the classic situation. They are a mistake. In interfacing, they are used to hide not-often used options. In other words, there will be less page views, and you lose.

This guy has taken 2. and gone wild on it. Navigation is NOT a game of mouse ability or exploration.Some nice points:

* strictness forces you to consider evaluation order
* once you've thought about evaluation order, you've probably also picked a good one

Whereas:

* laziness means you can delay picking a specific evaluation order
* but you might skip the tuning of evaluation order at a fine grained levelCome on! I'm serious. After the [apotheon saga](http://programming.reddit.com/info/14oi9/comments?opt_csort=highscore)...It's an interesting story but in real life there are lots of easier ways to get an appointment than to go sit to company lobby.Back in the days security didn't matter, and buggy products were expected. Nowadays, months and months of QA is needed to ship even the most basic product. This slows you down tremendously. Especially if your competition can afford to be far more lax in this domain.

Additionally, MS has at least partially become so successful because of 'not playing fair'. Judges nowadays don't leave it at a slap on the wrist, so unless the potential win is huge, breaking the law is not commercially viable anymore. Competing by making a genuinely better product is really hard.

As for the zune, I think they just dropped the ball there. I don't think it's representative of MS in general.Ok, I didn'tThe problem is that Reddit's number one most important feature is the one that can't be easily copied: the number of people who have it bookmarked or delicioused.  Without an audience, no betterreddit.com will matter.  And it's true - I could write a better Reddit in a couple of days without the slightest difficulty, in any of three dozen languages (even that old one with the parentheses that used to get mentioned a bit before Haskell took over the hearts and minds).  But I couldn't attract people to it, so why bother?"C is often described, with a mixture of fondness and disdain varying according to the speaker, as "a language that combines all the elegance and power of assembly language with all the readability and maintainability of assembly language." (MIT Jargon Dictionary)The obvious conclusions are that:

* reddit's value is not technical;

* the *technical* service provided is good enough not to significantly harm the *actual* service people are looking for here.

One of the toughest mind shifts required from startupping developers is to realize how important technical issues are when compared to others: they grossly overestimate this importance (or, if you prefer, grossly underestimate the importance of the rest).

Given the choice between a buggy LESS-THAN and unwashed hordes of diggers, I take the former any day--hence my being here.

As for better content tagging/browsing, sure that'd be cool, but not really necessary today: the amount of data to handle while reading prog-reddit is not big enough to truly require these. And given the actual production of interesting contents, bigger volumes of data would be achieved merely by  lowering the signal/noise ratio. Which would be a real mistake, as prog-reddit's value is precisely its very high signal/noise.&gt; The obvious conclusions are that

Half of these are prepositions, and the other half are false: we -are- harmed, every time we go back to reload a thread to look at the new comment that... isn't really there.  Every time we either go to the trouble of de-&lt;'ing some code, or just leave it in for other people to go to the trouble of reading past it.  Every time we discover some new fascinating markup bug that destroys a comment until we work around it.  Every time we go through tinyurl.com just to submit our own links again with the correct title.

That reddit hasn't been suddenly struck dead by (my own) cursing doesn't mean that its bugs aren't harmful.

The real conclusion: that it is bizarre that reddit doesn't let this subreddit help it with these things.Actually, the module gravity well  (and it's UML representation)  reminded me more of Haskell.one reason not to fix stuff (or, better, just "half-fix" it):

-- bugs that are not fatal or extremely annoying give people something to bitch about without chasing them away

-- bitching about the same topics builds communities

-- people on the outside get the "reddit is so neat" version of things; they keep joining

so there. it doesn't need to be right, it needs to be cool. not too cool, as that would make people uncomfortable.

Windows ring a bell?[removed]Absolutely. I took "impure" in the context of "strict, imperative-functional" programming languages; more precisely, "impure, as in the ML family".Well my point was more about the fact that [syntax highlighting wouldn't even have to be done](http://pygments.org/) since it's a solved problem.I believe that (this may be urban legend, I'm not sure) but some mallocs don't cope so hot with malloc(0), so the 1 is there just incase n = 0. (And so you don't have to write that conditional).Yes, Dabble DB runs on Squeak.  It's a great development platform but the UI takes a *lot* of getting used to, which is a barrier to adoption that the vast majority of people don't ever make it past.&gt; I don't appreciate the insinuation of religion. I am trying to explain briefly why the people who designed Haskell chose to do things the way they did, and bringing religion into is a non-sequitur.

I'm sorry, you're probably right. It's just that I found your choice of words too emotionally loaded for an argument about why is it difficult to make the crossing from imperative to functional. I can see that it is a *passionate* argument, and I don't disagree that Haskell is a better all-around language than most of the lot.

I personally don't appreciate the dissing of ML, and really don't care one way or another about Haskell; I stand by my opinion that the discussion is, at heart, one of pragmatics (or of the politics of programming) rather than one on the technical merits of the languages involved. Peace.Right. They have no excuse.

Reddit: syntax highlighting of code fragments, at least in the subreddit. Please!&gt; [...] good enough not to **significantly** harm [...]

In a business context, **significantly** should be taken as: "badly enough to lose a significant number of users". And to measure it, you must take into account all factors, like existing alternatives, migration costs, non-technical unduplicable assets, etc. If these bugs were significant, an alternative community would have built around an alternative site. Or, much more likely, these bugs would have been fixed in reddit. As stated many times, hacking a decent clone of reddit's engine can be (and has probably been) done in a matter of weeks.

Why don't they adopt an open source approach for reddit development? I can't answer for them, but my guesses would be:

* open source development is less reactive, which is lethal in early stages. Try to imagine how things would have gone, if the move-from-lisp-to-python decision had to be taken in an open source context...

* it eases forks. And a fork in an open source community context has chances to take a share of the user base away (that user base being the only real, critical asset). That would be much much more dangerous than mangled LESS-THANs.

* they wanted to keep the option to sell to a big company. Such companies, when they buy a community, want to believe they control them, and the easiest way to make them believe that is to give them exclusive property of the technical substrate: the engine's sources. Imagine the selling discussions with conde-naste, if the sources were open and free: "well, we're sort-of selling you the domain name reddit.com, because there are a bunch of fine chaps hanging out there, and... well, that's mostly it". You'd have to be a damn amazing salesman to close a good deal under such conditions.&gt; Maybe he should have shared his knowledge, then, rather than declaring me obviously wrong without substantive comments and otherwise being kind of a bad citizen about it.

Didn't i [explain](http://programming.reddit.com/info/14drw/comments/c14kj6?context=4) the definitions, give [examples](http://programming.reddit.com/info/14drw/comments/c14n7y?context=6), and [point](http://programming.reddit.com/info/14oi9/comments/c14p2u)  you at the appropriate references?Salient quote:  _"about the same amount of thought has to be put in to make either kind of program efficient, only that with non-strict default, I get to choose when (and whether) to put in that effort."_
&gt; I've wished in the past that someone would simply put up 'betterreddit.com' and blow these fools away, but I'd be even more happy if they'd just get a clue.

Why not try it and see what happens?

I've been either involved with or a close observer to two of these splits:

1. Fanfiction.net vs. FictionAlley
2. LiveJournal vs. Plogs

In both cases the network effects were even more pronounced than Reddit: people would have to leave behind existing content and reputation that they'd developed at the original sites.  The results were very different between the two, however.

The FictionAlley vs. Fanfiction.net split succeeded.  FA eventually evolved into a large, 100,000+ member community of its own.  This is rather interesting, as FictionAlley's technology (as it existed then, and possibly as it exists now) is *worse* than Fanfiction.net's.

I suspect the critical success factor is that FA was started by a dozen or so *prominent* members of the existing HP community on FF.net, and when they left, they took their content with them.  So right from the start, FA was the place to go to find certain popular fics.  Moreover, they didn't try to appeal to *all* of FF.net's audience; only the particular subcommunity that they were part of.

The Plogs vs. LiveJournal split was also done by several prominent LiveJournal developers, but they made a marketing mistake.  Instead of making the site appeal to themselves, they tried to market it towards the nebulous category of "business/professional blogs".  Meanwhile, they had no particular experience in that field.  The site basically collapsed when they accidentally erased a database and had no backups, something that's definitely not enterprise-ready.

So, if you can convince a dozen or so of the core posters on Programming Reddit to come along with you, you might have a good chance with BetterReddit.com.  The thing is, I'm really not all that dissatisfied with Reddit.  Their bugs are a little annoying, but they are nothing compared to FF.net's elimination of forums, constant server outages, and capricious deletion of fics.  Usually sites fail when they become actively hostile to users, not when they don't have time to fix some small detail.I think several verbal disputes are at the core of this thread, and I would like to avoid one more.  The term "superficial" is ambiguous.  In one sense, it concerns visibility ("on the surface"); in another sense, it is an expression of a value judgment ("insignificant").  I think you are using the term to mean the latter.

&gt; The particular notation doesn't affect what a program does, hence it is superficial.

So, syntax is superficial from the point of view of the user of the program, or the programmer thinking about the computation.  But the syntax (or notation) of a language is _not_ insignificant to the person using it.

&gt; Then again, maybe my problem is that I think that what a program does is what matters most. Not what notation I use.

I agree, but I also think notation is still very important.&gt;He suggests that a typical developer will write everything in Ruby or Python.....Anything that needs a speed-up can be redone in Objective-C.

I don't think so.  Their are plenty of verbose languages out there that do NOT have to do this for performance making this combination a bad idea.As I understand it, they sold the rights. So it's CN who would have to open the source up.[removed]While I like the idea in principle, in practice I think it would lead to comments that are too long... But I guess you can try it in spirit.Right. So no hope there I think. And the bugs will continue to go unfixed, and the features missing. *grumble*vastitude? yeuch.No, it was a bug :) The dude fixed it, then everything was nice and rosy and valgrind loved his program (more important: he got results that made sense).It's in the I/O-monad and the IO monad never goes out of scope (at least as long as the program is running). 

And even if it would: Could the compiler really decide to clean up afterwards? Every computation which makes any sense has to return a value. And the compiler has to either prove that the resource in question can't be referenced by this return value or let's do the GC its job.

So it still look like the cleanup can only be done by the GC here.
&gt; What is so wrong with starting with the basics like "This is how you ask the user for two numbers and display the total."?

I agree that this is the best approach for some people, and tutorials along those lines exist, e.g. http://haskell.org/haskellwiki/Tutorials/Programming_Haskell/IntroductionThe site is an experiment in user interfaces.

Is it possible that a no-click interface is better?  Maybe.  That's what this site seeks to examine.That's 5kb of code, not 5k lines.interesting that they chose a dictionary....&gt; Semantics does not exist without syntax, just like the
&gt; taste of a grape does not exist without the grape
&gt; itself. But they are clearly distinct concepts.

Usually the semantics of a programming language are created before the syntax exists for programmers to express themselves with. If you're saying that semantics serve no real purpose if there is no way to 'access them' through some syntactic element, I agree. But the semantics are often formally defined completely independent of the syntax.

As a thought experiment, think about Java. You can reason about the semantic implications of call-by-value versus call-by-reference (Java currenly only has the former), even though Java has no syntax to pass parameters as reference. By saying "suppose that 'a' is passed by reference, then what would happen?". You can even extend Java to handle the situation gracefully, even if there's no syntactic element that signifies 'pass this by reference'.

As an alternative thought experiment. Consider mangling the grammar of Java to such an extend that it becomes impossible to pass more than 0 variables to a function. At this point, Java is still pass-by-value, after all, the language's internals haven't changed! But there's no way to pass a value to a function, therefore there is also nothing to pass-by-value. Same thing.[deleted]Arrrgh Random PDF links!That's true: monads are a helper library and they expand into plain, lazily-evaluated functions (both side-effecting ones and not). You can manually write exactly the same code without them, if you really wanted to.
---
[Registry cleaner](http://www.free-web-browsers.com/support/registry-cleaner.shtml) by [Free Web Browsers](http://www.free-web-browsers.com/)I suppose you yanks don't know too much about it, but on this side of the sea Pro Evolution Soccer is a massive game for ps2 x-box and p.c. The best football game by far, stealing students time europe-wide.

My question is: has anyone got any information about how they coded it? I googled lots and couldn't find anything, not even what language. I'd love to know how its done.&gt; The prediction of pseudo blobs appearing is a bit steep

Not to mention in complete contradiction to the facts.

BSD guy:

&gt; &gt; &gt; Signing NDA's ensures that Linux gets a working driver, sure, but the internals are indistinguishable from magic. It is a source code version of a blob.

Linux guy:

&gt; &gt; I'm guessing that you did not read the followup FAQ about the program at:
&gt; &gt; http://www.kroah.com/log/linux/free_drivers_faq.html

BSD guy:

&gt; I did read your FAQ but I can't see how it rebuts what has just been said.

The actual FAQ:

&gt; Q: How are you going to write a GPL driver by signing an NDA? Is it going to require a binary blob or some other way of obfuscating the code?

&gt; A: No, not at all. I have written many drivers after signing NDAs with companies. They are usually signed either to keep information about the device private until it is announced at a specific date, or to just keep the actual specification documents from being released to the public directly. **All code created by this NDA program is to be released under the GPL for inclusion in the main kernel tree, nothing will be obfuscated at all**.

Good going, BSD guys.  Learn to read.
This does have code, sometimes. See e.g. adaptive Huffman coding or Morris-Pratt algorithm.Psh, that's part of the refuctoring :PI find that sugarcoating distracts from the point you're trying to make. If you are direct you leave less room for misinterpretation. Often, it's challenging to find where the actual disagreement in a discussion lies (at its core). The more words you use, the harder it gets.

When playing a political game, the tactics described in the excerpt may be very useful to convince others. But in a mostly merit-based environment, I don't think it's the way to go.

I try to live by these rules instead, as far are debates/discussions are concerned:

1. respect people (no insults and ad hominem attacks)
2. don't respect opinions
3. respect arguments (if valid within context)
4. don't let people get away with logical fallacies
5. don't get sidetracked (almost impossible)
6. give acknowledgement when somebody makes a valid point, especially if you can't rebut it
7. don't participate in a discussion if your goal isn't to change your _own_ opinion as well as your opponent'sCurrently there is no language with the usability and expressive power of Ruby/Python and the efficiency of C (or C++ or Objective C).  I wish there was.yeuch? sploinks.In Python, probably the best ways to drop to C (or C++) are pyrex, swig, and ctypes.  These all have their idiosyncracies, but they pretty much take care of reference counting for you.The difference between now and ten years ago is that the "scripting language to C interface" tools have gotten better, and computers are much faster.  The former means that "dropping to C when needed" is feasible for more projects, and the latter means that it's needed a lot less.this is weirdWhy all the angst over PDF documents? Is there a platform where they're not well supported?C has been the "new assembly" for about 5 years now.  The signpost for me was the Windows driver development kit for XP.

If you think about it in 'layers' or 'planes' we're now three steps up from assembly (assembly&gt;C&gt;C++&gt;Today[C#/Python/Etc.]).

There's really very very few cases where the trade of performance for development time is worthwhile.  Those few places include driver development and high performance graphics.  For the latter we have HLSL which is the equivalent of C for code that runs directly on the video card, for the former we've been using just C for quite a few years now.
&gt;Is there a platform where they're not well supported?

All of them on occasions.  For some people, PDFs lag or temporarily freeze their system/browser as Adobe tries to load it.  The latest Adobe reader has changed that for me, but until 7.0 I *hated* clicking on a PDF and having it load in my browser.The article being ample evidence of why he wasn't offered a job.It's not angst, it's a plea for courtesy.  From the reddiquette (http://reddit.com/help/reddiquette) list:

"Put the file type at the end of the title if it is something other than html or text, like [pdf] or [video]."

Some people come to the content via RSS and since that is a redirect it's not always easy to tell where a link is going.  Putting that tag in there helps that.No, I didn't mean that genius or wise or smart only applied to either functional or imperative or any other *kind* of programming. It's about breadth vs. depth of knowledge, at least that's what I got out of it.&gt; there is no language with the usability and expressive power of Ruby/Python

You can match or exceed the usability and expressive power of Ruby or Python easily, if you accept [that 'usability' is more than 'approachability'](http://modeemi.fi/~tuomov/b/archives/2006/09/29/T23_35_30/).

&gt; and the efficiency of C

And although this is more difficult, if you accept the previous you can at least find systems that lend themselves more to efficiency than Ruby or Python do.

If you refuse to accept that, or if you accept it only insofar as you agree that you meant to lament about approachability, I think you will still find some happiness in this 'D' language that people talk about.What's wrong with it is it isn't true. The capacity of a pipe is a function of the material of the pipe and is pretty much impossible to change after it's been laid down. 

For fiber optics it's a completely different game since we are far, FAR from using up their physical capacity. Rather, their capacity is that of the lasers lighting them up. Which means that if you run out of capacity, you just add more lasers.lol, doesn't windows suck???Yes, PDFs are not well supported on electronic viewers.   You know, like a computer monitor.

PDFs are designed for printing, not to read onscreen.   So they work baddly on my screen where my window isn't (after any magnification) the exact size of one page of paper.   That means I can't just scroll down to read, I have to read one column, then scroll over and back up to read the next because the text doesn't reflow to fit my screen.

Now this is a slide presentation so the scrolling problem wasn't as bad this time (but I still had to scoll over and then back or expand my window).   Some PDFs are so unreadable that I give up despite interesting information.What I don't understand is why you would WANT this type of security.   I get bored working on the same code all day every day.  I want my code to be readable and easy for any idiot to maintain.   I don't want to be called back to change some minor every few weeeks."Point being that in some circles, Assembly is still very much alive and C is still thought of as high level. Lets all take a moment and thank the embedded programmers ’round the world for allowing app devs to think of C as Assembly 2.0."ya but if you're in a small IT shop then refuctoring doesn't even matter since you're the only one who will probably be using it.OMG!! PONIES!!![removed]I think the downvoting is unjustified - he makes a valid point. Not so much about garbage collection, but about having to make sacrifices in general.

Even if you use the ruby-gc so you don't have to worry about your heap allocations you still make the job much, MUCH, more difficult by dropping to C. You have to worry about issues such as stack depth (when using recursion), about the different allocation schemes, about pointers, about initializing variables, about _everything_ else.

Not to mention it's still possible to confuse the ruby-gc and create memory leaks with your C extensions that may manifest themselves only after 30 hours of heavy computation. (Not 100% sure about this in Ruby, but it's the case with other vm-based languages)

Combining C and Ruby is not pretty.

Arguably, the drop in abstraction between Ruby to C is far more significant than the drop between C and assembly. If you were to use _Java_ to speed up Ruby code, THEN we're making a fair comparison. The difference in abstraction between Ruby and Java is about the same as between C and assembly.&gt; Craziness is doing the same thing and expecting a different result

Hurray for threads!I think this rule simple enough:

1. Behave in a manner that will stir admiration from your years-from-now self, when she notices these pages.Yes, how terribly puerile of him to expect that a wealthy and self-consciously respectable corporation would hold to its own promises.[deleted]Assembly is for wimps.  Real men write in machine language.
[qhasm](http://cr.yp.to/qhasm.html) is the new assembly.Maybe this kind of thing is a sign that Reddit should automatically detect obvious file types and append that type or it's general classification.

More clarity, less comment threads consumed with bitching.
&gt; You can only find truth with logic if you have already found truth without it. -- Gilbert Keith Chesterton

This happens more often than some mathematicians lead on.

Textbooks are written as if one theorem logically leads to next, and so on. In reality today, however, many mathematicians have computers crank out numbers and look for patterns. Then, they try to construct a proof that the observed pattern holds for all numbers.&gt; so why bother?

I believe one considered reddit business direction was to provide a content ranking service for business or publication sites.  Remember the Salon sub-reddit?  I guess Wired wanted something like that.

You know how Amazon gives you decent customized recommendations and does a good job showing what's popular in a category?  Many sites could use that sort of functionality but they can't build it themselves.  Make it so they can paste bits of javascript into their pages and then retrieve useful data from an API to show good recommendations, or just get a reasonably good but customizable reddit-like front page.  The default "sub-reddit"-type page is obviously completely unsuitable for most businesses because it's insufficiently branded.  You'd also have to scrap the voting system and work it with page views.  Normal people don't want to upvote/downvote content, and especially not on the typical site.  And we're talking about much more complex algorithms for analyzing relationships between categories and products/articles.

It might be a viable business.  Charge for API calls above a certain level.  However, it's also a much harder problem.  The Slashdot "me-too" business model was probably never a good target considering Slashdot does not rake in the cash.

The idea of expanding Slashdot to all the content on the web was flawed because only a tiny fraction of people are interested in the Web as a participatory experience, and they're disproportionately geeks.*New*?&gt;-- bugs that are not fatal or extremely annoying give people something to bitch about without chasing them away

&gt;-- bitching about the same topics builds communities

Interesting observations.  I'll keep them in mind when trying to start communities in the future. :)See also [How Microsoft Should Have Played the ODF Standards Game](http://stephesblog.blogs.com/my_weblog/2005/12/how_microsoft_s.html). If they had joined the ODF standardization effort at the beginning, they could have thrown enough sand in the process to provide ODF support in Word *and* maintain their monopoly.Pretty funny, this kid thinks you actually receive expense checks!! Welcome to corporate America!more liberal spim/span
&lt;snicker&gt;Yup. I don't expect them to open the source, but some compromise should be possible. I recently suggested they come up with some sort of NDA-based bounty agreement to let some of us at their code. Alexis and I are both waiting for the lawyers to get back to us :)&gt;The former means that "dropping to C when needed" is feasible for more projects, and the latter means that it's needed a lot less.

Dropping down to C, no matter how nice the bridge is, still leads to serious headaches and problems.  Ruby apologists generally use this whenever someone notices how incredibly slow it runs (I have yet to hear a Python programmer use that as an excuse).  Experienced C programmers know that you only use inline assembly as a last (and I mean last) resort.  

Sorry, I would rather stick with a single language that runs well than the Frankenstein monster you are advocating just to get decent performance.Making a DLL the way you want works just fine.  What doesn't work at the moment is having this DLL dynamically linked against other Haskell DLLs.
It's a shame, but there is just so much man-power involved in ghc.
For more information see the classic "Gang of 212" book[deleted]&gt;&gt; Even after the function has returned, and its local scope has been destroyed, the local variables remain in existence as part of the closure object.

&gt; So storing a block of code into a variable makes it a closure.

No, it's sort of the other way around.  What makes a block a closure is when it uses variables from the function it was defined in.  The block "closes around" those variables, which would normally be popped off the call stack when the outer function exits.  If the function returns a closure, any local variables used by the closure are allocated on the heap and are accessible to the closure as long as it exists.Hm, I don't -see- any evil in this, which only means that the evil is far too insiduous for me to take in just yet!Windows is all right, but Adobe Reader sucks. Try [FoxIt Reader](http://www.foxitsoftware.com/pdf/rd_intro.php). It's so much better and faster.I have to admit that for the sort of software they're writing (fancy GUIs for creating interesting books) they have a pretty unimaginative sight.  At least I felt that I really had to look around to find some screenshots and was only rewarded by a grey one with some flaps.&gt;"scripting language to C interface" tools have gotten better

Not true.

In VB6 (similar in Delphi or some .NET language): 
    ...
    Private Declare Function FindFirstFile Lib "kernel32" _
       Alias "FindFirstFileA" _
      (ByVal lpFileName As String, _
       lpFindFileData As WIN32_FIND_DATA) As Long

This is the simplest possible. Note that these languages can just call C routines directly.

It's more complicated in [Python](http://docs.python.org/api/api.html) or [Java](http://java.sun.com/j2se/1.4.2/docs/guide/jni/spec/jniTOC.html), where you have to go through the language facilities to call C.

It wouldn't be simpler in, say, SWIG. What language/tool are you talking about?

What changed it that everybody knows that a language is dead in the water without C interface and act on it. But better than before? I doubt it.Are you seriously asking why people don't get around to building an improved version on a site targeted at chronic procrastinators?Judging by the halving of the reported comments on this link, and on another, I think the phatom-comments bug just went away.

EDIT: ah, nevermind.&gt; A bunch of problems: their 'number of comments in this reddit' regularly indicates one more than the -actual- number of comments.

That's a side-effect of the way we block spam. Spammers still see their comments, while other users don't.

&gt; You can't post a link with a broken title, delete that link, and re-submit it.

Yes you can.

&gt; You can't have OMFG LESS-THAN SIGNS-IN MARKDOWN CODE. 

See below...

&gt; People regularly ask for additional subreddits, or tags, or tags instead of subreddits, and such things, and the reddit people for whatever reason cannot respond to this requests, or try them out, or even appear to have any progress with adding value or fixing bugs.

We are working on much of that as we speak. We have been slow for a while; however, it should be fairly obvious as to why.

EDIT: http://webpy.org/track/browser/markdown.py is the markdown compiler we're using at the moment. Knock yourself out; send a patch.

EDIT AGAIN: If you actually want to mess with markdown (and have it show up on reddit), send the patch to steve @ reddit.
[removed]If there is one bug that annoys me, it's that when I click on the feed from google's RSS link to get to the reddit-new page, I no longer can find the articles I had listed.  For some reason, the new listing is always oddly orderedWell good luck on your $100. I've had mixed results over the years...but the point is, it's not uncommon to have an expense check go AWOL.There are two sorts for the new page- all and rising. "all" is simply sorted by date, and rising, which is the default, which is similar to new, except it takes into account how many times a link has been drawn, and how many people have voted up on it.

The problem is probably that your RSS feed and the reddit page are showing different sorts.I just got to this page with that quote in my clipboard.

Concrete evidence of [sapir-whorf](http://en.wikipedia.org/wiki/Sapir-Whorf_hypothesis), perhaps: if you program imperatively you're more susceptible to premature optimization.[deleted]&gt; I find that sugarcoating distracts from the point you're trying to make. [...] The more words you use, the harder it gets.

Amen!

Write arguments like you write code:  as concentrated as possible.You mean Ass-Enter?&amp;nbsp;8. In a public debate, don't expect to change your debate opponent's opinion. You are arguing to the unseen masses reading your posts, not your putative debate opponent.

You two wouldn't be debating if you weren't already so convinced of your rightness that you're willing to defend it in front of an unbounded number of people. (There's nothing wrong with that, _somebody's_ got to be that convinced or there's no debate, and that would be bad overall.)

In particular, don't keep posting and posting ad infinitum "until your opponent sees the light". That's just annoying, because it's not going to happen. 

I do try to post until my opponent seems to _understand_ what I'm saying, which is a larger challenge than it should be (quite a few people simply refuse to understand anything they don't believe; it's worth a moment to ponder what that means), but given my parenthetical I tend to give up after about the third try. Some people just can't seem to help reading "My scarf is deep blue with a hint of green" as "What? You think the war in Iraq is _justified_? You're a Nazi! Nazi Nazi Nazi!" or some other equally inflammatory thing.Ah, thanks for the tip :)

*Edit:* I tried both views but neither match to the RSS feed that google seems to be parsing.C was the "new" assembly in the 1970's. Glad you guys are finally catching on.Looks like there's a ton of other cool things on the NIST-SQG site:
http://www.itl.nist.gov/div897/projects.html

They have a code repository for security vulnerabilities here:
http://samate.nist.gov/SRD/view.php
Besides Cocoa#, the [Dumbarton](http://www.imeem.com/developers.aspx) Objective-C/C# interop library seems quite interesting (used by the [imeem](http://www.imeem.com) "social" messaging client)ah, colorForth, the Duke Nukem Forever of operating systems...This is fantastic.  Previous GPGPU programming methods had been quite complicated.  Now almost anyone can write GPU programs.

For other general-purpose GPU programming information, check out gpgpu.org[deleted]Believe it or not, there are companies big enough to hurt Microsoft.&gt; That's a side-effect of the way we block spam. Spammers still see their comments, while other users don't.

Ah, so Reddit tries to delude spammers into a world where their messages are not dealt with, and delude all the other people into a world where spam does not exist, and then there's that many-worlds interference that causes all those people to ask: what the hell?  Trendy.Here are slides from an nVidia presentation describing the architecture of the new GeForce 8800 for which this is a compiler.  Very cool stuff.

http://www.gpgpu.org/sc2006/workshop/presentations/Buck_NVIDIA_Cuda.pdfIf anything, we strive to be trendy.You mean to say that in the future, I will be a woman???You are right, here are a few other links to look at: http://www.futureofthebook.org/content/Mellon.pdf
http://weeklysqueak.wordpress.com/2006/12/20/oopsla-there-it-is-oopsla-2006/

The second one is a video that demos Sophie.  The demo starts at: 23:32.

Hope that helps, I'll update the article to provide these links.  Thanks.I noticed the screencast was almost exclusively mouse-based navigation. How hard anecdotally is using the keyboard for all those operations, for typing in various pigeonholes where I'm used to having all my code in a single flat file?Instead of listening to the FoTM fan "boiz":

There's a few C++ functional options, but the easiest is a few sections of the Boost (boost.org) libraries.

boost::function gives you some almost-sorta first class function object, boost::lambda gives you some pretty decent lambda creation, and boost::bind for binding arguments of both function objects and lambda objects.

And example from the docs (reddit is going to butcher the formatting, so I added some dots in there to make it readable):

std::vector&lt; std::vector&lt;int&gt; &gt; a;

int sum = 0;

std::for_each(

. . a.begin(), a.end(), 

. . . . bind(ll::for_each(),

. . . . bind(call_begin(), _1),

. . . . bind(call_end(), _1),

. . . . protect(sum += _1)

. . )

);&gt; What language/tool are you talking about?

[This Mercury example](http://www.cs.mu.oz.au/research/mercury/information/doc-release/mercury_ref/Impurity-Example.html) shows the old, obsoleted interface to C.  [This Mercury page](http://www.cs.mu.oz.au/research/mercury/information/doc-release/mercury_ref/Language-specific-bindings.html) shows the new, preferred bindings to C, Managed C++, C#, IL, and Java.

For all of these, you:

1. supply the standard predicate/function assertions that you do for any normal Mercury code;

2. have something like `:- pragma foreign_proc("C", c_time(T::out, IO0::di, IO::uo), [will_not_call_mercury, thread_safe, promise_pure],` instead of the normal-mercury-code `c_time(T, !IO) :-` for the clause;

3. write the target language in a Mercury string, referring to the variables T, IO0, and IO.  The IO stuff doesn't appear in the compiled code, and only blesses us who are about to do IO;

4. remember to have `:- pragma foreign_decl("C", "#include &lt;tcl.h&gt;").` or whatever, and to add -ltcl and such flags to your Mmakefile (not a typo);

5. compile your Mercury program just like any -- the build system will manage everything else.

This is pretty simple, and as Mercury compiles to these backends that you write code for, there's very little overhead for their sake.  Languages without lengthy compilation steps and presupposed static natures might need some extra work, but such work is not a curse that all languages must suffer: gforth, a forth implementation written in GCC-C (mostly: forth), offers C interaction at least as simple as in your VB example.  You can't write C inline in gforth (as in Mercury and Perl's Inline::C), but wrapping C functions requires all of:

    require lib.fs  \ the FFI, written in forth as a library.  unmagical.  much.
    library libc libc.so  \ finds "libc.so" the normal way, dynamically loads it, creates a LIBC word to deal with it
    libc sleep int (int) sleep
    libc open  int int ptr (int) open
    libc lseek int llong int (llong) lseek64
    libc read  int ptr int (int) read
    libc close int (int) close
    \ that's all obvious, isn't it?
    \ The only difficulty here comes in not getting to use C headers,
    \ and having to deal with things as they really are.
&gt; You two wouldn't be debating if you weren't already so
&gt; convinced of your rightness that you're willing to defend
&gt; it in front of an unbounded number of people.

That's a depressing way to look at debating... It leaves no room for people debating because they want to better their understanding of a topic.

If you read an essay, all kind of counterarguments pop up in your mind for every paragraph you read. Unless you make an effort to analyze the merit of each of these counterarguments, you'll be tempted to dismiss the conclusion of the essay (unless you happen to agree with it from the beginning, in which case you'll reinforce your belief). It's hard to persuade people through essays, because of this. In a debate, these counterarguments are often easily shot down by your opponent. Arguments that sound convincing in your head, often sound dubious when written on paper. So you're forced to think  more and to phrase what you say more carefully - and I think that is a goal in itself.I have many years experience with both Python and C, so "approachability" (which I think is a useful concept) isn't something I think about with respect to these two languages, except when considering others.  By the definition of usability given in your link, I know of no languages that are significantly better than Python, and none that are close to it that are significantly more efficient.  I wish I did.  Probably the closest thing I see is OCaml, which isn't quite as usable, and is pretty far from the mainstream.

I keep hearing about 'D', so I'll have to check it out.  I have little hope but that maybe it has most of the advantages of C++ without its many horrific disadvantages.Good point. I stand corrected, but still reserve the right to complain when it's compared alongside general languages. &gt;:)

Coincidently, I've been reading On Lisp, which describes the "building up the language" concept rather well.I have one of my own:

"Redefine the problem and it often goes away"Wait, it's written in Java?  Vim or Emacs as the programmer's text editor, I can see (the former has a scripting language and is extensible with native code, the latter has a custom Lisp), but I don't know about this.&gt; I have yet to hear a Python programmer use that as an excuse

Oh, ye of little history.

&gt; Experienced C programmers know

The more experienced they are, the more they'll remember when they did more of it.
[deleted][deleted]I think counting page views alone might differentiate better content titles, not better content; i.e. what if you read something that is a shit sandwich?you have a point.. i did find it calming, but it could be just the colour scheme though.Hmm.  If you're willing to trade away usability for performance, shouldn't you do all of your programming in assembly?

The multilanguage programs are not pain-free--that's for sure--but writing everything in C entails a lot of pain, too.  For the work that I do, for 95% of the programs, I never need to drop to C at all, and that's a big win.[Eh?](http://www.colorforth.com/install.htm) :)Stupid idea. Ugly, unreadable, unportable, antiquated, nonstandard, useless.&gt; I know of no languages that are significantly [more usable] than Python

By the definitions in my link, I'd argue that Perl is much more usable than Python, and still easily more efficient, as it (Perl) does not feel ungraceful when it adds array pre-sizing like `my @arr[5000];`, whereas Python will gracefully and slowly realloc the array hundreds of times as it increases and decreases in elements, with no programmer control.  There: now you know one :-)

Although O'Caml is far from the mainstream, it has an excellent system: an intepreter -and- a compiler, with powerful macro capability, and OO, and strong FP, and a system compilable from portable, maintained C (take -that-, GHC!) that also bootstraps itself.  Less usable?  I don't know: the static-type folks think they have some kind of advantage.
I think the mythical Lisp operating system has him beat when it comes to vapor. Hell, there was even one project actually named "vapor" (No longer hosted, of course).&gt;We are working on much of that as we speak. We have been slow for a while; however, it should be fairly obvious as to why.

Did you even read his comment? The **whole point** is that if you want things to go much much more quickly, **open the source** and let us fix it for you.Go meta. You can be convinced of the rightness of your desire  to learn more. You can be convinced of the rightness of the need for more information. 

I know these things are truly beliefs and not "cosmic truths all accept" because I can name names of people who actively don't believe those things. If they popped up in this argument, you would not be able to convince them of what you are arguing for. (I'm refraining from naming the names, lest I summon them.) And from experience, they would viciously attack the idea that they don't know everything they need to know to make their 100% solid concrete declarations of truth. I've seen it with my own eyes.

You're only going to debate what you truly believe in, but I have no problem believing and debating contingencies and conditionals.&gt; You mean to say that in the future, I will be a woman???

The future will have some kind of sexual drop-down box, or perhaps you'll select yourself and a glowing sphere of pastel-colored controls will surround you.  And then you'll glance at a billboard and be turned into a woman by neocyberfeminist malware.  So, yes -- but you'll be sympathetic and understanding to yourself, I'm sure.  You jerk.To follow up on this post, Mark Finkle (who also works in Developer Relations for Mozilla) has just posted an [online/offline-capable web application](http://starkravingfinkle.org/blog/2007/02/firefox-3-offline-app-demo/) that demonstrates much of what was talked about in this post.

There's definitely a lot of potential here, we'll have to see how it pans out in the end. At least these problems will be easier for web application developers to work around - we just need the user interface to catch up to the rest of the process.Dunno.  Been watching only from a distance.I'm willing to go so far as to state that people who are not willing to seriously debate a topic from another person's position every once in a while, are not worth debating in the first place.

If somebody has a problem with debating a conditional (or the implications thereof) then how can he possibly understand what his opponent is saying? After all, it is just the conditional: "If my opponent is correct, what do my arguments look like compared to his own?".

When two people are stuck in a debate, we need a referee to shout "SWITCH!" to make both parties switch sides. If a party is unwilling to switch sides, he loses by default.+1 for mentioning a Prolog-like language.  I used sicstus and quintus prolog, and even CLP(R) for a couple of years.  Really, really nice, but absolutely impractical outside of a fairly small domain.  (Allocating lists of millions of integers for graphics algorithms seems inconceivable, for example.)

Mercury looks interesting, but I would find going through the compile cycle pretty unpleasant.  Also, no Debian package, which I count as one indication of whether a language is plausible for production use.  (Realistically, I'll probably never meet a Mercury programmer, so it'd be kind of difficult to justify using it on a paid project.)You know, it's impossible to be both standard and innovative at the same time. So, it is important for some things to be nonstandard. Also, this may have been a failed experiment, but that doesn't mean it was a stupid thing to try.ColorForth is ready to use.I think we will see some JS libraries making use of whatever offline capabilities a browser supports. And as is the usual case, the libraries will have to create a common denominator API across browsers.

But this will certainly help developers add some level of offline support to web apps.More like command line.  Surprising what you can still get done in Bash with a bit of knowledge.  Even GUI programs in Linux can often be invoked at the command prompt.  The more I'm at the home row of the keyboard, the more efficient I am.I did read his comment, and I replied to the portion that wasn't a personal attack against myself.Awesome, a fucking PDF BOMB on the front page.  THANKS ASSHOLE.[removed]I have been programming in Java since before version 1.0. I always have to look at a book when I want to do IO - I guess I must have a mental block about Java IO! 

I can usually guess what to do in Ruby or Haskell.&gt;...shouldn't you do all of your programming in assembly....but writing everything in C entails a lot of pain, too.

 There are more programming languages out there besides just Python, Ruby, C, and Assembly.  And many of those programming languages have a nice balance of performance and usability.

&gt;I never need to drop to C at all, and that's a big win.

Then why advocate doing it?  Hey, I never jumped off a bridge because I could get killed but you should do it!&gt; Stupid idea. Ugly, unreadable, unportable, antiquated, nonstandard, useless.

Enth/Flux took many of his ideas and put it in a more usable package, also a native x86 system.  It had 'Enth', a standard Forth system, and 'Flux', a Color Forth system, that could call each other and that you could switch between with a keypress.  All of this written in (standard) Forth, in blocks, with a block editor.  Flux shocked me with its core simplicity: it turns STATE into a jump-table!  ANS Forth has a whole cell for STATE , with only -1 and 0 as meaningful; Flux had STATE with as many values as colors in Color Forth, with the colors as simply whitespace tokens in the source that STATE got set to, affecting following interpretation, and incidentally affecting how the editor displayed that source code.  Chuck's other stuff like 256-byte blocks and huffman encoding and hybrid dvorak input -- all ancilliary to a literally beautiful concept and a few language niceties.

([My PC speaker driver for this system, here in ANS Forth](http://www.quartus.net/cgi-bin/twiki/view/Main/EnthPCSpeakerWords) -- I also wrote it in Color Forth, under Flux, but that hardly matters for this code.)Folks, I have attained enlightenment and can now code the entire universe with just the number 1.That's a final version? It does mention things called 'floppies' in the page...

Forths are a bit different than traditional systems in that regard, but I'd like to see some independent users and especially some applications before I'd regard it as somewhat mature.[deleted][Shedskin](http://mark.dufour.googlepages.com/) is along the same lines as pyrex (compiling Python-like syntax to C).

[Weave](http://projects.scipy.org/scipy/scipy/browser/trunk/Lib/weave/doc/tutorial.html?format=raw), which was developed for speeding up Scipy / Numpy's number crunching routines, has very complete support for embedding C code in Python.  It also has a feature called `blitz` which can convert certain Python expressions to fast C code.

There's also [Cinpy](http://www.cs.tut.fi/~ask/cinpy/) and [PyInline](http://pyinline.sourceforge.net/), which are a little more lightweight, but very simple to use.[gpgpu.org](http://gpgpu.org) for those slightly lazier than myself.&gt; Mercury looks interesting,

It is both purely logical (I've seen Mercury people sneer at prolog for 'logic programming' -- it's wonderful!) and much, much more suited to industrial use and efficient algorithms such as your graphics one.  The compile cycle is a mixed blessing, as it always is, but not everything can be Erlang and Smalltalk :-)[deleted]&gt;Oh, ye of little history.

Please feel free to enlighten everyone with your vast historical knowledge of everything Python.  That seems like a noggle point all things considered.

&gt;The more experienced they are, the more they'll remember when they did more of it.

No, we didn't.  Assembly does not guarantee better performance than C.  Most books on assembly even state that the language should be used in moderation.&gt; That's a side-effect of the way we block spam. Spammers still see their comments, while other users don't.

It's still unfortunate.

&gt; http://webpy.org/track/browser/markdown.py is the markdown compiler we're using at the moment.

Excellent!  Thanks![1] Our verbose peers would undoubtedly ramble.  [2] I agree about the principle and I [3] have learned that comments could go long, [4] but this comment has less words than yours.It's not supported very well on Windows Mobile.I think that you can generally get the same performance in Python by saying "arr = [0] * 5000".  It is true, though, that Python doesn't visibly distinguish between how many elements a list has and how many are "allocated"--I suppose this might matter in a few cases.

Generally, though, I just disagree on Perl.  The only less usable language that comes to mind is C++.  For example, what does "/$foo[bar]/" mean?  How is "baz +1" evaluated?  If you think you know, you're almost certainly wrong...And yes: it is a year old.[deleted]I hope you are right, and I just misunderstand things.

Cause right on that page, manual says that you have to create a separate C file:

    #include &lt;windows.h&gt;
    #include &lt;Rts.h&gt;
    
    extern void__stginit_Adder(void);
    
    static char* args[] = { "ghcDll", NULL };
                           /* N.B. argv arrays must end with     NULL */
    BOOL
    STDCALL
    DllMain
       ( HANDLE hModule
       , DWORD reason
       , void* reserved
       )
    {
      if (reason == DLL_PROCESS_ATTACH) {
          /* By now, the RTS DLL should have been hoisted in, but we need to start it up. */
          startupHaskell(1, args, __stginit_Adder);
          return TRUE;
      }
      return TRUE;
    }

and then compile it with с compiler, and then get back to haskell and include it into compilation and that will give you dll.

This does not look like what i want.
&gt; It's much easier to criticize than to do.

The N&gt; thing is my way of getting to 0-base my comments in somewhat readable manner.  If you want to 1-base yours, you may as well use the provided markdown.
Not if you could fix the circular reference problem. 

Mark-and-sweep, Mark-and-compact, and their ilk are pitifully slow. If real garbage collection worked the same way Java garbage collection does, the garbage man would drive his truck up to your front door every monday, walk into your house, pick up each thing you owned, in turn, and ask "Are you still using this?", and loading it onto his truck if you didn't answer "yes". 

Really, reference counting works just fine (and fast) for 99% of objects, without needing a typical stop-the-world garbage collector. Fix it for the other 1%, and we'd never need use anything else.

(Actually, I figured out a way to do this a while back, but it has to be built into the design of the native types, since it must differentiate between pointers on the stack and pointers on the heap; and I'm too lazy to write a new language without being paid for it, as compilers are quite a bit of work. Perhaps when I go back to grad school...)I am sorry, permission denied. Microsoft has already patented the 1's&gt; If you think you know, you're almost certainly wrong...

Learning the language is an 'approachability' issue.

&gt; I suppose this might matter in a few cases.

I has never mattered to me, as I don't use Python, but I witness some educated griping on the subject :-)
Ruby apologists? I didn't realise it offended you so badly.I looked on the Maxthon Web site but couldn't find any mention of their Linux version.  Do you happen to know when that will be available?  ;^pI'm advocating a strategy of doing most programming in a high-level language (e.g., Python, Ruby, etc.) together with dropping down to a low-level language (e.g., C) only when needed.  I follow this strategy and so far the results have been pretty good.

If there were a single-language strategy that worked better (broadly, considering *all* of the constraints we deal with), I'd be all over it.  It has to actually *be* better, though--not just the best of the single languages.  I'm open to suggestions.&gt; In a demo released by meqon, there were 'zombies' with procedural, physical animation.

For what it's worth, you were able to handle this sort of thing with Novodex/PhysX way before they bought out Meqon. All you need are position and velocity motors. You basically set up a rag-doll and attach motors to the joints that try to match an animating pose. So the zombie would start out 100% kinematically animated, and when a physical object collides with the zombie you'd quickly blend from kinematic animation to physical simulation, and eventually blend back to 100% kinematic animation if/when the zombie has recovered his stance. For the game I'm working on we do this kind of thing for a subset of a skeleton to deal with reactions to minor hits, like gun shots--so if you shoot someone in the shoulder with a weapon with a lot of stopping power, you see that part of the upper body jerk back realistically.

&gt; I think his description of the AI is a bit over-glorified... Most of the work is likely done by the physics engine - fakey Inverse Kinematics using the physics.

That way of faking IK sucks really hard. Heck, even the usual ways of doing IK suck really hard for games, aside from really limited, controlled cases like aiming. Something I've been experimenting with at work is getting the same effect as IK by blending pre-authored poses. The idea is that you author certain poses and associate each pose with a point in space and an influence fall-off factor. To determine, at run-time, the pose corresponding to any point in space, you blend the associated poses of surrounding points using radial basis functions parameterized by the fall-off factor. I think the Mech  Warrior 3 guys did something like this, and there has been recent academic work in the same vein by Peter-Pike Sloan, et al. Anyway, the main reason this approach is really attractive, at least compared to traditional IK, is that it hands over control to the artists, where it should be.

&gt; Not saying it's not cool, but I don't like sensationalism in such matters. And not giving credit where it's due "You'll only see this at Lucas Arts" - yeah, right. I've already seen it in physics demos...

Yeah, I know what you mean. Last time I checked (which was admittedly a few years ago), Novodex's fracture system was really heavy on the CPU, especially the dynamic tetrahedralization. If that's really what they're using for this game, I think they have a lot of scalability headaches coming up. As far as I know, no game has ever shipped with that part of Novodex's tech before, so it's largely untested in a production setting _and_ it hasn't been receiving any from Ageia due to a lack of developer interest. Catch 22.&gt;Ruby apologists? I didn't realise it offended you so badly.

Replace "it" with "they" and you are correct.&gt; Cool, in the future you should label stuff as being 
&gt; a pdf though. :)

good point! It completely slipped my mind.Quick, someone make all my [VSTs](http://www.kvraudio.com/) run on this! I need 15 more layers of Absynth muddying up my mix yesterday, each with its own convolution reverb.Wait, you mean I actually have to click that to go there?http://common-lisp.net/project/cl-markdown/

:-)
Errr... you need GeForce 8800 to play with this stuff first. So hold your pants on.According to ctkrohn's linked pdf, Nvidia's version blows CPGPU out of the waterActually the CUDA development kit allows you to emulate GPU programs on your CPU... so you can play with it on any Linux or Windows computer.

But you're right, unless you have a GF8800, this is definitely in the "cool but useless" category.&gt; If you're saying that semantics serve no real purpose if there is no way to 'access them' through some syntactic element, I agree. But the semantics are often formally defined completely independent of the syntax.

Yes, that was what I meant. I realize that argument was a bit silly.  My idea is that apotheon in a way conflates syntax and semantics to an overlapping mess, and I tried to clearly show how they were separate. The technical expression is that they are orthogonal issues?

Your thought experiments are a good addition to the discussion, thanks.[removed]In theory they are indeed orthogonal. In practice they are almost always orthogonal. A few programming languages turn it into an overlapping mess, and that's generally considered bad form.This will be grrrrreat ![LLVM](http://llvm.org/) is worth a mention.Excellent resource to get that final 1% performance increase you've been shooting for.There is no "final" version. It easily fits on a floppy (cd or dvd would be a big overkill).

ColorForth for windows: http://www.geocities.com/eleks_76/The latency would kill you. :)[deleted]I was reasonably involved in that project. I was too inexperienced at the time to implement some of the more difficult ideas. It started when I was in high school, so that is to be expected. The hope for incredibly fine-grained orthogonal persistence was probably the most 'far-out' feature, but it is one that might yet be possible to implement in the future.

Vapour is still my dream, but I am not sure it will ever happen. Now that 64-bit hardware is more ubiquitous, it is at least possible. Regardless, some of the ideas we discussed for Vapour are still going to be used in the more immediate future. For example, now that I have more free time, I am actively working on the compiler that I designed back then.&gt; EDIT: http://webpy.org/track/browser/markdown.py is the markdown compiler we're using at the moment. Knock yourself out; send a patch.

&gt; EDIT AGAIN: If you actually want to mess with markdown, send the patch to steve @ reddit.

Will this patch actually make it into the (open-sourced) Web.py distribution, or would it be Reddit-only?That's not programming so much as using a calculator.

Well then can you show me one? 'Cause that one doesn't ask the user for any input, it only shows file input.

Console IO is important because it can lead to other discussions such as what happens when the user inputs "Tree" when you ask them for a number.
I'd add: they _have_ sold to a company, and from what I'm aware, a significant portion (most?) of the income generated by reddit actually comes from setting up reddit clones for companies.  If it were open-sourced, that revenue stream would disappear.How??Dudes, what was his idea?  I can't gleam it from the website.To be honest, I'm not sure if Aaron intends on maintaining markdown.py.

I'll certainly pass any working patches along, but I've never pulled from webpy.org itself since there tend to be painful (for us) bugs.

We could keep our copy online somewhere if it's different, though.[removed]vastitude, yeuch, sploinks

all good. i will keep those handyScott Aaronson posted this "anti hype" faq and included a reaction from Lawrence Ip. It appears that the talk itself made no crazy claims and was founded in firm science. It is not a universal QC by any means. Nor do they claim in their talk that a universal QC would be able to solve NP-complete problems easily. 

So I guess this is just a case of market-speak drones abusing terms to gullible journalists in order to get gullible investors. It's like 1997 all over again! ;)

http://scottaaronson.com/blog/?p=198
This is supposed to be funny, but it just ends up being very true.

I used to work in a medical device company regulated by the FDA.  FDA inspectors have to verify that you have followed the FDA regulations.  If you do not then they have legal powers to fine you, shut you down, or in extreme cases even arrest you (in theory a magistrate grants them warrants and judgements but, short of gross abuse of powers, their word pretty much goes).  And it was exactly like this.

Paul.Any reason not to use [python-markdown](http://www.freewisdom.org/projects/python-markdown/)? It's got a few nice extensibility features, and doesn't suffer from some of the more annoying bugs in the version Reddit uses (particularly, it's smart enough to ignore underscores within a word).Interesting, and thanks for saving me for a potentially costly mistake.  I think I may go with Pylons instead...

Incidentally, could the Markdown bug simply be that &lt; was left out of the list of special chars?

    escapechars = '\\`*_{}[]()&gt;#+-.!'

I don't see any reason for it not to be there: I'd initially thought security, but if security's a problem, then &gt; shouldn't be in the list either.[deleted]They've previously released another C-like language for programming the gpu, which was decent. What was it called? C... something.Quick and dirty:

    import System.Console.Readline(readline)
    import System.Exit(exitWith,ExitCode(ExitSuccess))
    import qualified Control.Exception as E(catch)
    
    forever :: IO a -&gt; IO a
    forever a = a &gt;&gt; forever a
    
    processEntry :: String -&gt; IO ()
    processEntry "" = putStrLn "You typed nothing !"
    processEntry "quit" = exitWith ExitSuccess
    processEntry num = do
        let a = (read num) :: Int
        putStrLn $ "You typed the number : " ++ (show a)
        return ()
     `E.catch` (\e -&gt; putStrLn "You did not enter a number")

    main :: IO ()
    main = forever $ do
        entry &lt;- readline "Your entry ? "
        maybe (return()) processEntry entry[deleted][deleted][Beyond3d](http://www.beyond3d.com) has nice summaries: [quick summary](http://www.beyond3d.com/articles/cuda-quick/) and a [longer overview](http://www.beyond3d.com/articles/cuda-intro/).

And there is also the [guide to ATI's Close To The Metal (CTM)](http://ati.amd.com/companyinfo/researcher/documents/ATI_CTM_Guide.pdf).:-)

No, I just wanted to show that we can easily do IO, have exceptions, build our own control flow etc...

Perhaps too much without any explanations. So, here are some details:

forever a is repeating the IO action "a" an indefinite number of times : it is an infinite loop.  So, I am building a new control flow keyword.

processEntry is just building an answer to an entry. If I type "quit", I get the IO action exitWith ExitSuccess and the soft is finished.

Otherwise, I try to parse my entry with a read (the read num). If it fails I have an exception (hence the catch) and I just display an error message. Otherwise I just echo back my number. 

And the readline is the standard Unix one.

IO in Haskell is no more difficult than in any other language.&gt; I'd like to see some independent users and especially some applications

There might be something interesting in the [mailing list](http://www.strangegizmo.com/forth/ColorForth/), [various](http://primarycolorforth.blogspot.com/), [user](http://ray.retroforth.org/)
[pages](http://www.profibing.de/colorforth/)
or the [wiki](http://quartus.net/cgi-bin/twiki/view/Main/ColorForth).Zune ryhmes with iTune or tune... without the 's' granted, but you get the association.  

That was the closest I could figure.

And if it's not "gibberish" then the name is harder to defend as a trademark.  Microsoft certainly learned their lesson with "Windows" and "Office".Having a small niche does not equal "failed experiment".&gt; that doesn't mean it was a stupid thing to try.

I disagree.

- color is *scalar*, so it's not *precise*. It's unlimited in theory but limited by human nature. The palette of colors you can distinctly use for semantics is bounded and you'll quickly run short of them. Only an overly simple language could get away with this.

- Color perception is *unreliable* among humans, both in the ability to distinguish fine shades, and in cases of color blindness even the ability to distinguish gross differences like red/green.

- Color is tied to display technologies. You can't edit it over a remote shell that doesn't draw colors. you can't print it on mono printers (eg: laser). You can't edit it on a no-color LCD screen, such as many small devices use. Also, color display at comparable precision usually costs (eg: color laser). It isn't justifiable just for code.

- Color pulls the semantics out of the plain text. All of the many, many tools and techniques we've developed for text processing suddenly become useless.

- Finally, color in its role of visually distinguishing syntax to help the reader is better handled by a colorizing IDE, tuned to the reader's preferences.Cool... Have you thought about contributing to any of the current Lisp OS projects?Great! Now just wrap it up in a tutorial that explains everything and throw it up on a website so people will stop bitching about it.Ok, fixed.I added that because the site hasn't been updated since 2002. I wouldn't consider a project a failure just because it has few users. I would however consider it dead if it is no longer maintained.I don't have any problem with Reddit.

I think the main Reddit has seriously tanked in quality, and I barely ever read it anymore (too many 'top 10', 'cool pictures', 'Bush is evil' posts), but I love the programming sub-reddit. I don't think there is anything else like that out there.

As far as the software itself is concerned, it has a few bugs, but they've never really gotten in my way, so I can't complain too much.

I just hope that if/when Reddit moves over to tagging it doesn't eliminate subreddits. I like that the programming section is completely separate from the main site, because it has retained a good signal/noise ratio. I fear if basic tagging is introduced, that won't be the case.&gt; that one doesn't ask the user for any input, it only shows file input.

Reading from the console is also file input;  just say `getContents` (short for `hGetContents stdin`) instead of `readFile foo`.[removed]&gt; I added that because the site hasn't been updated since 2002.

I don't think Chuck Moore is exactly the online type.  It wouldn't surprise me if he put up the website and then mostly forgot about it. :)&gt;By the definitions in my link, I'd argue that Perl is much more usable than Python

How? 90% of the time spent programming is reading code...often  written by other people. If you think understanding average perl code is easier, then tell me why perl is commonly called "write only." Also, properly written perl is often just as verbose or even more verbose than python.

&gt;and still easily more efficient, as it (Perl) does not feel ungraceful when it adds array pre-sizing like my @arr[5000];, whereas Python will gracefully and slowly realloc the array hundreds of times as it increases and decreases in elements, with no programmer control.

This isn't necessarily true. Also, python has arrays. Lists are not arrays. Also, arr = (0,)\*5000. Also, arr = [0]\*5000. Besides, python 2.5 is for the most part faster than perl and about even on memory usage.


[It's fixed now](http://programming.reddit.com/info/14v8a/comments/c14vci).

I wonder if it really was just a [single character](http://programming.reddit.com/info/14v8a/comments/c14y5o)I think you missed the point. The discussion was about the usefullness of tutorials, not whether or not the capability existed in the language.I posted a link to some of his work 5 months ago:

  [Chuck Moore: from Forth Creator to DIY Chip Designer (video)](http://programming.reddit.com/info/h897/comments)

I was surprised that it didn't get picked up by the reddit crowd. This guy's work is amazing, even if "practicality" is not one of his keywords.  His NIH got him to design chips down to the transistor level!'Design' is just another word for thinking.  It doesn't matter  whether you do it before you code or while you code, just as long as you do it.That is not how colorForth uses colors:

* There's a fixed set of only 5 or so colors, period.  They are treated as tags, not scalars.
* They are not tied to any display technology:  you can render them just as well using different typography, pitch/intonation (in a screen reader), or even (\*gasp\*) conventional Forth punctuation. :)"...and he had to keep telling the rabbits, 'only two!'" -- Bill CosbyThanks!I was not pointing out that the capability exists (obviously it would), but that as far as said tutorial is concerned, reading from a file is reading from a file, no matter whether said file is on disk, or standard input.Thanks a lot. At least some of the links don't date back to 2002, maybe it's time to look at it again. Haven't heard from the project a long time, but then again, Chuck Moore doesn't seem to be the most 'connected'... Maybe ColorForth needs a DHH of its own ;)[deleted][deleted]Sick thought for today: Color APL&lt;&lt;&lt;&lt;&lt;&lt; THANK HOLY GOD &lt;&lt;&lt;&lt;&lt;&lt;I'm not so sure - how old is this? One thing you'd want to keep in mind, is how far CPUs have gotten ahead of memory, in speed. It has become cheaper to waste tens of CPU cycles if you can avoid a cache miss. I don't remember the details but, there's a discussion in the 'Judy Tree' papers. Here, for instance, 'use int instead of char': that may or may not apply to individual variables (and may change between compilers / architectures), but certainly, given the above, you'd want to use char _arrays_ instead of int arrays. Another one that fails to take the above into account is 'Make structs be powers of 2 in size' - it wastes a lot of memory (and presumably cache efficiency) for a teensy CPU speed improvement.PDF warning please.[removed];D Point well made. I'm glad we mainly agree. As to brevity, you had a goal, and the result is somewhat grammatically strained.Macs have [multi-button](http://store.apple.com/1-800-MY-APPLE/WebObjects/AppleStore.woa/wa/RSLID?mco=EB02B9EE&amp;nplm=MA086LL%2FA) mice. Besides, ctrl-click never killed anyone ;)It has a pretty decent plugin architecture and allows some scripting via the builtin BeanShell (basically a Java interpreter). So I'd say it's right between Vim and Emacs when it comes to extensibility. Compared to the usual IDEs (Netbeans, Eclipse, IDEA) it doesn't wreak as much havoc on CPU and RAM. Couple it with the JGoodies look-and-feel and you've got something you can live with.
I'm guessing Wozniak (forget Jobs) had to wait 6-8 weeks for many of his parts. Unless he nabbed them from HP.Are there unit tests anywhere?

Unit testing a complicated string-&gt;string conversion were how I got started with unit testing, after I got tired of whacking one mole, just to see two more pop up after deployment, one of which I'd already whacked. Given the horrible interconnectedness such problems tend to exhibit, trying to go without them is usually insane; changes propagate like mad.Buying a graphics card to improve your audio performance is a sure sign of oncoming Apocalypse.Touché! I was in need of an afternoon caffeine fix.  Yours is executed much more succinctly.  There is no debate needed there!Sorry, I can't resist: See, there you are, arguing something you're passionate about, in front of an unbounded number of people, while nobody else (so far) jumps in.

Can you make a serious case that it _is_ worth debating with someone who is not willing to debate the topic from the other person's point of view? (Or do you feel too passionately about it?)Funny but also an interesting point.

Assuming you've seen the click-free UI prototype here:
http://dontclick.it/

How would you implement a click-free web browser?  More specifically, could you navigate the entire web as a single continuous mouse trajectory?  Usably?It seems common that powerful special-purpose hardware gains in capabilities until it is folded back into the main system:

http://wiki.linuxquestions.org/wiki/Wheel_of_reincarnationHm, the things he liked about Esh sound a lot like the features-list of [Chicken Scheme](http://call-with-current-continuation.org/).[deleted]Oh man, one click away from freedom...enh, maybe later.Sure. If the debate is the means to an end, and not the end itself, then debating with stubborn or closeminded people can still be meaningful. For instance, in a public debate you can pressure somebody into switching to your point of view by getting the audience on your hand. In my experience stubborn people are unwilling to hold on to their opinion when they lose support all around.

Alternatively, debating with somebody who's unwilling to really consider your opinion can be worth it ssimply out of pure curiosity. Religious topics spring to mind. Most religious folks are unwilling or incapable of understanding the atheist point of view. Still, the debate can be fruitful, because you still get a glimpse of how your opponent thinks. That may be worth more than the outcome of the debate itself.

In fact, it's pointless to debate with people who are willing to consider your position seriously in the first place. After all, if they are that openminded, they are bound to do the research on their own accord, and therefore the debate will serve no purpose. A debate is to force somebody to question something he has always taken for granted, so a closed mind and a stubborn attitude is not something to be annoyed with - it is actually a necessity for a meaningful debate.

------------------------------------------------------------
No I don't believe what I just wrote. Most of my arguments can probably be countered fairly easily. Still, I think my reply is sensible enough to qualify as a defence for the other point of view.I did not mean to imply that ML is not a useful or interesting language family.

&gt; ...it would no longer be an interesting language to learn and research. Just another ML.

Should be interpreted as "ML already exists, so why repeat it?" Not "ML is uninteresting."
&gt; Is C is the new assembly?

Duh.  It's been that way for twenty years.http://future.winning.lottery.numbers.comYou're probably thinking of Cg, nVidia's shader programming language.

There's also Brook, a C-like language for stream processing with a GPU backend.I didn't know Neal Stephenson was a redditor.

Captain Murph: You're not the boss of tigerbot Hesh!&gt; How?

I certainly must have hit a chord with you, to deserve this machinegun of cliches and nonsense.I prefer to just stab people in the face, hear the lamentations of their women, then apologize if the face-stabbing is found to be unwarranted in the following discussion and/or flame war.Yeah, Cg, that's it. Thanks.* You can't add methods to classes written in C in Python.  That's not a major limitation.  Just subclass.
 * You can add methods to base classes on the fly.
 * You can do anonymous functions (lambda) just as easily in Python as in Ruby.
 * Creating a DSL is just as sanctioned and just as frequently used in Python as in Ruby.  See Spark or SQLObject or any other similar package.

The only thing that Ruby can do that Python can't is add methods or attributes to classes written in C.  Pardon me while I yawn about that oh-so-significant superiority.Now that they're gazillionaires, they could shell out for copies of Allegro/LispWorks.Hey, is &lt; working in code now?

    &lt;

Hey look, it is!main = do putStrLn "Please enter first number: "
              num1 &lt;- readLn
              putStrLn "Please enter second number: "
              num2 &lt;- readLn
              print (num1 + num2)[deleted]This sucked so bad I laughed out loud.So you think the Ruby language needs to be apologized for because of the people who use it.  Given some of [DHH's antics](http://www.flickr.com/photos/planetargon/127984254/), i can see where you're coming from; i do like the language's syntax however.  The language implementation definitely needs some work though, which fortunately should be coming along with either JRuby or Yarv.Wow, Captain Obvious has spoken...I'd go even further than that. Arbitrarily cleaving an inherently sequential process in two, attempting to run it in parallel, and then *blaming threads for breaking your wonderful code* is less a race condition and more a brain-damage condition.&gt;GHC compiles through GCC. 

You think GHC converts Haskell to C?wat and wat is this website 
Qapla!  Praise dirk tuk for this glorious glorious day in the history of the universe!!  No longer shall we wallow in the pits of assembly!Don't piss in my office and tell me its computing.  Qapla!No.  Qapla!Have a look at the site where the pdf comes from:
http://waterfall2006.com
Look at the other articles. A great prank!
Does anyone have extra information?
Respect Qapla!You can't even destroy a thread properly. That shit is dangerous.[removed]This is a book review of *On the Edge: The Spectacular Rise and Fall of Commodore*.  It's not great as book reviews go, but that shouldn't stop you from reading the book.  The book's  biggest strength and biggest weakness is the same: it's very much an aggregation of stories and recollections from some of the people involved, most of whom were engineers.  You will likely find it fascinating if you ever owned a Commodore computer, and maybe even if you didn't (I cut my teeth on an Apple IIe).

Heck, just the chapter on the development of the 6502 microprocessor is probably worth the price of the book if you care anything about the history of the personal computer.

And yes, some of the stories in this book reminded me of *Soul of a New Machine*, even though this book is not nearly so well written.  It's still pretty interesting, just the same.
What else is best in life?
At the time, our version was the only version fast enough. I think python-markdown is relatively new compared to our copy, and I try to avoid swapping out libraries if we don't have to.It wasn't; but I came across the actual bug (it wasn't in markdown.py) while investigating your comment.A big problem with mouse-over navigation vs. regular click-based navigation is that on a denser page it's much easier to unintentionally trigger it if you aren't paying close attention to what you're doing. Ever accidentally open the "See All 36 Categories" tab on Amazon while just mousing over to another browser tab or menu? If a densely-packed site like Amazon used this type of navigation EVERYWHERE rather than just select places, the user would have to navigate with his mouse/trackball like it's a minefield if he wants to move his cursor off the page.

It's even worse on a computer like my aging G4 that is slow at processing Flash and/or flashy JavaScript navigation.Franz actually gave us copies of Allegro, but that was long after we switched.The story as I most recently heard was that Trevor was a language nut and was frequently rewriting Viaweb in other languages.

At one point Trevor rewrote Viaweb in C++, which was around the same time as Paul left Yahoo. Yahoo eventually decided to stick with the C++ version because [insert opinion here].Yeah, and that code isn't quite right either.
It would be easy to generate a wrapper like that, but ghc doesn't.
It just shows that DLLs and Windows is not a popular combination.

I should supply a better wrapper, because I have one.  And I have Windows DLLs generated by GHC that are used in production.One of the footnotes in *Hackers and Painters* mentions that lots of Viaweb was done in C++ and perl -- the LISP part was the store editor itself, I think.

Actually, comparing different versions of a legit program like Viaweb would be really instructive. It's another data point in the "X lines of C++ could be written as only Y lines of MyFavoriteLanguage," debate.Well, the first rule of code optimization is: there is no rule.
It all depends on the application, the CPU, the programming language, the compiler...
You aren't using them?
:)
My Common Lisp is a little rusty, perhaps you can help me out.
I wanted to replicated the Haskell example, so here is what I did.
(I'm using the clisp port on a MacBook.)

I removed the definitions of f, g, and h from your file and put them in another file.
I compiled this other file.
I removed the defpackage and in-package (because I don't remember how packages work).
I start the REPL.
I load my compiled file.
I load your file.
I get the error:
*** - COMMON-LISP:*: X is not a number

I thought it was supposed to show f symbolically?
Are you running Markdown on each page render? Or only on an insert into the database?

I ask because I've gotten into the habit over the past year or so of keeping two DB columns around whenever I'm using Markdown or a similar filter: one for the original input, and one for the generated HTML, with the HTML only being generated on an `INSERT` or `UPDATE`. It denormalizes the schema a bit, but it goes a long way toward killing the performance hit of using Markdown or other text-to-HTML conversions.We do it on render, and memoize it.

We could denormalize it in the db, and may actually do that in the next version of the db. If I recall we didn't do it in the first place because we didn't use markdown in the first place, so it was just simpler to bolt markdown on afterward.I would imagine it's on each page render.  All the comments with broken &lt; signs were fixed immediately after spez posted, yet if they were in the DB, he'd have to run a cleanup script that manually reparses all the text.  That would likely take several hours, at least.There are many times when I wish we were still using Lisp. Chicken Scheme is my current Lisp of choice, though.Tacos afterwardDesign First, Then Code &lt; Not valid anymore, IMO.
Should read: Code a bit, test a bit.Thanks for looking into this!Someone else, later.Right, Mr mhd!  You can just come over here and clean the exploded brain bits off my keyboard!  And *don't do it again!*Did you have to repost this?
[Duplicated here](http://programming.reddit.com/info/14yvg/comments).The link was broken.Repeat after me: A "web OS" is not an operating system.I have been looking around for that Dan Weinreb post for long time. Very important post.Sorry, the reason for the broken link is that the f-king blogger software created new pages after I did some small last minute corrections on the article. I saw this when the post showed up multiple times in google-reader and deleted the duplicate posts immediately. 

But someone has submitted the link of one of the duplicates to reddit before I noticed it and so the other link is dead now. 

Welcome to Web 2.0: Where one web-app still has problems with the the back-button and another web-app don't allows to change published links.
Most of the Lisp OS projects are not very forward-thinking. A lot are just some Unix system with a Lisp system glued on top, and others are essentially an attempt to implement Unix in Lisp. The C language was made for Unix. Taking the ideas in Unix and simply implementing them in Lisp is not an improvement, and it is probably a step backwards from the current situation.

After the failed Lisp OS mailing list project, there were a number of spinoff projects. I remember some of them being similar in spirit to Vapour, but I don't think they really went anywhere. If people are interested in seriously working on a successor to these projects, I would be glad to help. Perhaps a wiki is in order? ;-)I was almost going to skip this link after seeing it was like 3 pages long, but it was totally worth the read. I'm changing jobs right now and the table at the bottom comparing Yahoo and Google is perfect for me. You can't find out things like monitor type (dual 21" at google??) and best perks just from an interview.My work is done here.yeah this is excellent love the bill g story, story about the interviews, as well as the chart comparing everythingresubmitted with a better headline as pointed out beforeHaskell is what you need:
http://shootout.alioth.debian.org/gp4/benchmark.php?test=all&amp;lang=all

More expressive than Python or Ruby with decent speed. Your final code will have less bugs too. It's a win in every respect except the paradigm shift involved in learning it (smallish investment) and the fewer libraries (but not so few as to cause a problem in my experience. Writing the ones you need is generally just another small investment).
Crazy stuff, though I wonder about their motivations for writing the kernel in asm only:

&gt; The design goal has been to remove the extra layers between different parts of an OS, which normally complicates programming and create bugs.

There's a reason people who actually really care about bugs in kernels, [don't write in assembly](http://ertos.nicta.com.au/research/l4.verified/).Another proverb for engineers (said to the business people):  
If you want it real bad, you'll get it real bad.Looks like the pdf was built using bitmaps rather than vector-based fonts.  Bleh.NERD FIGHT!
"my code is cleaner than yours!"
I love it lolSo, why does Linus think the GNOME folks think users are idiots? What is the particular functionality he's missing? Anyone know?

That article was seriously deficient in context. I mean, personality conflicts are interesting and everything, but I would like *some* data on the actual conflict at hand.Are you retarded?

QFT:

&gt; jemfinch
&gt; 
&gt; * You can't add methods to classes written in
&gt;   C in Python. That's not a major limitation.
&gt;   Just subclass.
&gt; 
&gt; * You can add methods to base classes on the
&gt;   fly.
&gt; 
&gt; * You can do anonymous functions (lambda) just
&gt;   as easily in Python as in Ruby.
&gt; 
&gt; * Creating a DSL is just as sanctioned and
&gt;   just as frequently used in Python as in Ruby.
&gt;   See Spark or SQLObject or any other similar
&gt;   package.
&gt; 
&gt; The only thing that Ruby can do that Python can't
&gt; is add methods or attributes to classes written in
&gt; C. Pardon me while I yawn about that
&gt; oh-so-significant superiority.

First off, fuckhole, I was not claiming Ruby is superior.  I use Python, because I think Python is superior.  I _do_ have command of some simple facts, shithead, and I used them in my reply.

&gt; You can't add methods to classes written in
&gt; C in Python. That's not a major limitation.
&gt; Just subclass.

How the fuck are authors of libraries you are using going to know about the subclass you just created?

Sometimes, you wish to inject some functionality into library code.  In more static languages, you would have to have access to the library source code, and maintain your own private patch.  In Ruby or Python, you can add methods on the fly, known as "monkey patching".  In Ruby, you can add methods on the fly to _any_ class, however implemented.  Also, in Ruby, "monkey patching" is always encouraged, where in Python it is sometimes looked down upon (rightly so, I feel).  Some Pythonistas are moving towards "generic functions" to solve similar problems, for the greater control and structure it provides.  Phillip J. Eby has a fine generic function implementation called "dispatch".

http://www-128.ibm.com/developerworks/library/l-cppeak2/

&gt; You can add methods to base classes on the
&gt; fly.

Enlighten me, shit-biscuit, and show us how to add a method to the base class 'object', and have that method suddenly appear in *all* object instances of *any* class.

    &gt;&gt;&gt; class K(object):
    ... \tpass
    ... 
    &gt;&gt;&gt; k = K()
    &gt;&gt;&gt; k.x = 3
    &gt;&gt;&gt; K.method0 = lambda self: self.x * 3
    &gt;&gt;&gt; k.method0()
    9
    &gt;&gt;&gt; object.method1 = lambda self: self.x * 4
    Traceback (most recent call last):
      File "&lt;interactive input&gt;", line 1, in &lt;module&gt;
    TypeError: can't set attributes of built-in/extension type 'object'

Oh, I am sorry, was the base class 'object' too *basey* for you?  Fuckwad.  In Ruby, such dynamism is often used.  It has the potential to lead to subtle bugs, so Python will never gain this ability.

&gt; You can do anonymous functions (lambda) just
&gt; as easily in Python as in Ruby.

Fuck you.  Learn the fucking difference between anonymous functions, anonymous blocks, and anonymous closures.

Right now.  Learn it.  We will wait.

In Ruby, with anonymous blocks and anonymous closures, you can effectively create your own control statements.  Guido thinks that anonymous blocks and anonymous closures allow too much difficulty to understanding code in-the-wild, so Python will never get them.

&gt; Creating a DSL is just as sanctioned and
&gt; just as frequently used in Python as in Ruby.
&gt; See Spark or SQLObject or any other similar
&gt; package.

Ian Bicking himself says:

&gt; SQLObject class syntax is a _little_ DSLish. (His emphasis)

http://groovie.org/articles/2005/12/08/is-rails-a-dsl-what-is-a-dsl-and-is-it-possible-in-python

What about Ruby encourages DSL?  Ruby's looseness about requiring parenthesis for function calls, anonymous blocks, and symbols are some features, and all are used below to create a Ruby DSL matching SQL (as much as possible).  The actual SQL is followed by the Ruby DSL.

    Select column1 from table1 
    where column2 = 12
    and column 3 = 13
    
    Select[:column1].from[:table1].where do
      equal :column2, 12
      equal :column3, 13
    end
    
    insert into table1 (column1, column2, column3) values (10, 'book', 'start')
    
    Insert.into[:table1][:column1, :column2, :column3].values(10, 'book', 'start')
    
    update table1 set column1 = 12, column2 = 'book'
    where column1 = 10
    
    Update[:table1].set[:column1=&gt;12, :column2=&gt;'book'].where do
      equal :column1, 10
    end
    
    delete from table1 where column1 = 12
    
    Delete.from[:table1].where do
      equal :column1 = 12
    end

(Example taken from http://jayfields.blogspot.com/2006/09/ruby-dsl-for-generating-sql.html )

DSL are not used in Python to this degree, for the simple fact that it won't look like Python anymore.  (Duh.)  DSLs are not sanctioned in Python, no Python standard library code uses DSL style coding, and no Python standard library code ever will.  By contrast, the flagship Ruby application, Ruby On Rails, is famous for its use of DSL.

I have run out of energy to abuse you to the degree you should be abused.  The pathetic thing is that I am not even a Ruby programmer.  But Ruby programmers have interesting ways of doing interesting projects, they seem to be quite productive, so I try to stay informed about Ruby.  But I have no plans to stop using Python.

*Edit* fixed link to jayfields.blogspot.com+1, and the Forth chips were FAST.[deleted]I have to agree with Linus. I'm on the GTK side when it comes to widget sets, but GNOME wants to lock everyone into a very dumbed down UI. Free software being what it is, hopefully if enough people agree with him we'll see a fork...Because, as all the Haskell tutorials out there prove, functional languages aren't that functional when it comes to real-world stuff.You would do a HUGE service to haskell community.
Because frankly it is MUCH easier to plug haskell dlls into your *normal* application GUI made with dotnet, java, python you name it, rather than trying to bring all those GUI libs into haskell.

Please if you have a tool that creates windows dlls, share it.
Methinks Linus should stick to the kernel, and leave the UI to the real experts.Totally. mozboz is acting as if it's some sort of truck that you can pile things on.So why doesn't he just use KDE and ignore GNOME?[deleted]&gt; Keep in mind that Mac OS X is a BSD.

The XNU kernel is derived from the Mach kernel and some components of 4.3 BSD (which is over 15 years old). Mac OSX has barely any "BSD" in it. I have no idea why people keep claiming Apple is a BSD. That's like saying Linux is an System V.

But, yes, the death of the BSDs is greatly exaggerated.

That was the most unfathomable thing about that piece -- yes, by having less abstraction layers, we obviously reduce bug occurance...

That just blew my mind. Layered abstractions can get complicated, but....seriously?

It is entertaining, though.Stop telling other people what to do with their source and sites. Well, telling is one thing, **demanding** is something else.

Open your wallet and let me lighten it for you. Open up your calendar so that I may fill it for you.

Managing open source software and an open source community takes time and effort, especially when starting up. Quit volunteering the time of others just so you can scratch your own little itch.

Personally, I like Reddit because it's *not* DIGG. It's *not* Slashdot. I seldom come here to read comments. I do come here to find interesting things to read. We don't need syntax highlighting - this isn't your personal blog to show off. If you have something significant to say, and want control over how you say it, post it to your own site and submit the link to Reddit.
As you may have heard us say before, we believe that ads provide valuable information when they are highly relevant and targeted to a user's query. In order to serve high quality ads to our users, we use the Quality Score to set minimum bids for keywords based on keyword clickthrough rate (CTR), ad text relevance, the historical performance of the keyword on Google, and the user experience on the ad's landing page. Keywords with a higher Quality Score are rewarded with a lower minimum bid, so it costs less for those ads to be eligible for display. Low quality keywords receive higher minimum bids, often making them inactive for search because their maximum CPC does not meet the minimum bid. In addition, since we also consider quality when we rank ads, higher quality ads benefit from higher placement on the page and a lower cost-per-click on average. So, high quality ads are not only more relevant for your potential customers, but can also help you improve your ROI by lowering your advertising costs.
 
 We're constantly working on ways to improve our Quality Score evaluation and provide you with more information about the Quality Score for your keywords. Over the next week, we'll be releasing two changes focusing on transparency and quality, which I've outlined below:
 
 Transparency - Later this week, we're releasing an optional Quality Score column that shows the minimum bid for all of the keywords within an ad group as well as a Great, OK, or Poor quality label for your keyword. You can select this column by clicking 'Customize Columns' in one of your ad groups (selecting this will also automatically populate the column for all other ad groups within that campaign). Use the quality label to get a quick overview of the quality of your keywords, or look at the minimum bid for a granular understanding of your Quality Score. Remember, the lower the minimum bid is for a keyword, the higher the Quality Score, and vice versa.
 
 Quality - Next week, we're launching improvements to the Quality Score algorithm that sets minimum bids for keywords in order to improve the quality of ads that we serve to our users. These changes should make it easier for high quality ads to enter the auction while also discouraging low quality ads. First, we're improving the way that we set minimum bids for keywords where we have limited data. For example, if the system does not have any data on a keyword, we'll try to assign that keyword a lower initial minimum bid until we have enough data to make a more accurate assessment of the Quality Score for that keyword in your account. Second, we're improving the Quality Score algorithm to make it more accurate in predicting the quality of all ads. This will improve the overall quality of ads that we serve by lowering minimum bids for high quality ads and raising minimum bids for low quality ads. We expect that the higher minimum bids for low quality ads will reduce the number of low quality ads we show to our users.
 
 So, what does this mean for you? As a result of this update, you may notice that the minimum bids increase for some of your keywords and decrease for others. To better understand the impact of this change, we suggest that you implement the Quality Score column. This will allow you to better monitor whether your minimum bids increase or decrease based on our changes. If you find that the minimum bids for any of your keywords increase, making your keyword inactive for search, please consider optimizing your campaign instead of raising your maximum CPC to the minimum bid. We recommend changes such as choosing a keyword that is more specific to the product or service that you offer or editing your ad text to make it more relevant to the keyword before you simply raise your minimum bid. By improving your quality we hope to provide the highest quality ads to our users while also providing you with the highest quality leads to maintain great ROI.
 &lt;img src=http://www.seroundtable.com/quality-score-googles.png&gt;
 Stay tuned to the blog over the next few days
  as i'll post again to answer any questions that you may have.The big news last night came by way of the &lt;a href=http://alieneliminator.wordpress.com/2007/02/15/inside-adwordsgoogle/&gt;AdWords Blog &lt;/a&gt;about Google making two big changes. As I explained in great detail over at Search Engine Land, the two changes have to do with "transparency" and "a new quality algorithm."
 
 In terms of the transparency, Google will be adding a quality score column, that includes a minimum bid CPC for all advertisers today or tomorrow.
I'm sure the patches are public somewhere.  Gnome's contempt for users is [nothing new](http://osnews.com/story.php?news_id=7344).Think Photosynth http://labs.live.com/photosynth/ .Viaweb has nothing to do with Haskell.

Do you think one day you and the rest of the "real world" programmers will finally learn or build a useful "real world" programming language? Java and C++ don't cut it.Another tip along the same lines: excise all inert "I think that ...", "It seems to me that ... ", "In my experience ..." garbage from your sentences, lest your writing become as weak and weighed down and tedious to read as code littered with manual error checks for every procedure call. This serves a two-fold purpose: it makes your writing conciser and punchier, and it is more likely to push someone into responding with a thought-provoking criticism or comment, based on their disagreement.Ah, Shivers... One hell of a CS professor.

If you happen to be at Northeastern and remotely interested in programming languages and co,pilers, I highly recommend you take his class on the subject. Even if you don't love the material, his stories will make it far worth it.
Hey, it could be worse...

Hmm, but how? Integrating the four chinese tonal variations?
&gt; Why would anybody use C# and mono on the mac?

Perhaps there are people who like C#. It's not a bad language, and certainly has a few things going for it over Java (though I personally prefer Python for most stuff). So why not let them use a language they like to write Cocoa apps? Is it somehow a bad thing to have access to your preferred language?If you ever need this stuff in Python, check out [dateutil](http://labix.org/python-dateutil).Having worked at their hotline, I kinda agree...&gt; Gnome's contempt for users is [nothing new](http://osnews.com/story.php?news_id=7344).

Hoboy, the amazing 'humans only understand things insofar as they build physical metapahors for them'!  Completely stupid and obnoxious, insofar as it is even true!

Next they'll put a GL figure in a corner, to say everything and just -scream- body language, because 'OMFG 80 PERCENT OR MORE OF COMMUNICATION IS IN BODY LANGUAGE', which factoid everyone loves to say, without wondering if it should really be expressed as '80% of a human's total communicative output (e.g., I am about to fart.  It is ow, bright here.) happens in body language.'&gt; No longer shall we wallow in the pits of assembly!

Why did you ever wallow?  The easiest and probably initially best thing to do with assembly is to build a Forth with it.&gt; Another tip along the same lines: excise all inert

Exercise the inert ones, but please continue to qualify your assertions properly.  There's something horrible in the way some people keep all of their factoids untagged and unGCable, indistinguishable from fact, and communicated as such.&gt;but I would like some data on the actual conflict at hand.

Feel free to dig through a Gnome project mailing list.  I used to be a part of two lists dealing with Gnome core apps and from what I read I agree with Linus (minus his zeal).  There appears to be an "inside" group of developers who refuse outside input.  And this same group absolutely (and I can't stress that enough) hates outside input from outside programmers.  I say fork it to get around them.  I may start contributing again if that occured.`They fixed the &lt; problem?`

Edit: Excellent!

Further Edit: Bugger, now I have to remember everything *else* I hate about Markdown so I can figure out that code and fix it.They drank some unfortunate spatial browsing kool-aid (which they've since relented on, no?), but that doesn't mean they necessarily have contempt for users. By neccesity, if you have a niche, geeky project that you are hoping to take more mainstream, you're going to have to piss off some geeks along the way (or just branch, a la firefox).

As for finding the patches: sure, but I'm lazy :):(

_"A monad is like a 'macro': All it does is a code-transformation."_

This completely conflates the difference between syntactic and functional abstraction.  In actual design and application, monads are no closer to macros than `map`, `filter`, or any other higher-order function.I think it's a similar form of craziness similar to what led Microsoft to put a silly little dog in the search window in XP, to try to make it more user-friendly. Totally retarded...if you have the context. It's easy to get caught up in theories and miss sight of what's practical.

Or, more generally, it's easy to be wrong. Happens all the time.

P.S. What's a GL?This always involves the writer making a judgment call. Qualify every assertion properly, and you risk turning your prose into a morass. The medium plays an important role. In a highly interactive medium, like reddit comments or oral discourse, I would tend to qualify only my most contentious assertions and leave the rest to be qualified on demand. Another point is that "I think ..." by itself isn't a very useful qualification; if you want to restrict the scope of an assertion, you need to be more explicit.The first 'INTERCAL Resources' on that page points to -postscript-.  The third points to a -research paper-.  And I hear this language has spooky new stuff like COME FROM?  You useless academics are all the same!  Wanting me to throw away my hard-earned GOTO faculties to waste time on these new-fangled concepts so that you can spit out -more postscript- about why my software sucks and how your next brand new thing will have -even more inane bondage gear attached-, oh I can't even imagine what horrors you plan to add to the poor compilers: mandatory documentation with statically-checked spelling?!  UNCLEAN and IMPURE assertions before and after any IO?!  Mandatory regular THANK YOUs to assure that I haven't gotten lost in my own -project-?!

No, this attempt to slip a language in through my defenses is the -very last straw-.  I'm going back to FORTRAN.  My mother told me that no academics had anything to do with FORTRAN!Another cool multimedia software written in Squeak. If you have children, take a look at the Scratch programming environment. Lots of fun while learning programming logic!&gt; P.S. What's a GL?

An OpenGL-, or rather 3d-rendered figure, I meant.  It'll be 60% head, anyway, but you still need some fluidity in the rest of the human to get the proper ratio of 'communication'.  Perhaps it'll have a therapeutic mode, where it senses you reading too many 20%-communication webpages and tries to compensate by flailing and grunting, pantomining both dancing and throwing up, and looking listless, and spinning its toe like a worried-before-authority child.I think the Reddit dudes are a bunch of chickens.
The chickened out. Could've rolled the thing in some Lisp (LispWorks?) or Scheme (if they would only ask the fine PLT people).
Reddit sucks big time when you the browser's back button. No continuations, huh? Well, welcome to Python. How *brilliant*.
(Mod me down, see if I care)Linus is one stubborn SOB. But that's why we like him. Seriously, though, if he doesn't like it, nobody is forcing him to use it. Personally, the whole KDE vs Gnome thing is much like the old Motif vs OpenLook thing. I preferred OpenLook, and I prefer Gnome. If someone else likes KDE, I say go for it. Competing projects tend to produce better products in the long run.&gt; So why doesn't he just use KDE and ignore GNOME?

That doesn't work in IRC channel management, or in [American politics](http://www.dailyhowler.com/dh020607.shtml), or in raising feminist consciousness, or in getting device documentation instead of binary blobs, or in OMG HITLER.

No, that's gradeschool-stupid advice that soulless teachers spread so as to quiet the little demons down with the least of personal effort.  You might as well have scolded Linus for saying something not-nice instead of nothing-at-all -- not because doing so benefits him or the people who ask him for these opinions, but because his not-nice from afar is somehow -bothersome- to -you-.

&gt;[The biggest trouble that plagued us was that we could never quite get Lisp reddit stable enough to sleep at night. There were weird threading issues that would bring the site to its knees a couple times a day and required constant monitoring.](http://lemonodor.com/archives/001301.html#c12730)

So there.This isn't a silver bullet, but it might be the NBL.[deleted]It's not completely a macro. That's why I used the ''. And I talked about the similarities and differences in the article, too.

But map, filter etc are functions. Even higher-order functions are only functions. A monad is a data-structure build from a minimum of one data-constructor and two functions. Also a monad is used for a certain purpose - and this purpose is (generally) code transformation. By providing the do-syntax Haskell makes the primary use of monads for code transformation quite obvious. So code-transformation (even if not on a syntactical level - this part is done by the do-block only) is the 'essence' of the concept.

The most prominent example for creating a DSL by using monads is the IO-monad, which is quite easy to understand if you simply look at it from the code-transformation viewpoint. Other views simply don't work (at least for me). Also other monads like State, STM, Continuation, Parsec etc. fit perfectly into this view. 

Without thinking of code transformation those monads have nothing in common - but if you simply see the monad as a means of transforming simple 'statements' into a more complex structure, it's quite easy to understand whats the common ground here.

If you want to understand how for example the IO monad works, you need to understand that the IO-monad simply creates code. This code contains I/O-actions mixed with ordinary Haskell code and is executed *after* the value is returned from the main-function. And the value simply contains the code to execute. The IO-monad doesn't output anything, it only *transforms* the code. And the output is then generated by executing the transformed code 'later' (the '' here are because of lazy evaluation). If someone don't see how this works, he will never understand what the IO-monad is all about - and in turn what every monad is all about.
[deleted]The thread in question: http://lists.linux-foundation.org/pipermail/desktop_architects/2007-February/001119.html

The patches: http://mail.gnome.org/archives/gnomecc-list/2007-February/msg00016.htmlUgh, no, I didn't like either of the pipermail URLs submitted.  [Have some lovely gmane.](http://news.gmane.org/find-root.php?message_id=%3cd45bd28e0702120047w261592eaia7200d6efec8a29a%40mail.gmail.com)It certainly looks nice, yes.  IIRC, one of the knocks on Haskell is that it can be difficult to predict or discover performance bottlenecks, because of the lazy programming paradigm.  Still, I'd enjoy trying it out if I get a chance.[deleted][deleted][deleted]Are you kidding? You'd actually choose one over the other just because of the monitor size?[deleted]for the same reason .net offers more than just c#. Some people might want to write a .net windows app in something other than c#. 

Someone might want to write a mac app in something other than Objective CAnd not to mention less bitching about bitching.I think my navel has more lint than yours.[deleted]Well I hope that he at least got to keep his full salary before switching to Google. Why accept 30% lesser when all the supposedly flat hierarchy bosses [are making billions in stock sales](http://finance.yahoo.com/q/it?s=GOOG).[removed]@#%$#, I hate it when people delete [reddit submissions](http://reddit.com/info/14zn5/comments) with comments.

On the plus side I get to submit the raw sources rather than [some magazine's sensationalized account](http://applications.linux.com/article.pl?sid=07/02/16/1937237) of them. Look past Linus's flames in the first couple of messages and it makes for very interesting reading. Especially in concert with the recent [coverage](http://programming.reddit.com/info/13uq1/comments) of [the support policies of 37signals](http://37signals.com/svn/archives2/useless_absurd_must_need_appalled_just_infuriating_essential_etc.php) (comments). And of [how to deal with flames](http://joel.reddit.com/info/14uud/comments).

All credit to the folks who responded to Linus for trying to understand him rather than giving in to the temptation to escalate. They got [this gem](http://lists.osdl.org/pipermail/desktop_architects/2005-December/000486.html) for their forbearance:

_"To me, open source *is* about flexibility. And no, I'm not talking about people re-compiling their applications and making changes to them.. the thing that open source really excels at, is the flexibility it offers thanks to having lots of users, and lots of users whose needs get *heard*."_

Startups have one consideration in common with open source projects: how to make the most of the feedback your users give you, and deciding what parts of it to keep or discard. I'm starting to appreciate the nuance necessary to rein in the disparate needs of different users without alienating them entirely. Reddit's been really good about this from day 1, but even they have had their [share](http://programming.reddit.com/info/14v8a/comments/c14vi7?context=5&amp;style=nested#c14vi7) of [complaints](http://programming.reddit.com/info/14xmf/comments).

Update: Ah, it's [back](http://programming.reddit.com/info/14zy6/comments)It might be calming for some, and it might be possible to get used to it, but I think it is not designed for the mouse.  May work much better with another pointing device, e.g. a finger.[removed][deleted]As should the GNOME guys.[deleted][deleted]I generally call flash embeds "movies" no matter what it is they actually do."I asked RMS when he was implementing emacs lisp why
it was dynamically scoped and his exact reply was that lexical scope was too
inefficient."

Huh. So *that's* why Emacs uses dynamic scoping. 

I guess what they say about premature optimization is true (or should I be referencing a saying about legacy software?).Translated:

&gt;I'm also an idiot. I can't configure most of of the options on my computer, and I started using Linux because Gnome made it easier for me.

&gt;I don't think everyone that sits down in front of a computer should have to be a super-programmer. There are many people who use computers for simple tasks and don't need to know how to configure every last option on their system. 

&gt;This is precisely why Windows is in the position it's in, because among other things it's concerned with being an OS that's simple for most people to use.

&gt;I don't tell anyone who uses Gnome, but GNOME IS GOOD because it's what an important part of society needs, and if you need to do more, don't use Gnome, but don't criticize it. 

&gt;I don't like bananas, so destroy banana trees!

&gt;Who's the NAZI?

&gt;Nothing more, this is for LINUS to think about.

&gt;LINUS, YOU DON'T KNOW HOW TO SPEAK SPANISH, SO WILL YOU BE AN IDIOT TOO?Not only is emfle correct, but when he left Nautilus was a slow, buggy, crashy, memory hog. The GNOME hackers lopped off a lot of the buggy features and (partially) fixed the core.Wow.

&gt; While spatial Nautilius is not perfect ... it is able to recreate the desktop metaphor that started the graphical desktop revolution with Xerox Alto and Star so many years ago. Please, don't stop all these good ideas coming back again. 

The desktop metaphor was, and is, an abomination.   (And so is comparing a Web browser to a a book.)  Is it so hard to imagine that computers really are *not* like a frickin' desktop, and that shoehorning new things into awkward metaphors will, before too long, become more of an obstacle than a help?The point here isn't what linus should be using, hes trying to look out for the rest of the linux system and its usability, linus can (and did) change the system to his liking, the rest of us can't just do that randomly when we don't like something, thus he has decided to complain where many others would not have, many people just switch back to windows instead.twm ftw ?[This](http://mail.gnome.org/archives/usability/2005-December/msg00021.html) is as far as I got searching for the answer.[removed][removed]He's basically calling everyone who likes GNOME an idiot.  I don't
use GNOME, but there are certain people who *demand* that the user
interface adopt the GNOME philosophy that Linus finds so offensive.

If GNOME didn't exist it would have to be created to fill an
embarrassing void in the Linux desktop offerings.  Like it or not,
"simple to a fault" is an option that a lot of users want and a lot
of pundits pay attention to.
[removed]&gt; LINUS, YOU DON'T KNOW HOW TO SPEAK SPANISH, SO WILL YOU BE AN IDIOT TOO?

Sulking like this is contemptible in any language.&gt; but you Kfanboys

Us who?   Who where?  What when?

[I share this URL too much.](http://www.perl.com/pub/a/2000/12/advocacy.html)

For my part, I use [treewm](http://treewm.sf.net); for what really matters to me about WMs, [GNOME and KDE are interchangeable](http://reddit.com/info/hdcg/comments/chhpa).  Of course, nobody here (but Linus, with his mouseclick-on-titlebar-gripe) means to refer to the window manager component of either system.Linus likes KDE
Guido lkes django.

Does either matter in the real world, no.If they had given you copies before you switched, would you have avoided switching altogether?

O what could have been ;_;&gt; a video showed up here on Digg

???

TRAITOR!!!via
http://iguanarama.com/blog/?p=8
Iguanarama Blog » Blog Archive » Haskell and HDL’s&gt;Of course, that'll make them want to switch distro's right away to get Linus' approval.

Why should you, me, or anyone else care what desktop Linus uses?  If I remember, he commented how he liked OS-X.  Bill Gates could like KDE for all I care.  I don't base my decisions on the popular few.

&gt;But this post is a sort of reaction against the extreme anti-Gnome mentality going on here.

I prefer Gnome over KDE.  Very little reason for that decision.  I don't think my choice is the best or the worse.    As for everyone else who hates Gnome: bite me.  For those who like KDE just because: you rock.Linus may be right to some extent, but I think this debate is a waste of time.  

There is definately a need for a simplified interface for newbies and casual users.   It is not surprising Linus doesn't fit into this group.  

But, there are alternatives for geeks right?  Why doesn't he use KDE, or more likely a something like twm and 20 xterms instead??  (and shut his pie-hole)&gt; It's long been said about Haskell that "if your program types correctly, it's probably correct". It's less often said about Haskell that "it doesn't come  with a debugger", but the two statements are really two sides of the same thing.

No, they really aren't.  I say the first about Mercury (also a pure language, also with such a compiler-directed shift of effort into the compile-time), but I don't say the second: Mercury [has a debugger](http://www.cs.mu.oz.au/research/mercury/information/doc-release/mercury_user_guide/Debugging.html) which, on top of the normal facilities, seems to have [one of unprecedented amazement](http://www.cs.mu.oz.au/research/mercury/information/doc-release/mercury_user_guide/Declarative-debugging.html).  It even has a few [research papers](http://www.cs.mu.oz.au/research/mercury/information/papers.html) (search for 'debugger').

Mercury calls itself 'a purely declarative' language, to sidestep impressions like yours, where 'declarative -&gt; functional', where Mercury people prefer the 'purely logical' (hint: prolog -isn't-) sublabelling.[removed]&gt; Why doesn't he use KDE [and shut the fuck up].

I addressed this below.  Search for 'OMG HITLER'.

&gt; but I think this debate is a waste of time.

So what?  Are you the arbiter of anyone else's time, here?  What sort of time wastage do you perceive in people uselessly pretending that other people waste their time?  Or do you actually mean, with that unqualified 'waste of time', that they somehow waste -your- time?
Haskell has [debuggers](http://haskell.org/haskellwiki/GHC/GHCiDebugger)! And a lot of [research into effective debugging](http://www.haskell.org/haskellwiki/Research_papers/Testing_and_correctness#Tracing_and_debugging). People seem to find it quite useful.[removed]&gt; O what could have been ;_;

Within the multiverse there are reddits in every language existing now, and then further on in languages that do not and never will exist for us.  A universe next door has reddit.com in dd/sh, but if you slipped into it you couldn't tell the difference -- not without the 150-comment "Reddit's markdown code." link, half-consisting of OH DEAR GOD MY EYES WHAT EVIL HATH WROTH THIS and so on.I wonder what it would take to get this working on an Intel Mac? And, being a fanboy of languages that consciously try and cut out the os middleman, wouldn't it be cool to see a forth, smalltalk, (factor?) face on top of this. Smalltalk's design goals echo the design goals of this project.Wouldn't you agree though?

Having dealt with users of varying skill levels, I tend to agree with the GNOME developers.I work in a big company and this is BS:

&gt; just to help the engineers figure out how to use the code repository

Use Subversion, it's good and easy to use.

&gt; It takes 2 weeks to plan for a one-hour task

If you can do it in one hour, don't wait for a plan, just do it yourself, I see this all the time and I work on my own now. Waiting is useless unless it's approval for something serious, but it's never serious in reality.Agree. He who does speak Finnish may throw the first stone.Modify an ActiveRecord object's attributes programmatically whilst still passing them through any overridden mutators (setters) and accessors (getters) instead of just the underlying values (like the ActiveRecord::Base#[]= and ActiveRecord::Base#[]) allow for.I always wonder, what people want to configure that GNOME makes impossible. Linus bitched and fixed "middle-mouse click on title bar". What's so annoying about GNOME?

All configurations i ever did to my GNOME were done with some mouse-clicks (use Tab as additional Ctrl) and sometimes gconf (remove Trash icon from Desktop). I never hit any wall.&gt; And a lot of research into effective debugging.

Haskell also has declarative-debugger work?  Excellent![removed]Linus has a [karate champion backing him up](http://en.wikipedia.org/wiki/Linus_Torvalds#Later_years) so my money's on him.He wants to configure, what happens, when you double click the title bar. His patches are for the metacity window manager and the GNOME control center.I am merely expressing an opinion.  Newsflash: it's what we do here.  

Just as complaining that ice cubes are cold, or candy is sweet, or grass is green ... yes it is a waste of *my* time,  similar to you wasting it now with your strange passive-aggressive remarks.

I see from the comments I am not alone.&gt; What's so annoying about GNOME?

I dislike metacity.  Kwm, too.

None of these spiffy desktop environments seem to care much about the wild world of window managers, and it's the -window manager-, more than configuration wizards or file managers or docklets, that constitute what a windowing system feels like.  Want a simple example?  Grab OSX, and try to work the way I do under certain X WMs: I have N terminal windows, grouped roughly by task (e.g., I might have irc and then also a few xterms for reading manpages or writing short programs in the course of answering questions on irc), and then also N transient xterms that I create rather than exit whatever the currently visible xterms have up, for extraneous commands.  No, GNU screen does not answer this: I want to see multiple windows at the same time.

If you try this, you'll find that OSX has the funny notion that windows of an application have more to do with each other than with other applications -- so you can easily switch between terminals, but not between one terminal and safari -- rather, you'll switch between all-of-safari and all-of-terminals, obscuring safari with other-task terminals.  If you run multiple instances of terminal.exe, you clutter your linear index into all-applications -- and you still have a harder time doing this when you want transient windows, than you would spawning a new terminal under the same instance.

OSX doesn't have desktops, either, so there's no escape (but X11 on OSX, and an X WM!) from its broken notion terminals must all sit together and not mix too much with other windows.  Maybe if you have a big enough screen, you can have two or three terminal-using tasks going on (but then you'll have to waste 2/3s of your screen at any time, and also find that a distraction).

Under nearly any X WM, you'd relate terminals to other windows simply by having the related windows all in their own desktop, workspace, whatever.  'Relation' here isn't some fluffy good-feeling term, either: it is the very physical reality of the relative effort required to move your focus between two windows.  Almost everything, for instance (OSX breaks this, with its application/window dichotomy), ties the-current-window and the-previous-window -very- strongly together: you can switch instantly between them with Alt-Tab!  Usually nothing is that smooth.

OSX is too friendly to let you change its window manager -- and so also, probably to a great extent, are GNOME and KDE.  Even though both of these systems consist mainly of branded applications using common technologies with their own special control and configuration paths (can't possibly keep the X resources system, can we?), they insist on a typical unix window manager, with a 2d grid of desktops, barely more options than twm had for window control, and vaguely windows-inspired keybindings.Or maybe he is busy writing a web server and a web browser in colorForth :]It amazes me every time the way Linus communicates to the open source world. Right or wrong, he communicates to the Gnome people like they are idiots. These are all smart people. If Linux wants to take over the (desktop) world, it needs an enlightened leader, someone who motivates people, such as those working on Gnome, to the bone. Even if they are seperate projects, a lot of authority comes from Linus. There's a whole bunch of ways to make people do what needs to be done without using this tone, even motivating them. Classics include the freeze-move-unfreeze method from change management, where you give the subject the idea, initially opposed, they came up with the idea. I don't say that would work here, but clearly Linus is completely unaware of and uncapable in these skills. He makes a horrible leader. "What if?" In many ways Linux is too successful, to be more successful. With a better leader, Linux would have been much more successful. But that will never change, because the current leader brought us the current enormous success. A paradox one cannot escape from, unless an entity 'bigger' and with more authority than Linus arises.- there is gnome, kde, gnome with patches, ...
- and what do you recommend me?
- macosx&gt; I am merely expressing an opinion.

No, you are merely telling someone to 'just use something else and shut the fuck up', cutely given as 'shut his pie-hole'.  And when someone objects to this?  OMG I WUZ JUST EXPRESSIN MY OPININ!  You may as well have whined that you were being 'sarcastic'.  No, my remarks aren't passive-aggressive: passive-aggression is -- duh! -- -passive-.  I'd have to silently downvote everything you've done recently, or piss in your coffee, to express passive aggression.&gt; it needs an enlightened leader, [...] who motivates people, [...] to the bone.

My friend, you need to read the Illuminatus! Trilogy.  Please do it quickly, before this desire for a God-King translates to the politics of guns and war, away from this peaceable realm of open-source and aggressive patch-submission.&gt; This happens more often than some mathematicians lead on.

Ahah, I always knew those dastardly mathematicians were but scientists in wizard's garb.&gt; Probably most of my programming language research for the next while will be on Haskell.

Probably not.

&gt; There are a couple reasons for this: Haskell is active, and being researched by a large proportion of the smart languages people in the world;

Where are your sources for this? I have facts that would contradict you.

&gt; Ideological languages are easier to infer stuff from, and thus are more useful in research.

Easier maybe, more useful absolutely not and that's what academy definitely lacks: experience in real world situations. You can do the same stuff in C: just don't use static or global variables and voilà! You have your independant function.

&gt; So the focus in programming language design then shifts away from describing how a program should run, and more towards what a program means.

Maybe not, it was called Prolog and failed badly. Functional programming languages are great, but they must allow me to write something I control, not something that could be infered at run-time (or not). Haskell and Lisp may be the languages of the future though, but it won't be a magical language: inferance does not mean "automagically written by the compiler."

&gt; I believe we’re entering an era where a compiler can write better code than a human. Period.

Wow, and I believe we'll all be out of business in two years because programs will write themselves. By robots with giant claws. Period.&gt; all good. i will keep those handy

Hm, I haven't updated this as much as I should have (and it lacks my sentence-qualifiers, like 'curiously': curiously, why don't you like her hat?), but you can find more of my language [here](http://ayrnieu.livejournal.com/tag/ayrhua); the 'ayrhua' tag combines an abbreviation of my name with Chinese pinyin for words (not quite 'language').&gt; Wouldn't you agree though?

No.  Users just often think different(ly from the developers and designers).[removed]I don't get it. He claims that Perl doesn't need an Iterator-pattern because you can use a **foreach**-loop. How the hell is he thinking this is supposed to work if Perl didn't internally use iterators for the looped-over object?While I agree with most of your post, I have to nitpick:

&gt; Want to have thumbnails for your videos instead of MIME icons? Sorry, can't do it. 

Unless I'm confused as to what you're looking for, KDE has been able to do this for a while. I've had thumbnails for my videos since 3.3, I'm sure... They are definately in 3.5, which was the last version I used before, er, migrating elsewhere.I think you misunderstand passive aggression.[deleted]It's hard to seriously judge the accuracy of such statements coming from harried startup founders, whose software is closed-source. Some months ago, I offered a link of some startup meeting where one of the Reddit founders counted switching from Lisp as a mistake or regret of some sort. I recall another case where they said their ISP then was a major problem.

Now, I can't easily dig up the links to support this (and the whole reddit/lisp thing is pretty much dead so I'm not particularly motivated), so YMMV. Could be wrong. But I've dealt with many a company which deeply misdiagnoses its problems. 

[**Edit:** forgot to make clear I meant "Reddit founders" in the first paragraph.]&gt; I guess what they say about premature optimization is true

People more disgusted than me with Emacs Lisp have told me that this wasn't even a sane excuse, at the time, as better lisps had already proven themselves.  And for dealing with the efficiencies of the day: my father still sneered at an editor he'd have to 'boot up' not ten years ago; the story of vi's origins suggests that Emacs-users mainly stuck to luxuriously fast connections to their servers, to even suffer such inefficiency in commands and drawing.

But I know none of this personally, and I anyway find it a bit wearisome that people today, flush with unimaginable wealth in computation and interconnection, bother very much with looking down on people who made decisions in states of now-unimaginable isolation and scarcity.  From recent discussion, I suppose that the most important casaulty of dynamic scoping in Emacs Lisp is the ability of Emacs (or anything) to get from an Emacs Lisp program anything approaching the efficiency that CL and Scheme systems get.
Gerard Huet did something along these lines with exceptions is CAML many years ago.  But you have to be very careful so the overhead of being clever doesn't cost more than the node allocation itself.
Lol, point taken, but it is not entirely what I'm saying. I agree Linux should not become like companies or the real world, or that there shouldn't be a dictator. I run an open source project myself, one of the main reasons is the freedom and the pure concentration on technology far away from management or God-Kings. Really, I know ;) What I -am- saying, is that he has no empathy-skills towards the other party. If you act like this, the other side will digg in. It is a juvenile and counter-productive way to propagate your ideas. Even if he's right, they'll think, that may be, but this 'idiot' is sticking with it a**hole. I don't think that's particulary smart.Yes you are right, Usability is hard because you have to find out what your users are thinking.

It's never the users fault. Ok sometimes...&gt; I think you misunderstand passive aggression.

Welcome to the internet.

We do not have the same sort of conversational habits here that you'll find 'on the outside' -- no, even our rhetoric comes a bit from the left field, relying sometimes on storied metaphors so indirect that we need to offer them as hyperlinks to an explanation.

Yes, on the internet, you'll find not one but many separate and individually peculiar forms of discourse, shaped by their lower-level protocols: the relative immediacies of e.g. IRC versus a forum, the threadedness of or not of forums, the capability or not of noise to drown signal, and the presence or lack of arbiters of discourse -- of 'mods' who can cut things off or sway matters anyway by existing at all.

On the internet, you'll find that some of your 'physical' forms of discourse fail: questions you post don't get answered.  Comments you make don't get answered.  Points you push without thinking get flayed alive.  On the internet, you'll find that people will attack your assertions with seeming hate and then turn and smile at you, whereas a physical interaction of this sort would have each attack almost necessarily half about -you-.  On the internet, people will get upset when you confuse yourself with what you say, or with third-party concepts (like programming languages).

On the internet, you'll find that the quickest casaulty of physical discourse is the 'rapidly-iterating' dialogue: "I have a problem."  "What problem?"  "Well, my mouse doesn't work."  "What seems to be wrong with it?" -- or -- "I don't like Metallica."  "OMG!  Metallica rocks!  Why not?"  "Well, I don't like the music."  "What, not even Enter Sandman?"  "... well, I kinda like that one."  "And?"  "I don't like S&amp;M."

ugh, no, discourse like that only survives in small enclaves -- and even in those, you'll discover a pressure towards larger, more complete packets, with more hooks to place disagreement or speculation on.

So, again, welcome to the internet.  I know this must all be very confusing, right now ("why did ayrnieu seem completely ignore my sneering, blank-wall assertion, instead of trying to get me to expand on it?"), but with time and practice you'll discover riches of communication and interaction that -- in wonder, and emotion, and enlightenment -- make not even the most desperately pleasant 'high-protocol' physical conversation one of much value to you.I find it funny that people are being so obsessed with their opinions, they not only read both digg and reddit, not only read the same articles on it, but also copy-and-paste their comments to both. Time to get a gf my friend ;)&gt; I don't think that's particulary smart.

Maybe he has more faith in their maturity than you do? :-)[deleted]Slava, I hate boring languages, bloated software and the stupid software industry probably as much as you, I like what you're doing with Factor and I too think that Lisp, Forth, Smalltalk, etc. are all great languages but, seriously, we're not fighting a religion war.

Bad tools still are tools and can be useful.
&gt; if Perl didn't internally use iterators for the looped-over object?

If Perl uses one internally, then programs don't need a 'pattern'.  Just like, oh, the [No Stinkin' Loops](http://www.nsl.com/) guy manages to deal with the fact that his APL system has loops hidden in it, away from his code.  Or how people in any high-level language somehow manage to avoid programming as if every line would cost them seconds of hole-punching.Great, but the same can be said of any language or framework.
I haven't found your "How every language and framework makes me a better programmer" today, so I just submitted this one. :-)
I like this.  Can anyone else point me to another 'how $foo made me a better programmer'?Linus is often presented as friendly and nice guy. I didn't think he could be that harsh.That's quite a post. Not exactly passive aggressive, but there is another word for it... arrogant. I'll never understand why some people feel the need to belittle and ridicule other people. Personally, I am of the belief that you should at least try not to be a jackass.

Btw, I've probably been on the internet longer than you have.&gt; It's not completely a macro.

It's completely not a macro, is what i'm saying.

&gt; A monad is a data-structure build from a minimum of one data-constructor and two functions.

Only in the same sense that (for example) `map` is a data structure (a functor) built from a minimum of one data constructor (list) and one function.

(Consider that `map`, i.e. the list functor, is literally just a partial list monad;  you just need to add the one missing function (`join`/`concat`) to get the complete list monad.)

&gt; Also a monad is used for a certain purpose - and this purpose is (generally) code transformation.

"Control flow" would be a truer characterization than "code transformation".  You don't see people describing `map`/`filter`/`fold`/[deferreds](http://www.python.org/pycon/papers/deferex/)/whatever as the latter.

&gt; If you want to understand how for example the IO monad works, you need to understand that the IO-monad simply creates code.

This is only true only to the extent that you define what `map` and all does as "creating code".

&gt; The IO-monad doesn't output anything, it only *transforms* the code.

This is extremely misleading.  The IO monad is unique in that it *does* result in transformed code, because the compiler treats it magically, optimizing it away and generating imperative code in its place.  However, (a) this is unlike any other monads, and (b) it's really the compiler doing the transformation, not the IO monad.&gt; I'll never understand why some people feel the need to belittle and ridicule other people.

&gt; Personally, I am of the belief that you should at least try not to be a jackass.

&gt; but there is another word for it... arrogant.

&gt; Btw, I've probably been on the internet longer than you have.

-guffaw-Running this is incredibly easy on linux: 

1) install qemu

2) download http://modest-proposals.com/qemu-c4-800x600.tgz

3) run qemu.sh

4) type C4 to run ColorForthSummary: *The implementation of a trivial program in 4 languages. Java is verbose*. Yawn.

    main = do
        a &lt;- getLine
        b &lt;- getLine
        print (read a + read b)
(nit-pick: exercise ≠ excise)&gt; By the way, KDE doesn't listen to people, either. For example, the single-click icon opening? People complain all the time, but KDE doesn't fix it.

You aren't comparing like and like.

KDE has included a configuration option for it ever since single-click was introduced.  Single-click is easier for newbies, but many experienced users prefer double-click.  You can't be good for both sets of users without a configuration option, and it's better to use a newbie-friendly default because experienced users find it easier to change the setting.

Linus, in this case, is complaining because GNOME doesn't even have *the option* to change the default behaviour to something that suits him better.


&gt; KDE seems to think that giving you 50,000 options, all right on the main toolbar, will make the user happy.

Hyperbole isn't convincing.  This is obviously not true.

&gt; I also hear lots of KDE Kfanboys ripping on Gnome people for thinking they're doing things right, yet if you offer a single complaint or suggestion to most of these KDE Kpeople, they'll Kblow-up at you and Kmod you down.

Well when you call them "Kfanboys", it's not surprising.  Complaining with a bad attitude is the easiest way to get your complaint ignored.  Why help somebody with a bad attitude when you can spend the time helping somebody who doesn't call you names?

&gt; While we're at it, you want to chat while away without sending your status message to someone every time they IM you in Kopete? Nope, sorry, can't do it. Obvious option, wanted function, yet not included in KDE.

Obvious option?  Er, no.  If you are chatting, then you aren't away.  This is a totally *non*-obvious option.

&gt; Want to have thumbnails for your videos instead of MIME icons? Sorry, can't do it.

Untrue.

&gt; Want TRUE transparency for you Konsole? Sorry, kan't do it

Until recent versions of X, it hasn't been possible.  The next version of Kwin can handle this for *any* window, and you can have it with the current version of KDE if you use a different window manager.

&gt; Want to show HTML content by default in KMail? Nope, sorry, can't do it.

Untrue.

&gt; Konq opens upon images and PDF's inside itself, which is horrible, because it CAN'T HANDLE THEM PROPERLY!

Works fine for me (and if you don't like it, there's a setting to change it).

&gt; I'm not trying to rip on KDE, because I try to use it.

You mustn't have tried in a while.  Some of your complaints are *years* out of date.
my $a = &lt;&gt;;
    my $b = &lt;&gt;;
    say $a + $b

There you go, Perl (with the backported Perl6 say), is the best of all of those for teaching computer programming.  You could get even more verbose:

    my $a = readline();
    my $b = readline();
    say $a + $b

[But we already knew that.](http://www.guild.net/~schwern/papers/Why_I_Am_Not_A_Java_Programmer/why.html)

In any case, great kudos to this person for realizing that scanf() would need a subsequent 'and now to tell you how awful this function is...'.kudos for using 'Pwn' in the title.Why can't I find any article titled "How Cobol is making me a better programmer"? Something is amiss.:- module add2.
    :- interface.
    :- import_module io.
    :- pred main(io::di, io::uo) is det.
    :- implementation.
    :- import_module string, int, require.

    :- pred read_int(int::out, io::di, io::uo) is det.
    read_int(N, !IO) :-
        io.read_line_as_string(Res, !IO),
        (
            Res = ok(S),
            (
                string.to_int(S, N)
            -&gt;
                true
            ;
                error("not an int:" ++ S)
            )
        ;
            Res = eof,
            error(":-("),
        ;
            Res = error(E),
            error("look at what you did: " ++ io.error_message(E))
        ).

    main(!IO) :-
        read_int(A, !IO),
        read_int(B, !IO),
        io.format("%d + %d = %d\n", [i(A), i(B), i(A + B)], !IO).I particularly like:

    error(":-(")

Must use that more often.I once wished I could use COBOL. But I was stuck with RPG/400 on the AS/400.

BTW: COBOL articles all get downvoted on reddit.
In trivial programs like these, I prefer that the program berate the user :-)  One of mine told me me to "please make [it] report better errors :-)", so I did.I worked on a website for a clueless government agency.  In fact, I did *all* the work: XSLT, PHP, Delphi back-end, CSS/HTML front-end, passed accessibility guidelines with flying colours, and all to implement a design produced in Photoshop by morons with no HTML design experience, to an artificial deadline created out of thin air by a minister trying to play impenetrable games.  I got it all done, and I hear it even won awards.

Do you think I got invited to the launch party?  Ha!

Mind you, after *huge* efforts and hype to publicise the launch, the deputy prime minister announced his retirement the same morning so there were *no reporters at the launch at all.*  I felt good about that, at least.

I'm not working there any more.  I now work for people with clue.  I'm *much* happier.Doh.&gt; Textbooks are written as if one theorem logically leads to next, and so on. In reality today, however, many mathematicians have computers crank out numbers and look for patterns. Then, they try to construct a proof that the observed pattern holds for all numbers.

This is an increasing trend in some corners of mathematics, but I don't think it's true that this is how "many" mathematicians (relative to the totality) do research, using computers to assist in generating hypotheses.

What _is_ certainly true is that almost never are theorems discovered and proved, or theories constructed, in the painstakingly rigorous and logical manner in which they are presented in textbooks. Patterns are noticed--usually by intuition, not computer--and hypotheses formed, often in a loose and playful manner that is the polar opposite of what you see demonstrated in the literature. 

William Thurston, one of the greatest living mathematicians, wrote a now-famous article which discusses how mathematics is actually done as contrasted with how it is taught or outwardly depicted. It makes for interesting reading for anyone interested in this subject:

http://arxiv.org/pdf/math.HO/9404236.pdfAmiga rulez!!
**We** are sarcastic.
[removed]Python is often dismissed as "just a scripting language" (Perl and Ruby also suffer from this silly bigotry). 

This is the same kind of bigotry as "Lisp is slow". It's not so much bigotry as laziness. This change so quickly in CS that me need to question our knowledge all the time. So interpreters were slow at first. 

Why no mention of SICP or the PLT work?MARK THIS UP. Seriously, the video is worth watching - I wanted to see that Great Wall video at the end, but unfortunately they clipped it short.

This research is spectacular.Some malloc() implementations return NULL if you request zero bytes (you're not going to write anything to that buffer anyway, right?), which complicates error handling.  From SUSv2:

&gt; The pointer returned points to the start (lowest byte address) of the allocated space. If the space cannot be allocated, a null pointer shall be returned. If the size of the space requested is 0, the behavior is implementation-defined: the value returned shall be either a null pointer or a unique pointer.
¿Que?

Kannitverstan.
Interestingly Linus has always presented himself as a beligerent asshole (in a similar way to Stallman), but because he "created" Linux he has this altruistic father like aura attached to him...  People seem to forget that Linus is an engineer not an evangelist...Really Cool! 

Now who wants to help me write an operation system directly in hex codes? I've already written part of the boot loader:

000000: A9 55 AD 78 AA 09 00 08

Sorry that it is in high level hex, but I thought binary might be too lengthly.

The alternative is to write in decimal, but really that will be ridiculous. What a moron will use decimal in building an operating system?!


What your doing is to pull all structure down to the implementation level totally neglecting the higher-level structure. For example the list-monad:

A (basic) list monad consists of the Constructor plus bind &amp; return. Those three form a new construct named list-monad which is used to abstract 'non-deterministic-computation' (this is the common description of the list monad in the literature). While this *uses* a list data-structure for implementation, it *isn't* a list data-structure. It's a abstraction (or in other words code-transformation) which takes something like this:

    do 
      x &lt;- [1, 2, 3]
      y &lt;- [4, 5]
      doSomethingWith x y

and transforms this into something which evaluates 'doSomethingWith' with every combination of the values for x and y and combines the result. Where do you see a list here? I could also implement this monad by using arrays, streams, loops or some compiler magic. 

If you mix up the *implementation* of this monad (which is based on lists) with the monad itself, then you are the one who 'completely conflates' the abstraction here. 

Some people often talk about that even imperative programmer use monads without recognizing them because they are build right into the language. That is true, and it also makes clear, what monads do: They create new languages. And how do they do it? Like every language is implemented: By transforming (and 'creating') code.

It's not only control-flow, its also about collecting return values or destructuring data which is easy to see in the above example.

If you look at it from such a misleading point of view, also a compiler don't creates code, because the code is written by some developer. This is true on a 'philosophical level', but it don't fits the common notion of the compilation process.

All what the IO-monad does is the transformation of code to 'state-chaining' code. This could also be implemented by using some other method (look into the paper 'How to Declare an Imperative'-paper of Wadler or the 'awkward-squad' paper of SPJ). It's implemented in Haskell as it is now because of performance reasons, not because it's the only way. But the semantics of the IO-monad again is a transformation: It's not doing the output, it only transforms code (written in a I/O-DSL) into new code which, when evaluated, is doing the output by some other means.
The coding style could be... erm... improved.Wow! That is really impressive.  I love learning about technologies that use the web to harness the previously uncoordinated work of many people.  It is so enjoyable to see what new things can be done with some creative thinking and smart development.Mine: O'Caml and Lua.  Lua I think just a swell thing, with a pretty name and unoffensive semantics, and simple pragmatics.  O'Caml I put on this machine to use for more efficient programs, but then somehow never minded writing them in Erlang instead.  As I never use either of these, I don't know them very well -- but still, they occur to me sometimes.'fanatics', 'highjacked', 'movement'. What's next? Calling OOP-users terrorists and advocating to deport them to Guantanamo?

I don't get it. I think too, that OOP is more a part of the problem and not of the solution. But calling people who try to bring a field forward, like Gamma et. al. definitely have done it, 'fanatics'? That's plain stupid.

So a vote-down for the title alone (but since the content is also predominantly wrong, there's no moral conflict here).
Play WoW, you'll get to use Lua :)   

I never use Pascal any longer but it's still my favorite.Lua is the language that I find myself longing for when I run up against something that is particularly painful to implement in the two primary languages I program in professionally, Java and PHP.  Some people complain that it does not give you object orientation out of the box, but a) that makes it fast and b) it gives you the necessary building blocks, tables and meta-methods.It is rather amusing, because Linus seems very upset, calling everyone F.I.'s ("Fucking idiots") while the Gnome people try to be very calm and explain things.Is that your experience with a continuation-based webserver or are you just guessing it would be so (regarding scalability)?  

I've used both the PLT and Scsh's continuation-based webservers (I like Scsh's better) and both offer the possibility to say "forget all continuations of this session up to this point" or "forget the continuations of sessions older than x days".

Sure, it doesn't seem reddit would profit much from using continuations, at least not directly.  But I think their
threads problems would not have happened with PLT or Scsh/Scheme48, since these use "light" (not OS) threads and these scale pretty well.  It just happens that at least Scsh/Scheme48 use continuations to implement threads...the python is 2.2 and the last one I had is just a beta but it has a few modules from 2.2 missing, pickle for instance.

A worthy tool but look before you leap :)

I found solace from the PyPy project, a python implmentation of python.Eazel closed shop more as a result of the VC funds drying up at the end of the boom - You don't seem to have read the link I provided. Other than cleanup, the people they have added very little to Nautilus - it's now minimalist to the point of being useless.You mean the rest of GNOME was great ? I have been using GNOME from the first release. IIRC, it was released with RH6.1 - does anyone even remember how unstable it was with 'Enlightenment' (sic) . Nautilus brought SVG for the first time to the free desktop world - If the Eazel folks had run our of cash, we might not have lost all signs of good taste in the free software desktop world. UI design is tough - being code monkeys doesn't necessarily result in a usable desktop.Common Lisp.  Lisp in general is incredibly elegant, and Common Lisp adds incredible power to that.  However, I've never run across a project where Lisp was the right tool for the job.  It's always slightly easier to use Python for webapps, PHP for really quick 'n dirty webapps,  Java for work, Flash for games &amp; animations, C for VMs and other lowly bit-twiddling, Haskell for compilers, and so on.

I said it on C2 long ago, but I'll repeat it here: Languages are used for tasks they make easy.  Common Lisp makes a lot of things possible, but it doesn't really make anything easy.  So overall, it may be the best language, but people don't choose languages based on how well they solve *everybody's* problems.  They choose languages based on how well they solve *their* problem, and there's usually an answer other than Lisp that solves their specific problem better.Brilliant ! Being a great programmer doesn't necessarily make one a good UI designer. Empathy - an important quality most of these code crunching bots that churn out patches totally lack.Nothing beats Scheme for absolute beginners :-)

    (define a (read))
    (define b (read))
    (display (+ a b))

I've used exactly this program to get an 8 year old started with programming. The good thing with Scheme is that syntax doesn't get in the way.What is so slow with it? I find it faster.
As explained on the site, because of the extra time to make a click.
Instead of always asking myself the question "click, or not to click?" I would directly rush along the paths to what I want to do, and if I'm wrong I'm back in few millimeters of mouse movement.
But that's clearly only for applications were you can't do much wrong, like entertainment or information. And if the 'paths' are too narrow it's like in the Windows OS under start-&gt;programs with all these company names,and subsubmenus till you get to your target - one slight drift under a big resolution and you start again.

They had some really cool ideas, like the buttons where the little 'Go' comes out on the right. I liked that.
Because if this adjusts to the movement speed you use, it would be faster than clicking.For what it's worth, I agree with him.

Most of the flames against him I've seen are to the effect of "Linus doesn't like our desktop! But he's the kernel creator - he isn't allowed to have strong feelings toward software projects like that! That's against the rules! _**THAT BASTARD!**_"GNOME doesn't treat users as idiots. It's treating users as people who want to get stuff done, whether it's work or leisure.

Recommended reading:
John Gruber's ["Ronco Spray-On Usability"](http://daringfireball.net/2004/04/spray_on_usability):
&gt; It wasn’t A.T. who couldn’t connect to a shared printer. *It was Raymond himself who couldn’t figure it out.*

Joel Spolsky's ["Choices = Headaches"](http://www.joelonsoftware.com/items/2006/11/21.html):
&gt; The more choices you give people, the harder it is for them to choose, and the unhappier they'll feel.&gt; GNOME developers think "users are idiots", says Linus

Users provide no supporting evidence to the contrary :)
Good lord. "NO, you will NOT abuse the metaphor of PHYSICAL CONCEPTS that DOESN'T EVEN WORK WITH COMPUTERS AS DEMONSTRATED! BAD USER!"Come on, OS X is very stupid in parts but with GNOME you can't stop running into the stupid.How does supporting object orientation make a language slower? It would make Lua bigger, and that's a bad thing, but not slower.Yes, definitely.

Seriously though, different people have different sleep patterns. By adulthood, most people drift towards a sleep pattern that makes being up for 9-5 fairly sensible. If programmers don't need to work together, who cares. If they do need to work together, maybe pick a central 6 hours (10-4) that everyone must be present for, and let everyone then decide if they want to be around 8-4, 9-5 or 10-6.

Your sleep pattern does not define your ability to code.
The patches referred to are for Metacity to allow configuration of mouse-click actions on the titlebar.  Metacity hardcodes the middle/right-click actions and allows limited actions on double-click.  He added individually configured actions for double/right/middle-click.  Using it now.  Simple and nice.

This is a simple issue, but for Linus I guess it is just one example of Gnome's limitations.If one considers the beginner to be a child or in high school, then Logo would come closr to beating Scheme.  But since the beginner for a CS department is typically 18 or older, then my vote goes for Scheme as well.Or the ability to use closures or reason about your code without having to take the whole system into account. That sort of unimportant things..."So JavaScript is one of the world’s widest-deployed programming languages, and also one of the least-well-known, in the sense of the base of programmers who can competently work with it. That’s a problem."

isn't that true of x86 assembly code?From what I see, he just expanded the mouse handlers on the title bar to include middle-clicking and right-clicking. Am I missing something? What happens when someone wants to triple-click on the title bar? What when someone else wants to chord?

It seems rather ad-hoc to me.Or, you know... Stop abusing regexes. Real parser &gt; cesspool of regexes. (*looks over his shoulder to make sure the sysadmin isn't around* ;)

Not that it'd make unit tests useless, but, hopefully, it'd localise the effect of changes to the code a lot better.mm, I've noticed Io, but it seems a bit weird.  Something I'd abandon for Slate, if Slate would mature.Io.  If someone tells you what the syntax is like you might go "ick" but when you look at programs in Io, they are unbelievably elegant.  It is an incredibly cool, pure language.[removed]I'm not saying you misunderstand monads, i'm just saying the analogy to macros is [vacuous](http://en.wikipedia.org/wiki/Vacuous_truth):  the similarities you talk about apply to monads only to the extent that they apply to *all* higher-order functions.  Creating the impression that there's something specific about monads in this regard is counter-productive.

If the subject of this post was _"A higher-order function is like a 'macro': All it does is a code-transformation."_ i'd have absolutely nothing to complain about;  there are many interesting things to be said about that.I wanna add that the child was fascinated that the computer would calculate the result even for huge numbers in no time. Imagine having to explain integer overflow, because of lack of transparent bignum support. Another nice detail is that DrScheme opens a distinctive input box on a call to read.C, of course. It was my first professional tool, and like cutting your teeth it was painful as they come at first, but how I chomped with it! More than 8 years of Java later (don't ask) I can't quite write in it anymore, and it just feels... strange. Ah, also, Italian. I used to speak it with my gran, now I can't say two words in it for the life of me, but I really love it. And Classical Greek. It's the programming language of mankind, so structured, so logical, so musical; nothing like Latin, which is quite ugly in comparison (my native tongue is Spanish, go figure).

Full disclaimer: my drug of choice is OCaml, and I can write pretty decent Mathematica code if I need to, but you know how it is this thing they call "making a living".[deleted]This follows [this](http://programming.reddit.com/info/151d5/comments).Be slow to take offense, and quick to apologize. Always be courteous. Strive to be discreet.

Don't hit the "Submit" button. Ask yourself if you are adding to or taking from the discussion. Put out your own flames.[This](http://programming.reddit.com/info/151d9/comments) follows this.I don't understand why Linus hates Gnome and its users.

Isn't the whole Linux mentality to have choice of what to use? And a lot of people like Gnome (myself included) so where's the problem. If so many people like it over KDE, then it must be doing something right.

Frankly, the reason I hate KDE is because it's so customizable. I've been using Ubuntu for a few months now and thought of trying out a few KDE distributions. Kubuntu was a natural choice. But like Ubuntu, I hated the default look of Kubuntu and quickly wanted to change it. And simply changing a theme in Kubuntu to my liking is a nightmare. And  really I never could find a theme to my liking. The default look of KDE is absolutely horrendous. The Plastik widget and icons are highly amateurish. While the whole Linux community in general needs more art direction, KDE suffers the most from this lack of good appearance. Anyway, I find Gnome to be so much easier to use, and customizable enough to MY liking. Yes, KDE is definitely more responsive (I use Konq as my file manager in Ubuntu) but it's lacking in the one thing that brings in more users towards the Linux community and it desperately needs and Gnome has nicely adopted - simplicity.I think that information will be kind of hard to come by.  Your best bet would be to locate and ask the actual dev team (Konami made the game) what they used, or ask someplace else where people know more about the game (GameFAQs?, [this place](http://www.talkpes.co.uk/)?)[deleted]vTables. Having to look up the correct virtual method to call in the vtable is a small but potentially noticable hit. That is why marking methods final in Java is supposed to make them faster.

These days I think there are far worse performance drags like XML parsing to worry about.
[removed]The T project hit a sweet spot in Lisp design that has rarely been hit since. Now that the code is available, I hope that somebody puts it to good use.Check out the JS demo's here: http://play1.codespeak.net:8008/ it looks very promising!let () =
        let a = read_int () in
        let b = read_int () in
        print_int (a + b)Fortran/OpenMP. The ability to do parallel programming just by tagging my code was awesome. I really wish they would apply it to higher level languages like C#/Java/VB.

http://en.wikipedia.org/wiki/OpenMP

Actually, I just noticed that VS 2005/C++ supports OpenMP. I'm going to have to give that a spin some day.
One of the reasons that Scheme is a good language for an introductory course is that you can explain the syntax in a matter of minutes. Effectively means you can concentrate solely on the ideas behind programming, rather than the machinations and historical baroqueness of many PLs.  That and it's possible to teach from first principles and get into things like how program languages are built rather more rapidly.

And though I think Scheme to be the best language for an introductory CS course(with some of the best introductory texts: SICP and HTDP), I personally won't use it for my day-to-day projects.  I consider Scheme to be an excellent vehicle for learning how to learn other programming languages.As a user of Lisp I have only contempt for all the other languages I never use ;)To my understanding, these hardware functional languages are a two layered beast: A synchronous layer that has little to do with 'functional' and everything with state, and a macro layer on top of it which expresses macros in a functional language.

I would be very intrested to hear about a closer interaction between these two layers, especially when it comes to verification. So far the tutorials/presentations I have read (Lustre/Lava) make ample allusions to the fact that the macro layer is fully expanded (thus irellevant) before anything else takes place. But perhaps I'm wrong. I would be very interested to hear differently.Douglas Crockford,Yahoo! JavaScript Architect , has a great set of videos teaching JavaScript.

http://developer.yahoo.com/yui/theater/

After watching the videos you won't think of JS as just a some "weak browser scripting" langauge.

There are some things that the language does really well, like the usage of Lambda.You have to admit, it's innovative.A higher-order-function *can* do code-transformations (this is how monads do it in the end). But it hasn't. 

What I try is to find working images for concepts. With monads I had problems doing it, because there were so many different ways to look at them. For example the conveyor belt image: While the image is of course true, it simple don't catches the essence of what a monad does and thus can even be misleading.

It's like saying: A while-loop is a structure build from a conditional jump and an unconditional jump. While this it a totally valid description of a while-loop, it fails to catch the real 'essence', which is: A while-loop repeats a computation as long as a condition holds.

Many examples about monads do something similar: They concentrate on the implementation and on details instead of finding a description of its 'essence'.

Calling it 'macro' has it's deficiencies (which I talked about in the article), but it's nonetheless a good image. A 'tree'-data-structure has nothing to do with real (biological) trees but it's still a good image to remember how a tree-structure works. 

Your objection is as if a biologist protests calling a certain data-structure a tree because a tree is a 'woody plant'. And calling a data-structure 'tree' reduces the intricate biological details to a simple structural similarity which isn't even the defining property of a real tree. 

So how would *you* describe the 'essence' of the concept 'monad'? You explained why you think that my image is misleading. Now I'm really curious to hear of yours. And please don't simply repeat the definition, this is not what I'm asking for.
This is a [duplicate](http://www.paulgraham.com/thist.html) ([comments](http://programming.reddit.com/info/dswq/comments)).

Johnathan Rees' [T page](http://mumble.net/~jar/tproject/) includes a list of his corrections to Shivers' article. (Link courtesy of [pjdelport](http://programming.reddit.com/info/dswq/comments/cdu7n)).Spot on. In my school, we learned programming in a mixture of C and C++. It can't get any worse. Permanently fighting the syntax, in two years we learned hardly anything that matters. Nowadays I have no problems to keep algorithms and data structures separate from the syntax used to implement them (in C, C++, whatever).I'd agree with C.  I like it for some reason, but it's hard to justify using it much these days.Haskell for me. But I'm expecting that it will graduate to a language I'll start using (much) more since &lt;a href="http://www.cs.rit.edu/~bja8464/lambdavm/"&gt;that&lt;/a&gt;.What's next on reddit?  vi vs. emacs?&gt;[No he does not.](http://www.perl.com/pub/a/2000/12/advocacy.html)

That is an excellent essay by someone who is not Linus, criticizing an
"us or them" mentality and talking about Perl.  It does not in any
way contradict what I said.

Suppose you say "I like X. I think it's done the right way, and that's
why I chose it over the many alternatives."  What does it mean for
someone to say "X is designed for idiots"?

This is not a "group" thing.  This is not a "You've insulted something
I like, therefore you've insulted me" thing.  This is a "You've
insulted something I like by saying that only an idiot would like
it" thing.  That's a little different, if you stop and think about it.

(I use "I" only for illustration.  I used GNOME once a long time ago on
 someone else's machine, and don't know what I think of it.  I've used
 KDE more often because it's the Knoppix default, and I think it's
 okay.)
OO support doesn't preclude plain function calls. Think of C++, Lisp, Python, Perl and probably many more.The main difference between KDE and Gnome that I've noticed:

KDE starts you off with an unusable, cluttered desktop that you'll have to spend ages tweaking to make it function in a sane fashion.

Gnome gives you a simple, neat &amp; tidy desktop. That is nice to work with, but you'll inevitably run into some minor feature that just isn't available to you.

Me, I generally avoid GUI tools for most things, so Xfce is fine for me.[deleted]I use it everyday, I know it well and... I hate it! C sucks. It sucks even for bit twiddling...I think that's the point. The 'pattern' is the foreach loop, the benefit is that it hides from you the internals of the iterator.

The map syntax is even a better 'pattern' because it even hides the loop itself.
[deleted]Just an example, instead of:

    function killCSSRule(ruleName) { // Delete a CSS rule

I'd write:

    function css_rule_delete(ruleName) {
From the [addendum](http://perl.plover.com/yak/design/samples/note.html):

&gt; Or people spend a lot of time arguing about how foreach is not analogous to an iterator pattern, or how it doesn't do the same thing, or how even if it does, I'm still wrong, because Perl has no analogous replacement for a model-view-controller pattern, blah blah blah. None of this has anything to do what the point of my talk. I really wish I'd left out the thing about the iterator pattern.
&gt; 
&gt; Why do people focus on pages 4, 5, and 6 of a 13-page talk, and forget all about slides 8, 9, 10, 11, 12, and 13, where I make the real point? I have a few theories. One theory is that most of these people are technical experts. They're equipped to consider technical issues, but not much else. When all you have is a hammer, all you can see are the nails. That's what happened to the guy above who said I "got it all backwards." What? The point of my talk was that we need to take a fresh look at Chrsitopher Alexander; if I got it backwards does that mean he thinks we should *avoid* taking a fresh look at Christopher Alexander? Or does he think that Alexander should be looking at us instead? No, he just forgot about everything after slide 7.There's an old joke:

What's the difference between a terrorist and an OO programmer?
Try negotiating with an OO programmer.
True, but some languages seem to. In theory, any late bound language is going to have as bad or worse performance for method calls than even normal vtable lookups. Instead of just indexing into the vtable, you must do string comparisons on the function name. (I wonder if they use hash tables.)

That said, I still maintain that other techniques currently in fashion cause a far more significant performace hit.
[deleted]Another problem with RFC2445 is that it forces you to specify the daylight saving time rule that was in effect for the rule's time zone at the time you created a recurring event, even though you almost always want the recurrence rule to be intepreted according to the current rule for the time zone.

For example: Suppose in 2006 I set up a rule for a 10am Wednesday meeting. In 2007 the U.S. Congress changed the start date of daylight saving time, (for the U.S.), but the recurring event still specifies the old rule.  Since time zone names are outside of RFC2445, I can't reliably look up the time zone by name to see what rule to use (and the daylight saving time in the recurrence is still wrong.) 

And, small devices like iPods shouldn't need a database of timezones and daylight saving time rules through history, just to present a few events.Hard to say. How much code is actually written in x86 assembly these days? From what I understand, low level stuff is usally C with a tiny bit of x86 when needed.I'm not saying the rest of GNOME was great at the time. Merely responding to Hertzfeld's complaints that the GNOME devs turned it into something mediocre. They in fact took broken Nautilus and made it much more usable, and one of the ways the did this was removing some of the more ambitious features that never worked well.[removed]Yea, it really isn't one of his shining moments.Well, I'm not so sure it is unfair to ding RMS for making a short-sighted decision. He set out to write an editor that would still be useful 20 or 30 years later, so that sort of implies fundamental design choices should've been made with an eye to the future.

Looking at it from a modern perspective, I'd say the current tragic problem with Emacs Lisp is that dynamic scoping makes it so very difficult to compile/translate/interpret/what-have-you Emacs Lisp into a Lisp dialect with a future, like Scheme or CL. For example, I suspect GNU Guile would have done a lot better if it had been the extension language for Emacs from the start.I'm surprised no one mentioned [J](http://jsoftware.com/), the descendant of APL. A little playing around with it is worthwhile for the brain-bending that it induces. The same could be said about Forth too.well, this post was part of a contest, you can see the other entries here: on-ruby.blogspot.com/2007/01/win-books-by-blogging.htmlMost languages use symbols instead of strings. There are many ways to optimize dynamic dispatch. But anyway, supporting this doesn't make a language slower: it's a local change. Continuations, for example, *can* make other parts of the language slower because they require global changes.[deleted]You didn't really read the whole thing, did you?  Check out the part where he says (approximately), "stop obsessing about the damn iterators and read the rest of the freaking slides."Definitely more fair.  Thank you.Actually, that is the only thing you _can_ configure now.  Right click and middle click are fixed functions.[deleted][deleted]Close, but not quite.  Perhaps you meant:

    main = print =&lt;&lt; join (liftM2 (+)) readLn

Though, to input both numbers on one line, one would use:

    main = do [x,y] &lt;- fmap (map read . words) getLine
              print (x + y)
QBasic, HyperTalk, and mIRC's built-in scripting.

Presumably "hold fondness for" isn't taken to mean "wish I still used".

Actually, I remember feeling like HyperTalk was kind of awful, even at the age of 13 or 14 with my only exposure to programming being line-oriented BASIC and DOS batch files, but HyperCard was in general a fantastically cool piece of software.C has a clean and elegant base but very little power. People look fondly back on it, remembering the language's simplicity, but forgetting that actually getting work done is a pain in the ass.Really? That has not been my experience at all. Perhaps I'm misunderstanding what you mean by "transparent interaction between compiled and interpreted code". But if you mean that a compiled procedure can invoke an interpreted procedure... well, it's trivial. 

I code like that a lot: write interpreted code, test it, then compile it into modules; run the program with a REPL at the console. When a bug occurs I re-evaluate procedures in the interpeter on a case-by-case basis, overriding the compiled versions. Later, when the bug's fixed, I recompile the new version. It's not as clean as, say, compiling in SBCL, but it's still incredibly good.

The module system has some esoteric corners, to be sure, but it doesn't really work that differently for compiled vs. interpreted. What particular differences are you noticing?

Keep in mind, I've never used Esh, so perhaps there's something striking about it, that stands out in comparison to Chicken, but that i don't see.The lightweight languages lists are so fascinating to read... I wonder why there's no ll5+? Has the interest in little language design and research waned in recent years?no one complains about not writing "XOR BX,BX".  in 10 years, firefox probably will be fast enough (thanks to new chips and a cousin of Tamarind) to support people writing in some framework that compiles down to javascript.  sure, you'll need some basics, just like it's helpful to understand that using the XOR saves you a byte of machine code, versus "MOV BX,0", but that'll largely be moot.wanna try that link in markdown?I don't think that analogy is a good fit. Compilers output machine code, not assembly. That said, I see your point.The section on Spore starts 37 minutes in (`00:37:00`), if you want to skip straight there.  The entire video is interesting, though.You could presumably change that latter one to:

    main = fmap (map read . words) getLine &gt;&gt;= print . sum

That would also let you generalize to any number of numbers.&gt; - Fits on a single floppy 

*That* is pretty cool though. And might be a way to reduce bugs.[removed]if you skip the part about the iterators, then the article says extremely little.  it says, "GOF tried to adapt Alexander's book to programming, and got it wrong.  but i cant really explain why, sorry..i used an example i never should have used (and i havent supplied a better one since).  you should all go read Alexander's book itself.  just trust me, they got it wrong".

the iterator example is hard to ignore, as it reveals the primary motivation for the article - he ran across a few patterns zealots who insist on applying GOF in its character-by-character literal form, which is obviously stupid, and then he assumed thats what GOF and the "patterns movement" recommends overall.  it doesnt.  GOF doesnt mean to say "these are the only patterns". it doesnt mean to say, "here is how you write your program".  there *are* people who think it says that, but theyre not producing any code, trust me...theyre just making life miserable for other programmers by butting in their work with stupidity (i speak from past experience).

the point of the book has nothing to do with establishing a common behavior, only about defining a common language, just like Alexander's book.  reading the preface/introduction/etc should make this pretty clear.  it has nothing to do with "copy this code line by line else you arent doing Strategy"...it means I can say to someone, "oh put those two different versions of the code into a strategy pattern", and he or she knows what i mean, they go off and do it.  thats it.  thats GOF..if Alexander is the genius and theyre not, i dont care; GOF took it to the programming community and for that alone i think its noteworthy.[removed]Yes, that's what I'm saying, and not ashamed of it.

Sounds like you have some issues to work through.  I'll leave you to that, but will recommend some sunshine and exercise.Python. I almost always use Cocoa (Obj-C) because then I can write an application—I can't do that with Python. (Tkinter doesn't count, I've not had time to tinker with PyObjC, and Cocoa won't work in Python until Leopard.)

When I write a command-line utility (that wouldn't be better served by Cocoa for one reason or another), I use Python. But those times are few and far between.

I even use Perl more than Python nowadays, since I can write simple filters in Perl and then pass them off to [ThisService](http://wafflesoftware.net/thisservice/) to create a [Mac OS X system service](http://highschoolblows.blogspot.com/2005/11/mac-os-x-services-menu-you-never-go-to.html). Python is usually overkill for these things.Not open-source: useless for real world usage, can't be trusted and will most likely close in a few months. No way to export my project.

I try to create a new project: "Project name is too long" because I typed 3 words.

No ID, no markup language, no way to share files or information efficiently: crap.

Please, don't try to compete with free professional tools like Trac when you don't have the skills. Sometimes too simple is useless, we're talking about a bug tracking system, it must be powerful.[removed][removed]He's not talking about the implementation, he's talking about the code *in the Perl program*.

Imagine if you had to (*in Perl*) create an iterator object to loop over that array. Now compare that to the foreach code. Which is simpler?[removed]Erlang?&gt; p.s. i am not sure why this story gets so much press

This is Reddit. There are two answers: Lisp and Paul Graham.[Ban Comic Sans!](http://bancomicsans.com/)[removed]Sad thing is that the "casual joe" user usually is.[removed]Aside from the fact that it's now fast enough to actually use and run. I use nautilus every day, but when it was still an Eazel product I couldn't imagine using it due to the speed issues. Nautilus does exactly what it should do at this point, which is work well and stay out of my way.[removed][removed]I know more than a few folks who use Mono, and personally I don't even own a Windows machine. But I know a bit of C# and don't mind having that as an option for developing stuff if I need it.[removed][removed][removed][removed][removed][removed][removed]Eek, you're right.  Sorry.[removed][deleted]Javascript is a camel-cased language (unfortunately), so I went with the local convention in naming.   People who are used to dealing with document.getElementsByTagName("p")[0].style.textDecoration='none'; probably would feel more comfortable with killCSSRule than with underscores.  Personally I would have just gone with $$ since $ is used by a lot of frameworks as getElements so $$ being get style would be pretty cool, but since the article also attempts a spot of training that was out.

I am genuinely interested in suggestions to improve the code however, I actually eat my own dogfood so any improvements directly benefit me, if not my ego ;)[deleted]Except for some very small improvements (macros,...) assembler is just another notation for machine code.[removed]Wow, this is really great.I'm working on embedded devices these days, and C is still the main language here. I like it in a kinky way: it manages to make challenging some stuff which would otherwise be awfully boring. Besides the obvious bit shuffling and memory managing issues, offering a cohenrent and intuitive API to a C library can be an interesting puzzle.

The top language I love yet don't use for real stuff would be Haskell. There's nothing as beautiful as a correctly designed Haskell program; unfortunately, that's not something easily concertible into money. And when it comes to open source development, when you get too much pleasure from the shear beauty of your code, you're distracted from the main goal of writing something actually useful for some actual people.inersting read
not sure i agree with it all.Yes, I think this is a perfect teaching tool especially for younger students :) &lt;/sarcasm&gt;Why would linked list be harder to teach in Python than in another language. 

    class Node:
        def __init__(self):
            self.next = None
        def add(self, node):
            if (self.next):
                self.next.add(node)
            else:
                self.next = node
        def poptail(self):
            node, self.next = self.next, None
            return node

You can extrapolate from that.It's actually a good example if you read in context.

The GoF style is to say, "we need an iterator, so you, the programmer must implement this set of objects and APIs". But the Perl `foreach` and similar constructs in other languages says "you, the programmer, do not need to implement much of anything; the language has anticipated this problem."

A lot of this goes back to the notion that design patterns in one language represent shortcomings and become built-in features in later languages -- if a language has a common problem for which it offers only a tedious, repetitive solution, then a better language should come along and fix that. But for the longest time, many patterns folk have missed out on this, and insisted that the languages should keep their shortcomings and that the programmers should continue memorizing the magic incantations to work around them.I think he may prefer the fact that GNOME is mostly written in C rather than C++.Definitely REBOL (http://www.rebol.com/). It's a fun little language, and some folks get a lot of mileage out of it. I haven't done a heck of a lot with it, though.He should be happy that linux has such diversity. What makes opensource great is that it was created for the people and by the people. Anyone who uses either windowing manager should get that choice. Obviously people want to use GNOME or it wouldn't be popular. If people want to use a system. Let them. Really it all just works adversely to the whole linux community to make childish debates like this.[Interesting comment](http://programming.reddit.com/info/14p7r/comments/c14v2i) elsewhere from jkerwin:

&gt; MONIAC is a nice idea, but it didn't handle rational expectations any better than Phillips' other invention. In fact, he used MONIAC to explain the Phillips Curve relationship, so to some extent that bizarre water simulator was directly responsible for stagflation. History is weird.If he's just complaining about Metacity, then he can use a different window manager. Gnome is configurable in that regard. From what I remember, Metacity was not meant to be very configurable at all. If Gnome ships by default with Metacity, then I can see how one would say it's a Gnome issue, though. I don't think that feature is well-advertised, though. It used to be in the GUI, but it isn't anymore.

main = getContents &gt;&gt;= print . sum . read2nums
    
    read2nums :: String -&gt; [Double]
    read2nums = map read . take 2 . linesGood, once the "mainstream" gets a hold of anything, it gets flushed into the ocean and becomes worthless, which is something I don't want to happen to Lisp[deleted]If you didn't ignore languages like Haskell and Erlang, you could have that already.My apologies if I'm misreading your post, but apologists are people who defend (not apologize for) a concept.[deleted]The suggestion I personally followed was to: not use poorly designed languages like Java.

You might be out of luck there fellow.
One day, programmers will drop down to Java.

*cackles*&gt; class X uses the underlying implementatin of Y.

Like the SpellChecker class?Does [Py2App](http://svn.pythonmac.org/py2app/py2app/trunk/doc/index.html) not work for GUI apps?This is the sort info that makes programming.reddit so worthwhile. Thanks for it.Scheme. I loved learning it and solving the exercises in SICP, and I still feel like it's the best language for teaching. Coding with `call/cc` was a mind-expanding experience, and the macro system was elegant and powerful. But these days, I find myself preferring Python for simply getting things done.LOL. 

That's the problem when trying to deride the importance of general databases in programming. So much programming revolves around databases that it's not even funny. Basic file I/O and simple databases that don't even support SQL are not that fun, at least not anymore, in this highly connected and available Internet focused programming of today.

Monads rock your boat, huh? :-)&gt; There are many ways to optimize dynamic dispatch

e.g. [PIC](http://citeseer.ist.psu.edu/hlzle91optimizing.html)Smalltalk in a big way.  It's a fantastically beautiful language design, but I never could stay in the environment for more than a few minutes, and I like building standalone apps.  But I might start playing with Pepsi.  When designing my own languages, I often found myself trying to solve a problem and realizing that Smalltalk just didn't have that problem.  My latest language is pretty much just a mash-up of Smalltalk and Python.
It must—otherwise there would be no point in making a .app bundle. But without an Obj-C bridge (either PyObjC, or whatever will come with Leopard, whether it's PyObjC or something else), I can't use Cocoa. I'm not about to *write* my interface in Tk or Carbon; I would much rather lay out my interface graphically in Interface Builder, and I've no problem with using Obj-C directly to get that.[removed][removed][removed]&gt; i dispute the ongoing tenor of these kind of posts. javascript is an okay programming language. it is definitely not a "great" langauge that we have not yet "understood". look at how associative arrays are hacked on to Objects as the-least-harmful way to abuse properties to simulate hash values. this is not the hallmark of a great language.

_Every_ language has at least one or two really ugly warts; that fact doesn't mean that there are no good languages.

As for the rest of your complaints, much of the design of JavaScript goes directly back to the environment it's designed for: client-side web scripting. In that environment, a URL is as good as an import statement, for example.[deleted]&gt; actually getting work done is a pain in the ass.

Thus my comment about it being 'hard to justify'.Could be. I've always felt GNOME's decision to use C was a little odd, because as awful as C++ is (and boy is it awful), OOP is way, way better for GUIs. The object mappings are relatively natural, unlike a lot of domains.Fnord.Well, FWIW, I heard that the next OS X will have support for multiple desktops.

mVi is for people who hate life. The kind who'd rape their own grandmother for drug money.

Emacs is for the chosen people. The ones who will ascend into Lisp heaven when the Rapture comes.

Just doing my bit... :D&gt; it processes your paychecks and stock trades among other tasks you care about.

Which, having worked on Java financial software at a couple different companies, rather frightens me.&gt; And where do people turn to the most when the want highly parallelizable code? Erlang? Haskell? Lisp?
&gt;
&gt; No. They use a variant of SQL.

Mind explaining that?[deleted]When you have a 'Policy on Policies' (true story)It is a really cool!  Check out my article about Scratch: http://weeklysqueak.wordpress.com/2007/01/23/scratching-the-surface/ 
I program in C every day, and it is possible to do [object oriented programming in C](http://ldeniau.home.cern.ch/ldeniau/html/oopc/oopc.html). I've done some GTK development too, and it is also OO. Here's the [object hierarchy](http://www.gtk.org/tutorial1.2/gtk_tut-5.html#ss5.2).anyone want to try writing the algorithm?Lisp an Smalltalk. My favorite ones... yet I never have the chance of using them seriously.

[removed]Scheme.  6.001 was one of the most enjoyable college courses I had, and I often found myself thinking back on the interesting problems they gave us to work on in that class.  But, I've never had the opportunity to use Scheme (or any other Lisp) in any context since then, however.Any? Oh you mean, any 9x9 sudoku. We're talking about the general case here, for instances of any size.[removed]The question was "what languages do you have a fondness for".  I thought APL/J was interesting, perhaps even "mind-bending" (25 years ago), but I have no unfulfilled longing to revisit it.  I think the Matlab scripting language  could reasonably be described as "readable APL", and that's now as far as I can imagine going back in that direction.  It may have intrduced some interesting ideas, but still, APL is one of those languages that I think died a well-deserved death.&gt; Sounds like you have some issues to work through. I'll leave you to that, but will recommend some sunshine and exercise.

Fuck you.

hmm!  Hey, I like my version better: no pseudopsychiatric claptrap; no "that's what I'm saying, and not ashamed of it." pseudopolite evasion; no excuses.

Fuck off and die, you sneering, simpering, useless, asshole.Good edit re "solving any puzzle in seconds".&gt; It does not in any way contradict what I said.

It should have suggested to you that you think more carefully about what Linus's words actually mean, without confusing yourself with the 'GNOME tribe'.  As it failed to do this, somehow, consider this: OMFG LINUS IS PART OF THE 'GNOME TRIBE!  'One of us' cannot possibly have insulted 'us' in this manner!  It must be some other complaint!  Perhaps against -gnome developers-?I don't get the context, but it by his claims it seems at least to be an ad-hockery that many other unspecified WMs already support.But in most OOP-languages those abstractions are part of the runtime lib. You don't need to define an iterator for standard collections. This is identically to Perl, where those collections are part of the language.

But if you have created your own data-structure in Perl, you also need to create a way to iterate over it. An iterator is one way to do it, a map and a fold function another way. But both are patterns.

And the GoF-style don't say 'you have to do this or this'. The idea of the book is to *name* certain patterns to make the communication more easy, not to force programmers into using them, even if they are inadequate.

Patterns are part of nearly everything humans do. If you go do work, you use the 'go by car' or 'go by train' pattern. If you communicate with other, you use the 'talk'-pattern - and if you program you use lots of patterns depending on the particular language you use. In OOP those patterns are different compared to functional programming, but patterns exists in every language. And having names for them is a strength, not a weakness.
Real hacker would just hack and not even think about it being morning or afternoon.Have you tried GNU Smalltalk?  It focuses on scripting and standalone programming, and gets performance that shocks Squeak people, at least :-)&gt; Why would linked list be harder to teach in Python than in another language.

-nod-, if you can only teach what the language already holds as concrete, you should just use Forth -- which has almost nothing concrete, allowing you to build anything without stumbling over existing syntax.What's considered Iterator design pattern? the usage of it or the protocol? IMHO, I think GOF focused more on the protocol.


Perl only replace the loop/usage side of the protocol, but it does not replace the other side of the protocol. 


I don't know much about Perl but (from what I googled around) foreach in Perl seems to works only with array and hashes. Iterator pattern is also about being able to traverse any kind of container, not just some fixed type.


Let's say a Perl programmer want to be able to traverse his custom collection, how could he do it and uses foreach if he doesn't want to copy everything in his custom collection to array first?


If he has many type of custom collection, and want to be able to traverse them all in the same way, then he still need to know iterator design pattern, foreach is not going to help him.


The better foreach is in C#, it is just syntactic sugar over  iterator design pattern, so it allow you to use froeach with you custom collection.


GOF book points out the pattern, there may be some variation in how you implement it in actual language because each language has its own limitation. But if GOF didn't point out the pattern, this pattern may not even be put in as foreach construct in your favorite language at all.


Also, you cannot be sure that you always get to choose the language that you want to code in, or that even the language you want to code in may have all the design pattern implemented in the language already, so GOF help you to realize that there is a way to simplify your code if you face a certain problem.


Does Perl programmer never have to use strategy pattern, lightweight object, or factory pattern? Are all those pattern in your favorite language as language construct? If you do use it in your code, will you immediately jump to the next language just because it happens to have 'lightweight' or 'strategy' as a keyword?main = do a &lt;- readLn
              b &lt;- readLn
              print (a + b)

Or for a more fanciful approach:

    main = print . sum =&lt;&lt; replicateM 2 readLnObviously you have never watched someone who has never seen a computer before double click.  It is hard.  Sure if you come from Windows or Mac you have learned.  That doesn't mean it is right though.   

Learn to single click, it is easier once you get rid of a bad habit.Scheme in its basic form has very few features, as well. But those features are easy to build out, because all the framework is in place.

XMLHttpRequest *is* an import mechanism. Maybe no one has come along and wrapped it up with a bow for you, but filling in the blanks in a basic form can be done in about 3 lines of code if you use something like the Prototype library to wrap the raw calls.

As far as package management goes, I sketched out a design for a package management system a while back that involved defining a Package class, writing a brief manifest for each package, and using XMLHTTPRequest to recursively load required packages. If my code base ever swells to the size where I really need it, or I discover I need runtime loading, I'll probably build it out.

And really, I expect someone out there has probably already built this. If not, it's hardly rocket science. JavaScript is a malleable enough language that it's not hard to take concepts like that and root them right into the language.

As far as the techniques for creating private vars go, AFAIK, they're all just based on lexical scoping, which is a fairly basic element of the language, and it's unlikely that anyone will suddenly decide to rip it out of the language.

And if you care, Prototype has a Hash class that gives you the elements most people traditionally expect from an associative array.

I don't know that I would say that javascript is a truly "great" language, but it's malleable enough that all it takes is some intelligent framework coders to build out whatever pieces you think it's missing. In that sense, it comes a lot less "ready-made" than something like perl, but a lot of people think perl has drastically too many built-in language features anyway.even better :-)

    let a = read_int ();;
    let b = read_int ();;
    print_int (a + b);;read a
    read b
    print $((a + b))
    kedit hello-world
    firefox
&gt; What's considered Iterator design pattern? the usage of it or the protocol? IMHO, I think GOF focused more on the protocol.

Woah, hold on: do you believe that `a + b` in APL, where `a` and `b` are huge and complex tables, is an instance of a 'looping/iteration pattern'?It's easy to be fond for programming languages you don't use.  Like Stroustrup said, "here are just two kinds of languages: the ones everybody complains about and the ones nobody uses."

I guess I'd throw out Smalltalk as the one that I'm not so much fond of, but find intriguing.  I'm fascinated by the whole app/ide/environment being bundled together thing, but end up banging my head against a wall every time I try to use Squeak, and I'm not sure why.I totally agree, but that's basically an orthogonal concern. :)No, they won't get media terror without calling it a 'virus'.APL!  I had to build an APL interpreter (partial) for school and fell in love with it -- so elegant!  So scary!  There was an old APL terminal down in a closet in the basement of the math building that I seriously contemplated stealing...sometimes wish I had.

Erlang's cool too.  I've been waiting for years for a project to come along that calls for it.[deleted]&gt; But if you have created your own data-structure in Perl, you also need to create a way to iterate over it. An iterator is one way to do it, a map and a fold function another way. But both are patterns.

And both require less code than the classical Iterator pattern. That's better language support for the problem, which is really what I'm getting at here.[Dojo](http://dojotoolkit.org) has a _very_ nice package system which can pull in modules via `XMLHttpRequest`; so if you need, say, the Dojo event module, you can write `dojo.require('dojo.event')` and it'll load. There are also some niceties in there for building a single file which contains all your dependencies.It will be much worse than the y2k-thing, but I guess it won't generate as much buzz because people won't understand the technical details.&gt; Java is verbose.

It also is the only one that talks about errot handling.
If you are willing to give up that, then it becomes:


    import java.io.*;
    public class Addup
    {
        static public void main(String args[]) throws Exception {
            BufferedReader console = new BufferedReader(InputStreamReader(System.in));
            int i1 = Integer.parseInt(console.readLine());
            int i2 = Integer.parseInt(console.readLine());
            System.out.println(i1+i2);
        }
    }


Which is not much longer than other language, considering that it is being compared with other dynamically typed language there (except C).

I don't know why people keep writing example about Java by trying to make it more unnecessarily verbose than other. From the original code:

1. Add error handling code even though other languages don't do it. In java you can simply forward the Exception to the caller, in this case the JVM, by declaring it in the method signature. Verbose? may be. But i prefer this to the C# way. I think Java got it right that Exception is part of the method signature, when you write critical code you want to be sure that you know when you are handling exception or letting it elevate up the call chain.

2. Declare variable outside of the block, not declare at the point of usage, just to make it longer.

3. Print description on output ("i1 + " + " + i2 + " = " + (i1+i2)") while other simply print the result, to make it look like even printing out takes more code.

4. Use System.exit while you can just simply let the method exit.


And from his article:


&gt; I'm actually kind of embarassed I had so much trouble with this - I've been working on a commercial Java package for two years


Yes, he should be embarassed.



Please, if you want to make a comparison, make it fair. Java may not be so elegant, but it is not as bad as you make it out to be.
&gt; No. They use a variant of SQL.

Could you please tell that to my boss?&gt; the word 'vaporware' is starting to appear in combination with 'arc'.

PG himself put the words together in [*Hackers and
Painters*](http://www.amazon.com/exec/obidos/tg/detail/-/0596006624)
(in the glossary if I remember correctly).&gt; Compilers output machine code, not assembly.

Compilers that output machine code output machine code.

Compilers that output assembly output assembly! :DRight. "They used 2 instead of 4 digits for the year." was easy to understand.

How do we explain the year-2038 problem? "Bits? Why should I bother with bits?"
Eiffel. If programming fashions made any sense, it would be holding the place that Java holds today. It is basically in the same class of languages as Java (statically typed, garbage-collected, object-oriented, "industrial" languages), yet it came out much earlier than Java and it's much better (IMO). 

Some of the reasons I think it didn't take off: 

* It focuses on correctness and quality, which is something most programmers don't care strongly about
* It doesn't have a C-like syntax, and most programmers are scared of learning new syntaxes
* It just happened not to be in the right place at the right time
Hollywood will make a movie about it.

But I guess they rename it to "Year 2040", because it's catchier.
No, if you cannot extend it by telling APL how to do + on your custom collection.

Iterator pattern is about explicitly specify protocol on how to traverse any kind of collection. APL does not tell you how you can interact with that protocol.

EDIT: I don't consider Perl's foreach iterator pattern, also. If it is not extensible then it is just a special case syntactic sugar.Is that any different from function programming? If functional programming became popular, do you believe that all the FOP code monkey produced will understand everything in SICP?I am in absolute, confounded awe.&gt;look at how associative arrays are hacked on to Objects as the-least-harmful way to abuse properties to simulate hash values

Um, that's not a hack - JavaScript is a [prototype based language](http://en.wikipedia.org/wiki/Prototype-based_programming).

&gt;no import statement beyond urls (which are at the html level, not the js level, so in fact js has no import mechanism)

I agree with you on this though - this is pretty silly.And they'll have the token IT guy (Jeff Goldblum, or Penn Jillette or somebody) jump up and yell, "It's not a virus, it's a *worm*!"Dojo also has some good examples of writing excellent javascript code showing how elegant the language can be.Some interesting stuff here.

I would strongly suggest that some Ruby and Perl6 folk start looking over PyPy and seeing whether you can merge with it and centralize your VM work.

(As a Python person, I can't say that I'd mind the ability to judiciously use a Ruby-based DSL, unified libraries can't be all bad, and it sounds like backing to PyPy may mean backing to .Net, the JVM, and straight machine language all at once. Please, stop running in 20 directions with VMs and run in something more like 3 or 4!)The problem isn't solving arbitrarily sized puzzles. It's solving it in a reasonable amount of time. The line quoted by the title is obviously misleading, since the people solving Sudoku puzzles aren't working on this problem.Seems like a fluff article. Really, the only bit of substance is that Sudoku is NP complete. The example of an algorithm restart as a "concrete advance" is hardly an advance in algorithm design. It also mis-characterizes P=NP. And those millions of people aren't contributing to one of the hardest problems in CS.You can't appreciate Shakespeare until you've read him in the original Klingon!

    REQUIRES DOUBLE

    : getnum ( -- d )
      0.  \ set up for &gt;NUMBER
      PAD DUP 10 ACCEPT 
      &gt;NUMBER 2DROP ;

    : go ( -- )
      ." First: " getnum CR
      ." Second: " getnum CR
      ." Sum: " D+ D. CR ;

    go
Actually it was pretty much exactly what I thought too.I tried that earlier, it wasn't working. Seems to work now...[deleted][removed]+1 Scheme

I never get to use it, but sometime I still manage to sneak in some algorithm modeling with it.  It's a great language to learn with too.puts gets.to_i+gets.to_i[deleted]Many people I talk to about this problem seem to focus on technical solutions. Maybe it's just the way some coders think, being steeped in computers and all, but it's so often that I hear, "If we only used technology foo, then all our problems would be solved." Of course, based on the article, I don't think it's that simple.[deleted]This article is highly misleading.  Solving a Soduko puzzle is  not solving the P=NP problem, it is merely solving on particular puzzle which happens to be NP-complete.  And Soduko is hardly unusual in this respect - there are countless NP-complete problems in ordinary life.  Suggesting otherwise is pure sensationalist journalism.[deleted]Oh come on. Bullshit.

Take java. Take databases (all that I know, anyway) - they all operate on 64-bit milliseconds since 1970, or even more detail than that. Will last until well beyond the heat death of the universe. There's lots more of those, by the way. It's really just C nowadays which is lagging behind.

But just like some ancient cobol implementations could potentially stumble over 'Y2K', which all fizzled out, by 2038, if the singularity hasn't come yet, this CERTAINLY won't be an issue.
[deleted]yeah, while this is pretty cool, it really is not a *true* OS.Lua is generally a very well-respected prototype-based programming language that uses an associative array (a "table", in lua) as the basis for its objects. So it's not like conflating objects and hashes is something that originated with javascript

Given that languages like lua influenced the design of javascript in the first place, I can't see why either having objects based on associative arrays or then turning around and using the Object class as an associative array is a hack at all.

The only thing that makes it problematic is the fact that you can't add new properties or methods to Object and declare them non-enumerable. This limits how you can extend the Object class to add new features, but isn't an insurmountable problem.pipeline

table data gateway

front controller

its just a vocabulary....this whole notion of "if you define a terminology, then i only know the language and dont understand anything", its like musicians who think learning to read music will hamper their creativity....so lameI never got into Eiffel too much myself, but I agree it would be nice to have it in place of Java.  I think its niche status is mostly due to its reputation as a bondage-and-discipline language in a class with Ada and Pascal.  The rep is deserved, but almost no one likes B&amp;D until they've been compelled by experience to appreciate its special charms.

Incidentally, while experience has indeed compelled me to appreciate Eiffel and other sorts of B&amp;D, I still prefer more loose and dynamic styles of development.
Solving large Sudoku-puzzles was one of the challenges in the Intel/TopCoder Multi-Threaded competitions. You were asked to solve an up to 4096 by 4096 puzzle on a machine with 4 dual-core Xeon processors.

The problem description can be found [here](http://www.topcoder.com/longcontest/?module=ViewProblemStatement&amp;compid=5624&amp;rd=9892), but you need to log in to read it. You can also examine everyone's code through the results page, if you wish.If they all would just compile down to JVM-bytecode, they could immediately:

* just ship the classfiles without requiring the user to install an additional runtime (since a JRE is a fact nowadays, whether FOSS-monkeys like it or not)
* use Java-libraries
* run on J2ME-phones (you know, the platform that Steve Asshat Jobs deems dead, the one that is supported by more than a billion phones  from Nokia alone)
* interchange and mix Python-, Ruby- and Java-classes at will
* have a cross-platform GUI-library (Swing uses native widgets on OSX and looks like shit on Windows and Linux, of course) and won't need to mess around with wxPython, wxHaskell or whatever half-assed variant Ruby uses

But of course, with your mad sophomore skillz, you all can write a better VM than Sun's engineers did for some tens of millions, right, FOSS-boys?

Just keep on reinventing the wheel.What the FUCK? This is the exact same deal as boo!

http://boo.codehaus.org
&gt; All that said, I don't really have an opinion about GNOME's C-ness or C++-ness

Good; that's a pretty silly thing for anyone to have an opinion on.

As for good design that happens to go into gnome's underbelly, though, please take a quick look at the glib library.  No, not 'glibc' -- 'glib'.  Simple and sensible facilities that you should want in any case if you plan to do much C programming.That will be no problem. They will just send someone back in time to get a 1970s IBM 5100.I'm not complaining that someone is taking the initiative to create a new language, but, with IronPython and [Boo](http://boo.codehaus.org/) being available, this isn't exciting news.  Boo is basically a statically typed python built for .NET (and Mono) that uses context to acertain types (you don't have to declare types usually).[deleted]Scheme is toward the extreme end of a school of language design that says that the language spec itself should be as simple as possible, and everything else should be an implementation detail left to libraries. Then the only question becomes one of how big the libraries are that you ship with the language. Big libraries require large base implementations (which can discourage ports), while small libraries mean programmers end up doing more of the work themselves or relying on third-party libraries.

The main downside of depending on third-party libraries is that you can end up with a somewhat balkanized programmer base. Code bases built on top of YUI and code built on top of Dojo (to pick two examples) are likely to be increasingly divergent over time.

That's not great, but it's not the end of the world either. You see the same thing with server-side web frameworks in Java, Python, and other languages.

As the library situation gets solved by add-on suppliers, though, the fairly strong feature set of the underlying language shines through. The way prototypes, lexical scoping, and lambdas are used provide you with a language that is enormously powerful and vastly extensible. And beyond those, the fact that it includes a powerful syntax not dissimilar from s-expressions that has been repurposed for JSON is a vastly underrated language feature that few other languages provide.

Javascript is certainly not perfect, but it's got a solid foundation, and virtually all of its flaws can be solved through well-thought-out libraries and careful choice of coding style.Crock.Follow [this link](http://boo.codehaus.org/Gotchas+for+Python+Users) for a more immediate contrast.Meh.  It won't excite dynamic-language programmers, and it won't excite programmers used to much more interesting type systems.  And if it excites the (numerically large) middleground of people who deal with statically-typed systems without rich type systems -- well, meh again :-)  But please consider that the languages in this middleground had significant force behind them and a (promised) niche in front of them; fully dynamic and richly-typed languages seem to get along a bit better on their own merits.

Warning: I've argued politics and hacked C Preprocessor mess already this morning; my mind is a bit blown.I really think 64 bit machines will be so pervasive by 2038 that we won't have to worry about this one.

How many 16 bit machines are around anymore? (Not counting your old NES)I can imagine the Viaweb guys didn't enjoy the big company atmosphere of Yahoo, but does anyone actually know why they left? Did it have anything to do with programming?[deleted][deleted]*well, it can be using a hack (once again) to place script elements into the head post-load, but that is beside the point, i mentioned urls themselves via statically declared script tags.*

You can also eval the code you download.

*i don't know why the argument that modularity must be supported at the language level meets with such resistance.*

If and when it becomes a universally supported language feature, this will be a very useful extension to JavaScript. But there are enough independent implementations in various browsers that this will be a long time coming.

The more compelling point, I think, is that putting namespaces in the language doesn't really do anything different from building an intelligent namespace system in an add-on library. It likely won't work much differently at all. The only difference is that it will be available "for free" instead of in a library.

*if you could code client side in python tomorrow, wouldn't you jump?*

Honestly, I enjoy programming in javascript more than python. Assuming that moving to python didn't magically solve the cross-browser implementation issues, not really. I admit, I would probably pick ruby or something in the lisp family above javascript, but for a language that gets as much bad press as javascript does, I'm surprised at how little I have to give up to use it.When matters get escalated to the Escalation Department for further escalation.

/Microsoft&gt; It's really just C nowadays which is lagging behind.

No, C doesn't even define the width of a time_tCobol? We're talking about unix and C here!  Much more prevalent, particularly in important systems.

I imagine 32 bit *unixes still have this issue, even if they are running on 64 bit hardware - they'll still use 32 bit ints.  The issue is very well known, though, so I suppose that new versions will address it.  Still ignores all the old computers and hardware, though.  I guess they could deal with 64bit time on 32 bit hardware.  It'd be slower, though.

Anyway, its certainly not bullshit.  Much bigger than Y2K, which no programmers really gave much credence.[deleted]Great article!

[Webdesign](http://www.oribium.se/) [SEO](http://www.oribium.se/)
[Webbyrå Göteborg](http://www.oribium.se/)
[Design](http://www.ottila.info/sv/)
[Pellets](http://www.jannebergskvarn.se/)
I don't get the "Tamarind" reference...A great article, thank you!

[Internetvärd](http://md-seo.blogspot.com/2006/11/internetvrd.html) [SEO](http://md-seo.blogspot.com/) [Webbyrå](http://www.oribium.se/)
[Webbdesign Göteborg](http://www.oribium.se/webbdesign-webbutveckling/)
oh, for shame.

    readNums :: Int -&gt; String -&gt; [Double]
    readNums n = map read . take n . lines

(I can't test that that works.)[deleted]Did you miss the part about PyPy targetting .Net?This is pretty much exactly as I initially understood monads, and how I communicated this understanding.  Mainly because I'd glanced at Mercury years before picking up Haskell, and Mercury explicitly passes this state:

        % io &lt;- fully known as io.io_state ; the state of the universe.  Optimized away.
        % di &lt;- mode for 'destructive input'; equivalent to: ground &gt;&gt; dead
        % uo &lt;- mode for 'unique output'; equivalent to: free &gt;&gt; unique
        % (where [free,unique,ground,dead] are some of the possible instantiations
        %   of a variable in Mercury)
    :- pred main(io::di, io::uo) is det.

    % all the following are equivalent.  The last is now frowned upon.

    main(IO1, IO) :-
        io.print("Hello, ", IO1, IO2),
        io.print("world!\n", IO2, IO).

    main(IO1, IO) :-
        io.print("world!\n", IO2, IO),
        io.print("Hello, ", IO1, IO2).

    % these next two syntaxes work for anything;
    % not just IO.  Also good for RNG state, &amp;c.
    main(!IO) :-
        io.print("Hello, ", !IO),
        io.print("world!\n", !IO).

    main --&gt;
       io.print("Hello, "),
       io.print("world!\n").

Of course, there's more to monads in Haskell than this simple threading of IO state, but connecting monads to Mercury's threading was all I needed to comfortably use IO in Haskell programs.  [The Concurrent Haskell paper](http://citeseer.ist.psu.edu/7078.html) helped, too, with its 'we singly-thread the IO state so that we can have the -safe optimization- of in-place update' explanation.&gt; $y = sub { my $x=$_[0]; return $x+1 }

"More verbose" if you do it verbosely.  I'd have:

    my $y = sub { shift + 1 }

... but you make up for it with the DO notation at the end.  -shudder- :-)[deleted]&gt; [parrot] seems further along than perl6 itself.

Why does it seem that way to you?
&gt; Incidentally, while experience has indeed compelled me to appreciate Eiffel and other sorts of B&amp;D, I still prefer more loose and dynamic styles of development.

I think that B&amp;D actually permits a less formal, more hackerish style of development. "B&amp;D" can mean imposing arbitrary constraints on the programmer, which is bad; or it can mean outlawing certain high-risk, low-payoff things, which I think is good. That is, I think there is bad B&amp;D and good B&amp;D. The lower the chance one has of creating errors (good B&amp;D), the higher the freedom one has in the way one does things. 
[removed]&gt; I imagine 32 bit *unixes still have this issue,

Let us all fixedly imagine that -all unixen everywhere- have this issue, right now.  OK... what?  2038 is thirty-one years from now.  Shivering in my boots, I am not.  The Y2K issues riddled through everything -- through fixed-width COBOL and database fields and "Year 19100" Perl programs and comparison operations; the 2038 issue is more locative.

And those of you suggesting that we'll all be on 64-bit hardware by then?  Pessimists like you sicken me &gt;_&lt;

:-)

(We'll have quantum computers, and surgically-installed ultra-personal-computers, and some digital rights worth a damn, and all manner of privacy and security hell from the outright failure of every single currently-existing cryptographic protocol.)a = gets.to_i

b = gets.to_i

c = a + b

puts c


Would be an equivalent Ruby program.  Interpreted languages are great for teaching.  Just fire up irb or the equivalent and try stuff out.  You don't have to write a small program, compile it, fix the bugs, compile it again, etc. like I had to do when I was first learning programming.  You get instant feedback.And then Jeff will use alien technology and/or dinosaurs (or dinosaur chaos theory) to defeat the vicious bit limit explained by a clever analogy about how worms cut in pieces keep living, but only dinosaur alien hybrid spaceships can convert the worm's 32 bits to 64 bits when linked up to his conveniently compatible iBook. All this is, of course, Web 2.0 (and available on an RSS feed).

...Damn, it's time for bed.&gt;It should have suggested to you that you think more carefully about
what Linus's words actually mean, without confusing yourself with the
'GNOME tribe'.

Linus's
[words](http://mail.gnome.org/archives/usability/2005-December/msg00021.html):

&gt;This 'users are idiots, and are confused by functionality' mentality
of Gnome is a disease.  If you think your users are idiots, only
idiots will use it.

How carefully do I have to think about that before "only idiots will
use it" becomes "idiots plus some other people will use it"?

Let me repeat for the third time that I do not identify with GNOME
users at all.  I also do not identify with Linus, nor do I see
any reason to lump Linus and GNOME users or developers together.
I have analyzed his words in and of themselves.  Stop assuming that
I'm engaging in a particular fallacy simply because of the position
I've taken.  I've explained my reasoning and it clearly is not based
on any sort of 'tribal loyalty' principle.

He is calling them idiots.  I do not feel insulted by this because
I am not a GNOME user, but I do think he's jerk.

I also think this is a case of one person's bug being another person's
feature.  I, like Linus, would see it as a bug.  But I realize that a
significant number of people see it as a feature, and I'm considerate
enough to let them use it in peace.
[removed]Forth.  The way it encompasses both low-level and high level at once is brilliant.  But it works best when the system is forth-based all the way down.  I'd love to write my own forth someday, but for now I'm just fond of the design.MFC's CTime is one of the culprits, which it will choose by default if you let its wizard build your recordset off of a table with dates in it.  I've tested our old code and it will fail real soon now when it can no longer load a proper 30 year treasury maturity.  If you find any CTimes in your code, COleDateTime works just as well in all the same MFC code.Hypertalk was easy to learn, easy to read and even as a beginner you could do a lot with it.  It was pretty exciting the first time got it to run our class' video disc player.Sounds like a serious case of NIH to me.Haskell and/or OCaml. Simply out of habit I use Python or Perl for small tasks because I rarely, if ever, need to look up anything. Whereas with Haskell or OCaml I have to remind myself which functions are in which libraries. Yes, all it takes is a quick look on a web site, but that's an extra 5 minutes I didn't need to take with the Perl script.

I do, however, find myself favoring Haskell and OCaml for larger projects because I know what the benefits are in the long run.[Helma](http://helma.org/) is precisely that.I'd be surprised if this problem didn't mostly solve itself through advances in computer architecture and software. The real question is, should we start preparing for the year 292271025015 bug once we migrate to 64-bit architectures?[removed]Within the context of the GoF patterns book, yes, Perl is light-years, leaps and bounds ahead, because within that context it's being compared to C++ and, later on, Java. There are tons of things I don't like about Perl (keep in mind I write Python at the day job), but I'd choose it over C++ or Java in a heartbeat.You could also calculate the average of t1 and t2 by doing:

((t1-t2)/2)+t2

which seems a lot neater than any of the suggestions within the article...It's strange how the writer doesn't even know the difference between UTC (Coordinated Universal Time) And UNIX epoch.

UTC has officially started in 1961 (see: http://en.wikipedia.org/wiki/Coordinated_Universal_Time ) and UNIX epoch in 1970 (see: http://en.wikipedia.org/wiki/Epoch_(reference_date)#Computing )there's a way of doing (linear) genetic programming about 200 times faster than normal: you directly evolve machine code right in memory and execute it without interpreting it. you have to take care of the possibility of division by zero and other stuff, but basically it's faster than greased dog shit.

the only implementation of that I know of is a commercial program called Discipulus.Consider the IBM System p5 595. It is a 12 million dollar server running 32 dual-core CPUs. What do they use to take advantage of that power? SQL PL. Oracle's PL/SQL and Microsoft's T-SQL is up there too.

http://www.tpc.org/tpcc/results/tpcc_result_detail.asp?id=107012201

Honestly, can you think of any other language family being actively used by virtually every major company? Sure C/C++ runs the operating system, but most companies don't have C programmers on the payroll.

And the remarkable thing about SQL is that it is a parallel programming language. Take T-SQL, you have to use special commands to make SQL Server NOT automatically run you code on multiple threads.

Another thing about SQL is that it is smart enough to recognize when a function has side effects. It can automatically tell if a function is deterministic or not and react accordingly. Can Java do that? Or C? Or any of the .NET languages? Not a chance. Microsoft is haivng a hell of a time right with PLINQ (Parallel LINQ) because they cannot figure out how to generically determine if a function is deterministic.

I actually told my mom and brother, both sudoku addicts, that the sudoku problem is part of a family of problems in CS that no one knows a good solution for.

The only other thing I mentioned was that the traveling salesman problem is also one.  There's no sense trying to explain NP completeness to people without the background to understand it.Of course not; I don't know what line of work you are in or the kinds of problems you are trying to solve. It would be foolish of me to say that you should use it just because most people have found it to be effective for their problems.It doesn't stand a chance. The name is too close to Corba, which carries some much baggage that anything remotely similar is doomed to fail.I'd second that, I spent one evening playing with it and found it to be very very elegant. Another attractive aspect is the incredibly small vm, though lua also satisfies this criterion.Linus's words:

&gt; This 'users are idiots, and are confused by functionality' mentality

Sneering at a mentality exhibited, Linus says, by the gnome devs.

&gt; of Gnome is a disease.

Asserting that the mentality, besides being contemptuous, is dangerous.  Has negative effects.  Can fester or be cured.

&gt; If you think your users are idiots,

More negative restatement of alleged mentality

&gt; only idiots will use it.

More negative expansion of a negative effect of the alleged mentality: a -warning-, not a 'all of your current users are idiots'.

None of this targets GNOME users.I concur. The only reason is use .net is to take advantage of its tight integration with windows. That's the only thing good about it. And if you're doing that, you're already sacrificing most other considerations, like speed, space, and scalability. Hell, .net garbage collection is so poor that you can usually cut your memory footprint by 2/3 just by tricking it into running a little more aggressively.

So why not, if you're already drinking the koolaid, just drink the koolaid? 
Another type, as seen in haskell, is B&amp;D which allows you to do cool things you wouldn't ordinarily be able to do.He rather missed everything about PyPy. Like JIT, science purposes etc.Give me the performance gains! =)

(Btw: I once translated a python levensthein downto C with pypy and had a performance gain of about factor 100. So for special cases, pypy already is the shit!)&gt; It's really just C nowadays which is lagging behind.

But most Unixes are written in C.  Even if your
application is OK, what if (for instance) the filesystem
fails?  Then your own app will also fail because it is
build on top of other things that fail.Well this was kind of the point of a blog post I linked here on Reddit earlier in the week. Some people took it the wrong way, but my point wasn't to say that PyPy was a failed or bad project (I am definitely looking forward to the 1.0 release).

My point was that doing the types of things the PyPy team has done is HARD. So hard that even with great funding, lots of experience, and quite a bit of time, they still aren't where they might want to be yet.

And therefore, for some of these other teams, it might make a lot more sense to harness the work the PyPy team has already done instead of doing it all on their own.

It makes sense from a developers' standpoint, and it really makes sense from a users' standpoint, because dynamic languages need to get to the point one day where they are all using the same libraries. It is wasteful for Python, Ruby, and Perl to all be doing their own thing with libraries.Give me the program, which solves Sudoku in P, and i will show you how to solve any other NP problem in P.Posting to say hello to people who search for the first mentions of this bug in the future in some database or another the way people search through Google Groups for the first mentions of AIDS, the Y2K bug, etc.

Check this out. Back in 2007 there were these things called "rainforests" and "glaciers." It was pretty cool.Dear reddit: your biases suck.  This is an absolutely amazing article.  No love, ayrnieu.&gt; B&amp;D which allows you to do cool things you wouldn't ordinarily be able to do.

Let's call this kind 'actual BDSM' :-)JavaScript might be improving to become what Python 3000 would wish, and lots and lots of languages are converging on JavaScript syntax/semantics, so it's not like it will exist in only one form. Much like Python and its several implementation versions, JavaScript is shaping up to take over the syntax/semantics from the mainstream languages.

Creature (JavaScript) might outlast/outgrow the Creator (Java).

BTW, one of the important potential mainstream features of JavaScript is the type declaration, which is not on the radar of the standard Python and Ruby main implementations.&gt; If I can't trust the programmers around me not to muck around in my guts without good reason, I can't trust them at all.

Wow, this guy discovered real life programming with multiple people! Maybe one day he'll get a job to experiment with this new knowledge.

This guy is a moron, he complains that public/private/protected is too complicated and at the same time wants multiple inheritance in Java...&gt; No, it isn't really. This massively-parallel human race sticht we have here has more energy that you could fathom if you froze the database and spent the rest of your life researching it on a mountaintop.

It isn't? How many libraries are Perl, Ruby, and Python gonna duplicate and copy from each other before we get a stable system in which they can easily use code from each language?

It is a very inefficient way of doing things for the users of all languages. Maybe not relative to the entire human race, but the last I knew, not every human being was a programmer.[deleted]ridiculous_fish is my hero!thats nice


I think dynamic languages are a waste of freaking time.Last week I visited a website which greeted me with the year 107.

And by the way: The popular e-mail client Kmail had a Sep 9 2001 problem!! Imagine what less popular software we will still use in 31 years. If they don't recompile it (missing source) it still uses a tiny time_t.
Sudoku has nothing to do with P or NP. It is solved in constant time.I suggest you to look up "NP complete".&gt;a -warning-, not a 'all of your current users are idiots'.

But it's a warning based on the idea that non-idiots will dislike
GNOME and leave it.

It seems pretty strained to argue he was talking about the future,
given how long the GNOME devs have had the same approach and the fact
that he calls the code "unusable" (in the present tense).

Even if you think he means there are people who dislike it but haven't
left yet, he's still calling people who *like* GNOME idiots.

I tend to think that most GNOME users use it because they like it, so
I've talked about "GNOME users" in general.  What I originally said,
though, was that he's calling everyone who likes GNOME an idiot.
If you prefer to imagine most users itching to leave but not doing
it quite yet, you can go back to that.

If you can find a link to Linus clarifying that GNOME fans are not
idiots, I'll accept that as his original intent.  For now, I'm pretty
sure he was deliberately attacking a whole design philosophy and
everyone who shares it -- developer or user.
Mine is brainfuck: Mind-boggling unreadability and simplicity rolled into one.[removed]It would have been better if he had cited some real examples rather than depending on the PHB stereotype.  In fact most managers in my experience are actually quite clever people, and well aware of the fact that software is not infinitely malleable.  

The problem lies not in the PHB nature, but in the fact that the real world is not static.  When a manager comes along and asks for a new feature its generally not just a random brain fart; they have reasons for doing this which have been imposed on them.  It may be that the regulations have changed, or that the end users have found a new way of working, or that the competition has come out with a new feature, and of course your customers will look through the feature list of both products.  So its not "I've had a good idea for an extra feature", its more like "We need this feature because of non-negotiable factor X.  Now whats the best way of solving this problem."

The best solution I have seen so far is agile development, where you basically start with the assumption that this stuff is going to happen.  Its also interesting that good open source projects seem to be better at coming up with flexible architectures than commercial software, although the evidence for this is only anecdotal.

So I'll be interested to see what the proposed solution is, but I won't be holding my breath.

Paul.Inform 7.Actually, it is. But you're right, up until now, those millions of people are not contributing.It's not that hard to explain, really.Well, the main problem I see is the following one: you can't name a function killCSSRule and add a comment to explain that it *deletes* a CSS rule. Just name it deleteCSSRule and remove the redundant comment. This is confusing; don't repeat yourself! (especially if you're not using the same words)

And you don't really have to comment each damn line of the code; comment a block to explain what is done and why (not how). Is it really necessary to tell where a while block ends?

But the intention was good, because you tried to explain.
He believes that multiple inheritence causes more problems than it solves?  No -- no, I don't think he really believes that at all.  He may as well have said that he believes in four-letter names of programming languages, or in languages with both a constant `STDIN` and a variable `$stdin`, for all that this 'belief' simply corresponds to whatever decisions matz happens to make.I wonder what people find clean, elegant or simple in C.
Well, let's admit one can find C simple (it is not). But clean and elegant?!
To be more precise, if a problem is NP-complete it means that all other NP-complete problems can be reduced to the current problem (in polynomial time). So a polynomial solution to one NP-complete problem is a polynomial solution to **all** NP-complete problems.

I think I got that right, it's been a while since I had it in computer science. :)Here's my own now:

Ruby is brighter than the sun to me! :-)

I came to Ruby from a Delphi and Java background, but I had worked with some interpreted languages as well, like mIRC Script, PPE (Bulletin Board System - PCBoard)...

In hindsight, the major feature of Ruby has been its interpretation, because even after so many years, I think Ruby's legacy has been to make the interpretation make sense. All the expressivity, the OO support, the handful of the commonly used types, the integration of the RegEx, and so on, all end up making so much sense in this interpreted world of Ruby, as in my experience, Ruby programming does scale from the mythical "One-Liner" command-line, passing by the single script file, and going all the way to the support of medium to large systems, because by the division of labor, by the creation of independent libraries, everything ends up being easily maintainable.

Ruby is the tool to look for when you need the creativity unleashed. I have created as much code as I have deleted already, and the result has been improved libraries and APIs. Without Ruby, chances are that I would be hiding from all the crap I would have written or participated in, by joining the Microsoft/C# camp which had the opportunity to learn from the mistakes of the others so they have been doing a good job overall.

I couldn't have done half of what I have been able to if it weren't for Ruby.

Cheers, Joao

The motivation has been this contest - http://on-ruby.blogspot.com/2007/01/blogging-contest-february-challenge.html[removed]Interesting comment. I'm an embedded software developer too, I hate C as much as I can (but I respect it because it played a very important role in computer history) and I'd like to use a better language.
I'd really like to start a company (in fact, I'm kind of doing it) to sell *real-world* tools for embedded development based on better languages.
I never used Haskell but I think it's probably too hardcore.&gt; For now, I'm pretty sure he was deliberately attacking [users].

That isn't clear at all.  But you've convincingly argued that they are 'collateral damage' in his deliberate attack on the gnome devs.Almost, not quite. I'm on the net wel prior to the introduction of the WWW, when mamma was still changing your dipers and we played MUD instead of SL or WoW. Poor thing, someone jokingly defies your copy-paste, and you turn into an amateur psychologist (and a rather poor one) on a cruzade. Exactly the kind of response I'd expect from an angry white man without a gf.Warning: stack overflowAlthough it's 3 years old, I'd never heard of Factor :-P
Looks interesting
I would love to see any reference, link, book, article or otherwise that could tell me the algoritme to do that.Live long and prosper!Use your common sense. There are finite number of boards. The rest is an excercise for the reader.Problem is that the article is outdated and criticizes the wrong things. For example starting an article with how important it is to be able to write one-lines like 'hello-world' simply doesn't matter if you're going to write at least some 1000 lines anyway. 

But OTOH the author wrote about *his* reasons to not use Java. Total respect for this - but highly subjective stuff is very seldom also interesting for others.
C is the language of the gods?
Fine, I'm an atheist.
apt-get remove emacs &amp;&amp; apt-get install vim-fullIo is my liked-but-not-used language, too.40 years ago : "But the buggy applications will be replaced well before 2000.. Duh!"

Today on reddit : "But the buggy applications will be replaced well before 2038... Duh!"
The people here make the exact same mistake as people 40 years did, so it's not even sad, it's scary.



Just for fun I changed the date of my computer to something past 19 januari 2038 and guess what?

-Opera crashes when you try to launch it.

-When you read a video with Quicktime it crashes with a message "a buffer overun has been detected".

-Yahoo messenger crashes when you try to send a message

-Cool edit (an audio software) crashes when you launch it.

And it's not like I tried hard, I just launch the application and do the most basic operation.
(I advice you NOT to try that on your machine).Reported.[deleted]You are correct; guess I never bothered to look up the formal definition on that one.Huh? In what sense does emacs lisp have no future? As part of emacs I'll bet it has a longer future than any of these Scheme in Java languages, even if it suffers from some poor design choices. I thought one of the interesting tidbits in the paper was the comment about prescheme in Scheme48 and the ability to handle multiple dialects.Wow.  The PowerPC has an `eieio` instruction."It's not us. It's them!"

"We're brilliant coders - it's the suits that ruin it all!"

Sigh.

The "It's not what you might think" title indicates that the article is at least a -little- original or insightful. But no, it's the same old drivel about managers and changing requirements being the problem, without even hinting at a better way of doing things. Just bitching for the sake of bitching. (Like this very post: oh the irony)

And the bridge analogy. Does anybody think it makes _any_ sense? Engineers building bridges make a plan and stick to it. They give guarantees regarding all aspects of the bridge.

* Q: "How many cars can the bridge support?"
* A: "I have no idea. We'll just drive trucks over it until it breaks; then we'll rebuild the bridge."

Where real engineers do the math and give real estimates and real guarantees, we software people give none.

This bridge analogy isn't even worth a thoughtful rebuttal - it's so inane.

I think we don't need complex and elaborate explanations for why software projects fail.

0. reading code is harder than writing it. Consequently, about half the time we write code in an environment we do not fully understand. We have to make guesses, so sometimes we guess wrong. This gets worse with bigger projects and with projects involving more people.

1. many programmers are 'winging it', me included. We learn on the job, and we never get the chance to completely master the environment we're in. Consequently, every once in a while we fuck up.

Corporate structure certainly influences the way we build software. But if the PHBs are to blame, how come open source projects are on average complete failures?

Even if you give a single experienced and highly competent programmer a task to build something - without changing requirements or any other complications... chances are the project will still buckle under its own weight.

Blaming others is not productive, and frankly, makes the software industry look immature.

Software is getting better every year. Be glad that everything is gradually getting better.

Or change professions and start building bridges. Building bridges must be so easy. Or become a surgeon, and get the respect you think you _deserve_. Right.

We're neurosurgeon-artist-engineer-rocketscientists! Why can't everybody acknowledge that!?

Pfft.Nice article, unique style.See [this thread](http://groups.google.com/group/comp.lang.scheme/browse_thread/thread/962df9e8e2a4cd27/0df6a7e29599bbfd?hl=en#0df6a7e29599bbfd) for a more objective (and realistic) view of the subject.I'm  just wishing happy new year to you and the community. I don't know how to use Reddit to do that other than posting something. I thought WIkipedia would be suitable.The GNOME developers are GNOME users themselves. So by Linus' logic, the GNOME developers consider themselves stupid also.

Not that plausible.

Although I don't completely agree with the GNOME philosophy (or the one of KDE/ice/fluxbox/xfce/ for that matter) I do think the devs should do what they consider best. It's what OOS stands for: people building what _they_ want to build. The end user decides which projects become successful and which don't. In theory anyway.

I think that Linus lashed out at GNOME because he's disappointed with the progress on the desktop in general. Linux kernel-land has gotten pretty stable in the last few years. Driver support has gotten to the point where you have to look for the things that are _not_ supported, instead of for the hardware that _is_ supported.

The desktop however still feels like windows 95. Unstable, chaotic, poorly documented, inconsistent. It pretty much sucks. If I were Linus, I'd be frustrated with that as well.Thanks for the link. Apart from the typical silly client side JavaScript most of my experience had been with a proprietary JavaScript clone, and I'm not sure it shared that...Jon Harrop is on a crusade against Lisp, for whatever reasons. He always brings the pattern matching argument and calls OCaml a "modern functional language", implying that Lisp is old and thus bad, completely missing advantages of Lisp. I don't take him serious anymore when he speaks about Lisp or pattern matching.Looks like you broke your UNIX ;)let a = read_int () and b = read_int () in print_int (a + b) ;;

Fits comfortably in a single line. Profits from the fact that `+` is commutative to avoid determining the order in which the `read`s occur. Is readable, even, if you squint a little `;-)`That's actually not a bad idea.  It would be trivial for most common forms of media and their sources.

Also there could be checkboxes on the submit form asking what type of media the link links to.[removed]Pattern matching is definitely one of the very useful features of functional languages such as OCaml, especially given what I see as the general bent of functional languages towards emphasizing a more data-centric and declarative approach. This is particularly true of the strongly and statically typed languages, where pattern matching is very much tied up with, and naturally follows from, the type system. However, all that said, I believe that pattern matching, while *desirable*, is hardly essential, and probably doesn't make a very good benchmark for comparing languages.&gt; and calls OCaml a "modern functional language"

It is.  And Lisp isn't.  Lisp is Lisp (or CL, or whatever one happens to mean at the time).

Although if he says this as a sneer at Lisp, I share your distaste.[deleted]Just for those who want to check pypy out a bit:

First download pypy, then run the translatorshell in the bin/ directory. 

    $ python translatorshell.py
    *snip*
    &gt;&gt;&gt; def ack(x,y):
    ...   if x == 0: return y + 1
    ...   elif y == 0: return ack(x - 1, 1)
    ...   else: return ack(x - 1, ack(x, y - 1))
    ...
    &gt;&gt;&gt; t = Translation(ack)
    &gt;&gt;&gt; t.annotate([int, int])
    *snip*
    &gt;&gt;&gt; t.rtype()
    *snip*
    &gt;&gt;&gt; ack_c = t.compile_c()
    *snip*
    &gt;&gt;&gt; import time
    &gt;&gt;&gt; start = time.time(); ack(3,6); print time.time() - start
    509
    0.19878911972
    &gt;&gt;&gt; start = time.time(); ack_c(3,6); print time.time() - start
    509
    0.00290894508362

The C version is nearly 70 times faster than the pure python version.

I do know that this is not the usual real life bottleneck that you run across, but it does somehow show PyPy's potential.Javascript does not really have anything to do with Java, does it?

Furthermore, Ruby and Python definately do have a significant edge in other aspects, like threading, networking, numerics, C interface, ...My dilemma has been answered in the comments to the [bugzilla ticket](http://bugzilla.gnome.org/show_bug.cgi?id=408898).

I wasn't missing anything, really.Character attacks aside.  The point he is making is actually quite valid.  Pattern matching is a very concise form of expressing action-sets for complex state awareness.  Not having to dive into boatloads of nested execution paths is one of the joys of reading OCaml, Erlang and the like.  On top of that, the ability to match against all built in aggregate data structures on arbitrary nesting levels and then rebinding their interesting members for the match expression adds to clarity and compactness (two concepts usually taken as mutually exclusive).I wasn't trolling.

Your first paragraph, in which you are trying to disagree with me, states exactly the same thing I said!

Your second paragraph is very subjective, as subjective as my opinion on the desktop environments. I think gaim is pretty lousy compared to Miranda-im or Trillian. I find Nautilus has many rough edges.

We may have different opinions, but that doesn't make one of us a troll. Pigeonholing people is sleazy. Don't do it.&gt;by 2038, if the singularity hasn't come yet, this CERTAINLY won't be an issue.

I'd like this issue sorted out *before* the singularity, thanks. Would rather not have to deal with overflow problems in my *mind*.&gt; Problem is that the articcle [...] criticizes the wrong things..  For example [...] one-lines like 'hello-world'

Good lord, did you read to tha point and simply stop?  Don't say 'criticizes the wrong things' without a better example.  As for outdated, it is only outdated in the small section that every Eclipse user will sigh at.The author doesn't know the difference between "its" and "it's".Don't forget that pattern-matching in OCaml is heavily optimized. There was a flurry of research on the topic in the first half of the '90s to formally *prove* that the compilation scheme OCaml uses to sort patterns is optimal, given a number of conditions. In short, it compiles pattern matching to nested computed `goto`s using Dynamic Programming, sorting the more specific patterns first so that you don't pay the full price of general testing upfront.

I'm sorry I can't provide bibliographical reference (all my papers are hardcopies stashed somewhere inaccessible at the moment).OH NOES, NOT TO THE NET-POLICE?!brainfuck is my 'language I loathe because it is not befunge, which is cooler in every respec, but resentfully gets less notice' language :-)The modern vs. old says nothing. Ruby is younger than Caml and is more like Lisp. The way he says it, it sounds as Lisp lacks pattern matching and static typing because no one knew about this great techniques back then. It would be similar bullshit if someone statet that OCaml lacks CLOS-like OO because the OCaml designers have no clue about OO.

Everything I've read so far from Jon Harrop was this "modern OCaml" versus "old Lisp lacking pattern matching and static typing".Enable In-Order Execution of Input-Output.

A classic PowerPC mnemonic, along with "lfsux" (I'm not making this up, this is "load floating-point single with update, indexed"). The original MPW disassembler always printed the comment "`; and it's a bitch and then you die`".[deleted]I looked into Sudoku-solving algorithms at one point. For a high-speed solution, Knuth's "dancing links" algorithm will solve any ordinary Sudoku puzzle in milliseconds. For a slower but more straightforward solution, just feed the rules of Sudoku to any constraint solver (say, the Oz programming language) and watch the answer pop out.

Now, in theory, it may be possible to construct Sudoku puzzles which require NP time to solve. But in practice, published Sudoku puzzles (even the "hard" ones), rely on a half-dozen rules of logical inference. A very large subset of all possible Sudoku puzzles (and essentially all the ones aimed at humans) can be solved trivially in polynomial time.

(There are similar behaviors in other NP-complete problems: Given a plausible distribution of input values, you can sometimes find an amortized polynomial-time solution, because the hard cases are so infrequent.)

So, yeah, I think this article is fairly misleading, though not actually inaccurate. What they could accurately claim: "Millions of people around the world are solving trivial examples of a potentially NP-complete problem, and would become massively frustrated if even a few of the more pathological cases ever showed up. The puzzle designers make sure this never happens."Did anyone notice that there is no comment on the merit of Linus's patches?  This is typical Gnome drivel about how "we are right because we know what is best" and they never once focus on the change submitted to them.  Noooo, lets focus on why Gnome is what Gnome has become rather than get in to discussion of changes that could be improvements.  According to Linus, his patch didn't add features but rather fix code, so this "we don't want to add new feature stuff" doesn't apply does it?Stop pigeonholing, and try to understand the argument I'm making. My argument doesn't require long rambling explanations, it's a hypothetical.

This is the argument:

1. I think that linux on the desktop is of significantly lower quality than the backbone of linux
2. Maybe Linus thinks so too
3. If so, then I can imagine he's not too happy about the way the GNOME guys run their project
4. Hence him lashing out at them in frustration

That's my argument. 

If I spend 2000 words explaining why I think the linux desktop is flawed it would only distract from my point.Of course when you rewrite your python in C and then want to speed it up more in a platform specific way, you can rewrite the slow bits in Assembly.Maybe he has a book to sell?Yes, I noticed that recently :-)

He ensured that I'll never buy it.Someone needs to explain the difference between "effected" and "affected" to this guy.Final paragraph:

&gt; Anyway, back to Linus and his irritation with Metacity. I can't not say if his patches will go in or not, its not my call. But I did at least add them [properly to bugzilla](http://bugzilla.gnome.org/show_bug.cgi?id=408898) for Linus to ensure they get reviewed and commented on at least.Sorry, but this is hardly the first time this is mentioned. I read about that problem at least 10 years ago (around the same time the Y2K craze began).Which means he has not looked at the patches, just commenting on Linus's comments.That's not what I mean at all.  I mean that the quality, both editorial and cognitive, shows why he wasn't hired.  He's also much to blame for not filing his expenses in a timely fashion and following up on them in the same manner.

You really think someone at Google handles processing interview travel expenses?  They outsource it the same as any large corp.Regardless of how willing the human race is going to be to recreate libraries that work in Ruby/C, Ruby/JVM, Ruby/SchemeVM, Python/C, Python/.Net, Python/JVM, Perl6/C, Perl6/Parrot, Perl6/Haskell, and a few I've forgotten, no matter _how_ diligently we work, we still won't have the ability to write a library in one language and use it in all that we can get if we share. 

I think there's an optimal number of duplicated efforts, where you've got enough people trying enough things that good ideas can still come out even if one team is unable to come up with them (perhaps the leadership is rejecting putting it in the main repository because they think it's a bad idea, but they are wrong), but not so much duplicated effort that it's just being wasted. That's why in my original post I said we should reduce down to 3 or 4 efforts, not all the way down to one.

We need to get to the point where writing Another Ruby VM isn't "cool".How to [beatify your Emacs](http://www.stallman.org/saintignucius.jpg).The first commenter on this article makes some very good remarks.By the way: How is OCaml's support for multiple cores?

(Honest question. Multiple cores are everywhere and Erlang looks a bit ugly. I'm still searching for a fast non-scripting language.)Maybe he visited #lisp? :-)
&gt; he complains that public/private/protected is too complicated

Where does he make this claim?&gt; he complains that public/private/protected is too complicated

Where does he make this claim?

No, I won't say it this politely.

HE NEVER MAKES ANYTHING APPROACHING THIS CLAIM.  MOREVER, THE EXTENT OF INTENTIONAL HOSTILITY REQUIRED TO READ THIS CLAIM INTO HIM IS NECESSARILY DISINFORMATIONAL.

Fuckface.I know, I know, and I like the song, but here are keybindings that make sense on my zaurus:

    map ,n :bn&lt;CR&gt;
    map ,p :bprev&lt;CR&gt;
    map ,, :buffer&lt;SPACE&gt;
    map ,. :buffers&lt;CR&gt;
    map ,d :e %:h&lt;CR&gt;
    map z. zz
    map - '

    " text menus, bound to the zaurus 'menu' key :-)
    source $VIMRUNTIME/menu.vim
    set wildmenu
    set wildcharm=&lt;C-Z&gt;
    set cpo-=&lt;
    map &lt;F5&gt; :emenu &lt;C-Z&gt;

And nobody with a good windowmanager needs to edit files one at a time :-)&gt; I don't buy that argument.

It -is- better than any pattern matcher CL could have, but for the reason that O'Caml is a statically, extensibly, and richly typed language, with pervasive constructors that it can pattern-match off.  Mercury and O'Caml and Haskell all have this, and so necessarily get performance and compiler checking and logical advantages that CL never will.

A CL pattern-matcher, however, always has the potential to be at least as good as Erlang's system.  So if you want ideas, look at it.  How is your bitarray-matching coming along? :-)Minesweeper is also NP-complete.He states that he dropped C++ as soon as he encountered public/private/protected. I'm not sure the implication is that it was too complicated, but he certainly doesn't like the idea. 

Overall, I think the guy expressed his opinions well, but I personally don't agree with many of them. The whole view seems to be skewed toward that of a lone hacker, and in practice I've found such a development style doesn't scale. That seems to fit though, because in practice I've found that any Perl development rarely scales past the lone hacker.&gt;Of course when you rewrite your python in C

Why?!

&gt;then want to speed it up more in a platform specific way, you can rewrite the slow bits in Assembly.

That is a common misconception.  Assembly does not guarantee speed over C.No, there is an infinite number of boards. There's a finite number of 9x9 boards, and 4096x4096 boards, but not a finite number of NxN boards.

When discussing algorithmic complexity, you always have some variable that approach infinity.Whew. Pagggh!  What is this, "Emacs for Dummies"?Io probably falls into that category for me, too. Never having the runtime at hand helps.&gt; I'm not sure the implication

There's no implication.  He finishes the bloody sentence with his explicit distaste for it.  Fuckface above -quotes- that explicit distaste; you presumably -read- it.  What alternate reality have I fallen into?
If an article starts with an example which is irrelevant and wrong, it don't really creates an urge to read on. But I still did and found nothing relevant (for me, others may look at it differently). But do I really need to comment *every bit* he wrote? So I simply picked his first example - because it was his first. But ok:

&gt; There's more to life than OO

Doh! But if somebody wants to do OOP, I would consider it quite sensible to pick a OOP-language. If OOP gets out-vogue later, then I would simply switch to the actual in-vogue language instead of using a half-assed compromise just in case.

&gt; No CPAN

True. But who cares as long I have Google? If I want to use 3rd party code, I have to evaluate it first, maybe check it into the repository etc. So the time something like CPAN would spare me is quite irrelevant.

&gt; No function pointers means no closures

Nonsense. Java has closures (name is 'inner-class').

&gt; No symbol table manipulation

Which is a good thing and would also be totally useless in a static typed language.

&gt; No dynamic method generation

If you really want it, you can have it. But it's not recommended and thus a bit difficult. In most cases you can archive the same by other means.

&gt; No eval

Fortunately. Even in Lisp they acknowledged that it's not a good thing. It's risky and can create unmaintainable code.

&gt; No multiple inheritance

MI is not a good thing. Interfaces combined with traits is much better. Java has no traits which is bad, but having no MI is still no real problem.

&gt; No here-docs

Again a good think. Putting long texts into external files via some easy to integrate template engine is much better. Here docs only lead to maintainability problems.

&gt; Bureaucratic privacy rules

Inflexible and not powerful enough: True. But every programming language is 'Bureaucratic', so this is again a 'Doh!'.

&gt; Mandatory strong types

Not a bug but a feature. Weak types are evil.

http://hedgehog.oliotalo.fi/ 

A curious thing.[deleted]&gt; for example there's no simple way to covert a Scheme procedure to a C function.

Granted, for some definitions of "convert" and "simple". :-) 

&gt; All the various "declare", "use" etc. forms were just too confusing, especially since they worked differently between compiled and interpreted code.

Declarations are ignored in interpreted code; they are compiler directives. It's true that they can change the semantics of your code; but most of the time you only add them if you know you want them. 

"Use" is a short form for "require-extension" and really is the only directive you need 90% of the time. It can load installed third-party modules, as well as both compiled and source files (.so/.dll as well as .scm) from your project directory. It's the closest thing to, say, "import" in Python.

Certainly, depending on what you are programming, you may *need* to compile your code. If you're writing against the FFI, for example, there's no choice. But I would guess that most people use compilation for two main reasons: the immediate performance gain, and the ability to build standalone executables. These aren't always important, especially with in-house and development work. And the Chicken interpreter is no slouch, performance-wise; it's much zippier than most interpreted languages.

As you probably guessed by now, I like Chicken. :-) It's not the purest Scheme, nor the fastest, nor the most popular: but IMO it really hits a sweet-spot for pragmatic, real-world programming.Excellent overview with lots of links for the curious to follow up.The OCaml pattern matcher is not necessarily better than any pattern matcher CL could have, because it is static and tied to the type system. If you want to do matching that goes beyond the type system, or some matching with dynamically generated patterns, you have to write your own pattern matcher.

Although, I seem to remember a lot of the people on Usenet like Jon Harrop saying that any language feature that can not interact with a Hindley-Milner-like static type system is not a feature that anybody should use. When all you have is a hammer, everything looks like a nail. ;-)I emailed him about it last night. His response this morning:

&gt; yep - all true
&gt; 
&gt; too lazy to update it
&gt; 
&gt; -paulvia
http://perl6.cz/wiki/OdkazyIs this what things on programming reddit are coming to? I'm almost inspired to write a new reddit-alike already.I assume that you by "better" mean "faster". In order to compare apples with apples, we need to assume that we give each pattern matcher the same amount of information. That is part of the pattern matcher is a macros for defining the data.

&gt; How is your bitarray-matching coming along? :-)

Are you thinking of my bit-io ports? 

http://scheme.dk/blog/2006/04/reading-and-writing-bits-bit-ioplt.html
No CPAN? You don't need CPAN when Java comes with everything you need.I actually like it. I think code quality and maintainability is an important topic in many markets (if not most) and a language supporting that design metric in the actual grammar sounds like a good idea to me. I don't understand why everybody is being to hostile to this ?The point of the article wasn't to blame Pointy Haired Bosses for the problems of the software industry. It was to blame the way we as an industry structure our companies, which, among other things, causes communication problems among PHBs and the coders they manage. The root cause of problem lies in the organization, not in any particular people.

Secondly, although I only mentioned this briefly, another real problem that this corporate structure causes is the unwillingness of developers to push back and in general communicate upward. This is just as much of an indictment of the developers as of managers, but in actuality the fault lies with the structure of the organization. In a hierarchically managed organization, developers can't be expected to say "no" to their superiors any more than managers can be expected to accept "no" when asking for unreasonable features.

As to your distate for the bridge analogy, well, it's just that: an analogy. Bridges are physical things, software isn't. Obviously they're not identical. If you've got a better analogy, by all means, let's hear it.

And in regards to open source software, often the measurement of failure is completely different. For instance if a particular open source project labors on in obscurity but is used daily by the developer and a few of his friends, then that might be considered a success. Additionally, a lot of open source software these days *is* developed in a hierarchically managed corporation, and so this article very much does apply to it.

Also, you complain that the article doesn't even hint at a better way to do things. If you had bothered to read all the way to the bottom of the text, you would've seen the little note that the subsequent blog entry would cover some potential solutions to this problem. I try to keep each entry fairly short and succinct.

Anyway, thanks for all the feedback, even if you didn't intend for it to be constructive. :)Changing and incomplete specifications *is* a real problem in software development. Most customers simply have no clue, what they want and often the only way to really find out, what it is, is incremental development. This is even true, if the customer and developer are the same group of persons.

The big problem with software is that it's really difficult to visualize. If someone wants to build a new house or a bridge, you can build a model, show it to the customer and modify it until he's satisfied. And *then* start with the process of really building it.

This is much more difficult with software. While it's possible to build prototypes or mock-ups, it often don't work, because the customer confuses the mock-up with the real thing. And if you tell him that it takes months or even years he thinks that you try to deceive him after the prototype looked so fine after a few weeks. With a bridge, nobody would mix up the model with the real thing, but with software this is quite likely to happen.

And while building bridges is not easy, it's a very old and very well known job. Compare how long humans build bridges and how short they build software. As a result, it's much more likely and often that one have to explore new grounds instead of moving on known territory.

I'm looking forward to the second entry. I'll hold on to my verdict until then. Let's see if an article called "Why the software industry is broken" lives up to its title...No, but it's hard for non-mathematical people to *understand*. ;-)Exactly. The analogy with bridgebuilding is completely flawed on all grounds, like you pointed out.Yes, I've heard about Hedgehog and it's exactly the kind of thing I'm interested in.
Not trying to be nasty at all here, but *did* you read the part of the article that explained how building bridges is in many ways easier because it's a physical thing you can point your bosses at, while building software is difficult in large part because you can't, and everything is abstract?Interesting and stimulating article comparing roles on a software team with those within a soccer club.I'm arguing that the whole analogy is flawed, and serves no purpose in the first place.

You can also argue that growing turnips is different from building software. I wouldn't disagree with that either, but that doesn't make it (a) a good analogy and (b) relevant.unfortunately the Ruby mode for Emacs was pretty weak the last time I used it.

the modes for Python, C, and Erlang are very strong, though.Actually, I know a guy who just left the software industry to grow turnips and other vegetables on a farm, so obviously there are some skills in common. :)Many web owners and web developers and designers are concerned about the information published on the “Contact us” web page on their web sites. It is known that there are software applications used by spammers ...

Is it worth to hide the e-mail address ?If i understood it right, you both agree with each other and i agree with you. The Linux kernel is at least one of the best kernels today. The GNU tools are also very good. The Desktop isn't.

GNOME isn't that stable. Evolution is a prime example of an unstable application. It breaks with every new major version and is hardly patched to a useable state before shipped. I don't use KDE, but my friends who do, report me similar experiences.

&gt; an idea that requests for config options was usually a result of broken behaviour in the window manager and thus feeling the behaviour should be fixed instead of a config option added to work around the problem

This describes the different philosophies of KDE and GNOME (spoken from the GNOME point of view, though).When he became a turnip-farmer... did he still complain that he didn't get the same respect as neurosurgeons?&gt; The troublesome thing about Sawfish was that it was written in its own Lisp dialect so as part of Sawfish you got both an extra lisp interpreter and GTK+ bindings for it. This meant that the C/C++ skills in the GNOME community didn't lend themselves well to fixing bugs in Sawfish.

Hey, this is reddit! No mention of the LISP paragraph so far?Maybe they upvote you, if you use aptitude? ;)Possibly, but I'd like to think that right now he's on some web forum discussing how turnip growing is or isn't like bridge building, and wondering whether there's anything that could be a better use of his time.You win.[deleted]If locks are your bottleneck - do it single threaded.

I don't think it's worth this effort today. When we get hundreds of parallel processors, they (hopefully) designed some better concurrency handling into them.

When you want to design for multi threaded speed optimizations today, use a futures or actors library. Protecting your data structures with locks, semaphores and monitors is also not that hard.The usual way?He's not telling because the site is meant to be "viral" ie. advertisement for his great idea. It's a mysssstery!![deleted]Search for word at point already exists in Emacs:

    C-s C-w

% is nice in Vim only because it lacks the movement commands Emacs already has, so it seems kind of strange to introduce it.

    C-M-f  ;; forward expression
    C-M-b  ;; backward expression

iswitchb-mode is the counterpart to ido-mode for buffer switching.  Absolutely essential.  It should be on by default.

I'm just guessing, but I'd say an alternate reality where you're right about everything and verbally assault everyone who doesn't agree with you. Well, the verbal assault part appears to be the reality you're trying to impose on everyone else. 

Anyway, I'll feed the troll one more snack here. There is a difference between excessive complexity and unnecessary functionality. The author appeared to be implying inheritance protection was the latter, not the former.It's an quite common mistake that the size of short and long code has anything to do with each other. There are for example languages which allow 'free' statements while others require to define some 'main'-function. Also some need explicit imports whole other import some standard libs implicitly. In a 1 line-program this can create 4-times overhead, but in a 1000-lines program (with all code in one file) it would add again only 4 only lines total: Resulting overhead &lt;0.5%. 

Now Java forces programmers to put each public classes in a separate file (which was also criticized by the blog-author), but this isn't even visible to me because it's completely abstracted away by all modern IDEs (thats why I wrote 'outdated'). Code which I don't have to create or maintain (like import declarations in Java or separating classes into different files) simply isn't relevant. I look at modern Java like at Smalltalk: The IDE is a part of the language.

Java has approx. a 2-times overhead over most languages. In certain cases this can increase up to 4-times, but the reasons are totally different.

And again in Perl there are also many concepts in this small piece of code. They are simply 'under the hood'. This may be useful for beginners but if you want to master a language you need of course to know what's really happening, anyway. So in the end it's irrelevant. I don't consider Perl a useful learning language. Really.

And I think that dependency injection is a general problem, but often it's simply ignored (totally possible in most relativly small projects) or solved by some ad hoc methodology if it becomes necessary at some point in the project. I see a similarity here with the expression-problem which is even worse in most functional programming languages (because of the ML type-system) than in OOP, but since there are much less real big projects done in FP the problem simply don't stands out as much.

[Linus](http://lists.osdl.org/pipermail/desktop_architects/2005-December/000395.html): _"..the thing is BY DEFINITION not usable if it cannot be used for a specific task."_

I think firefox gets this right, by providing useable configuration for a small number of common knobs, and relegating the rest to [about:config](about:config). A knob doesn't cease to exist just because you decide it doesn't merit configuration on useability grounds.I see, I did misinterpret you. I've never done much elisp other than various cusotmization hacks. I recall at one point the FSF was funding the guile work, as a rpelacement for elisp. I don't know what happened with that effort. I never could understand why Scheme hasn't become more of a mainstream language.I agree that concurrency is rarely worth the effort on a (say) 4-processor system.

&gt; Protecting your data structures with locks, semaphores and monitors is also not that hard.

In theory, it shouldn't be. But I've written a lot of lock-based code for true multiprocessor machines over the years, and--honestly--it always turns into a nasty slog.

I'm an extremely careful, pedantic programmer, to the point of writing proofs of correctness for certain tricky bits of code. And I still make dumb mistakes as soon as complicated locks get involved.

I've become a huge believer in transactions, message passing, and other forms of deterministic parallelism.
&gt; (regarding the alioth shootout) Our old, good, not so tough to learn, not so “enlightening” Ruby language often ends up with shorter programs than those written in any of the “languages of the gods”.

This argument is laughable. The entries for Haskell/Lisp/OCaml show that you can write C-like code in these languages, giving C-like speed. And that is a good thing. I can tell you, Rubyists would _love_ to be able to do that, too.

Additionally, the shootout programs are tiny, abtraction capabilities don't matter there.I never thought about it that way before, but I think you are dead on.In Lisp?
When they have cover sheets for the TPS report. And you know what a TPS report is!
can you possibly be more offensive? if you're frustrated, contribute, or shut up.Or if you weren't posting anti-Emacs comments to a helpful article for Emacs users.[deleted]It basically does if you're decent at it.  I suppose it's possible that the C compiler has figured out absolutely the most efficient method, in which case you're totally stuck.

Then again, I'd imagine its possible for introduction of ASM to interfere with optimizations in other code, as you cannot make any guarantees (not that you can make many with regular C code...).Nothing guarantees speed, but in the right hands assembly does give you a lot more control over the machine than C. That may translate into more speed, especailly if you can take advantage of CPU-specific op codes.Probably loads more than you think... Embedded systems hugely outweigh PCs in population.  Many of those are even 8 bit.Sweet. I'm going to go write a web server in SQL right now!I'll give you a hint. You use the solution against werewolves.It's a single entry point. It's a single fix. If a problem like this is to become earth shattering, it specifically must NOT BE restricted to core and library areas, it must be something all end-level-code gets wrong. See much bit fucking going around regarding dates these days?

Me neither. We use libraries. Y2K will have been wore than '2038' when that comes around.&gt;but in the right hands assembly does give you a lot more control over the machine than C. 

But there is the magic of C: just use inline assembly with conditional compilation.  Doing this gives you that same awesome hardware performance with the bonus of portability.Hah. Hah. Hah.[removed]I'm beginning to wonder if the problem with Parrot/Pugs/Perl6 is simply that it isn't needed.  Personally, I don't mind the delay at all.  I don't see too many people outside of the respective projects complaining either.  Maybe people just don't care, and the devs sense this and so don't put in all that much effort.

Then again, I switched to Python a while ago, so maybe that's why I don't care...[removed]They are *not* closures, and even if they were, their syntactic cruft makes my eyes bleed.

I *cannot* achieve dynamic method creation by any *reasonable* means.

MI is not *intrinsically* bad; it can be abused, just like operator overloading was when C++ came out. Java doesn't support trivial modules, mixins, *or* MI, which leads to a lot of duplicated effort.

Java is NOT an OOPL, it's OOPL-like, and it's very irritating to be hamstrung by its lack of capability.

&gt; every programming language is 'Bureaucratic'

Yeah? How about "some languages are more bureaucratic than others" then, which is a no-brainer, so 'Doh!' yourself.

&gt;&gt; Mandatory strong types
&gt; Not a bug but a feature. Weak types are evil.

That's ridiculous. (...and my argument is just as valid as your completely content-free one.) Bad developers are evil.
Whew; I was hoping that was a joke. Thank goodness."But I didn’t seen any particular too much wonderful Lisp applications written by ESR…"

What was the last wonderful application you saw by ESR in any language? ESR is a writer and maintainer, at best.

As for the listing applications stuff, is that really useful? If I come here and say, I use Stumpwm, Emacs, darcs, GHC, lambdabot, all in a Haskell OS, does that show Lisp/Haskell is good? No: all such an argument can hope to show is that popular languages... are popular. I don't think anyone disputes that knowing C/C++ is *useful*; the arguments belike that Haskell/Lisps are intrinsically superior on the merits of the language itself, irrespective of how much is already written in it and what sort of other network effects inhere in it.[deleted]Damn, that stuff is insane.Is there any other way to write assembly? I cannot imagine anyone still using stuff like MASM.&gt; Where real engineers do the math and give real estimates and real guarantees, we software people give none.

There was a blog post on Reddit recently that said that's not actually how real engineers build bridges.  There's apparently a lot of guesswork, empirical tests, back-of-the-envelope calculations, and intuitive leaps involved before the engineer delivers a design and construction begins.  They usually do the math afterwards so they have some numbers to show, but many aspects of engineering are just as fuzzy as computer programming.

[Here's](http://www.vanderburg.org/Blog) the post, and it may be worth checking out the Wikipedia entry on [Robert Maillart](http://en.wikipedia.org/wiki/Robert_Maillart).Quote from IRC a while back:

&lt;atob&gt; Hackerdom's most revered demigods are people who have written large, capable programs that met a widespread need and given them away, so that now everyone uses them.

&lt;atob&gt; ESR gave away fetchmail and they gave it back. :(Interestingly, you could almost replace Perl with Ruby or Python (except for CPAN arguably) and the article would read the same.  

The main point of the article is basically that Java isn't flexible in how it lets you do things.  Want to program functionally (with ease)?  Sorry.  Want to write witty one liners that do something useful?  Sorry.  Pretty soon Java won't be the most powerful language on the JVM (JRuby, Jython) so it doesn't really matter.&gt;Is there any other way to write assembly? 

I wouldn't, but that doesn't appear to stop [others](http://www.menuetos.net/)Thanks raganwald, a lot of use who don't use C++ on a regular basis forget those details.Knock off the insults, they have no place here.[deleted]Some people have way too much free time on their hands.Upmodded for the humor alone; it's rare to see in technical writing amusing gems like:

"With it went a lot of our enthusiasm. When people come into a mailing list or IRC channel with a quick implementation of an off-beat idea, droves of righteous knights rise up to slash them down and set fire to their bodies."

or my favorite, a quote from Glidden's writing:

”[GNOME] Development strategies are generally determined by whatever light show happens to be going on at the moment, when one of the developers will leap up and scream *‘I WANT IT TO LOOK JUST LIKE THAT’* and then straight-arm his laptop against the wall in an hallucinogenic frenzy before vomiting copiously, passing out and falling face-down in the middle of the dance floor. There’s no whiteboard, so developers diagram things out in the puddles of spilt beer, urine and vomit on the floor."That same exchange has a later
[email](http://lists.osdl.org/pipermail/desktop_architects/2005-December/000471.html)
by the Metacity guy that mentions about:config.  It also says this:

&gt;I did in fact spend some time hacking on it, and didn't get a patch
I liked. The current status is that I don't personally have time to
do it, no paying customer has ever had this on the top of their list,
and I've never gotten an adequate patch. It's really that simple.

Near the end he says he'll keep an eye out for the patches on their
bug tracker.  So it sounds like the feature itself won't be considered
too complicated for the users.  It is, according to the Metacity guy,
just a matter of doing it and doing it right.

One thing I noticed is that he talks about targeting the corporate
marked.  That could be thought of as "the customers think the users
are idiots."  (He didn't say that, I'm saying that's something you
find a lot in the corporate market.)
According to the early Christians, the language of the God is logos endiathetos.

But if you worship at the Temple of the Gnuphrates, the language of the gods is clearly Lisp. :)Yeah, do it right this time :)This is from 2002, wtf.So C is the language of the gods? Well, doesn't that just neatly explain all the problems of the universe.&gt; Pretty soon Java won't be the most powerful language on the JVM (JRuby, Jython) so it doesn't really matter.

It already isn't ([Scala](http://www.scala-lang.org/), [CAL](http://labs.businessobjects.com/cal/), [even Haskell?](http://www.cs.rit.edu/~bja8464/lambdavm/)).I use GNOME.  I don't consider myself to be an idiot, and I don't think the GNOME developers are treating me like an idiot.

There are lots of things I want to do with my computer, but deciding exactly what the title bar does when I center-click it is not one of them.  What's the point?I have to agree that Unix-in-Lisp is a bad idea. A true Lisp operating system should take advantage of all the strengths that Lisp provides.

However, I would be cautious about starting off too forward-thinking. I would like to see a minimal (but functional) Lisp OS that can be used as a launching point for truely forward thinking projects.

OK, time for a blatant plug. I have a Lisp OS project on sourceforge ( http://sf.net/projects/losak ), and I would love any input from someone with experience in the subject.[deleted]In all seriousness, not trolling here, how many people are still using Emacs on a daily basis?  More importantly, are there serious Emacs users who have been using it for less than 5 years?

I used Emacs a lot for about a decade, but that decade ended over a decade ago...[deleted]The missing word in that top clause was "module". I don't advocate writing all your python in C but for some modules, if it's a bottle neck and you aren't changing its interface anymore, go for it.

Assembly does not guarantee speed over C but if you look at the disassembly and you know the platform there are gains to be made. There are people at my workplace who can squeeze a ridiculous amount calculation out of assembly.Lol, never has anyone proved my points for me so easily, troll--I'm truly in awe.

Pot, meet kettle.I find it hard to trust an argument claiming a language is superior unless it considers what was made with it.

Consider the bejeweled golden hammer. It is beautiful to behold, with is elegant curves and dazzling stones. Only a fool would dare to compare the simple carpenter's hammer the rest of us use, with its dirty and cracked wooden handle and chipped face.

And hardly anyone has seen something built with the golden hammer. Everyone knows it would put the carpenter's hammer to shame, but for reasons not know to us, the owners of golden hammers only talk of building other tools with their hammer. They seem to actually build anything, though they claim how easy it is to do it.

What had really perplexed me is that for all their talk of tool building, their toolboxes always seem hard to come by. When I look around, all of the tool and boxes for sale seem to be made with and for our simple tools. It is as if they are happier building a new saw or crowbar every time rather than just picking one up. Perhaps that is why it takes them so much longer to produce anything of value.

Or maybe, just maybe, their golden hammers are just show pieces that would dent if actually used. 
Does anyone really care? It is a blog entry, not formal writing.Why I am not a Java programmer: everybody who uses Java has a religious conviction that Java is flawless and simply cannot be improved upon, and that James Gosling is God, and that Java is the Final language. Smug Java Weenies. You guys might have the most expressive language in the world for I know, but the community has serious social problems.Any project that brings new ideas to the table is valuable in my book. Progress in technology, especially in computer science, is often a matter of progress in small increments. Once in a while really smart people put together the best of those ideas to create something that is perceived to be a major breakthrough, but often couldn't have been achieved without a whole bunch of lesser geniuses creating lesser milestones. The worth of any project is not only defined by its own immediate capabilities, but also by the influence it has on other thinkers. Here is an interesting read by Michael Abrash that illustrates that point very well: http://www.bluesnews.com/abrash/chap64.shtml. In short, at least he's trying something new - what have you done lately to improve software engineering?[removed]Inline assembly syntax is really awkward. If you need to write some assembler routines, put them in a separate file and use NASM/GAS/FASM/whatever, then link the result with your binary.I still use it on a daily basis. It sits inside a `screen` session, always open, and at this point the key combinations are so firmly wired into me that I have trouble using anything else.&gt; Haskell/Lisps are intrinsically superior [...] irrespective of how much is already written [...]

If you define superiority by the quality of the personal experience lived by the programmer while using them, I tend to agree with you.

Many people, however, would consider that superiority is rather about "what kind of improvements have been brought, by programs written in this language, into the lives of non-programming computer users?". 

Try to translate it to another field than yours if it helps: I would consider a truck superior if it ensured faster and cheaper fedex deliveries to me; whether it provided a truly enlightning experience to the driver, I couldn't care less.

Let's be clear: I'm a big fan of some of self-acclaimed superior languages, e.g. OCaml, and I often curse when I miss macros in whatever language which doesn't support them (e.g. in OCaml: sorry, but camlp4 sucks far beyond unusability). The pleasure I get by working with such tools is immense, especially when compared to chasing a `NULL` dereference or mem leaks in C code. Yet I have to admit that these languages' contribution to most people's lives is really tiny when compared to C or perl, or increasingly Java and Python. 

I don't know why it's so, although I'd really want to know. I don't feel I'm entitled to dismiss this inconvenient truth as "what sort of other network effects inhere in it": Lisp had all the time it required to create whatever network effect it might require on its own merits; ML must be something like 30 years ols as well; yet they didn't. Meanwhile, OO programming made it to the mainstream very forcefully.

I'm not sure why it is, but this doesn't look like a coincidence nor a conspiracy: there is something wrong with "superior languages" which prevents them from turning computer into useful stuff, although that's exactly what should be expected from them. Period. Dissmissing it just because we don't know why exactly is sterile denial.Someone fill me in. How is CPAN different than the Java-based entries in sourceforge?[deleted][removed]It doesn't mean that at all.  It means he can't make the call as to whether they go in or not.I use it for Lisp because SLIME is by far the best Lisp interface I'm aware of. I'd definitely consider using Emacs for a project in some other language, because I generally prefer the  Emacs interface to something like Eclipse. Looks like the Python mode has some nice features, and I've gotta imagine there's plenty of good stuff written for C.Disclaimer: I'm the author of the article and I've used Emacs for about a year now.

Not all people using Emacs are doing so out of old habits or because we don't know better. We do so because we like to use a powerful editing enviroment which will survive and continue to prosper long after the latest IDE's are gone.I agree. There is too much fragmentation in the ALGOL community. The top ALGOL programmers in the world need to get together and design a unified ALGOL dialect to replace Java, C#, Python and Ruby. Then code can be shared and resources can be pooled. No point having hundreds of slightly incompatible dialects of the same language.I agree. Sun needs to assume a leadership position in the ALGOL community and create a unified, standard ALGOL for the 21st century. Take the best parts from Java, C++, MUMPS and Intercal.Who the hell implements a parser with lex/yacc??!It's just like python.  Only one thread can run at a time.  You need to have cooperative processes to take advantage of multiple CPU's.Well, there are small subtle differences with both '%' and isearch-forward-current-word-keep-offset.

C-M-f and C-M-b don't work as you'd think when the point is right over a '(' or ')'. 

The search for thing under point command will automatically search for the complete word - even if the point is somewhere in the middle of a word.

Try them out and you'll see.I still don't see what the big difference is compared to a Scheme closure (besides mutation). Can you give me an example (which don't do mutation) which isn't straight forward translatable to Java? 

If you need mutation, it's a little bit more difficult, but  mutation in closures is seldom needed anyway. And if, it's easy to write something analog to Ocamls Ref-Type (or simply use an array).

The Java solution is btw very unique and IMO good integrated into the Java concept.
Man, no wonder Alpha was so fastSpecifically, it's *a blog entry* by someone who doesn't know the difference between "its" and "it's".  That means it's a blog entry by someone with, at best, junior high-school english literacy.  Granted he's Polish, so his polish is probably better than mine, but he's not writing in polish.

It's one data point; not the only one, but one."OCamlMPI provides Caml bindings for a large subset of MPI functions. MPI is a popular library for distributed-memory parallel programming in SPMD (single program, multiple data) style. MPI offers both point-to-point message passing and group communication operations (broadcast, scatter/gather, etc). Several implementations of MPI are available, both for networks of Unix workstations and for supercomputers with specialized communication networks."

http://caml.inria.fr/distrib/bazar-ocaml/ocamlmpi.tar.gzJava's syntax isn't designed to write programs in a functional style. So using closures requires lots of code. But if I want to program in a functional way, I simply don't use Java. So what?

It depends on the 'range' of dynamic method creation. If you want to use it like in Ruby, sure, forget it. But you can create new classes and methods on the fly by using a code generation tool (which uses a custom class loader) and call them dynamically via reflection. Depending on what you want to do the difficulty varies between relativly simple and nearly impossible. But if you want to do it on a border scale, I would recommend Python or Ruby instead. It's simply no the way to program in Java and it's not often necessary. 

All languages are bureaucratic. One simply syntactic error and they don't compile (or worse and fail at runtime). And one little mistake and the program won't do the thing you intended. And as long as we don't have strong AI, this wont change.

I've programmed long enough in C and C++ to know that strong typing is a must if you want safe programs (besides system programming. There it's still an evil, but a necessary one). 
No ofcourse not, but you don't have to be an ass about something just because you don't get its merits either.I'm talking about anonymous inner classes. Sorry, that I haven't made that clear enough.
It's not a good analogy - people argue Lisp is better at expressing algorithms, which is what a programming language should *do*.  Nobody seriously argues Lisp's horrendous-amounts-of-braces, cryptic-operators syntax is *pretty*.

The golden hammer is just useless frippery which detracts from it's function as a tool.  The "beauty" Lisp-fans talk about relates to it *doing it's job well*.Yeah, I still use Emacs almost exclusively.

Every once in a while, I use an IDE for a few weeks or months, just to stay in touch. :-) But sooner or later, I need to move to a different OS or programming language, and I move straight back to Emacs.

My theory: If you need to work in massively diverse environments, pick one highly-portable editor and master it. Otherwise, it's too easy to wind up using 20 different editors badly.I'd never heard this Larry Wall quote before:

"The very fact that it's possible to write messy programs in Perl is also what makes it possible to write programs that are cleaner in Perl than they could ever be in a language that attempts to enforce cleanliness.  The potential for greater good goes right along with the potential for greater evil."

I love this, because I've been saying it for years, myself: Perl makes it /possible/ to write very readable, well-organized programs (which is not true for shell scripts or for awk).  Sure, it has it's flaws - and I can run off a armload of them - but Perl was a huge step forward from what was available when it was first created.  I'd never choose it (now) to write anything that I expected to be a large (or even medium-sized) program, but it's still a wonderful tool to have on your belt.
Well, we do believe that P &lt; NP, but also that NP &lt; PSPACE.&gt; Why I am not a Java programmer: everybody who uses Java has a religious conviction that Java is flawless and simply cannot be improved upon, and that James Gosling is God, and that Java is the Final language.

I'm smelling quite a bit of religion in that statementI think it would be neat if we could start a book exchange here on programming.reddit.org.  Perhaps would could make requests/lists of books we are willing to sell/trade and use the private messaging system here on reddit to do the transactions?

I suppose the matter of trust might be a slight issue, but not too big of a problem...what is everyone's thoughts about this?Just join #bookz doodz.[deleted]My thought is that it's doomed to failure, that people have thought about it a long time ago and that's why it was never done.I don't use it on a daily basis; I use it occasionally in Viper mode for a decent debugging environment, because vim still doesn't offer much in that area, and I don't care for ddd.That's true.  I guess I don't really miss % in Emacs because there are so many other useful structural movement commands.  You don't actually pay attention to the parentheses.
[YASM](http://www.tortall.net/projects/yasm/) is the successor to NASM.I use emacs on daily basis in Linux and even when I'm forced to work on Windows.I'm smelling sarcasm, as usual :-)

Edit: I meant Slava's comment.The definition of "closure" within the context of language design and implementation is well defined and widely agreed upon. This view, coupled with your offhand dismissal of CPAN (and, thereby, any third party library in general) being "but...I might have to check something in!" leads me to believe you're a total nutjob.[removed]The code looks like it was written by a Common Lisp user who doesn't understand or use typical Emacs conventions. Ugly. When in Rome, and all that...&gt; if I want to program in a functional way, I simply don't use Java. So what?

Because the choice of the language isn't always *up* to the developer, and it sucks that Java's restrictiveness make it very difficult to write programs in the best way for the job.

&gt; you can create new classes and methods on the fly by using a code generation tool

IIRC, I said *reasonable* way. Generating bytecode at runtime is not a *reasonable* way, and guess what: Johnny McOutsource can barely program Java, let alone that kind of magic.

&gt; It's simply no the way to program in Java and it's not often necessary.

Hey, if you get to choose your language and environment, bully for you. But lots of people don't.

&gt; All languages are bureaucratic.

**time-warp** Yeah. And then *I* said "some languages are more bureaucratic than others". Java is very bureaucratic. Lisp isn't. There is a wide range of bureaucracy.

&gt; I've programmed long enough in C and C++ to know that strong typing is a must if you want safe programs (besides system programming. There it's still an evil, but a necessary one).

*lol* Well *I've* programmed long enough in C/C++, Java, Lisp, Smalltalk, Forth, and a slew of others to know that what you just said is a load of poo, and it frightens me to hear people talk like that. I'm not even sure I could *count* the number of fully productized embedded systems I implemented in Forth.

Stripling.

Obviously typing has its place, but you're making things up if a) you say it's necessary for safe programs, and b) Java is a good example of a useful OOPL.Because I can get CPAN modules with a single command? Because almost everything anybody has ever done in Perl that's useful (and several that aren't) is available from a single location instead of scattered all over the web? Because updating is another different single command? Ooo, golly.&gt; Could Weak Sapir-Whorf apply?

Maybe. I use closures in Java very seldom, because other ways are most often more appropriate. But the same could apply the other way around: If you use a language like Ocaml then you would maybe use closures for things where a class would work even better (just a thought). I've never had good experiences with 'programming contra the paradigm'. It's possible, but in most cases I regretted it later.

I agree that mutation isn't such a good idea for arbitrary local values. There are people who consider it good style declare all locals as final as default and only remove it if absolutely necessary. With iterators mutation can be removed even from most loops (of curse the iterator get mutated, but this mutation is encapsulated). If it's OTOH a good idea to remove mutation completely is still the big question. Even Haskell and Clean can't live without it (they encapsulate it good, but it's still mutation). I'm not really sure about the topic, while I really like the idea of getting rid of mutation, I just don't know if it's worth all the trouble, if even Haskell seemed to give it up and only restricts mutation instead of avoiding it.

Why doesn't Ocaml create Refs automatically? The answer: It was a design decision.

In Java they implemented anonymous inner classes with 'full mutation' in the beginning (it's not difficult and only requires a little bit rewriting). But because at the time, performance was important and implementing it would lead to 'invisible' allocations (like boxing), they removed it and created the 'final' restriction instead. Auto-boxing was build in later and maybe they will also build in auto-boxing for closures in the next release too.

But I'm not really sure if it's a good idea to improve closure support in Java: If it's necessary, it's there now. But if closures should become ubiquitous in Java, to make them really useful it would requite a total redesign of huge parts of the core libs - while still supporting the old ones for legacy code. So the language would become also much worse as it is now: A kitchen-sink language.

In this case I would say that it's better to leave Java as it is (maybe few add small moderate fixes and extensions) and create a completely new 'Java 2' (maybe based on Scala) from the ground up and with a clean overall design. I hate kitchen-sink-languages.
Java wasn't even the most powerful language on the JVM when it was the only language on the JVM.People who call others 'total nutjob' are not worth an answer. EOT.*hahaha* I didn't even see that weird CPAN comment.

Yep, it's usually quicker to write everything yourself, hey?!Do you mind giving your full name corentin? I want to properly give you credit for that quote, it's great :DYes, I have been using it for about three years now. It's not "pretty", but since its purpose is to edit text, and it can render text pretty much as well as any editor, that doesn't matter much the way I see it. The programmability and general feel and power outshines any other editor, especially when you also start reading Usenet and email in Emacs.[deleted]Okay. Downmodded for idiotic sarcasm.If there are better languages to do the job, than your boss may see his mistake when your competition runs circles around your company.

Are you a 'Johnny McOutsource'? If not, why even thinking about him? Creating self-modifying code is always risky and simply no playing ground for 'Johnny McOutsource'. But if you really consider using a code-gen-lib in Java as 'magic'...

And I also can't always choose the environment, so in the moment it's most often Java. In the end I have to make the best from it, that's part of the job. I also have to accept the 'specification' from the customers and can't change it in a way to make my life easier. That's part of the job too.

Lisp is also bureaucratic. Sure, you can make more kinds of errors without having the compiler shouting at you. But the program will bomb nonetheless. I prefer it to get hints from the compiler as early as possible.

Embedded systems is most often system programming. So we have weak types as a necessary evil. Weak typing is problematic because it can much to easy lead to security holes and can make maintenance a pain. With dynamic types I get at least a runtime error, but with weak types I get undefined behavior which manifests itself sometimes as totally different points in the program. 

And you talk of a simple Java code-gen as 'magic? Against building big systems with weak typing it's a piece of cake.
You'll never see yourself in the mirror with your eyes closed

edit: I hit the guy with a pillow and suddenly I'm Hitler :(You make a good point, but I personally think that Perl and Python have their own odd silliness. If you really want to see a cool language, try server-side Javascript. When one of these interpreters is written in C, it's an amazing delight to use. It frees you to work on business logic rather than syntax errors. What frustrates people most about Javascript is the linkage to the DOM and DHTML, and the browser-specific stuff, not Javascript itself. Javascript can also be used for a multi-purpose language, CGI language, or Apache module. Moreover, if you're a web developer, you can't get away from not knowing Javascript, so you already know how to get going pretty much immediately.It seems to me that there's some confusion as to whether D-Wave presented a general purpose, incredibly useful NP problem solver.  It's just a 16 bit adiabiatic quantum computer, and it's not general purpose.  It can currently solve a certain subset of problems, and currently slower than any typical digital computer.  The idea isn't that they've broken through every engineering barrier to having a useful general purpose QC.  But if their design is as effective as it seems, and assuming they didn't fake their demos then it is, then it's certainly a notable achievement.  And if they can scale it up to the KQb scale, then it will be as monumental as everyone is so disappointed that it isn't already.[deleted]I use it on a daily basis, for pretty much everything. Out of curiosity, what are you using instead? A full-featured IDE? I've tried, tried and tried to convince myself that a "real" IDE will make me more productive ... still, in 2007 it hasn't happened. It all comes down to how amazingly useful it is to truly master an editor. An editor worth mastering (not just Emacs) does take a long, long time -- but once you've done it, it's really hard to downgrade in exchange for some language-specific fluff.

Just last week, after kicking a complex editing task's ass with a multi-buffer macro (via M-x grep), with p4-mode edit hooks (a real annoyance in most IDEs) and a regexp-search for making the batch edit at the right point, I sat back in my chair and smiled. Damnit if Emacs doesn't make me feel powerful sometimes.

I wrote a post on this a while ago:

http://derekslager.com/blog/posts/2006/12/the-case-for-emacs.ashxI *never* use sarcasmFor version 0.1?  Everyone.  For version 0.2 they come to their senses.That's not what your mother told me last nightHave you really thought that way when you were part of the Java community? 

I use Java and I think that Java has many flaws. Yes it can't be improved because it has reached a local maximum (which is far, far from being a global maximum but each incremental 'improvement' would make the language even worse). I don't care about Gosling and find the concept of a 'final language' laughable. So consider your statement as falsified.

The worst language community I've seen until now is the Lisp community. *Those* guys are smug. The Java guys are total novices in smugness compared to them. But even the Lisp guys are far from your description. The hard-core Apple fans, those may match your description (with Steve Jobs as God of course). But Java? In which world are you living? 

Or is all this just some self deprecating way of trolling and you're not even the real Slava Pestov (because I've never seen a guy which is so stupid to troll under it's real name).

[deleted]&gt; Knock off the insults,

Hm, I'm going to helpfully rewrite this to

&gt; I don't like insults

so -- OMG!  YOU DON'T UNDERSTAND INSULTS?  HA HA HA.What do you mean by, "up until now?" And what does "it" refer to?I use Emacs on a daily basis too, but often in conjunction with an IDE.

For example, I'm doing some contracting work on a Java application and the rest of the team uses Eclipse. I swap back to Eclipse to do refactorings, code formatting and source code insertions (e.g. auto-generation of classes and methods like HashCode).&gt; Pot, meet kettle

No, only you tell others to 'get sunshine' in lieu of telling them to fuck off.

&gt; I'm truly in awe.

You may as well have complained that you were 'only being sarcastic'.  Puerile cliches like these have gotten old.As usual, you don't get the joke (but it was a ha ha only serious type joke) and never miss an opportunity to take a swipe at the Lisp community.

I'm not sure I have any way of proving that I am who I claim to be (or that indeed a person named "Slava Pestov" even exists); but do you really think that anybody who has ever written a successful piece of Java software must be a blind zealot for the language?I've been "using" emacs for the past ten years, but only in the past six months have I even begun to get serious about it.

"Unfortunately", I discovered that the emacs keyboard macro features are rich enough to actually be _usable_, which is a first. Now I'm stuck in emacs, because nothing else works nearly as well.

Also, the window frame commands are just too damn convenient to give up.Haven't you heard? Anybody who uses a third party library is a **total hack**.As in, so far. Who knows, the next Turing could be doing a Sudoku right now. My niece is 4, and she has a Sudoku game. And the article does seem to describe a concrete advance, unless you are saying the IEEE is wrong about Carla Gomes' work.&gt; you're right about everything and verbally assault everyone who doesn't agree with you.

Hm, I'm going to helpfully rewrite this to

&gt; I dislike verbal assault.

-- bzuh!?  You don't understand verbal assault?  HA.  OMFG.  You kill me!  I only hope that when someone swears at me for taking this perfectly logical interpretation of 'I dislike foo', that someone will come along to helpfully explain that, no-- there's some kind of implication.  But at least people like you will get stiff-backed and uncomfortable -only- when -someone drops the F bomb-.
[deleted]So, I suppose I should mention that I would really love a copy of "Introduction to Functional Programming" by Bird and Wadler.  I'm willing to trade "Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp"&gt; Or is all this just some self deprecating way of trolling and you're not even the real Slava Pestov (because I've never seen a guy which is so stupid to troll under it's real name)

He's probably just Paul Graham messing with usThe bridge analogy is a bit rubbish but mainly because he's drawing the analogy on the wrong aspect of construction. You only have to read the papers to know that construction project's are frequently very, very late, cost several more zero's than estimated, can often be despised by the client, and very often have bits tacked on during and after construction to satisfy the clients whims or in some cases where the engineers haven't taken into account things like energy usage or even the effect of certain crosswinds...Maybe you need to work a bit on your joking abilities? For example by making them... funny? And you missed that I also took a swipe at the (hard-core) Mac community.

And you're the expert blind zealot detector here. Have you tried it out when looking in the mirror? 
&gt; You make a good point, but I personally think that Perl and Python have their own odd silliness. If you really want to see a cool language, try server-side Javascript. When one of these interpreters is written in C, it's an amazing delight to use. It frees you to work on business logic rather than syntax errors.

How is a server-side JavaScript implementation written in C any better about syntax errors than Python or Perl? I couldn't say for sure, but from just a glance over the two languages, it seems to me that Python has a simpler syntax than JS.This was complete drivel[deleted]Well, half the 'applications' you're using these days aren't on your desktop anyway.

Most of the interactive websites use PHP (which I am not a fan of, but whatever).  Or Python, Perl, Java, or something else.

When is the last time you saw a database-driven website written in C or C++?  Sure, Apache is in C, and the database itself, but everything on top of that is something higher-level than C.

So while the author is right from one perspective, he's also wrong in another perspective.[deleted]The author (bkz) is a Redditor, so he'll probably read your comment, and very likely accept any help you feel like offering.  He mentioned that he's new to Emacs, so that would explain the lack of idiomatic elisp style.

Note: I'm not saying "fix the problem yourself or stop complaining", because I loathe that attitude.  I fully support your right to complain, as long as you don't demand a solution on your timetable.  The standard disclaimer for any website should be "satisfaction guaranteed or your money back" -- and what did you pay?  Nothing!  But offers of help are accepted by any sentient, sane human being.Good point.

So basically it's our task to lower the expectations of our clients even further.

+1 for insight.Editors come and go as people invent new fads and get tired of them.  Emacs remains.

Actually, the number one killer feature, besides extensibility, is full keyboard support.  You never need to use the mouse, and that alone is worthwhile.  Also, being able to turn off all the stupid waste-of-space toolbars.  vim share these properties of course.
1. It exists.[removed]shit or hit?&gt; … everybody who uses Java has a religious conviction that Java is flawless and simply cannot be improved upon, and that James Gosling is God, and that Java is the Final language.

It is final because it was declared that way.It's a little annoying that, despite the stylistic quirks and the lack of emacs experience, he felt it deserved an "(advanced)" suffix in the title as he posted a link to his own article.[deleted][deleted]Please excuse my misunderstanding. According to your proposition "the only language on the JVM", and the 19 rules of first-order logic as I understand them, then the logical implication "Java is the most powerful language on the JVM" holds. Furthermore, so does the statement, "Java is the least powerful language on the JVM".

To me, it is like a child telling his mother, "you're the best mummy in the world", when of course, there is only one mother and so of course, the 'best' criteria applies (as does the 'worst' criteria).

From where I sit, there is an absurdity, so what have I missed?I believe it's an accounting requirement. QuickBooks needs to make all sorts of entries in the transaction journal that are date related (such as initial account balances). It's not so much of a problem with QB, but with accounting requirements.

Of course, that's not to say that QuickBooks (and Quicken) don't have all sorts of bone-headed UI and work-flow decisions. This just isn't one of them.Non-advanced Emacs tips still centre around the complex issue of how to hold down the Ctrl key while pressing another letter, and also the vexed matter of what to call your .emacs file (hint: `.emacs` is a good name).

Given that some of his stuff is Lisp, even if it's not idiomatic, I think the parenthetical suffix is appropriate.&gt; So how is \_\_main\_\_ any more simple than main(), or "return lambda x"? What's a lambda? (Rhetorical, but wanted to make a point.)

\_\_main\_\_ isn't the same thing as main() in other languages. Infact you could do main() in Python, it would be a function, but \_\_main\_\_ is just a special name that Python assigns to a module when it is actually being run.

It lets Python programmers do the following trick in order to have a different action between importing a module and directly running it.

     if __name__ == "__main__":



&gt; And what's a tuple? Every other language for several decades implements an array, structured array, or hash (associative array), but in Python it's a tuple?

In Python a hash is known as a dictionary, an array is known as a list, and a tuple is basically an immutable array. Its used when you absolutely do not want the contents of a list (array) changed.

Of course Python is going to seem strange to you and you are going to have syntax errors in it; it doesn't sound like you've used it much. You really just compared apples and oranges as far as syntax is concerned. 

I'm not saying that Python syntax is perfect, but once you know it, it really isn't something you forget, and I just thought your claim that a C-based implementation of server side Javascript helped you be more productive due to less syntax errors was strange.

I mean really, unless you use a really complex language, syntax errors become less and less of a problem the more you use it.I'm happy to be wrong about thinking, myself, that parrot was doing much more poorly than chromatic describes.

However, the bitterness he refers to reminded me of another bitter episode: the one in which chromatic decided that he didn't want his Pugs contributions to be under a new license, and so wrote a puff-piece about how he refused the new license and how the Pugs people might have someone spend more than a few hours digging his code out -- if they were able.  In reply, audreyt rewrote everything chromatic'd touched in a few hours (literally three, IIRC), using svk's praise/blame/somethingelse command to identify his contributions.  At the end of the purge, she commented that only some non-copywriteable whitespace remained, attributable to him.  chromatic probably should've had this as a shorter disclaimer: "I tried to childishly sabotage the Pugs project."

moral: svk is a wonderful VCS.

But don't take this to as reason to doubt any of his descriptions of the parrot project; I only mean it as my own immunization against too much sympathy for chromatic's bitterness.

EDIT: sorry, I characterize chromatic's 'puff-piece' too harshly, here.   You can see it yourself, in a use.perl link in another comment.&gt; It's called a web forum, dipshits, and it's pretty damn effective. 

No, it is called Gmane.&gt; What frustrates people most about Javascript is the linkage to the DOM and DHTML, and the browser-specific stuff, not Javascript itself.

What frustrates me most about Javascript is that it does not have HERE docs.  Otherwise?  It's decent enough, but I think I'd be happier, somehow, with Lua.[removed]&gt; Plus, I have to create my language in a kind of assembler with PIR? No thanks.

No, no: write it in some better language, and compile down to PIR.  This is what everyone does for everything: I dasn't guess why the culture of writing directly in PIR has persisted in parrot's case.  But look: you don't even need to make this 'better language' something hosted itself by parrot.  Use -any language-.  Metacompiling forths are an obvious example, but if you throw a few rocks at wikipedia's list of programming languages, you hit more than a few interesting systems which already have more than a few internal layers of compilation and translation, with intermediate languages and multiple backends.After reading Planet Lisp and comp.lang.lisp for about a year I always thought of you as one of the sensible and pragmatic fellows in the CL community. I can't possible understand why you found it necessary to flame me in such a public way.

1. I put a disclaimer telling the reader that I'm not a Emacs guru and that many tips were collected from the gnu.emacs.help mailing list and the Emacs Wiki. I'm not distributing a complete package and assume that people can manage their own namespace "prefixes" if they decide use some of the supplied code.

2. I wrote the article with the intention of communicating some cool ideas to the Emacs crowd. What difference does it make if I submit the article myself or not?

3. The Emacs userbase is quite large and not everyone is implementing SLIME or writing crazy major-modes of their own. What is "advanced" for you probably isn't the same for the majority of Emacs users out there.So you're critical of his punctuation, but can't get your capitalization right? (polish -&gt; Polish).

Geez. I agree with grauenwolf. Who cares?

&gt; Creating self-modifying code is always risky and simply no playing ground for 'Johnny McOutsource'.

Code generation != self-modifying code.

If you want to see real self-modifying code, go dig up the Wolf3D sources... texture mapping done with self-modifying 286 assembly.Fair enough. I'd like to see it support that because it's simple and makes sense.&gt; [...] you have to write your own pattern matcher.

Sure, a CL pattern-matcher can evolve more easily, starting with sequence destructuring (which conveniently already exists) and then move on to Erlang-style binary matching, and then with matches over structs and fields of objects in the manner of Erlang's records.  Even list destructuring can be wonderfully helpful and readable if you get into the corresponding habit of throwing around (ok . Object) (error . Reason) (ok (Code Headers Body)) conses, although this habit itself seems opposed to the objects and exceptions that CLers would rather use.

However, the reason I had most in mind as I wrote my 'It -is- better', is that static languages can reason about the constructors you match on:

        % as from reading from a port, &amp;c
    :- type io.result(T) ---&gt; ok(T) ; eof ; error(io.error).

    :- pred prompt(string::in, string::out, io::di, io::uo) is det.
    prompt(S, A, !IO) :-
        io.write_string(S ++ ": ", !IO),
        io.read_string(Res, !IO),
        (
            Res = ok(A)
        ;
            Res = error(E),
            error("error: " ++ io.error_message(E))
        ).

Here, and in O'Caml and Haskell, the compiler can complain and tell you that you didn't handle the case that Res would be `eof`.  That `io.result` type might be in the `io` module, you might never refer to it by name in your code, and these systems can still know that Res is of that type.  O'Caml is so friendly, it will print your own code, underline the offending pattern-match (IIRC), and then give you an example of what your match fails to test.  And after you do test everything correctly, it is indeed faster.
In the Lisp world, flaming you is just the standard way of saying hello.  If he rips your head off and vomits down your neck, then he *really* likes you.

EDIT: And remember: if you don't like it when a complete stranger responds in a rude and abusive way to you, it's because [there's something wrong with how *you* relate to people](http://en.wikipedia.org/wiki/Psychological_projection).  Obviously!Nope, did that deliberately.  I use "Polish" and "English" for the nationalities, and "polish" and "english" for the languages.  It's allowed by english (and English) convention.

And if you don't care, that's fine.  Poor spelling is the equivalent of a dirty t-shirt and ripped jeans: it's your right to dress how you like, but it's my right to glean messages from how you dress.  Nobody's rights are being infringed if I see you differently than you see yourself.They could make it less sucky by explaining what the 'start date' is! Or maybe I'm missing something....Ah. Well, the article is implying that the people solving Sudoku puzzles are actually somehow working on P=NP. While it's certainly true that the person who answers it definitively could be working on Sudoku puzzles right now, that doesn't seem to be what the article is suggesting.

Based solely on the description from the article (which could very well be lacking), I'd have to disagree that restarts are a new thing. In particular I'm thinking of [simulated annealing](http://en.wikipedia.org/wiki/Simulated_annealing#Restarts), as well as [evolutionary computation](http://www.google.com/search?hl=en&amp;q=genetic+algorithm+restart&amp;btnG=Search) ([here's](http://citeseer.ist.psu.edu/fukunaga98restart.html) a particular paper that proposes using restarts in a very similar way). I'd have to dig deeper to give you truly firm evidence that these techniques predated Carla Gomes' work, but based on her [publication list](http://www.cs.cornell.edu/gomes/new-papers.htm), I'm confident in concluding that restarts in these contexts came first.

[removed]This has to be the most bizarre use of regular expressions that I have ever seen.[deleted]I hadn't seen that one, thanks. So gnome does indeed have some sort of framework for hidden options.That's the best you've got? I mean, you could have kept it on topic and argued that the article had already expressed the author's disdain for arbitrary complexity. I don't agree, but at least it's a debatable position. Instead, you just fell all over yourself and couldn't even pull off a passable cheap shot. Worse yet, you apparently had the conviction to use the term "fuckface" earlier, but felt the need to soften it to "F bomb" in some weak attempt to acquiesce.

Seriously, I was giving you an opportunity to not come off as a complete ass, but you don't even have a cogent point to make. Come back when you have something to add to the discussion, or can at least stick to your convictions. At this point you're simply not worth any of more of my time.&gt; but felt the need to soften it to "F bomb" in some weak attempt to acquiesce.

No, I used 'F bomb' out of contempt for the sort of hand-wringing can't-we-all-get-along people who complain about it in those words.

&gt; but you don't even have a cogent point to make.

No, I do: I point out that the author never makes anything approaching the alleged claim, and that seeing that claim requires an absurdly disinformational level of hostile reading.  I pointed that out some time ago, and since have mainly responded to -your- attempts at 'cogent points'.

&gt; passable cheap shot

Let's see, I have:

1. SPOKEN IN ALL CAPS.

2. Named someone 'fuckface', and referred to that person again as such a second time.

3. Mocked two people for objecting only to my 'insults', by way of pretending to take the same otherworldly misinterpretation that I initially objected to.

What shots do you imagine I cared to make?  Would you like to take a tally of what I've offered and what -you- have directed against me?Once again ... I believe that Macintosh computers are immune.

:-D
Tcl:  The entire language syntax is specified in 11 bullet points (!!). Once you get used to it the things you write end up looking surprisingly better than you expect. Throw in Tk for good measure which looks reasonably native on every platform of note and you have a great little language. Calling C function to extend the language it is trivial. Oh, and tclwrap rules for making stand-alone windows programs.

But for some reason I just never write anything in it any more.http://tuatara.cs.uni-duesseldorf.de/benchmark.htmlHoboy, another one who can't bother to distinguish "it's" and "its".  I fear for these people's code.'whose'.  For example, a pharamicist 'whose' understanding of foo is in bar.(Backslash-escape those underscores.)What a wonderfully selective story!

Audrey wanted to relicense code to which she was not the sole copyright holder.  It's hardly fair to contributors to change the rules after they've contributed.

The choice of licenses was at odds with what TPF can accept for an official blessed Perl 6, and all of the Perl 6 code I've contributed to Pugs I would like to see used in such a project.  Worse yet, the particular license had particularly difficult legal implications regarding warranty for use, as I understand from previous discussions with TPF's legal counsel.

Now Audrey lives in Taiwan, which has a completely different interpretation of that particular license, and she meant primarily the Haskell source code.  She also offered the chance for contributors to object.  However much I may disagree with her initial announcement, I believe she had honorable intentions.

When she clarified her intentions and I clarified my particular objections to the license, we found a workable compromise and dropped the issue.  I haven't even *thought* about it in over a year.

Impugn my motives all you like, but I believe you'll get the same story from Audrey herself.Come on. You have to substantiate these claims as to how the typechecker catches bugs. I am not disagreeing with it, but unless you provide food for thought, this is yet another "random Joe feels strongly about xyz" post.I really can't help you.Bugs?  No: bugs are meaningless.  [Errors, however, must be corrected.](http://programming.reddit.com/info/j74u/comments)  I'd find "My programs don't have errors." of annoyance, anyway, as it suggests a mentality in which programs either don't exist or else they exist full and complete and error-free.  Real projects take real amounts of time and have a real lot of intermediate stages which necessarily have errors of omission and whole hours of not being able to build due to the here-unacknowledged errors that the compiler won't let through.&gt; You have to substantiate these claims as to how the typechecker catches bugs.

Why?  Why must this person work to prove something that anyone knows implicitly, who has ever learned or begun to learn one of these languages?  The compiler contributes pretty tangibly in these languages, telling you that you didn't check for zero-length lists, or that you passed a Frob to a Frabophage, or that you duplicate an object you've instantiated as unique (anywhere else in your program).I assumed they were talking about something different than the mutation in genetic algorithms. It would be quite silly if the author couldn't get something that simple right. Or maybe Ms. Gomes just came up with a novel heuristic.Some thougts on meetings and software.&gt; What a wonderfully selective story!  [...] Impugn my motives all you like,

The following quote from your [use.perl on the issue](http://use.perl.org/~chromatic/journal/29419) left a bad taste in my mouth, and this distaste is what I communicate here.  The story is of course selected by what I observed.

&gt; It should be a small project for an interested Pugs hacker to find all of my contributions and remove or reimplement them -- at least a smaller project than convincing me to change the license. If that is what the members of the Pugs project want to do, I support their efforts.[deleted]Indeed, and it turns out that I was wrong with that prediction.  Audrey *did* convince me to change the license.Mutation is not the same as restart. Restart is starting the simulation over again. Mutation is the process of making small changes to the genome.

That the article is a little silly was my original point.[deleted][deleted]I remember running it once - IIRC, it booted from a floppy. It was dead slow - surely because it used native VGA, but still.

Edit: tried again, in VMware. Still slow, even with "VESA acceleration".

This is not to diss them, fantastic effort.It is just an old story I half remember. What analogy's you apply are your own business.*So, is C the new Assembly…*

so what ?lazyweb: bookmooch.comHe did give an example.How will that fix anything? It isn't like running on a 64 bit machine will magically change all binary file formats to use larger date fields.[deleted]I think the biggest problem is that it will be very hard to detect. It won't fail everywhere all at once like the Y2K bug, so the errors will have time more time to corrupt data long before the drop dead date.Do you want to dig through a couple decades worth of old C code looking for places to make that change?&gt; Nobody seriously argues Lisp's horrendous-amounts-of-braces, cryptic-operators syntax is *pretty*.

Hell, of course they do!  Nobody argues that it's everyone's *taste*, but i think most Scheme hackers will tell you that they find the syntax truly and deeply beautiful.[deleted]This is a tongue-in-cheek post. One definitely doesn't need to know the categorical definition to do useful stuff with monads. However, this is certainly a lot more authoritative than all the recent "What is a monad?" posts :)Except for point 3 (I think there are at least less than a dozen colleges using it), Mercury is in a very similar state.

This language has an strikingly odd syntax, but it gets interesting pretty quickly, and is otherwise decently engineered and useable.  Some of the docs are amazingly enlightening, too -- especially those that teach constraint programming.  Please give this language a good chance.

(Oh, and also except for the C++ part of point 4.  What an odd suggestion!)rm pmfIf anyone wants an authoritative and understandable answer on the computational importance of monads, they need look no further than Wadler's or Moggi's papers. Even Moggi's original papers that introduced a computational monad as an aid in modularizing a denotational semantics are very readable, at least as long as you skip the heavy domain theory and category theory.&gt; make the exact same mistake as people 40 years did

The mistake isn't in the 'duh', but in actually failing to replace the applications.&gt; However, this is certainly a lot more authoritative

You *do* realize that you linked to wikipedia, don't you?&gt; everybody who uses Java has a religious conviction that Java is flawless and simply cannot be improved upon

really?I do not. But, honestly, you should always use this, else you can only safely use up to half the capacity of whatever data type you're using...Why is it that everyone is rediscovering shit that's been known for 20 years?

C is the new assembly, Smalltalk is a better language, functional languages are the shizznit...

*This shit was 20 fucking years ago!*I'd like to see a parallel raytracer written in SQL.

I know you're trolling, but monads are not accidental complexity. They're a data type, just like your little enterprise apps have 'Employee' and 'Contract' data types.I just thought it would be interesting to have this posted. Sometimes the programming community hops on real strange bandwagons - Perl4, Ruby, PHP - while project that are much more solid are not picked up, because people are afraid to think outside the box and just assume, because there are academics behind it, that it has never been put to Real World (TM) use.
Let 10000 languages flourish!Oh, yeah, and re-mod me up! (I knew I was going to get kicked for posting Mozart/Oz stuff). ;-)
BTW, I think the real cool thing about Oz is concurrency and soft-real time, all the while having OOP and HOP, and good tools. I really hope it gets more attention and is able to generate a positive feedback loop.Start with an existing color theme that's close and then just modify it to fit.

http://www.cs.cmu.edu/~maverick/GNUEmacsColorThemeTest/index-c.htmlPerhaps he meant to infer that writing in byte code directly was better?No, Ruby is not "more like Lisp" than OCaml. Only on the surface. Lisp and OCaml support functional programming. Ruby, doesn't. Ruby is an imperative latent-typed object-oriented language that is terribly slow and I don't even know if they solved the namespace problem they were having. It seems, though, that Ruby has its place in the world, so what can I say? Where you confuse things is where Common Lisp intersects with that feature-set.I think it's also important not to loose focus on the speed issue. Some of Jon Harrop's post are pretty interesting, IMHO.
And, judging by the current research on multi-stage programming (MetaOCaml - a recent post of mine here - modded down into oblivion, as always), and the stuff regarding concurrency in Termite, Erlang, Alice, and Oz...To say nothing of garbage-collection and libraries (such as graphics)...I'd say Lisp is sort of getting old...Common Lisp is frozen in time (Scheme is not) - and, actually, this might be a feature, not a bug, depending where you're coming from.
BTW, watch as OCaml (F#) is slowly creeping in .NET. This will be really fun to watch - quants on Wall Street coding in F#.&gt; I'd like to see a parallel raytracer written in SQL.

So would I. Well not SQL specifically, but some sort of set based langauge. It would be interesting even if it turns out to be untenable.

P.S. All data types are accidental complexities.SQL Server 2005 supports it, though I have no idea why.

Really though, how much explicit parallelism does your typical web server need? Most of the ones I have seen that actually performed well used completely isolated threads so they were essentially seperate processes with none of the normal difficulties of parallel programming.
Sudoku is defined by being 9x9, not NxN. Anything larger is not Sudoku.Discussed here:
http://japple.blogspot.com/2007/02/conors-rule.htmlFind out from Mr. Monad himself.Whiny, content-free, short blog alert...good find... though that picture there on the bookmooch page has the look of something done by someone in a psych ward...Yeah, but my guess is that the perspective Harrop is coming from is that pattern matching and combinators go a long way in providing a near quasiquotation mechanism akin to macros.
I guess he didn't *say* that. Anyways, my 2 cents. I don't really grok OCaml that much.&gt; In category theory, a monad or triple is a type of functor, together with two associated natural transformations. Monads are important in the theory of pairs of adjoint functors. They can be viewed as monoid objects in a category of endofunctors (hence the name) and they generalize closure operators on posets to arbitrary categories.

I believe I've said before: any minute now, Geordi's going to pop up and tell me to reconfigure the phase emitters to produce an inverse tachyon pulse; it's the only way to escape this temporal flux!&gt; Didn't i

1. [explain the definitions](http://programming.reddit.com/info/14drw/comments/c14kj6?context=4) -- No, not really.  You just disputed what I said, and claimed I had it "backwards".  Not much explaining there, other than "explaining" at length the fact that you really really though I was wrong.

2. [give examples](http://programming.reddit.com/info/14drw/comments/c14n7y?context=6) -- Sure, but your "examples" weren't really illustrative of your position.  They just reinforced the fact that you actually had a position, and made it clear what your position was, at least as far as you explained them.

3. [point you at the appropriate references](http://programming.reddit.com/info/14oi9/comments/c14p2u) -- Whee!  You can link to Wikipedia!  Now compose an argument with specific references to those sources that actually disputes what I've said (rather than just claiming that they dispute what I said).&gt; Lisp [supports functional programaming]. Ruby, doesn't.

CL only appears to support functional programming more than Ruby (or Perl) in that it focuses on lists and therefore discourages the sort of lamely destructive operations that are cheaper with Ruby's arrays.  Otherwise, nope--  No amazing FP support in CL.  It certainly doesn't huddle closer to O'Caml in these matters.&gt; The class of Sudoku puzzles consists of a partially completed row-column grid of cells partitioned into N regions or zones each of size N cells, to be filled in using a prescribed set of N distinct symbols (typically the numbers {1, ..., N}), so that each row, column and region contains exactly one of each element of the set.

Sure, the original Sudoku puzzle is only 9x9, but that does not mean a bigger is not Sudoku. The generalization follows all the Sudoku rules and everyone except you calls it Sudoku.Setup and install went as smooth as I have ever seen a Linux install go. I have Yellow Dog Linux 4 on my PowerBook, and even that was a chore in comparison to v. 5 on the PS3.Nice article, Babar. Don't mind the haters.

One quick tip. There's already a built-in function that moves the current line up, namely transpose-lines (bound to C-x C-t by default). And you can do the same as your move-line-down by moving the point to the next line and doing a transpose-lines (since moving line n down is the same as moving line n+1 up). Finally, your move-line function can be implemented by repeated transpose-lines applications, although I don't see how this generalized version is very useful.How about using `(..+)\1+` to find unary prime numbers? :)&gt; And it looks like Alan Kay is leaning toward a graphical programming language using tiles.

Where can I find more information about that?(Nitpick:  they're not just a data type, but also a specific operation on said data type, of course.)&gt;The most famous is Fred Brooks's 1986 essay "No Silver Bullet - essence
&gt;and accidents of software engineering", which claimed that there's simply
&gt;no way 10x productivity increase could possibly ever happen.

Brooks was looking at start-to-finish time, not the time it takes to
bang out some code.  His argument was that writing and debugging the
*spec* already took most of the time, so nothing that helped you bang
out code was going to knock off 90%.
The presentation of the attack is great, really insightful.

The programming tips are a bit weak. Nowadays, every SQL binding comes with a mechanism that does the escaping for you (which might be error prone to get right manually). So there's no reason to reject certain email addresses as suggested.Sorry, but cartoonists outrank bloggers in my twisted online hierarchy. The language of the gods should have been Lisp, but it's mostly Perl. C isn't even mentioned.Seems like that would find m^n for m&gt;=2, n&gt;1, not primes.

EDIT: I'm a retard. Obviously this matches m*n; so if it doesn't match, you have a prime.Anybody with the equivalent of an undergrad math background should be able to understand the categorical definition of a monad, as well as related concepts such as adjoint functors. It's not that hard.It goes without saying that this is not because programmers are better educated or more able in any way. I think the reverse would be true, and if Dijkstra was worried about CS education then he must have given it up as a bad job by the time he died. How much longer would it take the blogger to implement something for which he had to scratch his head rather than look up some library methods? This measures the amount of effort designers of languages and tools have focused, especially  on this sort of task.The title, though spectacular, doesn't reflect the message of his posting.

What he describes is, as I see, the "real" meaning of Alexander's Design Patterns for Programming.  See also [this discussion](http://programming.reddit.com/info/151c7/comments) here on reddit.It would be cool if there was an interactive compiler for Haskell. ghc generates fast binaries and ghci facilitates interactive development, but you can't have both.Yes, you regularly confuse anonymous inner classes with closures.&gt; Erlang looks a bit ugly

i would say "bizarre", due to the prolog-inspired syntax, but once you start dwelving in the very small core of the language(Erlang has very few semantic and is conceptually fairly clean) and extensive use of message-passing processes and pattern-matching makes for an awesome languages.

Haskell's another language you may want to look at for parallel computing (one of the big improvements of GHC6.6)You can mix compiled and interpreted code in the same GHCi session though.

The use case is that you have an established, stable part of your program, which you compile, while the bits your're hacking on get interpreted and reloaded on the fly. The GHC runtime lets you freely intermix compiled and interpreted code in a single GHCi session.

Is that the kind of mixture you were thinking of?No, I'm thinking about hacking on, and replacing at runtime, compiled parts of your program.Ah, right, more like [hs-plugins](http://www.cse.unsw.edu.au/~dons/hs-plugins/) then. But less heavyweight. I've used it in some coarse-grained hot swapping scenarios, such as lambdabot. 

However, the best way forward in this area is to use the ghc-api package, to link the GHCi interpreter into your Haskell app. Needs some polishing though.I could have sworn this was posted here before, a little closer to when I first wrote it...It's Wikipedia, you can recurse until you get to definitions which you understand. :) That said, this is not really all that useful for understanding what monads have to do with programming.Thanks. 

I've rewritten the move-line-* commands to use transpose-lines - but the subtle difference is still intact - we want the point to remain at the same place while we move lines up and down.It's a beautiful paper.[removed][removed][removed]Pretty good into to the concept of SQL injection for newbies, I think. I happen to prefer more specific articles rather than generic articles that try to cover all platforms but, overall, I think it is a good intro.Don't write Emacs Lisp in Common Lisp style.

If that's a flame to you, adjust your skin-thickness.I use it every day, even for email.  The C and python support are brilliant for my work.[removed]Yet "millions of people around the world" are not tackling the problem you quoted, but only one case with N=9. It's like saying "millions of people are tackling Goldbach conjecture when they're saying that 2+2=4".&gt;I make the bold claim that most people who are faced with the task of programming a computer are not computer scientists or programmers by training

I make the not-so-bold claim *that you don't know what the fuck you are talking about*&gt;C was always the new assembly.

Yes and that merely needed to be a comment, not an entirely new blog post that moron redditors could upmod.I never said they did. It's a stupid thing to say.Ah, yes, of course, those of us who believe the standards are a good idea can't code. It's not the fault of people who don't check their pages, clearly.

However, that's not even what the comic says. What the comic is objecting to is the people who want to apply insane standards to websites. Like, XHTML Strict.

Okay, sure, if you're actually serving pages from a server that can spit out text/html or application/xml+html as appropriate, and don't need any of the features that XHTML Strict helpfully deprecated with no replacement (like centering for anything that's not text), maybe. That seems... unlikely, however. Having a valid DOCTYPE, and ensuring your pages validate to that DOCTYPE (or as close as you can get and still have it render in IE), is important. Validating to a random DOCTYPE someone likes that day is not.

Semantic markup. Great stuff, but fundamentally if you're laying out your page as a table, just use a bloody table. Equally though, if what you want is two columns, which don't necessarily have to be next to each other, use CSS.

Using px instead of em/ex is fine, if you need to match image widths. I use the term fine, relatively, here. If you don't need exact pixel widths though, it's sloppy.

I'd care more about labels if anything used them.
[deleted]Hmm, I believe http://damnsmalllinux.org/ is smaller ? :)QB - 

Up and running in: 10 minutes
 
Fixing your configuration mistakes: 6 months[removed]Nice save.The title is bad. The first step should be to realize that no two easy steps will lead you to write good programs.If your threads have so much shared state that locking contention is significant, the first thing you should think about is whether you've correctly partitioned your problem between the different threads (including thinking about whether you should have partitioned it between threads at all).

Threading can be a good idea when there is little access to shared state, but having multiple threads buzzing busily over the same data set (locking and unlocking as they go) isn't a good design, imho.Xhtml strict is an "insane" standard? Funny, I've never seen a web standards evangelist promote anything *but* that particular flavor of xhtml.very basic and eventually misleading ...You know about save-excursion, right? Instead of manually undoing your movements of the point, you can just wrap the code in save-excursion.Yeah, fuck blind people, they don't deserve to have access to information on the Internet!Gaussian elimination for REs is not a very used technique. It's presented in volume I of Aho &amp; Ullman's *The Theory of Parsing, Translation and Compiling*, as fundamental as it is unreadable.

It is best used in starting with a DFA, writing down the regular grammar and eliminating the productions to arrive at a single RE.[removed]The author complains about lack of unit testing tools for apple development - I have used at least two tools for unit testing objective c and they both had xcode integration. I believe one of them even got the Apple seal of approval and got integrated into xcode by default.

InterfaceBuilder must be close to the best tool for designing GUI currently available.Yes, but save-excursion wouldn't be of any of great use in this case. The point still has to be moved vertically along with line - we only need to preserve the column offset.Because of ubuntu.Hints?&gt; Can you tell me what was the most difficult bug you faced while programming and what you did to resolve it?

It's that people can't tell me what they need, just things they want. I have some workarounds, but it isn't resolved yet.I'd be interested to hear your evidence, if it's not a bold claim. I work in academia and associate with programming language theorists, so maybe it's true that I don't know what I'm talking about, and it would be very useful to me and my colleagues to be enlightened.

However, programming is just a tool to let you get your real work done, not an end in itself. Whenever I tell (or perhaps troll) my physicist, engineer or sysadmin friends that they ought to learn Haskell instead of sticking with Fortran or Perl, they complain that they don't want to know because they're just doing number mangling or text processing and they don't want to get their heads round algebraic data types, monads, etc. This seems fair enough to me - they just want to concentrate on the task at hand. Some of them *are* interested in learning more, and that can only be a good thing, but it shouldn't be necessary.

P.S. Yeah, I am naughty, maybe I should have left 'easy' out of the title ;).

P.P.S. Okay, I know, I shouldn't rise to the bait...
Because questioning basic assumptions is important.Adding/removing/changing a method of an existing class qualifies as 'modify code'. And if this is done by the same program which is modified this way at runtime, then I would call it 'self-modifying'. 

It's simply the high-level version of the assembler trick you speak of (which I used on my Atari 400 lots of years ago, too) But at those times the programs were small, and dirty hacks where common.
Anonymous inner classes are more powerful that ordinary  closures. So while a AIC is a closure, a closure isn't a AIC.
May I refine a bit your question?

How many of the people using Emacs (or Vim, for that matter) for less than five years use it in all of its power? Most of the people I've seen use only a minimal part of the great power provided by these tools. For example, I see lots of people using the mouse and the X clipboard to copy text.

So, have you spent time in learning the power your editor has available or do you use it just like a plain text editor with syntax highlighting?Really?  I stand corrected.

It's just that in my experience, even when talking about syntax Lispers will tend to praise its succinctness and efficiency in expressing ideas, not try to argue that half-a-dozen letters surrounded by thirteen pairs of matched brackets and braces are *aesthetically pleasing*.

YMMV, though.No, I meant to imply ;)

Obviously dibblego *completely* missed my sarcasm, which in itself is a little scary.

When Java came out most of us that were still of the opinion that Smalltalk would win because Java was so overwhelmingly inferior realized that the only way Java could possible become useful was to put something different on the JVM.

Java prompty punched us in the face by way of massive marketing hype and became one of the defacto programming languages despite its obvious shortcomings.

Only (relatively) recently are non-Java JVM-based languages getting the attention they deserve. Sun has, however, has missed a wonderful opportunity by focusing on JRuby when other languages would have served them better. Let's bear in mind that Groovy (which is better than Ruby) was the first "official" non-Java JVM language (JSR-241) but is still largely neglected.

If Sun wants the JVM to remain relevent (long-term) they need to do more than just hire JRuby developers; they need to get (in particular) Groovy and Jython, and perhaps Scala and a Smalltalk derivative on board, help create tools for them, and shout about it.
&gt; A higher-order-function *can* do code-transformations (this is how monads do it in the end). But it hasn't.

No, they really do.

Taking `_` as a placeholder for the code being transformed, consider the transformations that the following monads and non-monads implement:

    Maybe monad ⇒ case x of Just z -&gt; _ z; Nothing -&gt; Nothing
    
    List monad ⇒ _ x1 ++ _ x2 ++ _ x3 ...
    
    State monad ⇒ \s -&gt; let (s', v) = x s in _ v s'
    
    map ⇒ [_ x1, _ x2, _ x3, ...]
    
    foldl ⇒ (... (_ (_ (_ z x1) x2) x3) ...)
    
    foldr ⇒ (_ x1 (_ x2 (_ x3 (...))))
    
    iterate ⇒ [x, (_ x), (_ (_ x)), (_ (_ (_ x))), ...]

*Exactly the same thing* is happening in all the above cases;  the only difference is the actual expansion.

What monads are all about is the properties of the *result* of this transformation (see the end of this post);  focusing on the transformation itself completely misses their point.

&gt; It's like saying: A while-loop is a structure build from a conditional jump and an unconditional jump. While this it a totally valid description of a while-loop, it fails to catch the real 'essence'

Right, and this is my objection:  the article's analogy is like saying "a while loop is a control construct" without mentioning anything about what makes while loops different from any other control constructs.

&gt; Your objection is as if a biologist protests calling a certain data-structure a tree because a tree is a 'woody plant'.

A tree-to-tree analogy would be either:

* higher-order functions ⇔ macros (this is really what the article is about)

or:

* monads ⇔ some similarly special kind of macro (the closest i can think of are Oleg Kiselyov's [composable macros](http://okmij.org/ftp/Scheme/macros.html#Macro-CPS-programming))

&gt; So how would *you* describe the 'essence' of the concept 'monad'?

In a phrase:  a *program evaluation strategy* that's *composable*.

That is to say:

* *program*...:  All monads operate on combinations of related steps (i.e. "programs" or "computations"), instead of just single/isolated ones.
* ...*evaluation strategy*...:  Every monad embodies a particular strategy for evaluating said programs/computations;  for example:  doing nothing extra (Identity monad), failing early (Maybe monad), trying all possibilities (List monad), providing a shared environment (Reader monad), maintaining mutable state (State monad), and so on.  The strategy defines the particular monad.
* ...that's *composable*:  Monadic operations can be safely and arbitrarily composed together into bigger monadic operations.  This guarantee is the foundation of much of monads' practical use.

These things are what monads are about;  without them, you don't have monads, but "just" more general code transformation (higher-order functions, macros).Fair play.  I was really debating this, anyway:

&gt; I find it hard to trust an argument claiming a language is superior unless it considers what was made with it.

To me, showing what was made with a tool only shows you how good the craftsman is - Michaelangelo managed to paint some pretty impressive, lifelike paintings armed only with 14th century technology, and even with all the power of Photoshop it's still possible these days for amateurs to bang out complete rubbish.

In particular with IT, where network effects abound, the most popular tool (and hence the one every new tool must generally interoperate with to be successful) is normally the best-marketed, not the best *tool*.

If popular == good, VB would be one of the very best languages ever designed by man.  I think we all know that's wrong, right?&gt; Because questioning basic assumptions is important.

Sorry, you have to substantiate these claims as to how such questioning has importance.Betamax was inherently superior to VHS, but VHS became the standard.  Sometimes good marketing + network effects &gt; good design.  Sad but true.

&gt;  I would consider a truck superior if it ensured faster and cheaper fedex deliveries to me; whether it provided a truly enlightning experience to the driver, I couldn't care less.

How about a truck which was harder initially to learn to drive, but which broke down a lot less often *and* allowed much faster deliveries to you?

The truck's hard to drive, so driving it makes you a better driver.  Unfortunately, it also makes it harder to recruit experienced drivers or train new ones.  From a business standpoint it makes more sense to buy crappy, easy-to-drive trucks and recruit from the shallower end of the pool, making up the drawbacks with over-capacity instead of inherently superior systems/processes/equipment.

It doesn't make crappy trucks "better trucks" than the "good" trucks - it merely means it makes more sense for a business to buy them.

Again, any theory which posits "popularity" is proportional to "technical correctness" has to explain *very carefully* why, say, VB6 is exempt from this rule.Part of the argument was given in his posts in comp.lang.scheme.
&gt; It's allowed by english (and English) convention.

Cite? I'm no linguistics expert, but that's the first I've heard of that. Wikipedia may not be a great source, but it's the only one I found in a quick search, and it disagrees with you:

http://meta.wikimedia.org/wiki/Capitalization#Capitalization_of_language_names

Also, I never suggested that you don't have the right to complain about spelling. I was just exercising my right to say "who cares?"...  :-)

* Edit: Spelling, for fear of etfb's wrath.
So they made a small script to some specs from 72 (probably not the whole of them, too) and made that into a "law"?

Science education is clearly going down. And I'm talking about the very basics here...So can a bunch of other existing apps/frameworks, JSRs.

It's too bad Sun is focusing on JRuby when groovy is already a JSR and a more capable language. Can we say 'bandwagon'?Please stop it. Raganwald explained the semantic differences between closures and AICs.

I program in Java and in languages that have closures (Lisp, Ruby). In Lisp/Ruby, closures are used extensively, my guesstimate is more than one closure instantiation per 10 LOC. In practice, the following points make AICs unusable in places where closures are a good fit:

1. Checked exceptions

2. Overly verbose syntax

3. Declare interfaces up-front

4. Wretched semantics (e.g.: final vars)

I've tried to simplify my Java code with AICs in some cases where closures are a natural fit, but the above four points combined made it often impractical. In these cases the alternatives were either code duplication or architecture-overkill. I've _often_ seen these two diseases in other peoples Java code where _real_ closures would have helped.

It begs the question: Are you proficient in at least one language that has closures, e.g. one of Ruby, Lisp, Scheme, Haskell, Smalltalk?Too many useful parts of HTML 3 were deprecated without replacement by HTML 4, and in strict (as opposed to transitional) they can't be used. Not to mention the whole mess of MIME types and IE not actually supporting XHTML (yes, I know IE 7 will ignore the XML intro tag as a workaround).If you need speed, which is one of the only remaining reasons to be on the desktop nowadays, Ruby won't be of much help.&gt; any of the features that XHTML Strict helpfully deprecated with no replacement (like centering for anything that's not text)

Set left and right margins to auto.

&gt; I'd care more about labels if anything used them.

Almost every mainstream web browser supports &amp;lt;label&gt; (Safari was the holdout, can't remember off the top of my head if they fixed it or not).
Like what?  I must have written thousands of Strict pages over the years, and I *certainly* don't long for the days of HTML 3.2.
Gmane appears to combine the worst aspects of web forums and email lists. :(  Or maybe I don't know how to use it correctly.  

Lists hosted with Mailman seem far more user friendly, without the focus-confusing layout of Gmane (or Google Groups, which also has the effect of breaking the space-for-page-down UI I love).They probably *will* be using Emacs (or Vim).I'm a big cocoa fan.  No really.  I did coding for cocoa since NeXTSTEP 0.8.  But RoughlyDrafted is a propaganda sheet which makes up all sorts of crap in the name of Mac fanboyism.  The number of items they get wrong in this article (and almost all their others) is amazing.  Why do they keep getting upmodded?  Because of their obnoxious pretty-pictures?

Reddit badly needs a "I don't want to see a link to this domain ever again" filter.Quite a nice app, but why was this submitted to programming instead of science...?This is something that could really make websites more robust (no more Slashdotting!).  It's sad that this was reported in the 90s, people were actually hoping to get this into *Mozilla 1.0*, and it still isn't in after six years.

Maybe it would be a good idea to provide ad-free versions of websites that only appear on the IPs that are advertised via SRV records as an incentive ;).

A similarly long-lived bug is [bug \#9101](https://bugzilla.mozilla.org/show_bug.cgi?id=shy) \(support for &amp;amp;shy;\), which is part of HTML 3.2 and onwards.
...when you can form a coherent argument against them.

Speed? We're talking about normal GUI applications, not video games. Normal GUI applications have a familiar interface with lots of control from the programmer and can be programmed in a pretty straightforward way. Swing might not be the best example here, but it's miles away from anything the AJAX world has to offer.

And with rich clients or similar architectures (heck, even goold old X windows), the desktop-client question is a moot point anyways.He could publish a new article every day for a decade and it would all be justified.Ah, I think it's more like Pan's Labyrinth. :)


It's not obvious, but there's some integration between BookMooch and LibraryThing.

LT group: http://www.librarything.com/groups/bookmooching
LT feature: http://www.librarything.com/blog/2006/09/arrr-swap-books.phpWhy can't kids learn both approaches? Why do we have to give up long division to teach abstraction? The problem with "new maths" is the zero sum game approach where each new topic eliminates one of the traditional topics. Rote memorization of multiplication tables does not harm children.&gt; Set left and right margins to auto.

And the (block-level) containing element to "text-align: center;", IIRC.

I think the problem he's complaining about is that either:

1. He doesn't know how to do this yet, or

2. You have to wrap one block-level element (the image) in another block-level element (with "text-align" set) just to get it to centre - this seems like unnecessary markup to get an effect that you should be able to get by setting a property on the img element itself.

FWIW I fully support web standards, but I can understand why some people (especially, in my experience, those who aren't fully at home with CSS) perceive there to be bits missing in the XHTML/CSS2.x spec.

CSS doesn't stop you doing anything HTMLx.x did, but sometimes it requires a different technique or a slightly more roundabout (but generally more semantic) route to get what you want.

Of course, there are also always the "change == bad" and "HTML 1.0 was good enough in my day, so it's good enough today" groups, but that doesn't mean XHTML and CSS are *perfect* either.ops! my fault :DYou only need a containing element and text-align if you need to cater to Internet Explorer 5.5 or below.  In modern web browsers and Internet Explorer 6+, to centre a block element, you only have to set the horizontal margins to auto.

In the case of images, they are inline by default, so remember to change them to block if you want to centre them without changing the parent element.
Where was the sarcasm? Oh, the "Ooo, golly" part? That's not sarcasm. That's making fun of you for asking here instead of just seeing what CPAN is. That's different than sarcasm.

Downmodded for idiotic use of the word "sarcasm".C99 has quite a few unexpected gems. I hate having to write C89 code, you miss little things like being able to declare variables anywhere in a block and in a for loop (like C++).Would have saved me years if the HTML4 spec had mentioned setting left and right margins to auto, thanks!

(Looking at http://www.w3.org/TR/html4/present/graphics.html#h-15.1.2 )A 12 line script shows that programmer productivity is 500 times higher than in 1972? And from this we can extrapolate that this will continue in the future? So that in 2048, programmers will solve this puzzle in 0.9375 seconds, or they're not good?

This article shows mainly that we have better tools/libraries for text processing.  In 1972 you might well have had to implement chomp, split, sort, uniq, etc.  That's great, but that doesn't necessarily mean that productivity in general is orders of magnitude higher.

And there hasn't been such massive improvement since the 1990s as the author claims.  A good programmer would have used a scripting language back then, probably Perl, not Java, to do this job in about the same time.

Also, the program won't work if there are hyphenated words.I've structured my entire career around avoiding statically typed languages up to this point. Better typing systems like Haskell may change my mind, but I'm not convinced yet (not enough large projects proving large, useful projects can be done, something time will either fix or it won't). But at the moment, I'm about as firmly in the dynamic typing camp as it gets.

But even I'm not silly enough to try to claim that typechecking _doesn't catch bugs_. That's stupid. Many type errors are bugs. Better type systems catch better bugs, and prevent more false positives.

The "correct objection" is that the _cost/benefits ratio_ for catching these bugs is poor, that it's a bad trade. For the gain of catching the bugs that the type system will rapidly catch, but dynamic type systems don't rapidly catch (a smaller set than "all bugs static type systems catch"), you are making great expressiveness sacrifices, and at least in the case of Python/Ruby/Perl vs. Java/C++, you're sacrificing entire valuable design tools like closures and such that enable better, cleaner, and more powerful solutions. Bad trade. (Note those tradeoffs aren't entirely expressed in terms of typing; I am aware of strongly-typed closures and such. This is a sketch of an argument, not an argument.)

But don't fool yourself into thinking there _isn't_ a trade. There is. It's one I'd take in a heartbeat in all but the most extreme situations (almost everything has it's place, strong static typing is no exception), but it's still a tradeoff.I do, and have for a little over 3 years.The proper way to say it: O'Caml is a functional language with a "modern type system".Interesting stuff, although as a biologist who actually looks at hundreds of organisms every week (fruit flies in my case) you see just that a large number of the individuals go wrong in development. From the outside, you see an incredibly stable and responsive system, but life is willing (or forced, we're not really sure) to sacrifice a certain number of individuals to achieve what it does. Engineers would very likely have to make the same choice in designing their systems.You propose literature is phased out so we teach both math programs?&gt; Groovy (which is better than Ruby)

That's your opinion. Apparently others, including me, think it's the other way round.&gt; No, I meant to imply ;)

I knew I had that wrong. Readers infer, the writer implies. Grr!

&gt; Smalltalk

The reason smalltalk isn't my favorite language is because I almost lost use of both pinkies from right-shift 1 to get exclamation points ;]

&gt; Jython

Jython is no doubt here to stay. (At least I hope so since I write almost everything I do in it lately.)Java:

    Map&lt;String, Foo&gt; someMap = ...
    someMap.get("KEY_DOESNT_EXIST").callMethod()

    java.lang.NullPointerException:
      at com.mycompany.SomeClass.oopsBadHashCall() (SomeClass.java:42)

Haskell:

    callMethod $ lookup "KEY_DOESNT_EXIST" someMap

    Couldn't match expected type `Maybe Foo'
           against inferred type `Foo'
    In the expression: lookup "KEY_DOESNT_EXIST" someMap
    In the first argument of `callMethod', namely
        `callMethod $ lookup "KEY_DOESNT_EXIST" someMap'

(All code untested.  I archived my Haskell-development VMWare image to make room for a Python-centric image, so I don't currently have a working Haskell compiler installed on my system.)

A short list of the Java runtime exceptions that will *never* occur in Haskell, because the type system catches them at compile time:

* NullPointerException
* ClassCastException
* ArrayStoreException
* IllegalStateException
* UnsupportedOperationException

A short list of common Java bugs that are caught by Haskell's type system, always:

* Accidentally overriding a method with the wrong signature, so that it doesn't actually override what you expect it to
* Breaking class invariants through injudicious use of protected members
* Breaking class invariants through exposing too much public API; Law of Demeter violations
* Forgetting to initialize instance variables

A short list of other bug types that will never occur unless you use State/STRefs/IORefs to circumvent some of purity/typechecking in Haskell:

* Forgetting to call an init method
* Forgetting that you've called a mutating method and the state of an object is not what you thought it was
* Silent state mutation in third-party libraries
* Mutating the wrong variable, eg. typoing an i for a j in an inner loop.Productivity. I do not think it means what he thinks it means.In Python, the end point in the range function is never part of the generated list. So shouldn't in te solution to example 3 all the range(0,9)'s be replaced with range(0,10)?Actually, I'm hugely in favor of rote memorization of the multiplication tables. They're the basis for all mental calculation.

What annoys me is the idea that children should spend a year or two of math doing long division by hand, and that any other approach is heresy. Frankly, if you need more than two digits of accuracy, blow $4 on a solar-powered calculator and have done with it. :-)

What I really do wish is that more people could quickly approximate answers without resorting to pen and paper. This requires a combination of deeply-memorized multiplication tables, some sort of clustering algorithm, and some practice. Even more important than that is to be able to work with powers of 10--how much money is $2 trillion dollars divided among 300 million people, to the nearest order of magnitude?

You can't be a well-informed citizen unless you can answer questions like that quickly, while reading the newspaper. And the traditional math curriculum absolutely fails people in this area. So I think it's important to open the discussion to a wider range of fundamental techniques, and not merely defend the standard techniques against any change (as the meteorologist in the video does).While it takes one more step, I prefer to use the Richard Feynman algorithm:

1)Write down the problem on a sheet of paper.

2)Think really hard.

3)Write down the answer.Well, actually this is an ASCII comic :)
Hmmm, and here I am working on implementing a DSEL exactly along the lines edwinb suggested and getting paid for it.
And some of my coworkers (none of whom are computer scientists or programmers) are already using phantom types and monads without realizing.
Where did we go wrong in our thinking?
I'm a web standards evangelist and I advise people to avoid XHTML unless they have a really good reason to use it. HTML 4.01 Strict works just fine, and comes without any XML baggage.... and the restrict qualifier."That's making fun of you for asking here instead of just seeing what CPAN is."

God, what an idiot. What does "seeing what CPAN is" mean? Is that a coherent sentence? Yeah, I looked it up, and it looked just like sourceforge for PERL, and so I asked. And I got a real, real childish response. And then I got another real, real childish response from the same person. Seriously, grow up.A language is an interface between human beings and computers, and its usability by typical people is part of its intrinsic qualities. A language "technically correct but unfriendly to most typical users" is just incorrect, as a hammer without a proper handle.

And as a delivery company owner, I wouldn't be interested into a "better" truck for which I would have to hire fighter pilots as drivers. Even society-wise, it's grossly wrong to require pilot skills to drive fedex vans. Similarly, a formula one car *is* a crappy car with respect to most of sensible metrics: it only shines at speeding for 500km on a circuit with a perfect coating, with a new motor and several sets of tires for each grand prix, and that's *not* a sensible metrics.

How come higher level languages claim to have both the most power and the smartest users, yet produce so few useful apps, either proprietary or open source? This is not a rhetorical question, and I don't have the answer, but a serious answer is required. If a technology fails to deliver its promises, the burden of the proof belongs to it, not to the other technologies which actually do deliver value.

And VB6 is a very enabling language: financial analysts can write their little stuff in it without significant training nor requiring a developer; suppressing the middle man (i.e. the developer) and formal training in many reasonable cases is something that AI dreamed of, and VB actually did. There are other use cases VB doesn't even pretend to address, but what it does, it does very well.

Note that I'm extremely happy not to work with VB or java, but that statement is exclusively about my own personal fulfillment.The importance of learning long division is not to calculate many digits of accuracy. There's a benefit gained by simply having students memorize and execute an algorithm.

Also, grade school is not the last time that math students will see long division. When they study rational functions, they'll want to know their asymptotes, which are found by carrying out long division on polynomials. Students who failed to really learn long division in grade school are then lost in high school.Let me see if I understand.

The browser tries an SRV lookup first, which (if it succeeds) returns the fallback information and as many actual A records as fit in the packet.

But if the SRV lookup fails (as it will for nearly all current servers), then the browser needs to make a _second_ request for the regular A record. Adding this second request may slow down load times by as much as a quarter second for every non-SRV-enabled domain.

Does the SRV standard have any way of avoiding the performance hit?Of course it's a trade-off.  And I'd take the opposite option of yours in a heartbeat.  I feel much more productive in Haskell than in dynamically typed languages.  And a powerful static type system gives me an expressiveness that I miss in dynamically typed languages (there are other sides to expressiveness than having random program fragments put together being accepted by the system).
many of us at my work use JEdit. I came up with a list of reasons, which are valid, but aren't really the point. 

* it looks and runs identically on Linux and Windows
* its install is easy 
(compare http://math.claremontmckenna.edu/ALee/emacs/emacs.html to running a windows installer.)
* it works well within its scope and doesn't crash
* it loads quickly and is not resource intensive
* it is free in both senses

but the real reason over Vi and Emacs is

* we are toolbar/mouse people, not ctrl key people.

there i said it. I simply don't remember multiple key and multiple keystroke control sequences very well and I prefer using the mouse. Yes you can use a mouse with Vi or Emacs but that is not its native mode and the implementations don't look the same across platforms.
*yawn*

CPAN's homepage says "All Things Perl". Two clicks in (FAQ, "What Is...") and it's already obviously different than the "Java-based entries in sourceforge".

&gt; What does "seeing what CPAN is" mean? Is that a coherent sentence?

It's been coherent to everyone I've showed it to so far (admittedly small sample size of 3, but I gave you the benefit of the doubt and sanity-checked what I said).

&gt; so I asked. And I got a real, real childish response. 

No, you got three immediate reasons why it is different than the "Java-based entries on sourceforge" along with an "Ooo golly" that got you all cow's legs up.

You're the one that's all pissy and sad--"grown ups" have enough common sense to simply ignore an "Ooo golly" and get on with their lives.

[Leave the woman at the river](http://everything2.com/index.pl?node_id=801725&amp;lastnode_id=0)  and grow up yourself, or go somewhere else where the bad people won't occasionally poke fun at you.Garlic Soup ?[deleted]If you wanted to stop it, why haven't you simply refrained from making your above comment first?

All of your 4 points have nothing to do with AICs being closures or not:

- Exception handling has nothing to do with closures. Even if you build in a lightweight syntax with type-inference, checked exceptions would remain a problem.

- Syntax and semantics are different things. Even if you need 20 lines of code to use a closure, as long as it has closure semantics, it's still a closure.

- Declaration of interfaces is analogous to declaring a closure. In a statically typed language without type inference it's simply unavoidable to declare closures. In Ruby/Lisp it's of course more simple, because those are dynamically typed languages. But this is a again completely different topic which has nothing to do with closures per se.

- If 'final vars' are a sign of wrenched semantics, then Ocaml has wrenched semantics too, because ALL 'variables' in Ocaml are final. But I've never heard that somebody denies the existence of closures in Ocaml. SO if it's not a problem in Ocaml why should it be a problem in Java?

I've using Ruby for some years now (for scripting purposes), know Common Lisp, toyed around in Scheme, have learned Haskell in the last months and I've also played a lot with Smalltalk in the past.

I've also created and implemented programming languages which had closures. One of them I implemented in Ocaml btw.  So I know what closures are and because of this it's totally obvious for me that Java has closures: AICs fulfill all necessary aspects of closures: They are anonymous functions which capture the lexical environment of the definition site.

But I've not written that Javas closures are as usable as Haskell's for example. The reason why closures in Java are not as good to use is not that Java hasn't closures or because of the final restriction (this is a piece of cake and was never the problem if I used closures in Java), the main reason are the type system and the checked exceptions. 

Checked exceptions make life difficult in various ways and I consider them as a really bad idea (even if the original intent was good). This don't applies only to closures, it's a general problem in Java.

And explicit typing blows up the code extremely, especially if you want to use correctly typed closures. It's not the syntax, it the explicit typing.

But: This don't removes the closures-abilities from AICs. It only makes them hard to use and often impractical. But since Java most often has other means to reach the same it's not a real problem (Javas problems are elsewhere).

If one really wants to understand what's wrong with Java (or other languages) it's necessary to identify the *real* problems first. And if one not even understands that Java *already has closures*, this would be rather hard.
&gt; There's a benefit gained by simply having students memorize and execute an algorithm.

I agree with this completely, because performing algorithms teaches students (a) how to follow directions precisely, and (b) something about the larger nature of algorithms. But (a) can be taught with any one of hundreds of algorithms, and to teach (b), you probably need to compare at least two algorithms for doing the same thing (which is what the meteorologist in the video is complaining about).

My argument here is that US curricula could spend a smaller amount of time on the traditional algorithm for long division (my school taught it on and off for the better part of a year or two, depending on a student's track), and more time curing basic innumeracy about orders of magnitude, estimation, and all the other things that adults in the US are so bad at.

&gt; When they study rational functions, they'll want to know their asymptotes, which are found by carrying out long division on polynomials.

I don't know about other countries, but any US student who even gets close to worrying about rational functions is already honors track, and even then, they will only spend a maximum of two weeks on the subject (well, maybe a bit more in calculus). At that point, they will routinely need to learn much harder techniques quickly (e.g., integration by parts).

Of course, all my suggestions are sharply constrained by the low quality of teachers in many US schools. Given that a large fraction of (say) Boston-area teachers are unable to read at a 12th-grade level, there is a point to drill-and-kill: Rote memorization may be the best the school can hope for, and actual insight may be off the table.graphviz is awesome. it can be used by doxygen to get you really neat &amp; fancy graphs for source visualization. makes it much easier to make sense ofa  codebase. 

--vatI have a terrible admission to make: I have always been disgusted by Python's particular emphasis on its role as a 'glue language'.  I don't like the metaphor.  I don't like glue.  Hearing people make this sensible technical assertion with this unfortunate metaphor makes me think of glue.  Ugh.Dang it, I linked to the last page instead of the first page:

http://www.samspublishing.com/articles/article.asp?p=26865&amp;seqNum=1&amp;rl=1I'm glad you applied my statement to itself - so you are starting to agree with it! ;)

If you question other people's assumptions, instead of just accepting them, you can either:
1) justify them better and understand why the things are accepted in this way, or
2) you find something wrong and can achieve better results with different assumptions (usually this is in combination with 1), happens after you find contradicting evidence - this is how paradigm shifts in science happen)
Fairly few software engineers have undergrad math backgrounds.&gt; and Erlang looks a bit ugly.

FWIW, after... I've no idea-- after at least two years of some familiarity with Erlang, I think it's probably the prettiest language I know.  Although-- I've spent those two+N years looking at it through color-theme-goldenrod in Emacs and my own corresponding goldenrod.vim :-)

I don't know the answer to your question, however, except that the last time I mentioned Jo'Caml (for concurrency, not parallization) here, someone lamented that it was unmaintained.Then, if you can only do destructive operations on data, you don't have referential transparency. It  follows you can't do functional programming. Lisp is multiparadigm. OCaml is multiparadigm. Ruby is not.Where has this been all my life?!?!

Argh. Wish I had found this before. Thanks kerouac3001.[deleted]Is it required to use emacs to use Oz? From the tutorial it seemed to be the case. Some of us are just unwilling to leave our `vi` castles.&gt; (there are other sides to expressiveness

There are indeed.  Morever, there are unlike expressivenesses: I think that 'modern type systems' allow me to do a great deal of reasoning about my program, more easily, without writing any of the program.  If paper-coding is a fun way to pass time, paper-typing is even better.  I suppose that the closest you'll come to such in a dynamic or a less interestingly typed language is UML.  Bleh.

&gt; than having random program fragments put together being accepted by the system).

Well, duh.  Expressing yourself in this biting way only makes me think that you haven't actually put much thought into what advantages-in-expression you get from dynamic languages.
I think a problem with the entire premise of this article is that it overloads what 'full-stack' means.

Full-stack has never meant 'frameworks where all the components were built in-house'. Rails is considered a full-stack framework, but components like eRB infact did not originate with Rails. It was 'glued' in, but it was glued in pretty well. Full-stack simply means a framework that provides the developer with everything they might need for web development, no matter how those components are assembled.

And if you consider it that way, it is terribly unfair to say that TurboGears is a 'glue' framework. If you'll remember, the entire purpose of TG was to provide a full-stack framework which used 'best of breed' components. Its goals are no different than that of Django and it provides no less than Django (infact it provides more if we are to consider Javascript), but its way of doing that was different than the way Django did that (using outside libraries vs using mostly in-house libraries).

Also, as ayrnieu has stated before, glue gives a rather negative connotation. It implies that something is not well integrated, that there are rough edges. This too is unfair to Pylons. Pylons lets you see the 'plumbing' if a project if you want to, and even to adjust that plumbing, but it still has a clean API and the plumbing doesn't rear its head throughout the framework. Not to mention, Pylons provides a set of defaults that make it a full-stack framework (with the exception of an ORM chosen for you in the default template, which I'd like to see changed and I have heard that a more full-featured template is on its way, AuthKit/SQLAlchemy integrated and all).

Pylons and TG are no less full-stack frameworks that Django is. Just because they use outside components and Django doesn't (as much), doesn't mean they are 'glue frameworks' or anything like that. The *real* difference between Django and the others, however, is that TG and Pylons both allow and enable you to easily customize the default choices. Django allows you to, but it doesn't make it nearly as easy. In fact it can be pretty frustrating to do so when the framework doesn't help you out and certain members of the community are hostile towards doing so.

And I would argue that not only can there be a mixture of 'full-stack' and 'glue' frameworks (to use your terms), but it can be the 'perfect' framework (as much as a framework can be perfect).

Django as it currently stands could more or less be duplicated as a Paster template layer on top of Paste or even at a higher level, Pylons or TG. Use a regex dispatcher (several exist throughout the Python community), use Jinja for templating, and then use Elixir on top of SQLAlchemy and you have something that is very very close to Django, and a lot more flexible (because it is built on top of something made to be flexible). Now these components aren't a perfect match, but with a little time spent tweaking the, I'm thinking they could more or less duplicate the Django experience. Let's refer to this Pylons/TG Paster template as neo-Django.

Now, yes, a Paster template on its own that duplicates Django isn't terribly useful unless you really want that kind of thing, but if this type of approach was taken when building a framework, it could be very powerful. Essentially you have a framework on top of a framework. The developers of neo-Django could focus on documenting and supporting the specific components that their framework uses. While they do that, the Pylons/TG team is the one focusing on the 'plumbing', on all the parts that form the very base of the framework and allow it to be as flexible as it is. What you get really is the best of both worlds. A new user can come along and use this neo-Django template and get the experience of a 'full-stack' (again your term) framework with good support and documentation, while at the same time having the ability to easily use whatever components they want to. The neo-Django developers really are only focusing on documenting and supporting a few libraries and a template, while the and Paste and Pylons/TG developers are focusing on the tougher stuff (how do we integrate this templating system, how do we enable this server setup, etc).

Such an approach would be very powerful and a much more efficient way of doing things. It is a better way of separating concerns.

I am hoping that in the future more and more Python frameworks see the advantage in doing something like this, and make their 'framework' essentially a wrapper/template on top of Pylons/TG. Again, it allows them to support specific components without dealing with the really low-level parts of the framework (which of course would be possible if needed).

TG and Pylons already are taking a route like this. TG 2.0 isn't necessarily a wrapper on top of Pylons, but will be a loose set of WSGI components that are implemented as a Paster template.

Flexibility and defaults can exist hand in hand. I personally believe it is a lot more difficult to take something that was designed in a monolithic manner and make it flexible vs take something that is flexible and provide solid defaults.

I do not buy for a second that 'neither' type of framework is better. I think that is a bit of an excuse for Django's position on using outside libraries and building on top of outside libraries (which as much as you might say otherwise, hasn't been a bright spot). Developers will always disagree with the ideas of framework designers, especially as they get more experienced in development. In that case it is objectively better that they be able to change out components, and still use their existing codebase, as opposed to having to move over to another framework because they realized that the way Django does certain things just won't work for them.

&gt; In an ideal world we’d all realize that the choice of methodology comes down to subjective preference, and that it’s a great thing to have the choice; Python web development wouldn’t be nearly as much fun if everyone were shackled into a single vision of the One True Framework (even if Guido were to Pronounce on this, which he really hasn’t — “as standard as PIL” isn’t much of a Pronouncement).

And that's the whole point. Pylons and TG and designed for 'choice'. They are designed to be as flexible as the user wants them. Django realistically locks the user into design decisions. It is almost lip-service to say otherwise when I look at something like the way TG allows the user to easily swap out templating languages and Django doesn't. The user is left on their own.

A few more statements of yours that I'm just coming back to as I read over the post:

&gt; A full-stack framework has to ensure that its components are of high quality and all work together; that’s a big enough job in itself without trying to make them work with any random thing somebody pulls off the shelf.

And that's the entire point. If Django was using outside libraries as opposed to creating its own (FormEncode vs newforms for example), it would have a lot more time to make sure its components were of high quality vs rewriting them from scratch. After all, something like FormEncode has been used for well over a year and thus there has been thorough feedback and testing.

&gt; And a glue framework has to ensure that its adapters and interfaces scrupulously follow various standards and specifications (like WSGI, which is not trivial to get right) and work with as many components as possible; that’s a big enough job in itself without spending development time on maintaining a particular arbitrarily-chosen set of components.

That's another aspect of it. If a 'glue' framework is focused on implementing standards and specifications, it isn't their job ot make sure that components fit them. They can chose to help those components fit them, but for developers to have to change outside libraries to fit a specification is totally missing the point of a specification.

I think what we are really seeing here isn't a framework design issue, it is a long-standing software design issues. You've got the monolithic Windows approach on one side and the 'pluggable components' approach of Unix on the other. The monolithic approach (which Django takes) does allow flexibility at some levels, but because it wasn't built from the ground up to be flexible, it isn't as easy. On the other hand you've got something like Pylons which in a lot of ways is really unixy and is modular, but it also has the same issues as Linux of being not nearly as approachable for newbies as Windows is. There are benefits and disadvantages to either approach.

What we really need is the 'Mac' of frameworks. Something which embraces these unixy conventions at the heart of the system, but packages it up well and puts a pretty presentation on it.

And I do think that is possible.&gt; As people are reading this, this point will stick: locks are slow

Yes, they are.  But not by counting the cycles spent by the lock insns themselves.  Locks are slow, because they cause an additional memory location, besides the protected one, to be shared.  Namely, the lock itself.  And above all, *shared for writing*.  Which means that any contended lock is in the cache of *at most one* CPU in the system and transitions of the state of the lock (for example between locked and unlocked, or, for reader-writer locks, change in the number of readers (!)), performed by a different CPU causes the cache-line, containing the lock, to bounce to the new CPU.  Which costs *hundreds* of cycles on contemporary CPUs.&gt; Looking at http://www.w3.org/TR/html4/present/graphics.html#h-15.1.2

If you follow the bold link that says "Deprecated", you will see this:

&gt; In general, authors should use style sheets to achieve stylistic and formatting effects rather than HTML presentational attributes.

So, if you look at a CSS specification, you'll see something like [this](http://www.w3.org/TR/CSS21/visudet.html#blockwidth):

&gt; If both 'margin-left' and 'margin-right' are 'auto', their used values are equal. This horizontally centers the element with respect to the edges of the containing block.
OK, I agree with this argument.&gt; But: This don't removes the closures-abilities from AICs. It only makes them hard to use and often impractical.

Function pointers combined with structs in C in practice make for better "what you call closures" than AICs. So far, nobody was ridiculous enough to say that C has closures.

I think we have a terminology problem here. Can we agree:

1. that lexical closures were first implemented in Scheme and that the term became became popular there

2. that closures in Common Lisp, Ruby, Smalltalk and Haskell are damn close to the original ones in Scheme

3. that there are significant differences, in semantics, and for practical purposes even more in syntax, between AICs and closures as in the aforementioned languages

Were we differ is, that for me, the differences (theoretical and more so practical) between Scheme-style closures and AICs, are significant enough, that it's clearer to keep different names with disjoint meanings.

&gt; If one really wants to understand what's wrong with Java (or other languages) it's necessary to identify the real problems first

You're standing in front of a huge problem and your eyes are closed.As I've said to other people, if you program in Haskell and wouldn't touch Java with a ten-foot pole because of it's type system, then you already agree with me on all fundamental points. We only disagree if you'd choose Java over Python/Ruby/Perl.

I'm serious about what I said about Haskell; interesting, but I'm waiting for some more large projects to be done in it that do something interesting. Making a small project easy is ultimately not terribly impressive. I'm patient, though, and I'm reasonable; this is not an opinion I intend to stick to no matter what the evidence, this is a position I've chosen exactly because of the evidence. 

(My current "large haskell project" list is: Darcs, GHC itself, and I'm honestly not terribly inclined to include the latter. My cut-off is "greater than one man-year of time spent, standard 40 hours a week". Feel free to add to the list.)Interesting in this context is Erik Naggum's [rant about glue code](http://www.underlevel.net/jordan/erik-perl.txt)&gt; Feel free to add to the list.

Pugs.Well, you're right about the woman. :pIt works doesn't it...thats microsoft's viewDoes Mercury support interactive development as in Lisp and Smalltalk?Same bullshit, different day. This was written largely to summarize arguments I've been having with you, and it still seems to be going in one ear and out the other.

As I have said in pretty much every single post to you: if you don't like the way Django is designed or developed, that's cool; there are other frameworks with other design and development philosophies and you seem to have found a couple that you like. Use them and enjoy the embarrassment of riches the Python web-dev community has right now.

But please keep in mind that the way _you_ want a framework to be is not necessarily the way _everyone_ wants a framework to be, and that telling everybody to conform to your preferred style is not going to win you any friends. There is no perfect Python framework.&gt; If popular == good, VB would be one of the very best languages ever designed by man. I think we all know that's wrong, right?

When using terms like "best", you always have to qualify them with what you are measuring. For example, VB clearly isn't the best language for writing device drivers, even though you could with a few shims written in C++/COM.

On the other hand, for writing rich client business apps on Windows, the only serious competition is Delphi and the minor players like PowerBasic and RealBasic. So while I cannot say it is the very best without seriously using the others, it is certainly in the running.

I find network effects to be quite interesting.

First is the assumption that network effects are market driven, not effectiveness driven. But is that really the whole story? Microsoft has heavily marketed other stuff that fell flat on its face. On the other hand, no marketing certainly hurts too. RealBasic has cross platform (Windows, Mac, Linux) GUI support that puts Java to shame, but hardly anyone has heard of it.

Another issue is that networks effects make a tool better. VB wouldn't be where it is today without all the third-party controls. Consider also dBASE. When Ashton-Tate crushed the control vendors, the dBASE community evaporated along with it.

Anyways, back to my point. If you wanted a cross platform language for rich client development, what would you do? 

1. Listen to the people bragging about the superior design of their favorite langauge? 
2. Read the marketing material?
3. Or would you do the responsible thing and actually look at what people like you were able to create with their tools?
The XML "baggage" is very useful for certain editor and tools.

It can make life easier for the designer/developer.Hey! I heard that!&gt; As I have said in pretty much every single post to you: if you don't like the way Django is designed or developed, that's cool; there are other frameworks with other design and development philosophies and you seem to have found a couple that you like. Use them and enjoy the embarrassment of riches the Python web-dev community has right now.

Yes, I do not like the way Django is designed, and I am going to promote other frameworks to people, because I do not believe that Django is flexible nor is it a good model for web development. I want to save others the problem I have right now of converting over several apps from Django to other frameworks because I realized that there were certain parts of Django that I'd rather not fiddle with to get it to work how I needed. Might as well use a framework that makes the fiddling easy.

&gt; But please keep in mind that the way you want a framework to be is not necessarily the way everyone wants a framework to be, and that telling everybody to conform to your preferred style is not going to win you any friends. There is no perfect Python framework.

Do you not see the oxymoron in your statement? What I want is a framework that is flexible. Django is not flexible. If I want any specific aspect of my framework to be something, it is flexible. That's why I like Pylons and TG, precisely because they don't force me to conform to any idea of how a framework should be, unlike Django.

I do love the 'riches' that the Python web development community is currently pumping out, but you know where the real developments are coming from? TG and Pylons. Toscawidgets, Routes, WebHelpers, Elixir, Paste, SQLAlchemy, WSGI adoption, Mako, Genshi, Buffet, Kid, CherryPy etc, not even counting the numerous WSGI middleware solutions like AuthKit) all are projects that have come out of or have been heavily supported by the TG and Pylons communities over the last 2 years (if we are only to count new projects that is 8 out of 12). All of them work well with others frameworks and are designed for reuse. What has come out of Django during that time that is easily reusable by other frameworks? Essentially, newforms, a library which FormEncode and friends have been doing for a while now.

It is my personal opinion that Python web development has such a bright future because it is smart about standards and because of that a lot of great innovation has taken place. You don't see the same type of innovation in the Django community (what has come from the community itself? everything is from the core team). I believe that if anything holds back progress in the Python web development world, it will be Django, and that's why I'm hoping to persuade as many developers as possible to use something else.

Its nothing personal against you or anyone else, I think Python has a good future and I want to see that embraced as much as possible. Django is contrary to that because instead of building on other people's code (and I just listed what has occurred due to that), they've gone and done everything theirselves. Now that might produce a good quality product, but it probably won't be nearly as effective as if that same time and effort had been spent building on top of tools that are designed to be 'portable' and have been solid for awhile.I was really hoping for a more critical opinion of how they are handling it. You know, like is there a better way to handle the situation.[deleted]A step by step tutorial with source code showing how to implement a simple captcha (security verification by typing in letters from an image) written in PHP.A much better explanation can be found here: http://mat.gsia.cmu.edu/classes/dynamic/node3.html#SECTION00030000000000000000
Truly awful marketing from MSWhat, that something that's stuck around for decades will likely continue to stick around for decades?  :)The gods sure have a taste for irony, don't they?↑, Gerald Jay Sussman.You are right. But you still missed the point of the article.

Posts like yours here is the reason why Haskell will probably never become mainstream. It reminds me again at my time learning mathematics in university: Everything has to by 100% correct, proved etc. I accept this a a necessity if one want to successfully do mathematics. But not everybody wants to be a mathematician. It's not even necessary, lots of great physicists where lousy mathematicians (compared to real mathematicians of course) but they still used mathematics as a tool to make great and important discoveries (and the mathematicians cleaned up the floor later by creating a solid mathematical framework).

In CS this is similar. In Germany CS ('Informatik') only exists as a sub-section of mathematics. But in fact it's just a field of applied mathematics, like engineering or physics. While mathematics is an important tool, it's not necessary that everybody who do CS is a mathematician. So people who like to tackle the field more from a engineering point of view are screwed and often quit studying CS.

Especially in FP I see strong similarities: FP was created by mathematicians and they still have 'control' in the area. But this leads to the problem that it closes out people without a strong mathematical background - and the mathematicians don't even recognize it. They think: "Why don't people use our wonderful principles and techniques even after we've simplified them so much. People must be stupid". This is quite a common misconception. I've done it myself often enough to other people until I realized that some things which are really easy for me seem nearly impossible to grasp for other people with a different background. But not because they are stupid, simply because not everybody can know everything.

Monads are such a thing which is really hard to understand for non-mathematicians. But because they are an interesting concept, why not create analogies which removes this difficulties - even if the analogy is not exactly right? The reason is that this runs totally against everything mathematicians have learned and believe in. They want correctness. Proofs are their life-elixir. They have to spot tiny inconsistencies from a 100 miles distance. And if you want to do mathematics, this is really the way to go. But this isn't about doing mathematics. It's about building programs which have to work out of incomplete specifications and can't generally proved as correct.

So I tried to find an image for the concept which most programmers who aren't mathematicians can relate to. I'm such a programmer myself and so I know about the problems.

It's true that all functions do code-transformation if you look at if from a certain angle but again it's wrong because the real semantics are term-rewriting. Not code is rewritten but data is matched an rewritten. Closures are 1st class data in fp so you can also do term-rewriting on closures and this is what monads do. Not always (in case of the list-monad) but for example in case of the state-monad.

But I don't want to look at it in such a pedantic way. I wanted to take a look from a distance. For what do you use monads and for what do you use 'ordinary' functions? Functions are like a 'black-box': You put some data in and get some data out. And in Haskell the same in-data always gives the same out-data. With monads (which are no functions but *constructs build from functions*) you put statements in and get a certain behavior. Just like the monad transforms the statements via the rules of a language with certain semantics which depends on the type of the used monad. How this happens isn't important here (at least in the beginning), important is that the monad creates certain semantics (or a 'program evaluation strategy' in your words) from an ordinary bunch of statements.

And if you have some experience in programming you recognizes this kind of behavior: It's a DSL, something which often is implemented via macros. Sure, the method is different, but looked at it from higher ground, it's quite similar. All monads have this principle in common, so why not simply look at monads as 'some kind of functional macro'? It works. I understood it much better after I used this image and after this I also had a much clearer view on the restrictions monads have. And so I thought to share my insights, because I don't think that I'm unique and that other people could benefit from it, too.

I was well aware, that the are of course lots of people who really understand monads and see the deficiencies of my analogy. And because of this I put a short introduction at the start of the article to tell about my background and about the intended audience. Even if it's hard to accept for someone who knows a field, but to lean things we often need to start out with a more simply model, even if it's not totally correct. That's the reason you learn about Bohr's model of the atom in school instead of jumping right into quantum mechanics. And why they start with simple arithmetic instead of the Peano Axioms.

&gt; In a phrase: a program evaluation strategy that's composable.

That's also a valid description for macros. Why can't you define setf in Common-Lisp as a function? Because you the standard evaluation strategy of the language won't allow it. So you need a macro to change this strategy. And of course 'setf' is composable. You can use it like an ordinary function - but with a slight different way of evaluating its first argument as real functions do.

If you say "a monad is a program evaluation strategy that's composable" this is simply much to unimaginative. To understand it, people have to translate this sentence first. By simply saying "kind of a macro" or "program transformation" you can much easier ring some bells. And you will even ring the right bells. Soon after the bells stopped ringing, it's possible to look at the shortcomings of the model and what's the limitations and advantages of monads compared to macros are, etc. But if you don't get some bells ringing first, you can't even start to think in the right way about it. If you only see a monad as a kind of data-structure for example, the bells which are ringing are the wrong ones and you will thus get stuck in a totally wrong place.

&gt; Yes, I do not like the way Django is designed, and I am going to promote other frameworks to people, because I do not believe that Django is flexible nor is it a good model for web development.

As I've been saying: you want everyone to conform to your preferred philosophy. That ain't gonna get you any friends.

&gt; What I want is a framework that is flexible.

What you want is a glue framework. What I want is a full-stack framework. This is Python and we're all consenting adults here: if you like oranges and I like apples, that's cool and there's no need to mount a propaganda war against the heretical apple eaters, mmkay?

If you want to continue this discussion, hunt me up at PyCon. I'll be having drinks with the CherryPy guy...&gt; As I've been saying: you want everyone to conform to your preferred philosophy. That ain't gonna get you any friends.

If the main thing I am driving at is flexibility, how is that wanting everyone to confer to my philosophy? That makes no sense.

&gt; What you want is a glue framework. What I want is a full-stack framework.

Those are terms you've made up. You've made two polar opposites that don't exist. TG is both a full-stack framework  and one that uses existing libraries (a 'glue' framework).

&gt; This is Python and we're all consenting adults here: if you like oranges and I like apples, that's cool and there's no need to mount a propaganda war against the heretical apple eaters, mmkay?

If I am 'mounting a propoganda war', you are doing it just the same. But heck, consenting adults have the right to form their own opinions, right?

That's all I'm doing, letting people know that there is more out there than Django. I don't think using Django is morally wrong or anything like that, and for many people, Django is all they need. But for a lot of programmers there are great advantages to flexibility, and I like to expound on those advantages.Function pointers don't capture the environment, AICs do. That's the crucial difference here: Closures are in principle function-pointers + captured environment. AICs have both. If you look at the implementation of closures in the Ocaml compiler, you will see that it's nearly identically to the way AICs are implemented (only difference results form the fact that AICs can have multiple methods, while a closure have only one evaluate method).

&gt; lexical closures were first implemented in Scheme

No. Other languages where earlier. Pascal had it and it I remember correctly Algol had it first.

&gt; that closures in Common Lisp, Ruby, Smalltalk and Haskell are damn close to the original ones in Scheme

Only because those are all dynamically typed languages or have type-inference. 

&gt; that there are significant differences, in semantics,

Don't agree, there are no significant semantic differences. There are only differences in syntax and in usability.

I would propose to go with the duck-meaning: If it's implemented like a closure, can be used like a closure and have the semantics of a closure then it is a closure.

&gt; You're standing in front of a huge problem and your eyes are closed.

And what kind of problem is this?
First comment draws a good parallel:

&gt;I couldn’t help but see a bit of an analogy with the age-old Mac vs. PC argument here, whereby the Mac is the full stack and the PC is the glue. Apple works with a limited set of hardware components (and often builds their own), and uses software to make them work really, really well together. It’s never going to be quite as flexible as a PC, which can work with thousands of different hardware components. But, the downside of the PC is that each of the components may not play quite as nicely together, because it’s simply impossible for software (like an operating system) to account for every possible combination, new releases, etc, etc.It makes it seem too much like Perl.Thanks a lot - this is one of the things worth saving.read the titleYes. Whenever I read one of their pages, it reads like someone who's only really been superficially involved with the industry for a couple of years, has sat down, carefully read through some wikipedia pages, and correspondant archive sites, and then carefully put together a thorough retcon that brings alive a parallel universe from the one I remember, full of misrepresentations, and distorted perspective. It simply reeks of second-hand, it's ersatz history.Oh, nostalgia. My first computer was a PET 2001. The one with the 'ridiculous chiclet keyboard' and a whopping 8Kb memory. 

It was not only a great tool for the first steps in programming, after I was able to get a copy of the schematics the poor thing was a great way to learning electronics, too. In the end there was nearly no chip left in the main-board without burn marks from soldering cables or other ICs piggy back onto them. Alone to open the case like the engine hood of a car was always fun. 

Today everything is much more 'virtualized': Instead of hacking in code directly in hex we use high-level languages, and instead of soldering SN74xx chips there are powerful FPGAs which allow the design and implementation of computers much more powerful as those old ones, with CPU, periphery etc all on one chip. Makes access to the basics a bit more difficult, I suspect.
Moral: Windows sucks.  It doesn't just normally, casually suck -- no, it sucks so much that people exposed to it write articles like this one and the "Johnny Can't Code" one, lamenting the loss of crappy systems with crappy languages that, together, manage to be slightly less barren and evil than a normal windows install.Not programming.[deleted]&gt; Function pointers don't capture the environment, AICs do

Thanks for the lesson, that's what the struct is for. I wrote:

&gt; Function pointers combined with structs in C

you:

&gt; (only difference results form the fact that AICs can have multiple methods, while a closure have only one evaluate method)

Bingo. That's one of the reasons why AIC syntax is verbose compared to closure syntax. AIC syntax:

    new Transformer&lt;String, String&gt;() {
        public String transform(String a) {
            return a + foo;
        }
    }

versus hypothetical Java closure syntax _with_ explicit typing:

    new String transform(String a) {
        return a + foo;
    }

Already a lot better, no?

("transform" would be declared as a function type, AFAIK C#'s delegate types would do)

"Perfection is achieved not when you have nothing more to add, but when you have nothing left to take away"

    Antoine Marie Roger de Saint-Exupéry

In other words, you are using a screwdriver with a hammer head where a plain screwdriver would be appropriate.

&gt; No. Other languages where earlier. Pascal had it and it I remember correctly Algol had it first.

AFAIK they had lexically scoped variables, but no closures. http://en.wikipedia.org/wiki/Lexical_closure agrees that Scheme was first.

&gt; Only because those are all dynamically typed languages or have type-inference.

I disagree, see example above.

&gt; no significant semantic differences

_Edit_: closed over variables must be final in Java. &lt;/edit&gt;

I disagree. You mentioned the first: any number of methods versus a single "activate". Another one, Java code:

    interface Block {
        public void call();
    }
    static void a() {
        loop:
        for(int i = 0; i &lt; 10; i++) {
            System.out.println(i);
            b(i, new Block() {
                public void call() {
                    break loop;
                }});
        }
    }
    static void b(int i, Block b) {
        if(i == 2) { b.call(); }
    }

Fails to compile with "undefined label loop". The label is part of the lexical environment and should be captured by a closure. The equivalent in Common Lisp (using `lambda`, `block` and `return`) is natural Lisp code.

&gt; And what kind of problem is this?

The choice between code duplication and architecture-overkill. (Please, don't let's descend into arguing with Turing-completeness.)I guarantee you that's the *only* time I have ever been right about a woman  :DI understand your point about Django is akin to Windows but are you also then saying that Turbo Gears is the "Mac" of Python frameworks?I must support ubernostrum. Let everyone choose a framework for himself and for each task. I really love django for content orientated webpages, I love it a lot. But just because of that I would never develop pocoo in django. That's just not the right tool for that task. And forget about TG. Pylons is great, TG is not. It's harder to deploy than django, it's less maintained, the IRC channel is nearly empty, at least nobody answers questions. The result of that is that in the German Python IRC channel more than 4 people moved over to custom WSGI applications, django and pylons. And that's pretty crass; in the channel are not more than 60 people...Not to me; I'm already working on the next "feature".&gt; * NullPointerException

Haskell: Program error: pattern match failure: ...

&gt; * ClassCastException

Haskell: Program error: pattern match failure: ...
or if you use 'Data.Dynamic' you can have real cast-errors too.

&gt; * ArrayStoreException

Haskell: Program error: Prelude.!!: index too large

    * IllegalStateException
    * UnsupportedOperationException

Depends on the situation, but something similar is again possible.

You can prevent pattern match failures by using some switch,  so I wonder why it isn't mandatory in Haskell (like it is in Ocaml) to make patterns exhaustive.

Haskell's type system makes it still much better to find certain kinds of errors, because it's possible to type certain things where you need casts in a language like Java because the type system isn't expressive enough or you don't want to create huge type expressions all over your program using generic types and simply use 'Object' instead.
TurboGears is the amiga of Python frameworks ;)No, I'm saying that the 'Mac' Of Python frameworks isn't here yet. While the Pylons documentation is fairly good, it still isn't at the level of Django (the Django docs are *excellent*), and as I said earlier, the default template still doesn't provide an ORM. I don't know if Pylons will ever get to the point to where it is as easy for newbies to web programming to use as Django, but if it does, it could be quite the combination: ease of use and flexibility.

I think more than anything Pylons provides a really good base to build on top of, its just going to take someone to do it.

My opinion on TG is that TG 1.0 was really great for getting the ball rolling, but it has design issues of its own. In a lot of ways it is still very coupled. 2.0 promises to change that though, and it is the basis of projects like Toscawidgets. We will have to wait and see how much the idea for 2.0 translates to reality.[deleted]Boy, it takes talent to pack that many buzzwords into one headline. :)

Edit: Bah, forgot the smiley face. Oh well.You do know about -fwarn-incomplete-patterns, right?

Anyway:

&gt; NullPointerException: Haskell: Program error: pattern match failure: ...

Not really analogous; the equivalent to null in Haskell is the Maybe data type.  The Haskell compiler won't let you use Maybe Foo where a Foo is expected; you have to deconstruct it yourself via `maybe`, `fromJust`, or pattern-matching.  Of course you can shoot yourself in the foot with `fromJust`, but the source code text will make it very apparent that that's what you're doing.

&gt; ClassCastException: Haskell: Program error: pattern match failure

Same thing: the Haskell source code makes it apparent that you've neglected to handle a case, and if you use -fwarn-incomplete-patterns, the compiler will warn you too.

Bets are off with Data.Dynamic, but you typically need that only when dealing with external marshalling/unmarshalling, so you test that part of your program and then use the type system to keep everything else safe.

&gt; ArrayStoreException: Haskell: Program error: Prelude.!!: index too large

That's not what an ArrayStoreException is in Java.  [ArrayStoreException](http://java.sun.com/javase/6/docs/api/java/lang/ArrayStoreException.html) arises because Java array assignment is covariant, while type-theoretically, it's only legal for mutable arrays to be *invariant* under subtyping.  The JavaDoc gives a good example.

It never shows up in Haskell because Haskell's type-system is mathematically sound and doesn't have holes like this.Django is one type of a framework, and Pylons + TG is another. I like Pylons but I use Django, which is good (but not perfect) as it's very fast for creating apps for me. For Pylons like frameworks I would have to spend more time looking for documentation of components it's uses  ;)I printed this out, thanks. +1&gt; So that in 2048,

2048 is quite a bit in the future.  Would you settle for 2015?

&gt; So that in 2015,

I'll have a 12-line script in ~/bin

-- which is to say, no: by 2015 we'll have newer, more interesting tests of productivity increase that still don't, themselves, continue to absurdity.How to write programs in two easy steps:

Step 1. Write Code.

Step 1.5. ?

Step 2. Profit!in response to you and augustss below: my career prior to computer programming was market research.  augustss is making the mistake of relying on "anecdotal evidence".  i've been at 20 programming jobs so my own anecdotal evidence comes a lot closer to comprising a statistically viable sample than does one job.  and speaking of sampling, it is well demonstrated that the audience of (programming.)reddit.com is skewed towards academia.

&gt; Thanks for the lesson, that's what the struct is for. 

Thats simulating it. With AICs it's done by the compiler for you. If simulation would count, nearly ever language has everything.

&gt; That's one of the reasons why AIC syntax is verbose compared to closure syntax.

Never questioned that. But that makes AICs not less a closure.

&gt; versus hypothetical Java closure syntax with explicit typing:

No, this is not the reason. The reason is that your second example needed a different declaration. If you declare 'Transformer' with 'Strings', the overhead would only be "new Transformer() {". Yes, it's overhead but that doesn't make it less a closure. It's just a closure with a more elaborate syntax. The reason why it is really a closure is that you can use 'foo' in the body.

I don't argue that there are ways to make closures shorter in Java. Thats absolutely possible. But it's syntax: It would make *already existing* closures easier to use, not create them on the first hand. I'm not talking about perfection here (it's about Java, remember?), I'm talking about the question if Java has closures and you've not brought up a single reason which showed the opposite.

The Wikipedia article is inconsistent: First they ciount Python under the "Programming languages with closures" and Java under "Closure-like constructs" and below they write "This technique is not limited to Java, and will work in other languages with similar restrictions, e.g. Python.". So both have the same restriction and Java is only "Closure-like" while Python is not? Thats stupid. But OTOH, we're talking about Wikipedia here. It's a nice resource but you always have to take it with a grain of salt because the content is more dependent on politics than on truth.

The difference between Pascal and Scheme is that Scheme allows anonymous closures, which Pascal don't do. But a function in Pascal also captures the lexical environment of the surrounding scope. So both are 'closing' over the environment which is IMO the central idea of closures. So maybe 'full' closures where first in Scheme, the basic concept is older.

&gt; I disagree, see example above.

But first you still have to declare the closure somewhere (like an Interface). And also you compared to slightly different situations.

&gt; closed over variables must be final in Java

Like in Python. Or Ocaml. Or Haskell. etc. If Java hasn't closures, those language won't have closures too.

&gt; [example-code]

'break' is no necessary part of closures. In Ocaml and Haskell you also can't break out of a loop from inside a closure. If you want to do it in those languages you have to use throw and catch. The same is possible in Java. So again no difference to languages which are widely considered as 'having closures'.

&gt; The choice between code duplication and architecture-overkill.

Java has this problem, but adding closures to it won't solve this. With traits and yield you could do more good and would remain in the original paradigm. But changing Java to make it half-assed functional language would do more harm than good. But this doesn't even matter, Java simply reached the end of it's 'life' and those changes could only lengthen its death struggle.
&gt; Does Mercury support interactive development as in Lisp and Smalltalk?

It isn't styled that way, no.  Mozart is a bit better in this respect.&gt; I’m getting tired of making the same arguments each time.

Uhm then stop baiting to them and go on with your life.&gt; Is it required to use emacs to use Oz?

No.  You may find it helpful to experiment in Emacs and write code in vi.The [Trachtenberg System](http://en.wikipedia.org/wiki/Trachtenberg_system) uses little to no memorization of tables, and is faster (with practice) than traditional methods, particularly for children who have trouble with the latter.i did like i usually do, ctrl-dYes, that's why I wrote "You can prevent pattern match failures by using some switch, so I wonder why it isn't mandatory in Haskell" (I was to lazy to lookup the exact name of the switch).

&gt; Of course you can shoot yourself in the foot with fromJust

Thats what I wanted to say. It's possible and if you use it, you get a runtime pattern match failure which is analogous to a NPE.

Problem is that in Haskell -fwarn-incomplete-patterns is off by default and you can write programs without. IMO thats a bad idea. It's still better as in Java where things like @NotNull is still vendor dependent.

&gt; That's not what an ArrayStoreException is in Java

You're right. I was to fast there.

But this kind of a problem can also be prevented in Java by using generic containers (as long as you are careful and don't mix it with the still valid old notation). The error is a relict from old Java times.

But of course Haskell's type system is superior to Java's. I just wanted to point out, that some of your 'non-existent' errors could nonetheless occur in Haskell - but of course with other names.
I think it does not refer to the "complexity" of the explanation, but to the difficulty to explain it so that someone understands it.

e.G. it is not any more difficult to explain someone a binary tree than a b-tree. The additional effort of such an explanation lies solely in the additional complexity of the more advanced approach.

Thus, if you have a lot more difficulties to explain someone something that is only a little bit better - than it's a bad idea.

(This is of course only my interpretation, and I think it's okay :)

[deleted]_"Over this entire domain, the one step I've most consistently found useful and with the biggest return on its investment is thoughtful improvement of source code style."_

Downvote for this one. This is eat-your-vegetables advice -- i.e., the best way to prevent bugs in your programs is to pay attention and stop writing bugs.  Well duh ...

I know that the article is really focussed on C, but almost no one writes C anymore.  Nearly everyone has access to C++.  And the one step I've found most consistently useful in _that_ language is to forget about the delete operator completely and instead learn how to use std::auto_ptr, vector, list and string (and boost::shared ptr as well).Thanks for clarifying on that point.  I looked at RoR before I looked at Django or Turbo Gears and I overall liked the direction that they all were going in.  Django just fits "my" need better so that's what I went with.  

I came from an "invent my own framework" and loosely couple them (from a PHP perspective) with PEAR, bTemplate, ADODB, etc.  I found myself spending more time working to be productive then I did being productive.  At the end of the day, I changed over and decided to pick one so that I could write cool projects that interested me vs working on disinteresting projects such as maintaining my own framework.  Now I enjoy my personal projects and my work projects and it seems less like work and more fun which IMO is how it should be.&gt; While the Java language provides many advantages over C and C++ (...)

I stopped reading at this point.&gt; if you like oranges and I like apples, that's cool and there's no need to mount a propaganda war against the heretical apple eaters, mmkay?

Since when did arguing a point constitute a "propaganda war"? Let the man express his opinion and have it stand or fall on the strength of his arguments.Added.

In the interests of honesty and transparency, I've [made my list public](http://www.jerf.org/iri/2007/02/19/2568.html).If only it could produce as nice graphs as [OmniGraffle](http://www.omnigroup.com/applications/omnigraffle/)
What is going on with TurboGears anyway? turbogears-trunk hasn't seen a single post in a week. django-developers and pylons-discuss are going strong...Funny, I independently arrived at a similar analogy. Despite jesusphreak's raving, I seriously think Django is the closest to the "Mac of the Python web-dev world" right now, with TurboGears and Pylons being more like Linux. Macs aren't totally as flexible as jesusphreak thinks is totally necessary but they are solid and tinkerable, like Django. The analogy really does fit.Yes, C99 certainly added a lot. I personally appreciate the addition of a dedicated `bool` type (never felt quite right to be using `unsigned char` or bit masks all the time). Unfortunately, given the spotty compiler support for many of these features, most programmers will probably have to stick to C89/C90.It is often unnecessary to get good performance.

When it is, you'll often have faster and more maintainable results with stock, widely-known techniques, like caching, memoization, lazy initialization, algorithmic changes, indexing, or partitioning.

Micro-optimization is a last resort, for when you've exhausted all the usual options and it's *still* not fast enough.  And if you get to that point, you're usually not writing in Python anyway; you'd want to use C, Ocaml, or some other fast language.&gt; Toscawidgets, Routes, WebHelpers, Elixir, Paste, SQLAlchemy, WSGI adoption, Mako, Genshi, Buffet, Kid, CherryPy etc, not even counting the numerous WSGI middleware solutions like AuthKit) all are projects that have come out of or have been heavily supported by the TG and Pylons communities over the last 2 years (if we are only to count new projects that is 8 out of 12).

The experimentation is a good thing. But as far as letting people get things done the tornado of new-and-improved-way-to-do-this-wait-is-it-really-better-or-not has only caused the TurboGears community to fragment and suffer.

Do you really think when someone realizes that their templating or ORM isn't powerful enough, the overhead of getting it to work within the framework is really what they're worried about? In a production site, rewriting your thousands of pages in the new templating language, or converting all your queries to use the new ORM is the big deal, NOT calling it from the framework. We're all speaking Python here, it's not hard. And in production sites people don't swap out these components based on the weather or song of the day.Nice theory. Any evidence? Studies?&gt; Thats simulating it.

That was my point. (OK, I agree that AIC is semantically closer to a closure than manual function pointer + struct.)

&gt; he reason is that your second example needed a different declaration. If you declare 'Transformer' with 'Strings', the overhead would only be "new Transformer() {".

No. The Transformer interface is from actual Java code I've written. A Transformer is a function in the mathematical sense (if you ignore side effects), it maps something from type A to type B, it's generic (that both A and B in my example were String was accidental). If you fix the the type(s):

* You have to write a separate interface for each input/output type combination.

* More important, you lose the ability to write generic HOFs like `map` in terms of Transformer.

In the hypothetical Java closure example, the function type could be declared as:

    public &lt;A, B&gt; function B transform(A);

Then you can instantiate it for whatever types you like. In my examples is was A = String and B = String.

The AIC syntax is good for what it does. It accomodates to the fact that a regular Java class has any number of methods. But it causes useless boilerplate if used to simulate closures.

&gt; Like in Python. Or Ocaml. Or Haskell. etc.

Python's "closures" are crippled, yes. I'm no OCaml wizard, so can't comment on that. Haskell has a different evaluation model than all other languages mentioned and no destructive update, so this point is moot.

&gt; 'break' is no necessary part of closures. In Ocaml and Haskell you also can't break out of a loop from inside a closure

AFAIK, they have no builtin `break` construct. In languages in which `break` is part of the lexical environment, as in Java, it should be captured by closures.

&gt; Java has this problem, but adding closures to it won't solve this.

Closures are nice for small utility abstractions. Examples are map, filter, reduce, which could replace most loops. The second big use case is the typical and often repeated `try ... [catch X ... catch Y ...] finally` pattern. As opposed to "full blown" classes, which are better for the bigger (e.g. MyApplication) or data centric (e.g. MyModel) abstractions. Closures are light-weight, generic and fine grained, it's important that they are concise, otherwise they are pointless (=&gt; like the simulation via AICs).

&gt; Java simply reached the end of it's 'life'

As much as I'd like it, I don't believe Java will go anytime soon.[deleted]A fine table except when you try to use 'standard' keyboard shortcuts in standard Windows app like Paint, etc. Ka. Boom.[removed]i'd like to hear a comparison of this to Steven's http://www.amazon.com/Programming-Environment-Addison-Wesley-Professional-Computing/dp/0201433079  (2nd edition includes Linux) since that book is acknowledged to be a masterpiece.Well, he claims in one place that all he's doing is

&gt; letting people know that there is more out there than Django. I don't think using Django is morally wrong or anything like that, and for many people, Django is all they need.

But compare with this:

&gt; I do not buy for a second that 'neither' type of framework is better. I think that is a bit of an excuse for Django's position on using outside libraries and building on top of outside libraries

And this:

&gt; I am hoping that in the future more and more Python frameworks see the advantage in doing something like this, and make their 'framework' essentially a wrapper/template on top of Pylons/TG.

And especially this:

&gt; I believe that if anything holds back progress in the Python web development world, it will be Django, and that's why I'm hoping to persuade as many developers as possible to use something else.

That isn't expressing an opinion, that's a smear campaign that later backtracks and tries to say, "who, me? I'm just innocently pointing out an alternative!" And all the while, he's bearing out exactly what I wrote in the linked article: it'd be nice if we could have constructive discussion or at least agree to disagree on subjective things like which architecture we want, but there's always somebody running a dirty FUD campaign who comes along and blows that out of the water.NOW you tell me.  It was such a bitch listening to that in my JAWS browser.A bit dated, but an absolutly great introduction to the basics of classic Information Retrival.In a discussion about reddit tags, I made this [comment](http://reddit.com/info/137pf/comments/c139vr) and it got me thinking. I also thought the reddit-delicious mashup was kind of neat. So I wrote a python script to fetch the first four pages of content from reddit, filter it based on a list of keywords I have in a file on my server, and render the results.

I'm not a programmer or a designer, nor do I know much of anything about website mechanics, so my code is sloppy and the page is....I guess bland is a nice way to put it :P There also isn't much in the way of error checking. In the future I hope to add user names and comment links and style the page better. I also hope to have the script log me in but I'm not sure if that's possible.

Anyway, it works. [Here it is](http://www.redshifted.net/cgi-bin/redditlite.py)...

For those who care, here is my list of keywords (work in progress) I use to nuke links:


congressman

congress

bush

cheney

white house

political

vote up

vote down

McCain

religion

religious

atheist

atheism

scientology

iraq

iran

You can get an idea of my tastes :)

Oh, since I use the **re** module to search the links, I should be able to use any python regular expression as a keyword as well, although I haven't actually tested that yet.

If anyone has suggestions on improving my code, that would be great...I really like Python and I hope to learn more.

Here is the [code](http://www.redshifted.net/redditlite.txt). Please don't laugh too hard :)

EDIT: I'm agnostic by the way :P

EDIT II: Added usernames and comments (pain in the ass...my code is slightly italian until I clean it up a little)... or just read *Kamikaze L'amour*  :-D[deleted]Off the top of my head, some multi-man-year projects.

To stay on the Haskell compiler front:
nhc/yhc;
jhc;
pH (parallel Haskell);
Lolita, speach recognition from Durham (I think the project is dead);
the Bluespec hardware description language;
the Cryptol compiler.
You act as if my statements contradict each other. They don't.

&gt; letting people know that there is more out there than Django. I don't think using Django is morally wrong or anything like that, and for many people, Django is all they need.

Okay, yeah, I don't think there is anything morally 'wrong' with using Django, I just don't think it is the best option available.

&gt; I do not buy for a second that 'neither' type of framework is better. I think that is a bit of an excuse for Django's position on using outside libraries and building on top of outside libraries

And? That goes right along with my thought that there is nothing morally wrong with using Django, it just isn't the best framework to use. You aren't evil if you use it.

&gt; I am hoping that in the future more and more Python frameworks see the advantage in doing something like this, and make their 'framework' essentially a wrapper/template on top of Pylons/TG.

Alright, that's my view (hope) on future development in Python frameworks. What does that have to do with Django?

&gt; I believe that if anything holds back progress in the Python web development world, it will be Django, and that's why I'm hoping to persuade as many developers as possible to use something else.

And that naturally follows my belief that a) there is nothing wrong with using Django and b) that there are better options out there. Naturally I'm going to try to convince others to use the 'better' frameworks, but that still doesn't mean I think its 'wrong' to use Django.

&gt; That isn't expressing an opinion, that's a smear campaign that later backtracks and tries to say, "who, me? I'm just innocently pointing out an alternative!" And all the while, he's bearing out exactly what I wrote in the linked article: it'd be nice if we could have constructive discussion or at least agree to disagree on subjective things like which architecture we want, but there's always somebody running a dirty FUD campaign who comes along and blows that out of the water.

So all I've said is a 'dirty FUD campaign'? You haven't bothered answering anything of what I've brought up. I have tried to have constructive discussion, but your response to it has been: 'more bullshit, 'propaganda war', and 'dirty FUD campaign'. How in the world do you expect to have a constructive discussion when you are labeling the other position any of those?&gt; First of all, what John is predicting has already happened to a good extent. On the server side...

Gruber in that quote wasn't talking about server languages. Certainly he wasn't implying C/C++ on CGI are still used.

&gt; On the desktop, many games have a substantial portion of their functionality implemented in dynamic languages like Lua...

[Gruber is aware.][1] And probably was was thinking of Lightroom when he typed that. Nits: Firefox already uses JavaScript extensively (XUL) and extensibility in a language doesn't mean the program written in that language. And XUL still dumps onto C++ code to convert to WinAPI, Carbon, GTK+, Qt, and on and on. Writing an abstraction layer is not ideal; certainly it doesn't encourage programmers to code in a scripting language what with being slow or taking memory or adding another dependency or feeling wrong across different OSes.

Saying WinAPI, MFC, Cocoa, Carbon, Qt, GTK+, Swing, &amp;c. are dead (right now) without numbers is the same fallacy induced when saying FORTRAN is dead for business code. Intuitive and comforting, but ultimately fails because it's difficult to measure that sort of thing. You certainly see the applications that make a big bang (XUL apps, Lightroom) but they are a small minority.

[1]: http://daringfireball.net/2006/10/brand_new&gt; The experimentation is a good thing. But as far as letting people get things done the tornado of new-and-improved-way-to-do-this-wait-is-it-really-better-or-not has only caused the TurboGears community to fragment and suffer.

The real problem with TG, imo, was that it was ahead of its time. It wanted to do the 'best of breed approach', the problem with that was what was best of breed when TG was created was not the best option 6 months later. TG started out with SQLObject which was quickly eclipsed by SQLAlchemy. It used CherryPy 2 which wasn't heavily-WSGI based, when only a few months later WSGI became the big thing.

That's really caused a lot of anxiety in the TG community because they've been forced to either update to meet these new standards (which is what they are doing in 2.0), or stick behind in outdated functionality. People don't want to have to rewrite their 1.0 apps in 2.0, but if TG didn't go to 2.0 people would be unhappy with the framework 6 months from now and be saying (why not SQLA, why not ...).

Problem is, nobody wants to spend time documenting 1.0 or messing around with it now that 2.0 is on its way. That's causes the community to dissolve quite a bit.

What the TG devs are doing with 2.0, imo, is commendable. They are using a lot of similar functionality (think of Genshi as an improved Kid), but they are working on modularizing and improving all of their components and making sure that TG really embraces WSGI. This was a hard choice, because as you have noticed, it has caused problems with the community, but it is a good choice because a few years from now TG will be much better off and prepared for whatever is thrown at it.

Django would be facing a similar problem if it decided to revamp its current system to meet new standards. I realize that Django is between a rock and a hard place. You want to keep backwards compatibility yet make sure your software is always getting better.

For this reason, I again think that there's nothing 'wrong' with using Django, but if I am guiding devs to what framework they will use, I'll suggest Pylons or what will eventually be TG because those frameworks will just be ready to adapt.

Still, that doesn't mean that new things Django develops can't use existing libraries or standards. My main example is newforms, and I've brought this up many times before. I know it was said that Django's reason for doing a lot of things its own way was because 2 years ago, nothing really existed. But that isn't the case now, so I have to wonder the reasoning behind duplicating functionality that already exists in libraries like FormEncode.

&gt; Do you really think when someone realizes that their templating or ORM isn't powerful enough, the overhead of getting it to work within the framework is really what they're worried about? In a production site, rewriting your thousands of pages in the new templating language, or converting all your queries to use the new ORM is the big deal, NOT calling it from the framework. We're all speaking Python here, it's not hard. And in production sites people don't swap out these components based on the weather or song of the day.

I could be wrong but I don't think that most people writing applications in any Python framework are writing sites with thousands of pages. Python web development hits the sweet spot for small to medium development. And in this case developers are constantly over time developing new applications or maintaining old ones, and between releasing one app and another, there is always new better stuff that comes out that will make their life easier. I'd rather it be that a framework can easily adapt to embrace this new functionality than to have to hack it into the framework.

As an aside to all of this, I will say that if someone is completely new to web programming or Python, I do recommend Django to them. Mainly because flexibility matters much less than good documentation to them.I've responded over and over again, and every time it feels like it's going in one ear and out the other.

You have said, repeatedly, that the Pylons style of maximum component interchangeability is better and that you think all frameworks should come around to that. I have said, repeatedly, that this is a subjective opinion and that not all developers and not all frameworks should be forced to share that opinion -- some people have different goals than you do, and you need to learn to respect that.

You're basically talking to somebody who has a bushel of apples and saying "oranges are so much better, everybody should get oranges instead".

That's bullshit.

And in pretty much every thread recently that's mentioned Django, you've posted long screeds with horror stories about Django and how everybody should stay away from it and use Pylons instead because we're "holding back progress" and we're anti-community and all sorts of other stuff.

That's FUD.

Taken together, it's a lot of propaganda for what is ultimately a subjective apples/oranges type choice. Some people like a framework whose primary goal is infinite component swapping, and you're one of them. That's cool, and I'm willing to concede that there are advantages to that approach. But some other people like a framework whose primary goal is a single set of components honed to work with each other, and I'm one of those. That's also cool, and I'd hope you're willing to concede that are advantages to _that_ approach. But instead I get the same bullshit over and over again.

I like apples and you like oranges. Django is by no means "the" Python framework, nor do I think it should be, and people seem to know that there are a lot of options (why else would we have "which Python framework should I use?" threads?). We have apples _and_ oranges in Python, and people can pick whatever they like and get something that suits them. But you can't seem to live with that -- you don't like the idea that there's a framework out there which doesn't adhere to your philosophy. Django definitely has a philosophy and Django is definitely designed for people who agree with that philosophy, but we don't go around trying to force other frameworks to do things our way. In theory, this means everybody should be able to happily evaluate things and make an informed choice based on their wants and needs. In practice, people come to those threads and see you writing novellas about how awful Django is.

Stop trying to pass off your subjective opinion as absolute truth. Stop trying to feed the BS and the FUD to people who are looking for useful comparisons.  Take a cue from the lead devs of a lot of the frameworks -- all of whom are quite happy to recommend the competition when the competition does something better. And get out in the fresh air more.

Now, as I said, I'm adjourning this discussion until I'm at PyCon and suitably boozed up.3. Rebol 
2. Sadol (created by a friend of mine - http://kewlnet.int.pl/~nooga/sadol/ )
1. Brainfuck, of course.Summary of the article:

1. Write a new domain-specific language for your problem (in Haskell).
2. Solve the problem with your new language.

Where are the "easy steps"?[deleted]"Building robust systems"

Step 1: Don't link to PDF's on the web for browsing.&gt; You have said, repeatedly, that the Pylons style of maximum component interchangeability is better and that you think all frameworks should come around to that. I have said, repeatedly, that this is a subjective opinion and that not all developers and not all frameworks should be forced to share that opinion -- some people have different goals than you do, and you need to learn to respect that.

This whole article describes the distinction that you make and that I don't make.

You believe that there is a difference between 'glue' frameworks and 'full-stack' frameworks. I don't. I think a glue framework is simply one that can have a 'full-stack', focus on a set of defaults, and yet easily adapt to using other components. On the other hand, a monolithic framework won't be able to adapt as easily. So on one hand you've got a framework which provides defaults (this is no different than Django providing a core set of components), but can easily be modified (this is something Django doesn't do well). Your whole argument is almost the idea that because Pylons and TG only have defaults as opposed to more or less 'chosen' components, that those defaults are somehow less integrated or less documented. Its just not true.

Thus I believe you either have flexible frameworks or you have inflexible frameworks. Flexible frameworks can be made to mimic an inflexible framework, but an inflexible framework cannot be the same.

&gt; And in pretty much every thread recently that's mentioned Django, you've posted long screeds with horror stories about Django and how everybody should stay away from it and use Pylons instead because we're "holding back progress" and we're anti-community and all sorts of other stuff.

I do believe that Django is holding back progress. The other frameworks support Python standards like eggs and WSGI. Django has its own 'app' and middleware systems. And as I've shown, look at the numerous libraries that have been supported and created via TG and Pylons. Django just pales in comparison. And I've never claimed Django is anti-community, but now that you bring it up maybe it is. Many of the libraries I just spoke of came from the Pylons and TG communities, not the core development team. As I asked earlier, what has come from the Django community? Virtually all development on the framework itself has come from the core development team. Now just glancing at the difference between the two, it seems to me that one is the healthier open source model.

&gt; And get out in the fresh air more.

That was a nice ad-hominem attack and pretty irrelevant, you spend just as much time replying to these posts as I do writing them. You say I'm not offering useful comparisons, but maybe they just aren't useful to you because they don't commend Django. Maybe I have a problem with wanting frameworks to be flexible. Maybe you have a problem with people who have criticisms against Django.

I don't think it really matters, but stop the cries of 'FUD', 'propganda', 'get some fresh air', because that is no more constructive than actual trolling (if that's what I'm doing).

BTW, I'm not going to PyCon, and you've been claiming for more than a week that you were going to adjourn the discussion. You still haven't, in fact you kicked off the discussion today. I'm not going to make any claims that I'm going to stop advocating other frameworks, because I believe they just offer a better model than Django. That is my opinion and I am as free to express it as you are to either respond to it or ignore it.Java 6 notes: Java 6 has a System.console() that'll take out the two buffer/stream lines. Similarly, it has a Console#printf() that'll simplify those nasty string concats. And you really don't have to use two temporary string variables.&gt; You have said, repeatedly, that the Pylons style of maximum component interchangeability is better and that you think all frameworks should come around to that. I have said, repeatedly, that this is a subjective opinion

And what, he's not allowed to express that opinion or explain why he thinks that way?

Give it a rest.  The article is comparing Python framework design, and his comments are completely relevant.

If Django can't take the tiniest bit of criticism, it must be a very poor framework.
I had to write some library functions in the process.It's important to note that ubernostrum (James Bennett) is just one (significantly vocal and acerbic) person in the Django community and doesn't represent any "official" Django stance on things.

Me, I'm less and less amused by all of these petty arguments and have been trying to think of an eloquent and diplomatic way of saying, "Holy s--t, get a life, people."

AdrianWhy are there so many implementations?That package stuff is important, because the ANSI Common Lisp standard says something to the effect that stuff in the COMMON-LISP package (like *, +, etc) can't be redefined and they're not CLOS generic functions, so they're not user extendable.  So I put stuff in a new package and made new generic functions for arithmetic (that's what the macros "unary-to-generic" and "binary-to-generic" do).  Here's an example session...

    ~/lisp$ cat test.lisp
    (defun f1 (x) (sym-arith::- (sym-arith::+ (sym-arith::*  x x)
                       (sym-arith::* 12 x))
                    7))
    (defun g1 (z) (sym-arith::expt z 3))
    (defun h1 (z) (sym-arith::* z (sym-arith::* z z)))
    ~/lisp$ clisp
    [1]&gt; (load (compile-file "sym_diff.lisp"))
    ;; Compiling file /home/greg/lisp/sym_diff.lisp ...
    WARNING in FUN&lt;-SYM-1 in lines 128..140 :
    variable X is not used.
    Misspelled or missing IGNORE declaration?
    ;; Wrote file /home/greg/lisp/sym_diff.fas
    0 errors, 1 warning
    ;; Loading file /home/greg/lisp/sym_diff.fas ...
    f(x)  = (- (+ (* X X) (* 12 X)) 7)
    
    df/dx = (- (+ (+ (* 1 X) (* X 1)) (+ (* 0 X) 12)) 0)
    
    df/dx = (+ (* 2 X) 12)
    
    df/dx = (* (EXPT X 3) (+ (* 1 (/ 3 X)) (* 0 (LOG X))))
    
    df/dx = (* (EXPT X 3) (/ 3 X))
    
    df/dx = (+ (* 1 (* X X)) (* X (+ (* 1 X) (* X 1))))
    
    df/dx = (+ (* X X) (* X (* 2 X)))
    213
    ;; Loaded file /home/greg/lisp/sym_diff.fas
    T
    [2]&gt; (load (compile-file "test.lisp"))
    ;; Compiling file /home/greg/lisp/test.lisp ...
    ;; Wrote file /home/greg/lisp/test.fas
    0 errors, 0 warnings
    ;; Loading file /home/greg/lisp/test.fas ...
    ;; Loaded file /home/greg/lisp/test.fas
    T
    [3]&gt; (f1 10)
    213
    [4]&gt; (f1 'x)
    (SYM-ARITH::- (SYM-ARITH::+ (SYM-ARITH::* X X) (SYM-ARITH::* 12 X)) 7)
    [5]&gt; (funcall (sym-arith::derivative #'f1) 'x)
    (SYM-ARITH::+ (SYM-ARITH::* 2 X) 12)
    [6]&gt; (funcall (sym-arith::derivative #'f1) 10)
    32
    
Dear People who are making Linux,

Notice how all commands in OS X start with "Cmd" not sometimes "alt" and sometimes "control" with no clear logic behind why? Copy that.Actually, I suspect that, in 2048, 0.9375s will be just about the runtime of the genetic-algorithm-generating strong AI one would feed the system rules he gave into.I don't see the point to argue anymore. I think I proved my point that AICs are closures and you (or other readers) can judge it from the given facts. I never ever objected that AICs are not as easy and concise to use as closures in languages like Ocaml, Haskell or Ruby. But ease of use isn't a necessary part of the definition of closures. Capture of the environment is and this is done with AICs.

If 'break' is part of the lexical environment is questionable, and even if it is, the problem here is non-local-control-transfer. This is also no part of the definition of closures as far as I know. They called closures in Smalltalk 'blocks' because of this difference: A block is a closure with build-in non-local control transfer.

In Ocaml all 'variables' are final so to change something you have to put in in a 'ref'. Like:

    let val = ref 10 in 
       List.map (function a -&gt; val := !val + a) [1; 2; 3];;

This is conceptually identically to using for example an array in Java if you want to mutate a outside variable from inside a closure. 

And I don't think that map, filter etc. would fit that good into Java. This can also be done via iterators. Common cases like the try-catch-finally pattern could also be done by a simple and unproblematic syntax extension like C#s using. Adding more convenient closures to Java would be much more difficult and would require lots of library rewriting. And we would get another different way of doing certain things: Developer would always have to decide if they would use or write an iterator or a map/fold/filter. This could reduce code reduce because of incompatible libraries. In a real fp this is no problem because iterators aren't an option there, but in Java there's much legacy code.

not sure why you're using if str(m) != 'None': in `stripCrap,' &amp;mdash; a simple if m != None: will suffice.gcc supports it just fine. What other compiler is there? :-)Thanks. But I thought that saying: "[Oz supports] (...)  declarative programming, object-oriented programming, constraint programming, and concurrency as part of a coherent whole. For distribution, Mozart provides a true network transparent implementation with support for network awareness, openness, and fault tolerance. Security is upcoming. Mozart is an ideal platform for both general-purpose distributed applications as well as for hard problems requiring sophisticated optimization and inferencing abilities" too would be just too much.Hilariously enough, Google Reader cuts off the post title for this reddit at "Why the software industr."But then this is not like the Haskell code.  With the Haskell code you could have compiled the functions f1, g1, and h1 even before you came up with the idea of sym-arith, and then after the fact you can do the differentiation.
That was the whole point of the article.  If you are allowed to change the operators before you compile the functions then it's not as tricky.
There's the current primary release (1.8), the future primary release (YARV in 1.9), the JVM version, the CLR version, the Parrot version, and another C implementation (Rubinius). So I'd chalk it up as due to a combination of 1.8 having terrible performance, there being a lot of VMs, and Ruby being well-liked enough to inspire a bunch of developers to work on it.You missed some funny parts:

"Hello World 9M
SMC Server 38M
SLVM GUI 60M
Component Manager 160M
TogetherJ 300 - 900M"

That is classic.

"Hello World", somehow they figured out how to turn a one byte (2 for unicode I guess) char into 800k a character.
"By staring at website header photographs of girls. I'm sorry, what were you talking about?"Would this approach have anything to do with:
[Hardware Design and Functional Programming: a Perfect Match](http://programming.reddit.com/info/1519g/comments), and in particular with the Lava HW design language?
Informed opinions welcome :)Hmm, I suppose the code I was presenting is not exactly clear to someone who isn't familiar with the [generic functions](http://www.gigamonkeys.com/book/object-reorientation-generic-functions.html) provided by CLOS.  Because of the quirks of history, the Common Lisp operators, like +,*,... aren't defined as generic functions (open functions that dispatch on the type of its arguments).  It's like the ANSI standard defines (+) as an ad-hoc overloaded function with a signature of `Int-&gt;Int-&gt;Int` and `Double-&gt;Double-&gt;Double`, instead of Haskell's `Num a =&gt; a -&gt; a -&gt; a`.  But, by using defgeneric/defmethod instead of defun, you can create a function in CL with a type signature more like `Num a =&gt; a -&gt; a -&gt; a` (ignoring the usual static/dynamic type system differences).

   So sure, in this very narrow sense, you can't do exactly the same thing as in Haskell.  But you can do the same "*compile the functions f1, g1, and h1 even before you came up with the idea of sym-arith, and then after the fact you can do the differentiation*" idea on anything that has been defined using defgeneric/defmethod.  As another example, now that we've redefined +,*, etc, as generic functions, we can now implement the Haskell equivalent of new instances of Num to do modular arithmetic, array operations, etc.  And we can load these new instances at run time, without touching/recompiling any existing code.

  Or, in other words, I believe that the code does live up to the spirit of Oleg's original challenge, but you have to work around some of CL's historical cruft.  Is that any clearer?
[deleted]First,  I assumed otakucode was exagerting to make a point that a large portion of the population is poor.  Is that a racist statement?  Is it racist for me to say that most people in country X are poor?  It's not a judgment on them as a race?

Second, invention &amp; innovation -- where does it say not capable?  China's role on the world stage was severely crippled by communism, and that effected their ability to invent and innovate.  Do you know how big a player they are going to be when they come into their own?  But it's not going to be tomorrow, they need to get a lot more people through the education system.

And just for the record, I didn't necessarily agree with the conclusion of his post saying that china wasn't going to take off economically.  I was stating that I didn't think that was a racist statement.  I believe he was highlighting large problems facing china that they'll have to overcome.

Just an observation (dont hate the messenger), java isn't totally dead:

"Java ME (MIDP2.0/CLDC1.1)
Market: 1+ billion devices (including MIDP 1.0 devices), almost all new phones are Java ME enabled
Pros: Largest market, free tools (a choice of) and SDKs
Cons: Fragmentation (support of APIs, screen sizes, memory, etc.) causes a lot of work in porting, some native access lacking (and no JNI)"Graphviz is also included in the Ubuntu repos.Damn. I thought Top Ten Moronic Points were reserved for the main reddit...Umm, there are quite a few OSX shortcuts that don't start with cmd, require a keychord that would put emacs to shame, just don't exist, or are application dependent.

Case in point:
Cmd-Ctrl-Shift-3 to copy a screenshot to the clipboard.

Dear People who are making OSX,

Notice how copying a screenshot in Linux only requires Ctrl-Print or Print not Cmd-Ctrl-Shift-3? Copy that.

dup [http://programming.reddit.com/goto?id=oclp](http://programming.reddit.com/goto?id=oclp)Apple keyboards don't have a "print screen" button (nor the completely worthless "screen lock" key either), so they have to use some combination in order to do it. Given that 99% of users never take screen shots, it doesn't have to be an easy one to remember. 

How do you change the volume in Linux? (It's not listed on the page. I presume there's no stand way to do it.) Apple keyboards have dedicated buttons for volume, since 99% of users will use this feature. Laptops have buttons for screen brightness. These are the things you can add when you ditch nearly worthless stuff like "Print Screen."

One of the problems with Linux is that power users insist on optimizing for the least common usage scenario. "Hey, we can't have just one window manager/package manager/modifier key! What about this really obscure usage case that 99% of people will never run across? We have to make sure that works perfectly, even if it would make the system more elegant to omit it."

Fortunately, this situation is shifting somewhat as people come to see the beauty of elegant design (eg. Firefox, Ubuntu), but the power users continue kicking and screaming the whole way…&gt; Ruby is an imperative latent-typed object-oriented language that is terribly slow

Yes, and at one time Lisp and OCaml was slower than C. And at one time C was slower than Assembly. Comparing the merits of languages on performance doesn't get you anywhere.

And I don't see how an imperative, OOP, slow language can't be functional without additional evidence. Nor do I understand the seemingly contradicting conclusion later made that Ruby is not mulit-paradigm despite the fact that it is imperative and OOP.That's the extra Enterprisy Goodness™. No other language can bring that kind of Enterprise per character to the game as Java.

:)

To be fair, if I'm reading this "ps" output right, just running "python" and doing nothing at the REPL uses 2704KB of RSS and 4720KB of VSZ RAM. I've never really worked out exactly what that translates to in real memory consumption (as I have other long-running python processes already and I know _some_ of that is shared with those processes), but it's not all that far from a 9MB memory footprint.

Further edit: I just noticed they talked about Python in the article. I'm not sure how to reconcile their quote of 1.6MB using the same metric as the 9MB Java image (presumably). Maybe they just count RSS. (Like I said, I've never worked out what those are. I've read many explanations of them and it seems like no two say the same thing.)Seriously, there is someone who hasn't used graphviz for years?Which the author said.Possibly paranoia about comparisons with NULL, brought over from SQL?  Could be.Who cares IF IT IS GOOD ENOUGH!

Seriously, I know python is slow.   However nobody is running python on a 1.67mhz Atari 400 with 8k of RAM.   We are running python on minimum 1ghz machines with minimum of 128 megabytes of ram.    

Sometimes python is too slow for the task at hand, but in most cases you are better off just buying a faster computer.  If a faster computer won't solve the problem, then even the best C won't be fast enough, and you need to figure out how to make your program run across several computers.

To be honest though, most of what people do with python would be fast enough on a 386 pulled from a dumpster.Doesn't support pipelines. That'd be an irritant.Yup. Thanks to the Bentley books, I was vaguely aware of batch drawing tools, but the Unices that I had access to didn't seem to have them (more like I didn't know what I was looking for). gd and ImageMagick were too low level, Tcl/Tk didn't draw into an off-screen buffer and that horrible X-Windows based vector drawing program (can't remember the name) was just *awful*. I wound up writing raw PostScript.

Kinda' the same way I thought that Lisp was only for torturing AutoCAD users until Sr. Graham changed my mind... ;-)I don't think you guys are talking about the same kind of 'leaks'. It is true lazy evaluation can bunch up memory usage towards the end of a computation when everything becomes needed, but is that really the same as when a C program fails to free uneeded memory again and gain, resulting in a conventional memory leak?I just think that the much larger history buffer is great.

If you're just interested in better performance though, check out [parallel bzip2](http://compression.ca/pbzip2/).[deleted]If you can't even be bothered to check your information, why should anyone pay any attention to you? Python uses reference counting with cycle detection - a reasonable GC, if not exactly the bleeding edge.starting??  the fun part in this instance is that its not aol users but rather people who are getting bored of myspaceMaybe the likelihood of switching is directly tied into how much pain the developer experiences working with their current platform.    

I came from a Perl background, doing web applications with the CGI library for years, so having Rails manage my ORM and form parameters automatically, well, that was kind of nice.[deleted]It'd be nice to hack in in-memory support so that rzip plays nice with pipes, but uses all that much more memory to do so.I didn't have the patience to read all the bug comments, so maybe this was already suggested: you could make SRV and A record queries in parallel.  In the currently-general case, you'd be using as much bandwidth as what you described (which is still negligible IMO), and this approach could be modified in the future if SRV for the WWW catches on.Obligatorily: [LZMA](http://en.wikipedia.org/wiki/LZMA)!

Not only does it compress better than `bzip2`, but even with with high compression, its decompression stays fast/cheap (closer to `gzip` than `bzip2`).

(There's also [lrzip](http://ck.kolivas.org/apps/lrzip/README), a version of `rzip` that uses LZMA instead of `bzip2` as the backend.)Note the tag on the end of the URL. :)But how long did it take him to specify those rules? That's the important metric.There's no date on this.  When was this memo written?rzip is great for big log files you don't need frequent access to and to disk images with significant amounts of uncompressed content, but rzip is much slower than bzip2 and for many common tasks does not compress much better.  Be careful with the `-1` through `-9` flags - those tell it about how much memory to use at a time, `-9` being 900MB.It would be cool if you added the little reddit button things next to every link so we could vote on them at your site.It's the eternal cycle of the inter-tubes.

New, fun, popular, interesting site comes up that is lovingly reared by a group of devoted followers.  Some more people come, get acculturated and begin contributing.  **A lot** of people come and shit things up.

Initial group leaves to find a new site.&gt; All orders are shipped via United Parcel Service (UPS). We currently offer Next Day Air, Second Day Air, and Ground services. As of January 2007, we now offer shipping to Alaska, Hawaii, Australia, and Europe.

Canada -&gt; shaftedA wikipedia article that more accurately describes the situation on reddit: [Self-fulfilling prophecy](http://en.wikipedia.org/wiki/Self-fulfilling_prophecy).

The more stories I see about how reddit is starting to suck, the more I believe it.Huh, I like that. I'll make use of this analogy... someday. ;)Yay YARV! Here's hoping it's enough to silence the "Ruby is too slow and thus doomed" crowd.

Yeah, a futile hope, I know...hahaha.  Part of the problem is that we'll start seeing more dupes and stuff with the increased number of people.  So what do you do?  Submit a dupe :)By that logic, the entire hoard of J2EE devs should be clamoring at the gate. While I suspect that the ability to recognize a better environment is also a factor, it still doesn't account for all the PHP devs switching. *rimshot*I'm in the market for such a new site. Do you know of one?Your team is writing dirty code? Be a dictator and make them write neat and clean code using this desktop based weapon system!I find articles that lament about how bad a site is on the same site pretty humorous.

I mean if these articles are voted up, apparently a lot of people agree that the site is getting bad, but it is probably these exact same people who are making the site get worse.

Nobody ever realizes they might be part of the problem. Right off the bat here I can see a few 'fluff' comments on this article that while they've always been okay at Digg, they almost were always regularly voted down on Reddit in the past. Its little things like this which have led to a slow decline in Reddit's quality over the last year.&gt; Python uses reference counting

Same with Perl, which was fast enough to be usable for a wide breadth of work back in ye olden tymes of '98.[removed]I'll double check. I seem to remember it wasn't working when I did that until I added str(). That was when I was running everything on my laptop, if that makes a difference (Windows)&gt; Be careful [...] -9 being 900MB.

That's kind of the point.They did.. [until last september](http://programming.reddit.com/info/y98u/comments/cyd92), actually.Do you want to invest in one?This whole article smacks of authorial laziness. Aside from the utter lack of novelty here, it's chock full of stupid errors (and I don't mean the intentional, instructive ones) and lousy explanations.Change happens, systems that deny change will die.Heh. No, it's not worth the money; the value in it disappears after a year or two. And, even if it were worth the money, I don't have that kind of money. No, I'd rather just participate in one.Can I get a commitment to participating out of you?

I mean, if graham thinks now's a good time to [make a new one](http://reddit.com/goto?id=15gkq) then I'd say it's worth a try.Here's a guy calling himself the Usability guru and he doesn't put a publication date onto his news items. Took me lots of scrolling up and down the page to find out that his article was published some time in 2007. Might be useful to let people know when an article was published so we can figure out if it is relevant to us?It seems like this isn't something that many people have to deal with, so I'm curious what reddit hackers might have to say.

I'm working on a system where I would have product1.domain.tld, product2.domain.tld, product3.domain.tld, etc.

Each product might be built upon a completely distinct set of languages / databases.

The important thing is that users can move among the products easily without having to register a new account or even login twice. Something like what Google does.

This is what I was recently looking for. I wanted the users to be able to [update: create their own accounts] and use the same basic Apache login credentials for a series of Trac environments, where they would be able to file new bugs.

I've found some free PHP scripts that would do the job by keeping the user base in a mysql db and sync it with the htpasswd file. But none of the free ones satisfied me, so I decided to write one myself using mod_python and sqlite ;-) I haven't finished with it yet.

Also, do not forget openID.
Says it in the first paragraph.'m is not None' would be more pythonic.Oh shit.  The AOL mother ship has landed...&gt; Also, do not forget openID.

OpenID is definitely interesting, but in my case (among trusted sites) I don't think it really buys me anything. I could pretty easily authenticate users from one product site against another through a behind-the-scenes API. I really want single sign-on (in both senses) and a way of sharing my user database intelligently.

I'm aiming for something Google-like.
Got your point. I have no such account management system in mind, but I would definitely be interested in a Google-like one. I'll keep watching this topic and hope that someone has a good recommendation.Reddit has received funding from the NSA. That link was removed. The post regarding that link was removed. My comment will be removed.
Reddit is part of a large umbrella project of the NSA gauging people's opinion on all matters politic, by performing vast data collection and mining on the web.I'd have to have more details. Basically, my ideal site is Reddit in fall of 2005: Lisp-obsessed, somewhat startup-interested, definitely PG-interested, but also an educated take on general news. Programming reddit fulfills part of that need, except that I need to hide all articles with Haskell in them because I hate typecheckers. The NYT also helps. I guess the YC startup news could also serve part of it, but purely startup news is peripheral enough to my current interests that I'd be wary of committing to participate regularly in that particular site. 

An Arc community site might do that, once Arc is released, which seems somewhat less unlikely than it used to.I agree. People should be far more proactive, grumpy even, in their intolerance for posts/comments that are useless. User-produced content has to go hand in hand with user-enforced standards.But if you think about it, need it be that way?

Two possibilities come to mind:

- as long as you can keep the rate of change of membership below a certain threshold (perhaps proportional to the total active membership), you can keep the acculturation.

- perhaps the majority of people are "just like that"

Testing #1 might be a fun thing to do for someone building a social site. It's basically a way of controlling your demographic, you don't want more than X% of your userbase having less than Y months on the site. (This feels very familiar to that recent reddit post suggesting populations with a high proportion of young people were more warlike).

It's worth remembering that Usenet had weathered all previous Septembers.Does it make sense to use one central database for all of the different applications?

If not, I would use one database as the central login authority and replicate the user table to all of the product databases. Writes are made to the central database, reads from the local databases.

I admit, I'm new reddit user. I joined both Digg and Reddit the same day, about two months ago, and now I only visit one of them. (I mean, how could I resist the beady eyed red alien in the corner who silently cheered me on with his perpetual gaze?)

The biggest problem for newbies like us is we do what comes naturally. We see something cool, vote up. See something stupid, vote down. See a poll? We vote without thinking a second about it. It's as simple as that right? It took me a while to figure out that there's more to it then that. How many newbs like me are going to actually read comments and take the time to learn? Probably not too many. With reddit gaining more popularity, newbies have to be educated otherwise they're probably going to do what I used to do -- vote for what they think is cool, and not really care about the intellectual articles of the site.Oh yeah, first thing I do in a new codebase, if it doesn't have doxygen already - make a browsable set. So nice.&gt; Why are there so many implementations?

Well, Ruby exists because matz wanted something to execute his Ruby programs.  JRuby exists because someone wanted the same for Ruby-on-Java.  Likewise with .NET.  Cardinal has a similar rationale, to host Ruby on Parrot.  Rubinius and YARV both sprung into existence as, AIUI, parallel efforts to write a better Ruby system than matz's 1.8

Ultimately, they exist because of limitations in the core ruby distribution (other languages have systems that target many of these different platforms from the same codebase) and because of a willingness in the Ruby community to press on with things.

These answers shouldn't surprise you -- 'JRuby' and 'Ruby .NET' give some of it away, and so also the relative obviousness of the reason for these implementations makes your unqualified question sound contemptuous.Actually sharing the same database/schema is definitely ideal, but it is more restricting for the products -- they have to support some foreign user database and schema. Not a problem when starting them from scratch, but it makes integrating existing applications harder (they have to be partially re-written).

I like the replicating method, but one problem I've had here is that the updates pretty much have to be synchronous for the user to be able to immediately use their account on slave sites. There are some tricks to optimize this (sync to slave sites they're actually using right now first), but it gets fragile when there are a lot of slave sites.

Thanks for your input, it's definitely helpful.That's just how atomic instructions work.  Assert memory bus lock and change value; any change in memory causes cache line invalidation for any CPU which had cached the value.
At first, I thought it was EclipseWake me up when September ends.Thanks.Spaf seemed to predict the downfall of Usenet very well, or at least leave at the right time.

http://spaf.cerias.purdue.edu/~spaf/farewellThis is some form of karma whoring I have never seen before&gt; Of course Ben claims that will all change with Perl 5.0, but to me that remains to be seen and I'd rather go with a bird in the hand (i.e. one with a lot more active current user base) than a bird in the bush.  But who knows, they say you should learn a new language every year; at any rate if he's right maybe I'll try and pick up Perl 5.0 in around 2012. :)

Why would he wait 5 years to use Perl 5.0?Yeah, I'll sometimes like to submit old content because it's useful and maybe "hidden" from users. This media is a great way to find old interesting stuff also.

And I can use reddit to collect my bookmarks ;)what link? Feel free to send it to me as a personal message, and me &amp; my dupes will help you spread it again. 

Why fund? They can gauge just by looking at votes, like everyone else.On the other hand I think people do now understand your POV. You don't really make yourself a favor by insisting like you do.&gt; When it comes to programming on the modern-day GUI (post-DOS) platform, the vast majority of my coding has been, in order of experience, using T-SQL, VBScript in ASP, and about equal parts classic VB (v3.0 to v6.0) and VB.NET.

Great gibbering ghosts! Why?!Ah well, Jakob should have published it five years ago. This is old news here in 2007.This is a nice find. I have just glanced at it. It looks more detailed than any other ocaml tutorial I have seen, including going under the hood quite a bit and comparing it to other languages.

There's absolutely no mention of module signatures or functors. However, it could still be very useful.It would probably also recognize that you had an unstructured dataset and offer, clippy-style, to come up with the correct rule to organize it.I didn't read the article, but did search it for the text 'valgrind' and didn't find it.

If you're on Linux and suspect a memory problem (leak, corruption, double free, bad ptr, etc) then this should be the first spanner out of the box.


OK, gone back and skimmed the article a bit.

- uses new and delete in an article about memory leaks without touching on exception handling (just because you delete later in the same function doesn't mean the delete gets run)

- talks about assigning to uninitialised ptrs, something gcc will happily and reliably warn about

- correct in that if a function which frees or allocs it's args or return value needs to be obvious. But rather then cheesy comment blocks, it is quite nice to have c-style ctor ad dtors, e.g.

Foo *create_foo(params needed for a foo);

void destroy_foo(Foo *foo);
Django is a very polished, easy framework to get going with, but it becomes very clear early on into using it that it is highly focused on content management. As such I would consider it a (far superior) successor to Zope (with Ellington a non-free Plone). This is not a criticism: Zope hit the sweet spot for content management a few years back, but was hampered by poor design decisions such as ZODB, Z-Classes etc. Zope 3 is wayyyy too complex and over-designed, reminiscent of J2EE (without Java's libraries, IDEs, community etc). So Django is really Zope 3, as it should have been (or even Zope 4 ?).

Now, Zope was cutting edge at the time (it was written back in '96) but ended up a ghetto outside the Python community, to everyone's detriment. This was largely because the Zope people had their own way of doing things, which meant libraries developed by the Python community had to be Zopified (with a Z-prefix, of course) in order to work inside Zope. Only now, with Zope 3, do we see Zope and Python being reconciled, but the web application world has moved on and Zope 3 is largely ignored (not in small measure helped by Zope 3 team's anti-marketing). Another problem was that people came to Zope first and Python second, and had a limited understanding of Python and its greater possibilities outside the Zope world. We see this happening now with people coming to Rails and Django. For example, I have been doing Rails development professionally for over a year now, and have yet to write a single line of Ruby outside of a Rails context. As a result, my Ruby skills are more limited than my Python skills, which I have put to use in many contexts, web development being only one of them.

Python web app development is slowly grouping around standards, like Paste, SQLAlchemy, WSGI etc. This feels very much like the way Java went a few years ago, with standards like the servlet API and JDBC, but with Python simplicity instead of Java complexity. This is a much more interesting development than the next Zope, however good it may be, and indeed more interesting even than Rails. With a single Python meta-framework as a base, we can develop templates for all conceivable needs, so you could choose one template for your news site, another for your AJAX project tool, another for your online auction app, etc. Furthermore, such a template does not limit your choices should demands change down the line, whether those be demands from clients or a better ORM, template engine, form processor etc that may exist in six months or a year from now.

In other words, you can have the cake and eat it too : Django could still be 100% Django on the surface, same documentation, same template engine, same ORM, same admin app etc. Underneath, it would be a WSGI, SQLAlchemy, Paste-created template, which more experienced developers can customize and tinker with to their hearts' content. Rather than a three-way battle between Django, TG and Pylons, we have a shopping list (probably a subset of Cheeseshop) where you choose the template best suited to your needs. Now that's real power.I saw a linking page dated February 2003.

[Pages linking to this article.](http://www.google.co.uk/search?as_lq=http%3A%2F%2Fwww.internalmemos.com%2Fmemos%2Fmemodetails.php%3Fmemo_id%3D1321&amp;btnG=Search)&gt; How do you change the volume in Linux?

If your keyboard has volume controls, you use those. Same with brightness. Most non-Apple laptops have both of them.That works! Thanks....Now try accessing some SOAP web services and throwing the resulting multilingual data in a relational database. Then you will seriously consider Scala or F#, instead.Yeah:

http://news.ycombinator.com/You can do this part muchbetter:

    n = 0
    while n &lt; len(firstlinks):
        m = exlinks.match(str(firstlinks[n]))
        if str(m) != 'None':
            #we found a match...add it to the new list
            secondlinks.append(firstlinks[n])
        n = n + 1
    return secondlinks

by doing this:

    for f in firstlinks:
        m = exlinks.match(f)
        if m is not None:
            #we found a match...add it to the new list
            secondlinks.append(f)
    return secondlinks

or even:

    return [f for f in firstlinks if exlinks.match(f) is not None]
[q=reddit+sucks](http://reddit.com/search?q=reddit+sucks)

Reddit sucked, Reddit sucks, Reddit will suck again.

[*Oh, look, there's that article again ...*](http://reddit.com/info/ehcj/comments)If you can support linked databases and triggers, you can have the insert or update immediately send the change to the other servers. Unless you are dealing with insanely high loads on your user table, it shouldn't be a significant performance hit. [Insert Standard Disclaimer "Make sure you load test" here]

*******

If possible, I would perfer to use dual writes and batch updating every five minutes or so. With batch updating, it is easier to just keep a list of target databases and when they were last updated.

I call it "dual writes" because inserts and updates occur on both the central database and the slave database the user is currently using. This gives the user immediate access to the site while ensuring the central database always has the current data.

You need to put a disclaimer that says it can take up to N minutes for their changes to appear in the other products. As long as you do the batch updates at least every (N/2) minutes, there shouldn't be a problem.

Users shouldn't be bothered by a N minute delay unless you expect new users to try out several different products within N minutes of first registering. 

*****

P.S. I am really enjoying this discussion. I think of these kinds of issues are far more interesting than the constant barrage of "my language is cooler" we normally see on reddit.Hi,

I'm the author of the Cobra programming language and here is my response to the numerous comments above:

The "what's new" in Cobra is not the individual elements such as contracts, classes, etc. It's the combination of everything that goes into it. Let me 'splain. No, there is too much. Let me sum up:

Right now, if you want a language with contracts, what do you use? Eiffel or D. What if you want static *and* dynamic binding? Objective-C or Boo. What if you want expressiveness for quick coding? Python, Ruby, Smalltalk, etc. What if you want runtime performance? C#, Java, C++, etc.

What if you want ALL of those? ... You're out of luck! And that's frustrating to me because none of those productivity-boosting characteristics are incompatible with each other. I don't want to choose between C#'s speed, Python's expressiveness or Eiffel's contracts. I want it all. And I'm going to have it through Cobra.

Regarding "there's nothing new here", Cobra was never intended to be experimental. It's a practical synthesis of already-proven features that are currently scattered across multiple languages.

Now for the other specific objections raised:

Re: the comment about how it is half finished and will never be finished: Yes, it's partially complete as of today, *but* it will get finished. The next release is in 2-3 weeks and there will be subsequent releases after that. In addition to my obsession with creating this language, leveraging .NET makes it feasible to complete. I don't have to build the standard library, the gc, the machine code generator, etc.

Re: the thought that no one will use this because it's not MS or C# (or Sun or Java or whoever), that's the same FUD slung at every new language. When C# came along: "Why bother? Use Java." When Java came along: "Why bother? Use C++." When C++ came along: "Why bother? Just use C and structure your code." Also, because Cobra leverages .NET, C# and VB programmers already know the standard library. Even non-C# &amp; VB programmers will be able to leverage C# and VB articles when trying to solve a Cobra issue ("How do I send email?" for example).

Re: "it's not an MS language" there are plenty of non-MS languages on the TIOBE Programming Community Index that are popular. Plus MS could "buy Cobra out" like with IronPython. Plus I have no qualms with a "Cobra for Java" or "Cobra for Mac". And those would be quicker to crank out than the original after finishing the design, lexer, parser, error checker, docs, etc. that I'm currently working on. But I won't start work on other editions of Cobra until the .NET edition is fully mature (including a VS plugin).

Re: "C# is the only choice for .NET", Cobra generates C# classes, interfaces, etc. It's the same "stuff" and the same speed. But Cobra also has more "stuff" like contracts, compile time null checks, etc., none of which are found in C#, nor planned for the next version of C#.

Re: Boo, when I found it, I thought that I might be able to skip building Cobra. But there were enough differences between what I wanted and what it offered to justify building Cobra anyway. And not being able to keep in touch with Boo's author didn't help. I'll have to write up a comparison at some point (like I did with Python) to give full details. (Btw Boo is not news to me as evidenced by the [Credits](http://cobralang.com/docs/credits/).)

Re: "just make IronPython better", IronPython's aim is to be as compatible with Python as possible. That precludes adding features or changing semantics for the sake of improvement.

Finishing up:

At the very least, Cobra is useful to anyone who wants the combination of language level productivity boosters that it will offer. And at the very least, that includes me. Likely it will include many others even if the numbers don't reach Java-popularity-proportions.

And beyond that, I'm excited by:
(1) what software development will be like upon finally having all those features in a fully mature form under one hood, and
(2) where Cobra will lead beyond that (new experimental features, Cobra for Mac, Cobra on Rails, etc.).

Thanks for your comments, both positive and negative. All were very energizing!

Best regards,

-Chuck
[CobraLang.com](http://CobraLang.com/)
[removed]Possibly because my circumstances and job responsibilities have been different than yours?You are taking things to seriously. It is mostly a self deprecating joke. :)&gt; Nice theory. Any evidence? Studies?

Your comment reveals ignorance of the subject.  Please, educate yourself.  I suggest you search with the keywords "cache", "cache coherency", "cache coherency protocol", "MESI", "MSI", "MOESI".I spent a year memorizing hundreds of rules to extract traditional four-part choral arrangements from melodies. Are they saying it was no use!?

Oh. I rather thought so myself, actually.Do social sites need to become more like the NYC nightlife scene?  A cool spot opens up, stays cool for a while, and then is shut down by the owners as it begins to lose its appeal.  The owners then open at a new address, with a new name and a new look.  It's not a perfect analogy, but is there anything to be learned from it?Yes, thanks.  So you can do the same in Lisp if you have enough forethought and change the standard operators to be the generic ones.
Out of curiosity, is that how people usually write CL?
Evidently. Fair enough. Let me be among those to recommend PHP, Ruby, or Python for personal projects.&gt; This is some form of karma whoring I have never seen before

Thank you for this excellent example of what this 'Eternal September' misses: the also damaging -puerile backlash- against an influx of newbies, wherein seven-year-olds sneer at five-year-olds, but still haven't learned to hold seven-year-olds in contempt.  This backlash is where you'll find plonk-as-'fuck you', and quickness in decrying 'trolls', and hand-wringingly upset drives to annihilate the culture of the adults who -- horror of horrors! -- don't always exhibit respect or concern for the inner feelings of those members of the newbie influx who still haven't realized that they'd do better to pretend that it is October.Beautiful. I was a little nervous about posting my code but I'm glad I did now.

EDIT: Works great, of course. I used the verbose example for now...Ah the irony.This is a horrible analogy.  In the first place usenet is a discussion board -- an UNMODERATED forum.  Both digg and reddit are moderated.   Forums are also more prone to disruption as flame fests and troll wars break out, digg and redit simply bump up interesting urls, the conversation is strictly optional, reddit doesn't even apply it to your karma.

Sure different kinds of stories churn up as people come and go but big friggen deal.  Digg and Reddit are parties, eventually people you know move on, new people come in, some you like, some you don't, maybe you clique with the new people, maybe you don't.  The party goes on.

This is nothing more than the old timer's tellin people to get off their lawn.

Neither Digg nor Reddit are going anywhere because they have found a niche that they serve well.  YOU might be moving on but there's always one or two newbies streaming in after you.

[removed]I know (a little) about these, but i'm not sure wether cache misses are the main reason locks are slow.

If the lock is physically close to the data it guards, it could be well with the same cache line and thus doesn't really slow it down. Spinlocks can be written, so they don't write on every execution, so the cache won't get dirty all the time.

I can think of more reasons than cache misses, which would slow locks down: additional data, non-blocked loops (in case of spinlocks), code overhead, CPU migration costs

Do you have any links, wether the cache is really the blocker with locks?For people like me who have evidently had too little coffee in the morning; if you encounter this error:
&gt; Failed to map buffer in rzip_fd

, be absolutely sure the file you're trying to compress is not a directory. D'oh!I'm excited about this development effort.  Combining good elements from multiple languages into one is a move in the right direction.

Its easy to be a critic.  If the person who starts the effort is committed to it (and maybe with a little encouragement) the project will be completed.  

I know I'm going to at least give this a try when its more mature.You can smooth this:

 1. read value (non-atomic)
 2. if set: goto 1.
    else: read'n'set (atomic)
 3. if set: goto 1.
    else: proceed in locked section

Most of the time the lock will loop with the non-atomic read and thus not make the cache line dirty.If you're after the most horizon broadening web-app programming experience try Smalltalk + Seaside. For commercial work I'm not so sure but it's thought provoking to say the least and it seems faster than Rails (though I haven't done any benchmarks).

Rails and Django are great pieces of software I'd recommend to anyone, they feel a lot better to work in than VB.NET + ASP.NET especially on dynamic content.&gt; Ah the irony.

Please share it with the rest of us.The list comprehension, as it's called, is by far the most pythonic. It's got its origin in Haskell and is inspired by the mathematical set notation.

Start using it today, imho it's one of python's best features.velco is talking about cache synchronisation between multiple CPUs, not about cache misses on a single CPU.

Doing multi-threaded programming on a single CPU is a sure fire way to fool yourself you've got working code when you haven't.dude, chill

1. his comment has nothing to do with the article
2. if he's spamming it's not for google rank, a product, or some stupid chain letter
3. a quick glance at his message history shows he's never done this before, all of his posts are in the programming sub.reddit

So yeah, pardon me for being surprised. And I hardly think I was being caustic about it. 

Unless you were being self-referentially ironic, in which case, yeah, hilarious.Oh please. I learned so much more from the original article than from your standard "use locks" introduction to multithreading. Regarding the technique with memory barriers ... is it unportable? The article says so all the time, goes into the specifics of several processor architectures and what the do and don't guarantee. ... is it unsafe? Right at the beginning there's a warning that you have to arrange your reads and writes in an exact order, and the rest shows how tricky it is to place the barriers correctly.

So the article did *not* say "Do not try this at home", a warning that might have been appropriate for readers of age 8 and younger. The rest of us learned some interesting things on multithreading at the hardware/OS level.

To put it another way, should introductory articles on x86 assembler begin with a similar disclaimer that whatever problem you have is likely much more easily solved in Ruby?So even if AICs are in practice unusable as replacement for closures, you are satisfied with them because semantically they are closures (I happen to disagree, but let's ignore that for a moment)? Well, that seems to be another crucial difference between us: I have to code in Java right NOW, useless theories have zero value for me.&gt; So yeah, pardon me

No.

Someone posts a joke and you 'surprisedly' observe that it, rather than being a joke by someone within a community, is rather a wholly insincere attempt to 'whore karma'.  I find this distasteful...that social groups naturally start, grow, reach critical mass and then start to decline and fragment...

grasshopper.I've made a small benchmark, which demonstrates the bounce costs:
http://programming.reddit.com/info/15kcf/comments
you're a hypocrite wrapped up in a hyperbole.
continue this at your pleasure, i feel no more need to respond.I hoped somebody would notice.I recently had to implement a single-login system for a bunch of internal websites and a trac system.

I did the following:

1) Wrote a custom authn module for apache in C. All this module does is connect to a local UNIX domain socket, send the credentials and wait for either an OK or FAIL. Unix domain sockets are very fast and very cheap to create/destroy, so being able to do this per request rather than mucking around with persisting data in apache in C was very useful.

Trac uses HTTP basic auth, so once apache OK's the login, trac then looks up the user in it's local sqlite database.

2) Wrote a service in ruby to listen on this domain socket. The service acts as a caching relay to connect to the real auth server on another box via web-services. The cache is just a simple in-memory 5 minute sliding expiry cache, simply because remote TCP connections using SOAP is expensive.

3) Wrote a pseudo-plugin for the above service so that whenever a user authenticated, it would check if they existed in the trac sqlite db, and if not create them.

Lessons learned which may be useful/interesting:

* This was obviously a big nasty hack and worked only for our specific system, but our specific system was big/important enough to warrant the custom code.

* Keeping the apache module as simple as possible and using ruby for the rest meant that the entire project took less than 2 weeks including full unit tests etc. 

* The apache module connecting to a UNIX domain socket model seems to work very well as it allows you a full speed native C apache module

* APXS (the apache module building system) is great. The 'how do we compile crazy apache modules to match the C compiler/runtime/CFLAGS/etc/etc' problem is reduced to just typing "apxs -c -i my_authn_module.c"

* The 'quick hack' nature of the project could be easily updated and made nicer in future just by cleaning up and formalizing the ruby socket-listening service. It wouldn't be too hard to make it support openID for example.That's a great point, actually. Any ideas one how to do that? I'm arrogant enough to hope that lots of new users will see this article, reflect on their behavior, and shape up - but that seems unlikely.Oh one other thing. The users still have to log into trac seperately from the 'main' system, because browsers do not share HTTP authentication sessions across sites, however the username/password syncing is done, and being able to tell people 'You will always have the same username and password across all the sites, always' has seemed to be the main hurdle as far as users are concerned, they really haven't seemed to care at all about having to 'log in' to each site.Post quality decline is not a fatality: http://thejordianapathist.infogami.com/a_message_to_fellow_redditors

Reddit has something that no forum (or usenet group, or anything) has ever had in the past: personalized recommendations through user clustering. People don't seem to think it's a big deal, but it can make a huge difference: if you start to use it, soon you won't be bothered by the contributions of people you don't like, since you won't even notice their existence.&gt; i feel no more need to respond.

Why did you feel only this much need?All Reddit need to do is borrow a page from Metafilter's model and require some sort of a reasonable payment for an account (ex. $5).  And for the current user base, grandfather them in. :)This is an obvious dupe, of course - check out the tag on the URL. =D

On a related note, is there a statute of limitations on dupes? That post (which is where I first heard about the "Eternal September" concept, incidentally) was 6 months ago, long before reddit was swamped with newbs and the associated polls and coolest pictures ever.Check my earlier [post](http://programming.reddit.com/info/15j08/comments/c15ke1).It's funny and sad that we keep seeing it, and I don't think it had to occur here at reddit. Clay Shirky writes of it [here](http://www.shirky.com/writings/group_enemy.html), which was reprinted in Joel Spolsky's Best Software Writing.

Basically, you need something of a constitution, or things go to hell. Too many (equally empowered) people -&gt; quality drops. I'm a little disappointed that Reddit wasn't a bit more farsighted about this, doing something like using some kind of karma regulator (if the 'original' redditors were better, giving them a lead, and also having them have a greater influence on who gained influence). I know it's not really egalitarian, but I'm not looking for MSN news.

Now it's too late for such measures, as with a retarded poll anyone can get a pile of karma.

Oh well. The programming sub-reddit is still good, but I miss the front page. Comments are still okay, but dropping quickly.Some proprietary compilers.
Heh, I already did, and upmodded it. That could work.From the article: "Scala isn’t distributed under a fascist asshole corporate-greed license."

On the other hand, F# isn't promoted by free software jihadists.
While I'm just giving stuff away, why not sell Reddit to the community?  Buy in, get paid slowly with the growth of the userbase, which will only recede if the community sours.  Cede all other decisions to the staff.Damn it, why is this on the programming subreddit? ah well one crappy post in a blue moon isn't bad.I think the problem we're seeing is that a lot of lowest common denominator stuff is getting through. No one really thinks it great but a good proportion of people don't think it's *that* bad. That, I think, is why Reddit used to work a lot better (for me at least, and I assume for others), it used to be that the Reddit user base was much more closely aligned in interests with each other. The programming sub-reddit seems to be working quite nicely for (I would assume) the same reason.Oh yes!  Please kill Herl in bioinformatics!
I think he's saying that people might confuse this with gzip compression flags. I thought it was a fair point.&gt; fascist asshole corporate-greed

Fascism and corporatism go poorly together.  (Seeing this admittedly takes a few steps of reasoning.)

&gt; free software jihadists

Free-as-in-libre and jihadists go poorly together :-)

Morever, you dropped an adjective in this reply -- did you think that 'software' paralleled 'asshole' well enough?  Or that 'fascist asshole' only counted for one?I would have gotten away with it too, if it hadn't been for those darn kids![removed]*comment about the typo removed because Zyroth has included the fix for it*http://programming.reddit.com/info/15kic/commentsSo true.

http://programming.reddit.com/info/15kic/commentsSorry if you saw this before:)

http://programming.reddit.com/info/15kic/commentsFriend me?spam, downvoted

i never get to read any more interesting articles here, i might have to find another 'whats new on line' kinda site
:s&gt; more users =&gt; more new users =&gt; more average users =&gt; average users have the same mindset and follow the crowd .. Eternal September follows. (too many fools can vote)

Only if the fools watch and vote on the *new* page. If 20 to 50 of the "intelligent" crowd were watching the new page, most of the crap would not get past the new page.[A Group Is Its Own Worst Enemy](http://reddit.com/info/15kjl/comments) is more viable explanation.
I hadn't, but I think I wouldn't've minded :-)  Thanks!...or before you go goOh, are confused too with the tab-based editing area ? Or is it the tree view of a filesystem on the left panel ? I hate this kind of useless gadgetry. Let's stick with notepad forever ![removed]I find it's enough to see how many of the front page links are simple e-mail forwarding fodder these days.[deleted]It is very humorous.
It seems to be the most common article subject now. "Reddit's just not the same, now that all these &lt;sneer&gt; new people are using it".
If you don't like an article, vote it down and reload the page. See, it's gone now! How easy is that. In addition, your downvote will help ensure that others don't have to see it.
If you really don't like reddit anymore, don't use it. The rest of us won't miss your complaining!
Use [OCamlNet](http://sourceforge.net/projects/ocamlnet) for transport, [OC-SOAP](http://merjis.com/developers/oc-soap) (if you prefer CDuce) or [OCaml-SOAP](http://caml.inria.fr/pub/ocaml-soap/), and [OCaml-FreeTDS](http://kenn.frap.net/ocaml-freetds/) to query your SQL Server.

Really, there's [poison aplenty](http://caml.inria.fr//cgi-bin/hump.en.cgi) should you need or want it.I've been wary of Nielsen in the past. Too often I find myself agreeing with some of his points, and strongly disagreeing with others. But this list is pretty much spot on. In particular, fixed font sizes and opening new windows are increasingly problematic.Thanks =)"Insane" isn't the right word - "not useful in the current climate" is better, seeing as most people send XHTML as text/html which means it isn't even parsed as XML in the first place (and sending XHTML as XML causes big problems with IE).  See [blatant self promotion](http://www.markng.co.uk/template/article/why_not_xhtml) and [Ian Hickson](http://hixie.ch/advocacy/xhtml) for why.Actually, cycle detection has linear time complexity to the sum of references and variables.[deleted]It certainly saves typing. I'm reading up on it...frankly, I've never heard of it before...To me it's evident that heavy optimizations must consider the actual hardware architecture. That's why some software is shipped with a concrete HW configuration, because the speed guaranties are valid only usually for a very limited HW config combinations.

Memory access must be cache optimized if it can be (if not, then it's advisable to review the data structures, maybe it's possible to create one which can be more cache friendly).

However if you are writing a server application this is not going to cause to much problem for you, because of the large number of concurrent requests.

So this is a kind of micro benchmark which tells something, but nothing in general which can't be predicted from the HW architecture.
I disagree on the educating part. It's like trying to kiss every grain of rice coming by the truckloads.

If it only takes a particular action of one person (i.e. submitting voting spam) to annoy many, then the system needs to be fixed. "Educating people" doesn't work.The Reddit Hot front page at 7 pm Monday (2-19) listed in the top 10 posts: 5 cartoons (4 of them sophomoric, IMO), 3 religious posts (1 fundamentalist rant and 2 atheist slammings of religion), 1 technical (Linux) and 1 well written satire of MS Vista.

The Recommended Top 10 was an article on censorship of children's books, a damn generator of random kitten pics, an interesting legal issue, a post on proposed web censorship, Alabama sex toy laws, Pres Bush news, pics of Russians supposedly praying for unlimited internet, a rant against Christian fundamentalists and news of stranded air passengers.
 Of that Recommended 10, all the news posts except the children's book censorship were readily available on the front pages of general news sites such as MSNBC. The kitten generator and cartoon were worthless, the Russian pics were pointless and the anti-Christian rant was trollish.
  In the New category, the top 3 were mp3 spam, video game spam, coffee spam... and I gave up and went back to playing an intellectually stimulating game of spider solitaire.

Made me think about advice I gave to my children about public behavior as they were growing up: Appreciate the difference between "being seen" and "seen being.""front page" is such an outdated idea. The idea of the same "front page" for everyone is so ludicrous I wonder how people can still think it's a good idea.This is terrific. I had a similar thought after becoming frustrated with a program I had to use.

Familiarity reduces cognitive load by making some actions second-nature, i.e., anything in vim. That application... it's ridiculous the number of modes, and settings you have to be aware of when you first use it. There is absolutely no way any of it is obvious when you are first introduced to it.

It's all about lack of brevity... new user or not.I have never bought the 'downfall of usenet' notion. Usenet has plenty of life left in it.
I don't think digg ever had a very clever userbase, and it does worry me a bit about diggers coming here. However, this is something that will have to be addressed by moderation, thats all.

Look on the bright side, at least the youtubers aren't migrating to Reddit!Here's the comment link for that story:

[Startup News: YCombinator enters the social news scene (news.ycombinator.com)](http://reddit.com/info/15gkq/comments)

There are about six comments by [Paul Graham](http://reddit.com/user/paulgraham).

&gt; [Then this summer we started working on Arc fairly seriously, and we needed some kind of project to drive it, so I decided to stop nagging and write one.](http://reddit.com/info/15gkq/comments/c15h8u)
If you think there is something wrong with Reddit, don't just say so. Do something about it. Submit interesting articles. Post insightful and intelligent comments. Vote up sophisticated content and vote down trash.

Complaints only decrease the signal to noise ratio. Give an example of something better.[deleted]Awesome!  I've been so careful to avoid breaking old browsers I never noticed they'd changed this since IE5.x.Reddit should have newbie bait.  Grab the latest vaguely amusing "lol amazing" pic off viral email, and put it on the front page on a daily basis.  Everybody who has been a member for over a fortnight doesn't see it.  Everybody who votes it up gets a "WTF do you think you are doing?  This is trash!" message and banned from the site for an hour.

Oh, but I forgot, that's not *democratic*, and democracy is always good, even when it causes unending floods of drivel into Reddit.
This is a personalization problem. Provide effective personalization features where the user has some control, and there would be no need to create new sites all the time after  existing ones have too many users with little in common.
haha. yah. really. i just started at this new company 3 weeks ago.

first thing i did was download doxygen, graphviz, et.al. and run it across the codebase.


It took like 6 hours (!!!), and i had to run it 3 times, cause i screwed it up. 


but man does it make it easier to make sense of it all.


--vat&gt; There's no way to punish bad peers for not sharing, or reward good peers for sharing more.

No, the tracker can punish you.  It is awfully written, but [read the spec](http://www.bittorrent.org/protocol.html).Because it is a common idiom, it makes it much more readable, too.[removed]But they are rather equivalent..The *real* WTF is that he's got Celine Dion MP3s in there.
I know next to nothing about compression and the like, but are you saying that an increase in compression equals an increase in memory use?&gt; A language is an interface between human beings and computers, and its usability by typical people is part of its intrinsic qualities.  A language "technically correct but unfriendly to most typical users" is just incorrect, as a hammer without a proper handle.

No.  A language is a method for expressing thoughts.  Algorithms are thoughts that define processes.

The technical correctness of a programming language is how well it expresses algorithms, and how powerfully it manipulates them.

A powerful language with an unfriendly syntax doesn't stop being powerful, just like a piano doesn't stop being a more powerful instrument than a triangle because it's harder to play.

"Less useful in a business context", maybe.  "Less technically correct" or "less powerful", no.

&gt; And as a delivery company owner, I wouldn't be interested into a "better" truck for which I would have to hire fighter pilots as drivers.

You aren't listening, or you aren't capable of making the right distinction:

A language's *technical correctness* or *power* is a function only of its design.

A language's *usefulness in a business context* depends on a myriad of other factors, including legacy systems, network effects, availability of trained employees, etc.

The faster, more efficient, more reliable truck that needs fighter pilots to drive it *is a better truck*.  It's not as good a *business choice* (for most businesses), but how useful it is in one particular situation is not the same as not being a good design - otherwise a scalpel is a useless tool because I can't pound stakes into the ground with it.

By all the metrics that are important to assessing the qualities of a truck (fuel efficiency, speed, handling, reliability) *it's better*.

&gt; How come higher level languages claim to have both the most power and the smartest users, yet produce so few useful apps, either proprietary or open source?

As people have said:

1. Higher barrier to entry to learn the language - if fighter jets are so much "better" than cars, why don't you see more parked in the Wal-Mart parking lot?  Because the initial investment isn't worth it.  The vehicle's much more powerful and can do many more things, but if you don't need the power there's no point incurring the expense.  This doesn't reflect at all on how "good" a vehicle a fighter jet is, merely that it's more than you need to get to the shops and back.

2. Network effects - Lisp is too hard for many people, and many people don't see the point.  So most people write software in other languages.  People want to interoperate with those operating system and applications, so they write their software in the same language.  Non-Lisp language(s) become the de facto standard.

3. Inefficiency - Lisp is extremely high-level and computationally inefficient compared to, say, C or assembler.  It's only relatively recently that machines have been fast enough (and we have advances in compiler optimisation and developments like JIT-compilation) that even a staggeringly high-level language like Lisp is *useful* for anything in the real world.

No mystery - Lisp was ahead of its time in every way.  It was too difficult to learn to write trivial applications, so no-one did.  It was too slow to execute to write large applications, so no-one did.  Network effects took over and dictated that, even now these obstacles are largely gone, Lisp is still not nearly as popular as it "should" be, given its power and expressiveness.

&gt; If a technology fails to deliver its promises, the burden of the proof belongs to it, not to the other technologies which actually do deliver value.

What promises?  Lisp was designed as a language - a tool to express thought.  Just because the most expressive, eloquent spoken language in the world is a little bit flowery for day-to-day business use doesn't make the language less expressive or technically impressive *as a language*.

&gt; And VB6 is a very enabling language: financial analysts can write their little stuff in it without significant training nor requiring a developer... There are other use cases VB doesn't even pretend to address, but what it does, it does very well.

Again, VB6 was a brilliant *business choice*, even while being a truly execrable language.

Not to be rude, but you don't seem to be able to separate an object from a particular purpose to which it's put.  It seems to be tantamount to saying that the Mona Lisa isn't great art because a reproduction would be too big to fit properly in a newspaper, and its delicate use of colour and line doesn't come across well in crappy newsprint reproductions.  Snoopy however is brilliant art, because you can print it in a paper and it looks exactly as it should.

Intrinsic purpose of a tool != one specific purpose you put it to.

Language are for thinking and communicating in.  Epic Norse saga is a pretty useless form to use in a business context, but that doesn't stop it being a more powerful and versatile method of communication than a three-word slogan.Yes, that's exactly how this algorithm works, basically.The "Eternal September" requires a constantly increasing user base, yet according to Alexa, Digg's traffic has been dropping since December, and Reddit's has been rather stagnant too.&gt; Initial group leaves to find a new site.

You're still here, so I assume you're in the "A lot" of people group?

I can handle what many here seem to be claiming as worthless links (try using recommended as others have pointed out), what I can't handle is when people feel elitist because they have been at a site a few months longer than someone else. I think people need to realize that they probably were part of a second wave themselves to Reddit, even if they have been here for a year already.Tagan log # time bzip2 -v test
  test:    14.123:1,  0.566 bits/byte, 92.92% saved, 7899624 in, 559352 out.

real    0m16.462s
user    0m15.071s
sys     0m0.083s
Tagan log # time bzip2 -d test.bz2

real    0m1.320s
user    0m1.132s
sys     0m0.086s
Tagan log # time rzip test

real    0m6.046s
user    0m5.434s
sys     0m0.124s
Tagan log # time rzip -d test.rz

real    0m1.710s
user    0m0.960s
sys     0m0.617s
Tagan log # time rzip -v test
test - compression ratio 13.301

real    0m6.781s
user    0m5.459s
sys     0m0.121s
Tagan log # ls -l test.rz
-rw------- 1 root root 593915 Feb 20 14:06 test.rz

faster but not better comprsion than bzip2 on a standard 7.6M log fileWOO HOO!

Jython Lives!!!One thing you may notice is that these wonderful arguments **only make it to the top only on off hours**, when the majority of the Reddit community is not online to police by voting. You will also notice that confessed ex-Diggers have spent time analyzing Reddit for the last month or so. 

So I would say that this "submission" isn't so much a complaint as a subtle PR campaign for the hearts and minds of the community. I strongly recommend that the devs give us the ability to self police so that we can stop people who are obviously trying to abuse the community.

Compare jkerwin's [comment on this article](http://programming.reddit.com/info/15j08/comments/c15kee)

&gt;This is an obvious dupe, of course - check out the tag on the URL. =D

&gt;On a related note, is there a statute of limitations on dupes? That post (which is where I first heard about the "Eternal September" concept, incidentally) was 6 months ago, long before reddit was swamped with newbs and the associated polls and coolest pictures ever.

to those of [neomeme](http://reddit.com/info/154e4/comments/c154un)

&gt;So did the submitter. Look at the end of the URL. 
permalink parent reply
\t
&gt; neomeme 7 points 2 days ago

&gt; You got me :)
\t
&gt;theycallmemorty 6 points 2 days ago

&gt;Why'd you do it?
\t
&gt;neomeme 7 points 2 days ago

&gt;I'm trying to see what it takes for a story to make the reddit front page. **An experiment in social networks, already replicated on Digg with great success**.
\t
&gt;r2002 3 points 2 days ago

&gt;How do you do the same on Digg? I thought Digg is much tougher on the additional additions to a dup URL?
\t\t
&gt;neomeme 6 points 2 days ago*

&gt;Surely you're joking, Mr. r2002. You can add a # to a Digg submission URL and it will go through(something reddit catches). Hell, probably half of my front-page Digg submissions were dupes, and I got a lot of stories to the front page. But I don't like Digg anymore... reddit is where it's at.

&gt;marianobarrios 8 points 2 days ago

&gt;I see this every month
permalink parent reply
reply cancel
\t
&gt;neomeme 6 points 2 days ago

&gt;Then you'll be happy to know I plan to resubmit this in a month.

I of course realized that this other flag story was an attack and I [informed noname99](http://reddit.com/info/154e4/comments/c015517), the commentor with the highest karma at that point on the thread, who also managed to pick it out as a dupe story immediately. I am now informing jesusfreak because of his reputation for analysis of Digg related manipulations. (hint hint)

When there was a increase in the number of political stories on Reddit about 9 months ago several low karma "right wing people" started to complain about "quality going down". It was obvious from their profile they had no interest in what they were saying so much as advocating community self censorship for political reasons.

This current campaign by Diggers complaining and advocating way stupid decisions that didn't work on Digg (like eliminating karma) is nothing more than a campaign by concern trolls to see how they can undercut social bookmarking sites. Such knowledge is worth a lot of money to people who would like to keep the monopoly control of media a viable option. 


Yes but it is a side effect.I look at it from two perspectives: From the language design view, where AICs definitely are closures (which I simply have to accept, usability aside). And from the usability point of view where AICs are a quite complicated way for use as closures (which may be bad or not, depending on the necessity having closures in a language).

Now I know how to use closures. I've written enough code in languages with closures support to really know what they good for. But Java has different ways to reach things and in most cases, closures can't solve the problems Java have. But they would add lots of different ways to do the same which is already possible in a slightly different way. And this is bad because it creates the chance that code reuse becomes impossible. 

If I know that I have to use an iterator to iterating over a certain collection then I just implement this iterator. But if I have to think: Hmm, is an iterator the better solution, or should I better use 'map' and 'fold' here? The problem with 'map' is that it's not possible (in an non-lazy language) to efficiently iterate over multiple collection at the same time. For example:

    boolean compare(Collection c1, Collection c2) {
        Iterator it1 = c1.iterator();
        Iterator it2 = c2.iterator();
        while(it1.hasNext() &amp;&amp; it2.hasNext()) {
           if (!it1.next().equals(it2.next()) return false;
        }
        return it1.hasNext() == it2.hasNext();
    }

Please do the same if you have only 'map' for both collections. One way is to convert both Collections to lists of tuples and than map over the resulting list. But this is a quite expensive operation. The other way is to create functions like 'map2' which iterate over two collections parallel, but this is again limited to two collections of the same kind.

So map isn't a general enough means of iteration: We always need to build and iterator first and can than use a map to use it a bit more comfortable (But only in the cases where we really need a map, which isn't far as common in typical Java-code as it is in Haskell for example). But since creating 'map' is much more easy to do compared to creating an iterator (because Java has no 'yield'), I suspect that most people would create collections with only map and fold-support, and if you need the more general iterator, you have a problem.

And to make closures useful, it requires tuples because with them we much more often need multiple return values. So this will be at 100% next on the extension list if they really add easy closures to Java.

Now closures can be used to create a certain much needed control structure: The C#-using which would be really nice to solve the damned resource-allocation and error-handling problem. But why not simply add this control structure to the language? It's easy and can be done on top of the existing 'try' syntax without introducing new keywords. All we need is making 'try' 'Closeable' aware to  create the code to close a resource automatically (and of course to make also the iterator-interface implementing 'Closeable'). The advantage is that we remain with a 'single solution', it's simply more comfortable and less error-prone.

And there could be small enhancements to the for-each loop: Add parallel-iteration. Like:

    boolean compare(Collection c1, Collection c2) {
\t    for(Object o1: c1; Object o2: c2) {
\t\t    if (!o1.equals(o2)) return false;
        }
\t\tcatch {  // called if one iterator has no elements back while the other has
\t\t    return false;
\t\t}
        return true;
    }

Again a small addition to the existing language and quite useful. Also there could be a loop-counter and the possibility to access the iterator inside the for-each loop. Like:

    loop: for(Object o: c1) {
\t    if ((loop.count % 2) == 0) loop.iterator.remove();
    }

This is again a simple addition but would continue the path Java is on for years instead of choosing a totally new direction.

And a really nice thing would be 'yield' to make creation of iterator more easy. The advantage here is that it supports the existing paradigm instead of creating a new one which isn't even usable in many (real) cases. If they now would add 'easy closures' to the language this problem wouldn't be solved, and after some time playing with closures, the Java community would say: Hey, closures aren't such a good thing anyway, please give us more. In the end, Java would turn into a real kitchen-sink language.

But I think that all this will happen: Java will get 'easy closures', people will use them in inappropriate ways creating less clean and less reusable code and cry for the next enhancement in the next release. But this won't make Java really more usable in the end. It only will hide Javas problems by giving people new toys to play with.

And what to do NOW?

Don't know what you do, but Java is as it is now. If you call AICs closures or not. Even if they build more easy to use closures into it in the next release, NOW you don't have them anyway. So just use AICs instead or find a different solution (which really is possible in most of the usage cases for closures). You're a programmer, be inventive.
&gt; One thing you may notice is that these wonderful arguments make it to the top on off hours.

Where do these 'off hours' occur?  Do they occur on the moon?  Is there a whole plurality of uninhabited timezone on the Earth for these villains to exploit the mid-day of?I've wisely refrained from bitching too much about the decline of reddit, because I hardly ever submit anything, so that would kind of make me a hypocrite. However, all these comments and submissions are starting to annoy me. They remind me of something I've seen on lots of forums: 

1. Something happens to community (usually an influx of new members who are not exact clones of long term members). 
2. Long term member posts bombastic thread about how community is not like it used to be, they hate it and they will leave it and never return.
3. Other members reply by saying long term member's leave is only for the better.
4. Long term member lingers at least a week replying angrily to every reply in the thread.

It's very ironic. If the site sucks so much, why don't you just leave it already?It looks interesting, but truthfully all I could think of was, wow I am glad I have C#/.Net and Python/wxWindows. I mean I realize a text editor should be snappy, but then again I also realize a text editor is one of those projects that I have no hope of from scratching a better one then vi/emacs."pythonic" is "moronic." I'm tired to trying to be more "pythonic." It's one of the things thats starting to turn me off about the community behind the language and the language itself. There isn't even a strict definition of what "pythonic" even is. So, while you might think "'m is not None" is morePythonicThan("m != None"), I say who cares. It's still readable and still works.&gt;Fascism and corporatism go poorly together. 

Actually, fascism is a brand of [corporatism](http://en.wikipedia.org/wiki/Corporatism). The word you're looking for is capitalism.
The majority of the population is US. Take a look at normal US bedtime hours. There is higher traffic during US work hours during the five day work week than any other time. Yesterday was a national holiday. 9 hours ago would have been midnight eastern time on the last day of a national holiday. The story gained momentum during the early hours of the morning in the US when most of the readers would have been asleep, in preparation to return to work after a 3 day weekend.

Many other crap stories and bs memes like this one seem to populate Reddit during the early hours of Friday nights. Saturday nights, etc. When the working crowd is not using Reddit.

Alexa shows Reddit to be at least 50% US with much smaller populations distributed throughout the globe. This is not so unbelievable, no?

Hopefully jesusfreak will be interested enough to also investigate this phenomena.&gt; The word you're looking for is capitalism.

I'll settle for 'corporations as we know them'.

also: oops.I can't even imagine the serpentine code that lies behind this thing.&gt; The majority of the population is US.

That settles things, then!  Those dastardly foreigners want to destroy everything we hold dear.Don't you think it would be better to work on the rss feed?Criticism I don't mind. "My way is right, your way is wrong, and everybody should do things my way" I do mind; I like having choices. But Adrian's right and I keep saying I'm done with this; time to hold myself to it.Dang it, Adrian. I like arguing too much ;)A new version of acts_as_rated for ruby on rails is out. Modified to work on MySQL and added a new rated_by? method.In the same vein as indigoviolet, poster 3 in this thread, the price of freedom is eternal vigilance.BTW, playing with yahoo pipes.

http://pipes.yahoo.com/pipes/tFzJPu7A2xGNVEqIIBeTaQ/

You can see how it's done, and create your own with your keywords.
Just put the new rss feed on the feedreader :)Perhaps this license is merely designed for people to make little tweaks to functionality offered by the software as standard, as opposed to allowing people to make full scale releases of heavily-modified software. If this is the case, the licence does seem to me to be the right tool for the job, much as I hate to admit it.&gt; When using terms like "best", you always have to qualify them with what you are measuring.

Languages are tools for thinking, and for expressing those thoughts.  An expressive language is a "good" language, and an inexpressive language is a "bad" language.

Now, some "bad" languages are better than others *for certain things*, in the same way that while a brand new Ferrari is a "better" car than a knackered old Range Rover, *for the purposes of driving off-road* the Range Rover is better.

Intrinsic value as an object != the precise purpose to which  that object is currently being put.

&gt; First is the assumption that network effects are market driven, not effectiveness driven. But is that really the whole story?

Not entirely - for example, the network effect that made MS-DOS (and then Windows, and then MSOffice/IE/whatever) the most popular choice was more a management cockup at IBM than market-driven.  If you bought an IBM you got MS-DOS - there were no real market pressures acting on Microsoft.

Once Windows became the default platform, the only real scripting language *available* for the machine because wildly popular (let's not even *mention* MS-DOS batch files ;-).

&gt; Microsoft has heavily marketed other stuff that fell flat on its face.

Yes, Microsoft has marketed things which fell flat on their faces, but this is almost always where they couldn't leverage their pre-existing operating systems and/or office productivity monopolies to bail them out.

Don't tell me that VB was as popular before they bundled VBA into the Office suite, and don't tell me if a wider selection of scripting languages had come pre-installed in Windows *quite* as many people would be using VB...

For a fascinating explanation of how network effects can promote crappy solutions *precisely because* they're crappy, try to find a copy of "[Worse is Better](http://en.wikipedia.org/wiki/Worse_is_Better)", by Richard P. Gabriel.

&gt; On the other hand, no marketing certainly hurts too. RealBasic has cross platform (Windows, Mac, Linux) GUI support that puts Java to shame, but hardly anyone has heard of it.

Bingo.  Also, perhaps, because VB (with its pre-installed accessibility and decade-plus-long network effects) occupies the exact niche in the worse-is-better world that RealBasic seeks to exploit.

&gt; Another issue is that networks effects make a tool better.

Not where a language is concerned - language has to stay relatively static or you can't use it to communicate (making it not "more or less efficient", but "useless").

&gt; VB wouldn't be where it is today without all the third-party controls.

Right - VB6 was a great *business solution*.  It was still a shitty *language*, however, because all the add-on libraries in the world can't fix fundamentally broken language semantics.  They can ease the pain by abstracting away many of the hoops you have to jump through to get something done, but there will always be situations where you're thrown back on the language semantics and they're no use.  Good libraries for a bad language can be band-aids on the various wounds, but you've still *got* a wound. ;-)

VB6 has more libraries available for it than you could shake a very large stick at, but a complete lack of built-in support for (for example) regular expressions renders it a language which, as regards expressiveness, is fundamentally crippled compared to Perl, C#... even Javascript.

(Note I'm talking about VB6 here - with each successive iteration of VB Microsoft has imported more and more concepts form other languages, until VB.NET is basically syntactically equivalent to C#, Javascript, etc, which rather proves my point).

&gt; Anyways, back to my point. If you wanted a cross platform language for rich client development, what would you do?

It depends.  Are we (1) designing a perfect language, or (2) trying to design a programming language we can sell to lots of businesses?

If 1, I'd spend a lot of time worrying about semantics and far-reaching design decisions.  I'd aim for elegance and consistency, and the principle of least surprise.  I'd aim for a small set of powerful syntactic constructs, and place any powerful, complex or domain-specific functionality in libraries.

If 2, I'd bang out a crappy scripting language which was piss-easy to learn, and just good enough to suffice.  I'd spend all my time making sure it was really, really good at interfacing with Microsoft/the dominant computing platform applications.  The language would suck balls in every other way, but I'd release patches and new versions periodically until it became more usable.

If I discovered that the language design was so fundamentally broken it couldn't be patched or updated and retain backwards compatibility, I'd break backwards compatibility, re-write it from scratch, call it the "next version" and pray enough people bought it that the new version would be successful.

Now, believe it or not, *I honestly didn't intend those two answers to be loaded in any way*... but (reading back) do you notice anything about the two answers given, the two aims in mind, and the two languages VB and Lisp?

Basically, all we disagree about is how to rate a language *as a language* - I say we should rate it in terms of what languages are for (expressiveness), and you're rating it based on how useful it is as a business solution.  Fair?Yeah, I looked at yahoo pipes the other day and couldn't make heads or tails of it :) I wanted to make a custom news rss feed based on Google News search queries. It's cool to see it works so well with what I wanted from reddit. Nice work :)

Still, it's fun to learn python and as I said above, I would like to see usernames and comment links in the future(*Edit:* since added). Sometimes I think I like the discussions more than the articles, unless they contain certain keywords ;)It's designed with _much_ larger filesizes in mind - try it on a 100+ meg log and then see what you get.[removed]True but this is a good exercise for anyone.  Creating a non MFC text editor, the knowledge you will gain from that awaits you.Here's a very easy way to do it, without having to create custom apache modules or use LDAP:

1) Have one server that is the authentication server, running plain old apache/iis linked to the user database.

2)  When a user needs to login, they are redirected to the auth server, with a token in the url to represent the site they originally came from.

3)  If the user has already logged into the auth server, or after a successful log in, the auth server contacts the original server directly (via HTTP, Rest interface) and gives the original server a token &amp; username/email saying, if a user comes to the site with this auth token in the next five minutes, he's really this user.

4) The user is redirected back to the original server, with the auth token in his url.  The original server sees this token, and let's the user use the services.The solution here is very simple. Mandatory IQ/Technology tests for all who wish to be members of reddit...&gt; althoug you get a perfect 50/50 distribution I doubt whether this is a good random number generator. The decision for a 1 or a 0 depends on the length of the previous interval and so the probability of a 0 following a 1 is larger than the probability of a 1 following a 1, if I see it right. Because if you had a delay that was longer than the average you will probably get a 1. Since it is more probably for the following delay to be shorter it will also be more probable to get a 0 than to get a 1 after having had a 1.

I think this could be fixed by only considering non-overlapping pairs and extracting one bit from each.  It will slow down your your RNG by a factor of 2, but I think it will be a much better RNG.Here's a Ruby reddit lite:

http://rafb.net/p/GkmWiF83.html

And a bayesian reddit filter server:

http://rafb.net/p/lrSYTq85.html

Run it, go to localhost:3000. Now you'll see reddit with good/bad links. Clicking them will feed the title of the entry to the classifier.I take it this guy doesn't like the GPL either. ;-)Why the hell this is in programming?I'll ignore your vitriol since you clearly have problems with civil conversation.

&gt; How the fuck are authors of libraries you are using going to know about the subclass you just created?

It's somewhat the point of OOP that they don't *need* to know about it.

&gt; In Ruby or Python, you can add methods on the fly, known as "monkey patching". In Ruby, you can add methods on the fly to any class, however implemented.

And, as I noted, in Python you can add it to any class implemented in Python.

&gt; Also, in Ruby, "monkey patching" is always encouraged, where in Python it is sometimes looked down upon (rightly so, I feel).

That's not a feature of the language, that's a feature of the community.  Ruby's not "more dynamic" because its community encourages monkey patching.

&gt; Enlighten me, shit-biscuit, and show us how to add a method to the base class 'object', and have that method suddenly appear in all object instances of any class.

The "object" class is written in C.  You can't add methods to it, as I already noted.  But you can add methods to base classes written in Python, as I noted, and they will be available in all subclasses.

&gt; Fuck you. Learn the fucking difference between anonymous functions, anonymous blocks, and anonymous closures.

I'm already well aware of the difference, thanks.

&gt; In Ruby, with anonymous blocks and anonymous closures, you can effectively create your own control statements.

Just like you can do with higher-order functions in Python.  Ruby has no improved dynamicity with its anonymous blocks than Python has with its functions.  Everything that can be done with blocks can be done with functions (anonymous or named), and more: higher order functions can take *multiple* functions as arguments, whereas Ruby methods can only accept a single block argument.

&gt; Guido thinks that anonymous blocks and anonymous closures allow too much difficulty to understanding code in-the-wild, so Python will never get them.

Syntactically, it would be hard to implement anonymous blocks in Python.  As Fredrik Lundh says, just give it a name.  The anonymity is rarely a significant win.

&gt; DSL are not used in Python to this degree, for the simple fact that it won't look like Python anymore.

Ruby DSLs still look like Ruby.  Lisp DSLs still look like Lisp.  Python DSLs still look like Python, so I fail to see your point.

&gt; DSLs are not sanctioned in Python, no Python standard library code uses DSL style coding, and no Python standard library code ever will.

First, that's a function of the community, not the language.  Ruby isn't "more dynamic" because the community encourages DSLs.

Second, you have a significant misunderstanding of the term "DSL" if you think that Python's standard library doesn't include any.  The % formatting in strings is a DSL.  The struct module uses a DSL for packing/unpacking values.  DSLs are not always complex grammars.

Third, if Python programmers were so inclined, the only changes to the Ruby DSL you gave as an example above would be minor syntactical changes.  The Ruby DSL still looks pretty much like Ruby, and the Python DSL would still look pretty much like Python.

    Select('column1').from('table1').where(column2=equals(12), column3=equals(13))

A literal translation of your insert syntax would be possible: 

    Insert.into['table1']['column1', 'column2', 'column3'].values(10, 'book', 'start')

but I think a Python DSL could be even clearer:

    Insert.into('table1')(column1=10, column2='book', column3='start')

The mapping between a Ruby DSL and a Python DSL is simple and direct.  Ruby DSLs don't look any less like Ruby than Python DSLs look like Python.  The only difference here is the respective communities.

&gt; I have run out of energy to abuse you to the degree you should be abused.

Perhaps you should have focused your energies on making compelling arguments, rather than littering your post with useless vitriol.Generally, I agree that non-programming shouldn't be in programming, but generally the programming reddit community is significantly less AOLed than the main reddit, and so it probably has more sympathy for the phenomenon than does the main reddit. On the other hand, it clutters up the programming reddit with a giant and fairly retarded comment thread.No, six months ago we were already [sick of it](http://reddit.com/info/ekoj/comments)So long as notepad is spelled e-m-a-c-s.&gt;Yesterday was a national holiday...return to work after a 3 day weekend.

Most private sector workers don't get President's Day off. Heck, my mom's a public school teacher and she didn't even get the day off.By using more memory, the compression program is able to look at a larger chunk of the file, giving it more opportunity to detect repetition.I would recommend that reddit implement a system like metafilter uses for new users.

When they sign up they aren't allowed to submit anything for one week. They also have to make a handful of comments before they can submit.

oh and maybe charging $2 to sign up wouldn't be a bad idea. Even the smallest financial commitment has a way of weeding out the crap.[removed][removed][removed]This is the what Habermas complains is happening to the public sphere.OCamlNet: suspiciously infrequent releases. Doesn't appear to support SSL, authentication, custom timeouts, custom retries, optional HTTP redirect following. FreeTDS hasn't had a release for a few years although I wouldn't use it because I don't use the databases it targets anyway.

Good luck installing all this stuff on a non-Linux machine: Cygwin is slow and defeats one of the main purposes of using OCaml, speed, while using a native build requires you to hack together a Unix build environment on a non-Unix operationg system. OCaml on Windows is in general a very confusing experience relative to the one-click installers that abound nowadays.

Piggybacking on the JVM or .NET lets you deal with things that are desirable but unlikely to be implemented by obscure languages; e.g., object-relational mapping, Markdown/Textile parsers, XMPP (Jabber) protocol libraries, network protocols other than SMTP/FTP/HTTP. and so on. In the particular case of the JVM, I think once JRuby stabilizes it will complement Scala very nicely.

I found that OCaml was excellent for implementing optimization heuristics published in papers in the last few years. Things like St. Jane's OCaml Summer of Code will help improve library problems, but there's a much bigger problem looming on the horizon. OCaml's maintainers have an antagonistic attitude towards concurrency and the GC is not thread-safe. If OCaml wants to avoid being somewhat irrelevant in 5-10 years, I think they'll have to rewrite large portions of the runtime system.&gt; designed for people to make little tweaks

It -allows- 'tweaks' of any size or nature.  It even allows you to make 'full-scale, heavily-modified releases'.  You can replace every single line of code, even, over years.  It does not allow you to keep any of this from Microsoft for any purpose of Microsoft.  And although you've non-exclusive ownership your own modifications, it does not protect you from Microsoft -- as -they-, having -real- ownership, can simply relicense the codebase and any contributions.  Reading only the quoted portion of the license.

[RMS found this license objectionable, nine years ago.](http://www.gnu.org/philosophy/netscape-npl.html)If you wanna piggyback on the JVM, and have a soft spot for Lisp, you might wanna take a look on SISC Scheme (Second Interpreter of Scheme Code). 
http://sisc.sourceforge.net/The songs listed in the image prove that Neo was not the "One".This is why you need a system like [Klipboardz](http://www.klipboardz.com) which allows you to create your own groups of users. There are already many layers of private groups for the old school users who just want to continue talking amongst themselves. There is still a vibrant front page but it's nice to be able to just talk about things among your friends.if you had bothered to rtfa, you'd have noticed, it's for *LARGE* files, not 7.6MiB files. 

take a 100mb or &gt; file, and try again. 

--vatI actually liked the game by the same name.First, if a solution doesn't scale (and that's what this is about), then maybe it's time to do it better or improve what is there. I think the recent proposal concerning tags (in order to eliminate messages you simply don't care) would be an improvement. More people also means people with more diverse interests.

Secondly, the environment changes and part of the failure of usenet was IMHO due to the fact that people clinged to a netiquette that, e.g., had no notion of spam.
[removed]That's an upper bound, not how much memory will actually be used.

rzipping a ~260 MB file (it happens to be an uncompressed tarfile of the firefox source) used a bit over 300 MB of memory, even with -9 set.  rzipping the same file with -1 set uses slightly over 100 MB.  

Using -9, the resulting tar.rz file is about 2 megs smaller than the same data compressed with bzip2 -9.  Using rzip -1, the resultant tarball is about 3 megs larger.I guess I didn't make it clear enough that I was explaining that Haskell can suffer from a different kind of memory consuming problem called a space leak.The fist part appears that they are simply trying to prevent people from claiming that stuff like Windows binaries are under the license.

The second is just them covering their own ass. Any reasonable company would do that.

What the hell? If license isn't "free" (for some definition of "free") it doesn't mean that it's bullshit. But then I guess, usage of epithets like "fascist", "asshole", "corporate-greed"  is surely going to convince everyone that you are right, even if your logic is wrong...

&gt; Furthermore, every torrent needs a "seed"-- a peer with 100% of the file downloaded-- connected at all times.

This is also a misunderstanding on the author's part. You can have a purely peered file and as long as the union of the clients have 100% of the file, you can get 100% of the file and get to "seed" status.

This guy has no idea of what he's talking about.Yup, I agree, this is a good article.

Like the best stuff you can read, it clarifies things you have already experienced, but had not been able to construct a coherent thought about.

We've recently created a new time tracker program for logging the hours worked on particular projects.  The two keys I kept pushing for (and mostly got) were 'quick' and 'simple'.  Because that's the only way you're going to get people to enter their time.[deleted]The problem is that people down vote good articles and up vote conspiracy theories.  I've been thinking about this a lot lately, and I've come to the conclusion that reddit is fine.  I would just rather go to a different domain running reddit where people populate it with things that interest me.  It's the best of it's kind around, but that's the application.  The content(save for the programming articles not about functional programming) is either irrelevant to working programmers, or the worst kind of tabloid politics.&gt; Monads are such a thing which is really hard to understand for non-mathematicians. But because they are an interesting concept, why not create analogies which removes this difficulties - even if the analogy is not exactly right?

No, the analogy is right, but it has everything to do with higher-order functions, and nothing with monads, as such.

&gt; The reason is that this runs totally against everything mathematicians have learned and believe in. They want correctness. Proofs are their life-elixir. They have to spot tiny inconsistencies from a 100 miles distance.

This simply isn't true (as psycotic happened to [point out](http://programming.reddit.com/info/14vcn/comments/c151ur) very well recently, so i won't repeat that).

&gt; So I tried to find an image for the concept which most programmers who aren't mathematicians can relate to. I'm such a programmer myself and so I know about the problems.

Same here.

&gt; It's true that all functions do code-transformation if you look at if from a certain angle but again it's wrong because the real semantics are term-rewriting. Not code is rewritten but data is matched an rewritten. Closures are 1st class data in fp so you can also do term-rewriting on closures and this is what monads do. Not always (in case of the list-monad) but for example in case of the state-monad.

...?  I'm not sure what you're saying here, but i can definitely say that monads do not involve any kind of term rewriting:  they're perfectly ordinary function application, all the way through.

&gt; With monads (which are no functions but *constructs build from functions*) [...]

No, they really are just plain functions.

&gt; And if you have some experience in programming you recognizes this kind of behavior: It's a DSL, something which often is implemented via macros. Sure, the method is different, but looked at it from higher ground, it's quite similar.  All monads have this principle in common, so why not simply look at monads as 'some kind of functional macro'?

This principle/similarity is what macros and higher-order functions have in common, not monads.

&gt;&gt; In a phrase: a *program evaluation strategy* that's *composable*.
&gt; 
&gt; That's also a valid description for macros.

No;  with almost no exceptions, macros operate on single forms, don't involve evaluation, and don't compose.  

&gt; And of course 'setf' is composable.

No, it isn't.  (See the presentation at that composable macro link i gave.)

&gt; If you say "a monad is a program evaluation strategy that's composable" this is simply much to unimaginative. To understand it, people have to translate this sentence first. By simply saying "kind of a macro" or "program transformation" you can much easier ring some bells.

Except those bells are not related to monads, but to higher-order functions, and creating the opposite impression just serves to confuse and mislead people trying to learn the difference.I too find this licence objectable. I find putting words like "bullshit" in the title of this post and using phrases such as "Fuck Microsoft" in the article a bit silly though. It all seems a bit like being angry with dogs for crapping on the lawn -- it's just what they do. We could hardly expect for Microsoft to start allowing people to make modifications under a more agreeable licence. They just aren't in the business of doing that.&gt; Out of curiosity, is that how people usually write CL?

If I had to hazard a guess, I'd say it occurs about as often as people who get fed up with Haskell's numeric type classes and rework them into their own numeric prelude to better handle monoids, groups, rings, etc.
I can't download it via the links on the page. Tried several mirrors.You're going to run into that problem with most "fringe" languages.  Besides, everyone knows that web services don't work. ;-)

http://www.artima.com/weblogs/viewpost.jsp?thread=162149
http://www.sdtimes.com/fullcolumn/column-20070115-02.html

F# does kick butt though, especially if you've got Visual Studio 2005.  I appreciate scala as a research language, but that whole 'make-the-syntax-javalike-so-we-don't-scare-java-programmers' goal makes it the worst of both worlds in some ways.  Still interesting though.[deleted]I actually like one part of this interface:

All the buttons/tabs/controls are text labeled rather than icons.  There's nothing more irritating than trying to remember if "Search" is the little icon that looks like a green toaster, the one with several files in a folder, or perhaps the strange block with pieces coming out of it.

Also, some of the problem here is not one of interface, but that too many unrelated things are integrated into a file manager.  A file manager has no business playing music: run winamp then minimize it; screen area wasted in filemanager = 0 pixels.  Also a significant fraction of the window is taken up with file-viewing/previewing.  While at least relevant, this is not functional.  The chances that your file manager app is also the best viewer/editor of the file type in question is vanishingly small, and even if that is the case most users most of the time do not need to switch between managing lots of files and working inside one of them often enough to justify using screen area for integrated viewers, previewers, or editors.  It's simply not that hard to double click on a file and open it for real.  Just removing these useless features gets rid of a significant amount of the clutter.  Likewise, setting and options should by reduced to one window/menu-option/tab.  There's no reason people need moment-to-moment access to them so packing them away from the main interface is a net gain.  These things alone account for &gt;25% of the screen usage in the pic.

Now for all of that, I have tried to use The FileMatrix in the past and have found the interface clunky, but mostly because  it is based around the idea of navigating via user-set buttons for various locations on disk with the actual directory tree as a back-up navigation option.&gt;This current campaign by Diggers complaining and advocating way stupid decisions that didn't work on Digg (like eliminating karma) is nothing more than a campaign by concern trolls to see how they can undercut social bookmarking sites. Such knowledge is worth a lot of money to people who would like to keep the monopoly control of media a viable option.

That's some nice paranoia. 

I'm going to leave my comment at that for the irony.Have a look at this site for some comparative testing results:

     http://www.maximumcompression.comYou make the choice to define languages as algorithmic modeling tools, rather than tools that turn computers into useful things. You're allowed to, but you have to realize that most people don't use that word in that sense, and you're playing Umpty Dumpty: 

&gt; "When *I* use a word," Humpty Dumpty said, in a rather scornful tone, "it means just what I choose it to mean - neither more nor less". 

*Human* languages are about mutual understanding between people, and the non-spoken common base of knowledge and assumptions is an integral part of it. 

As for "usefulness in a business context", evidences indicate that you should rather say "usefulness in business, open source, and most other practical contexts".[removed]I wonder, would it be able to reduce the size of divx movies, for example? It seems to me video compression algorithms do not take advantage of long-term redundancy. Anyone care to test?Also, I don't know about anyone else, but, when I browse through the new section I don't downvote articles I don't like, I just don't upvote them.

How many other redditors do this?Wake me up when September ends.[removed]Open source advocates usually argue that open source software is superior because

1. it allows people to inspect the sourcecode and check it for bugs and backdoors
2. it allows the end user to fix bugs and add/adjust features
3. the sourcode itself is of tremendous educational value
4. peer review makes the software better, even if you don't look at the sourcecode yourself.

Programs released under the shared source licence have these 4 advantages, to the same extent as GPL.

So what exactly makes this licence 'bullshit'?There are people reading here who don't know this?care to substantiate that with a reference?September 1993 was when I stopped reading and posting in usenet.  It got too crowded with spammers.  Usenet was great, fast, no ads.  With the right reader you could quickly find discussions and mark others as read.  I rarely log on to my unix account as the service I have (freeshell.org) doesn't have the same netnews reader as the old one I used in the 90s.  But I remember buying an IBM XT computer with a 12" amber monitor just to read news and access my account shortly after I left school in 1991.  Back then it was exciting when I could buy a 2400 baud internal modem at the computer store for $49.99!&gt; That Microsoft is granted back, without any restrictions or 
&gt; limitations, a non-exclusive, perpetual, irrevocable, 
&gt; royalty-free, assignable and sub-licensable license, to 
&gt; reproduce, publicly perform or display, install, use, 
&gt; modify, distribute, make and have made, sell and transfer 
&gt; your modifications to and/or derivative works of the
&gt; Software source code or data, for any purpose.

Change **Microsoft** to **everybody**, and BOOM, what do you get? The BSD licence. If you submit a patch to FreeBSD Microsoft may profit from it, in exactly the way described above.

I think the Shared Source licence is not unreasonable, considering Microsoft has poured at least a few hundred grand in the development of F#.From a BSD perspective, the GPL is a bullshit "free my ass" license.

It's the same type of comparison.  It seems like it's almost a corporate version of the GPL.He does that nearly every year. Well, at least 1996, 1999, 2002, 2003, 2005 and now 2007. That's all the links in the end.

And yes, they are all different.I wouldn't hope for it. MPEG compression will always be better than lossless algorithms. Same goes for MP3 files. You can probably compress them slightly (1-2%), but nothing major.[deleted]&gt; Languages are tools for thinking, and for expressing those thoughts. An expressive language is a "good" language, and an inexpressive language is a "bad" language.

Even expressiveness is subjective. 

If you are writing code to handle compex mathematical formulas, languages like Lisp are painful because they don't support infix notation. You either have to use special libaries or turn all of your equations inside out.

Likewise, neither langauges like Lisp nor our current crop of 'mainstream' langauges like C/C++, .NET, or Java are any good at expressing set notation needed for database access. Older languages like xBase are far superior in this respect, and of course SQL.

On the other hand, if you want self modifying code nothing can beat Lisp's philosophy of code=data. Other langauges can do this, but it involves either string manipulation or abstract syntax trees.

&gt; If you bought an IBM you got MS-DOS - there were no real market pressures acting on Microsoft.

Actually the first IBM PC's didn't come with an operating system, it was a seperate purchase. One reason MS won is that that MS-DOS cost $40 while CP/M was around $250.

&gt; Don't tell me that VB was as popular before they bundled VBA into the Office suite, and don't tell me if a wider selection of scripting languages had come pre-installed in Windows quite as many people would be using VB...

VB was immediately popular, even without the influence of VBA. At the time of its release, Delphi didn't exist and writing Windows applications was just plain painful.

&gt; It depends. Are we (1) designing a perfect language, or (2) trying to design a programming language we can sell to lots of businesses?

Actually I was asking from the position of someone who had to use an existing language and didn't have the luxury of creating something new. But I did find your insight on the other aspect interesting.
The predecessor of flapjax?What's worse than being an OCaml tutorial? Not knowing where to set the apostrophe, perhaps? Or making stupid jokes about grammar?Ah, another open source zealot who's angry he can't have something/everything for free.  *The* one driving force behind the open source movement - something for nothing, and *goddamn you* if everything isn't given away for free.

Shame on MS for trying to make a buck or two back on their (I'm assuming here) usually sizeable investment.  Those bastards![removed]I think that one big difference here is the licence says it is possible for some of the code to be open while other parts of it are given to the consumer as binaries, which mustn't be reverse engineered or decompiled.

As far as I know this isn't possible under the GPL.Amen!Well, think of "pythonic code" as the best idiom to express that thought in python. Writing pythonic code is one way of becoming a better python programmer.

The reason 'is not None' works is because in python you are not supposed to use equality operators on None types. 'is not' is testing object identity, rather than equivalence. See [here](http://www.peterbe.com/plog/is-equal-in-python).Dude - lighten up.Generator expressions (introduced in Python 2.5) are even cooler. Unlike list comprehensions, generator expressions are lazy. The syntax is almost the same: Just use () instead of [].fairly common sense stuff bloated with heaps of pseudoscientific bullshit[removed]This isn't bad, I think I still prefer http://ocaml-tutorial.org/ however.You can ship closed source black box components with GPL products, if you want. Some wireless drivers under linux have closed-source firmware.

Legally it may be different, but I think that it's conceptually very similar.I was talking about MPEG + rzip of course, not rzip on a raw video file :)

MPEG uses a simple entropic coder, much simpler than rzip or the like. So maybe it would be possible to gain a few percent?&gt; Yup, I agree, this is a good article.

Well, I think that this is a rather complicated way to say that a UI shouldn't be unnecessary complicated and slow.  By the way, the concept of cognitive load has been introduced into user interface research long ago.[removed]Lisp IS only for torturing AutoCAD users.&gt; The solution is dead trivial with higher order functions [in Haskell]:
&gt; 
&gt; \taddTenAndConvert = map (("***" ++) . show . (+10))

I find Python's version easier to read:

\tten_added = ['***' + str(x + 10) for x in src_list]

(Or, in 2.5 and later, change [] to () for a generator expression, which is lazy.)

If you insist on using Python's map function, the result is odd-looking but equivalent:

\tten_added = map('***'.__add__, map(str, map((10).__add__, src_list)))

(Or, in 2.3 and later, change map to itertools.imap, which is lazy.)[deleted]Exactly. By definition, the front page is going to yield whatever constitutes the lowest common denominator.

Personal recommendation pages and subreddits are the most useful parts of reddit.Haskell has list comprehensions, too:

    tenAdded srcList = [ "***" ++ show (x + 10) | x &lt;- srcList ]Yeah, christ, it's like the internet circa 1999. Or wow, this new DVD thing is amazing - all those pictures on one little disc!&gt; So long as notepad is spelled v-i-m.

Corrected that for you.

Don't you feel silly making a mistake when you're supposed to be correcting someone!Bah.  The 'eternal September' was one I was around for.  At best, it shows you're a victim of your success.  At worst, it shows that your design was too short-sighted to take into account mass popularity.  Retro-fitting USENET into something that can handle the zillions of users that read it would be too hard (see the Usenet II website, last changed in 2001).

Always assume that your product/project will be more successful than you think it is.  That way, when you do have the large influx of people, you're ready to handle it.  If it isn't that popular, you spent a few extra hours in the design phase and a few extra days in the development phase that were wasted.  Far better than spending months trying to get performance out of something that wasn't designed right in the first place (see slashdot for an example).

On a side note, what really killed USENET wasn't the Ethernal September, but the spam that started arriving not long after (see Canter and Siegel).Web development is "programming" now? I thought it was just for art students...maybe a few percent, but at a big price - time and CPUWhy, oh why, isn't wxPython included with Python per default?

Developing a cross-platform-application with wxPython is nice and sweet, but **deploying** it is a nighmare that made my use Scala+Swing instead. Py2exe, py2app, all of which need to massaged to do what they are supposed to do and always manage to work just 98% ... it's fucking sad.
The author of the article picked a pretty trivial pattern to implement in Haskell. What would happen if you tried to implement some of the patterns that are designed for handling the sharing of state? Then Haskell would probably not fare so well. ;-) And on the flip side of things, one could even say that writing programs in a monadic style is a design pattern to get around the prohibition of side-effects.[removed]Why do so many sites talking about UI have such horrible UI's?

I jump to the middle of a page and the top inch of my browser is eaten by tags.  The text is riddled with hyperlinks and cute-but-useless things like strikethrough text.

Great stuff, here's a markdown-styled link: [maximumcompression.com](http://www.maximumcompression.com)That's an improvement, but I still like Python's syntax better. :)

(And yes, I'm aware that Python borrowed list comprehensions from Haskell. Wasn't aware of the syntax, though.)[removed]has nothing to do with who is manipulating. Has everything to do with when the most people are online using Reddit.You are the exception. Most federal holidays close most government services and banks. Which is the whole point. The majority of Reddit users were not online after midnight last night EST. Most nights after midnight EST he number of users online will be less than during th workday. This is true for many many sites, particularly those with dominantly US users.These test data files all appear to be tiny (&lt;10MB).
Those practice multi-processing game stations look cool! And cheap!And still, most web standards evangelicals promote xhtml strict...|Welcome to Academia.

lol. Fair enough, but there is nothing wrong with framing avenues of enquiry in technical terms, if the enquiry is valid or has potential. Computers depend on rules, simple though they may be at the basest level. Possible rules of interface design, however provisional, should be considered on their own merits, tested, refined and reformulated as necessary and appropriate.

Feel free to reformulate the three hypotheses as you see fit. It doesn't look like the majority of software designers are employing 'common sense' yet.I agree that if a situation occurs enough to merit naming a pattern, then we ought to be building the pattern into the next language (or the next version of a language).  Once and Only Once suggests we not repeatedly implement a pattern--put it in the langauge (or library) and move on until we find another pattern.  Repeat.

The worst thing would be to stick with a simpler language and write thousands of copies of the same 50 patterns.

Should we stick with this:

print "A";
print "A";
print "A";
print "A";
print "A";

Or, is there a Repeat/Iterate Pattern called "for (int i=...)"?Agreed.

"inversely geometrically proportional to..."

...wow.I don't think we need the word 'bullshit' in our reddit headlines. Modded down.&gt; Most egregious is your assertion that quantum computers can solve NP-complete problems in “one shot” by exploring exponentially many solutions at once.

someone had to say it.So how long before we get a redditlite domain, sort of like slashdigg or whatever, except rather than merging both, it just shows the "good links"I figured the Dwave folks were full of it.http://friends.reddit.com/I find it hard to respond to your post because you, out of the seven or so other people that responded, took it upon yourself to assume that I counted myself as some kind of elitist or reddit front-runner.  It's a shame that normal and rational people can so miss a point or even the intent of a post and make themselves look like a douche.

Oh well, we are al anon so it's not like it really matters, right?There's nothing wrong with Microsoft's "shared source" license. The license fulfills its use. It helps both the licensor (Microsoft) and the licensee (you).

Now, it naturally looks quite alien to you when you compare it with open source licenses, which is why it's decidedly *not* called "open source". It's not intended to be remotely similar, aside from having to do with access to source code.[deleted]I believe the irony was that you were railing against the people who rail against the 'influx of newbies'.Haha, tooting your own horn for the win!

Edit: Oh wait, this is Reddit. Ignore the above, I should have written: "OMG M$ SUX LIEK BILL G4TES NEDS 2 DIE"&gt; When designing human computer interfaces (including web UIs):

&gt;    * Minimize the number of text fields in your interfaces down to the absolute minimum necessary.
&gt;    * Minimize the number of click/keystrokes/gestures necessary to accomplish actions in your interface.
&gt;    * Make your interface as responsive as possible - minimize the latency of each and every action a user might take in your interface.

Really?  Wow.  That's so simple!

Oh, wait; it's not.  "[A]bsolute minimum necessary" is defined as ... ?  And you can be sure you've reached that point by doing  or checking ... ?

I doubt most UI designers add gratuitous clicks and fields. Stuff gets added to a UI because someone believes it provides a real value.  The difficult part is in determining such value, identifying cruft, and re-structuring the UI to be most efficient.

It's not that the advice in this article is wrong, it's just that it's a bit like telling someone that to be healthy you should eat right, but then never explaining how to select foods or portions.[deleted]And why is this on the programming subreddit?http://www.cduce.org/ was used to implement SOSS
http://www.caterpillarjones.org/soss/
SOSS is an implementation of a SOAP server for OCaml. "It is not, however, a completely general SOAP server. It's goal is as above, to define an OCaml service and reveal it as a SOAP service. As such it provides no support for more general SOAP features, such as SOAP with attachments."To put it another way, in Haskell, the space leak is necessary for what the program is doing - that memory is still doing stuff. In C memory leaks are by definition aren't needed. Why is the former bad like the latter?[removed]Make it ten bucks and I'll guarantee that all the newbies will go away.Cognitive load is not just dependent on filling in forms and clicking, but also in the interpretation of the interface necessary to successfully complete the interaction. Wordy interfaces, interfaces with meaningless distinctions, interfaces that do not communicate essential versus inessential interactions, and interfaces that remove fields by making other fields more complex can also increase cognitive load.

I do not find much surprising in this article except for its omissions and claims to originality.&gt; Normally when someone denies saying something they do so fairly soon after the event

Why would he waste his time denying an urban legend? I wouldn't.

&gt; fifteen years after it was allegedely made

So you admit that he could not deny it "fairly soon" after it took place because we don't even know the real date.

&gt; he’s trying to rewrite history

As much as I hate Microsoft, it's just a stupid urban legend disguised as a quote, he's not rewriting history.

By the way, why did it took YOU eleven years to publicly state your opinion on the story? Are you trying to rewrite history? Or just trolling...[removed]&gt; Human interface cognitive load is *inversely* proportional to interface latency

[emphasis mine]

&gt; *Reducing latency also reduces cognitive load because...*

Even if you're measuring latency in weird inverse units, like Hz, you've got it backwards one way or the other...Just because...

It's a little prettier with string formatting:

    ten_added = ['***%s' % (n + 10) for n in num_list]&gt; No, the tracker can punish you. It is awfully written, but read the spec.

Moreover, some tracker also keep a record of share ratio per IP/login and will regulate your connection to the tracker.

Of course, you can always get around tracker regulations using DHT.I think he means Perl6, when it comes out in 2012.I dont think its that big a deal and really doesnt matter. No body is perfect and people say things that are incorrect all the time no matter how smart they are? To err is human.http://www.squarefree.com/shell/shell.html is way better.I think comparing computer languages is like comparing natural languages. No body knows which one is better. It all depends on which one was taught to you in your infantry (or intro to programming class). Writing good code depends on the programmer and does not have to do a whole lot with the language itself. Just like English is a great language but how  well some people can use it express themselves depends on how well you know how to use it.No, you can't distribute closed source with GPL..

End-users can combine binary blobs and GPL, but distributors can't.[deleted]1. The main point on the fifteen year thing was that most people would have a hard time remembering if they'd said something fifteen years ago.
2. Note that he did specifically contradict himself between the 1993 &amp; 1996 interview with regards to his opinion on 640k in the early 80s. It could just be that he couldn't remember what he thought at that time. But in which case how sure can he be that he never said that line ?
3. The post ends with phrase “undetermined or ambiguous veracity" which is the term snopes uses to refer to an urban legend which is plausible but has no firm evidence either way. Which seems a perfect fit for this situation.
&gt;Oh wait, this is Reddit.

I think you are confusing Reddit with Slashdot.I agree, the statement in question was probably perfectly reasonable thing to say in the early 1980s. It's the denial that makes it interesting...&gt;Change Microsoft to everybody, and BOOM, what do you get? The BSD licence.

And if you remove that section entirely you get the GPL.True, but my feeling is its far worse now. Back then you could at least still find cool content on the main page...now it's more or less impossible.&gt;From a BSD perspective, the GPL is a bullshit "free my ass" license.

From a GPL perspective, the BSD is a bullshit "own my ass" license.

Edit: if that confuses you, see my following response below.Well, no. Because you have to add a section that removes the freedom to keep the changes you make to the sourcecode to yourself (if you distribute the new binary).What does that mean?

Sourcecode released under the BSD licence will remain forever free for everybody to use/modify/sell/redistribute, no matter what.

Consider a BSD Licenced product, say, FreeBSD.

I decide to make changes to it, so it becomes the perfect operating system for digital watches. Now I can sell the product (with changes) for any amount I want. So if I ask $100 per licence for this new version, the end user is paying for the **added value**, because the original version is still freely available. I think charging money for added value is perfectly reasonable, especially considering you will still have to compete with the original (free) product.&gt;as long as you can keep the rate of change of membership below a certain threshold (perhaps proportional to the total active membership)

Um, that would mean exponential growth.Uhm, yes they can. Where did you get that information?

Ubuntu already contains non-GPL closed source binary drivers for wireless cards.

You're not allowed to compile GPL and non-GPL stuff together into ONE binary, and then release that binary without the sourcecode. But you're free to do whatever you want as long as your software is sufficiently modular.The links on that page didn't work for me either... The page is linking to `jython_Release_2_2beta1.jar`, which doesn't seem to exist, `jython_installer-2.2b1.jar` is what you want.  Try [this page](http://downloads.sourceforge.net/jython/jython_installer-2.2b1.jar?modtime=1170966671&amp;big_mirror=0) or this [direct link](http://umn.dl.sourceforge.net/sourceforge/jython/jython_installer-2.2b1.jar) to the download.We all know that both languages are better.
¿Quién es el NAZI?
da uzaz be stoopid,yo.



It could be ;)  From a Lava paper I read that they used Monads, but it seems to me they could have used Arrows as well.  For a nice application of Arrows see also [Yampa](http://www.haskell.org/yampa/).I don't think I have ever seen a linux thread not devolve into a flame war.
Suop wit dat, yo?
Heh...the freekin thread on this at slash is already over 1100 posts. Heh.
[deleted]Unless it involves Penn &amp; Teller.[removed]What bothers me about the article, and the one before it, is that the author never shows us the Haskell code for all the variants that the java programmer is expected to offer. 

For example, the function 
    static List&lt;String&gt; addTenAndConvert(List&lt;Integer&gt; list, String s, boolean prepend) 

As far as I know, this isn't a one liner in Haskell if you actually want the ability to set or clear the prepend flag at runtime. 
&gt;If you want to critique management, it might help to understand what they do.

Did you actually RTFA, or did your knee just start jerking once you saw the first sentence? 

While it is hardly an earth shattering observation -- and the entry even explicitly states that, several times -- the core "complaint" seems to be that *programmers* endlessly drum up comments about SLOC. It wasn't a chastisement of management at all.

In fact, much of the rest of that blog seems to be more pro-management than pro-developer.So what if he said it? (Not that the post presents any proof that he did.) He didn't say it will be enough for everyone *forever*. If I said today that 1 GB (or make it 2 GB if you want) should be enough for everyone, wouldn't I be, you know, right?Show me a witness that actually heard him say that and I'll belive that he did. Otherwise, I have to belive that it is just hearsay.

And either way, who the (bleep) cares![deleted]&gt; and extensive use of message-passing processes

The simple examples aren't that bad. But then you read about how complicated it gets to make such a system stable (error handling). And you end up using server modules instead of this simple message-passing syntax.


&gt; Haskell's another language you may want to look at for parallel computing (one of the big improvements of GHC6.6)

I've only tried Haskell once. I think I have to use an other functional language before trying again. Nope, programming in Common Lisp wasn't helpful.

Decent article, but it does ignore learning curve.  Eg, vim, emacs, etc allow many manipulations within several key strokes, however, it's very difficult to actually learn them.OK, I admit, I got to the Lemon metaphor and gave up there.&lt;i&gt;It all depends on which one was taught to you in your infantry&lt;/i&gt;

This is my programming language. There are many like it, but this one is mine. ;-)How fast you can run a race depends on your leg muscles, heart , and lungs, but you still want the best shoes for the type of race and road surface.

All  languages come with a point of view; all software is opinionated.  Better to choose something that aligns with how you think rather than use a tool you will always fight.

Sometimes there isn't much choice; the best tool for a job might just have an annoying bias.  A good programmer will just suck it up, learn the tool, and get on wth stuff.

But with Ruby and Python, since they are roughly equally suited for more or less the same set of tasks, it's fielder's choice.And don't forget the possibility to use those for sets and dictionaries:

    &gt;&gt;&gt; set(i for i in range(3))
    set([0, 1, 2])
    &gt;&gt;&gt; dict((i, i*2) for i in range(3))
    {0: 0, 1: 2, 2: 4}Unclear why this is in the programming section, but damn funny anyways.It's misunderstood. He was actually talking about 640K bps: a 640000 baud rate ought to be enough for everyone. But that was before people started downloading porn.I'd like to see where he got the numbers to support that theory.My eyes are bleeding.He does show it:

`addTenAndConvert = map (("***" ++) . show . (+10))`Reducing clicks takes concious effort, even for simple things like letting users double-click on a list item rather than single clicking an item and then clicking the Ok button.
&gt; What would happen if you tried to implement some of the patterns that are designed for handling the sharing of state? Then Haskell would probably not fare so well. ;-)

Why?Yes, and you'd expect exponential growth for a social site, with occasional spikes (or I'd expect it, anyway). The background level of growth is likely to be proportional to the number of users, due to word of mouth, hence exponential. A link in from another large site is likely to create a spike. I'd love to see the growth curve of reddit users. (Any chance, spez and folks?)

If the factor is "double every month" then at any time, 50% of your userbase are n00bs. If you're growing at 10% a month, you have less of a acculturation problem (but a smaller population).

I don't if know the social site guys are trying to control their demographics in this way (I've not seen limits on sign-ups except on non-social sites like gmail), but I suspect the successful ones will need to if they want to keep their culture.nice lo-fi quality, cool pixHooray, you replied.  I have a cold, so I will try to get an energy boost abusing you more, but I will try to be more civil when I feel I have scored points.  I could stand to practice civility, at the same time I practive argumentation.

Anyway, I have learned a lot more about Ruby, and have had an opportunity to make distinctions between the two languages explicit for myself, and I think I could read Ruby code with more confidence.  So for that, I thank you for the motivation.

&gt; It's somewhat the point of OOP that they don't need to know about it.

Only sufficient if the author of the library implements a hook where you can insert your functionality.  Since it is impossible for the library author to anticipate all possible uses, you will run into situations where you need to explicitly patch, monkey patch, utilize generic functions (as suggested in my reply), or write your own version of the library.  In Ruby *everything* is available for monkey patching.

1) You seem to know that not everything is available for monkey-patching in Python, as it is in Ruby.  Your inability to say Ruby is more dynamic in this regard makes me doubt your commitment to truth and valid argumentation.

&gt; as I noted, in Python you can add it to any class implemented in Python.

As _I_ noted, all classes in Ruby, no matter how implemented, can receive new methods on the fly.

2) You dismiss the distinction between classes implemented in Python and classes implemented in C code as functionally irrelevant.  Your dismissal makes me doubt your commitment to truth and valid argumentation.

&gt; That's not a feature of the language, that's a feature of the community.

Give me an example of a language that exists without a community.  Now tell me how much real-world use that language gets.

If we are talking about developers being productive, we much consider the language and the community as a package.

3) You insistence to deem a language's community as insignificant seems petty.

Quoting you twice:

&gt; You can add methods to base classes on the fly

&gt; The "object" class is written in C. You can't add methods to it.

4) Your glossing over of contractions you have made in this thread makes me doubt your commitment to truth and valid argumentation..

You claim to understand the difference between anonymous functions, anonymous blocks, and anonymous closures.  To be perfectly fair to you, I would appreciate to hear your preferred definition of each.

Here how I define anonymous block and closures, for starters:

Anonymous blocks: a description of a thread that can be started and re-stared at will, and has access to a particular state/context assigned by the caller.  Can be defined with the language's syntax, inside or outside an expression, with no need for explicitly naming it.

Anonymous closures: same as anonymous blocks, but the thread also has access to some or all of the state/context of the context with which it was created.

Python has anonymous functions, named function definitions, generators, and now in 2.5 generators with a syntax for receiving arguments from the caller with each restart.  No attempt was made to implement anonymous blocks or anonymous closures, in fact, their addition was specifically ruled out.  In Python, you are encouraged to maintain your own namespace whenever you need functionality like closures.

5) I would appreciate your considered, definitive definition of anonymous functions, anonymous blocks, and anonymous closures.  I cannot take your assertion you understand the difference just on face value.

&gt; Everything that can be done with blocks can be done with functions (anonymous or named), and more: higher order functions can take multiple functions as arguments, whereas Ruby methods can only accept a single block argument.

Only if you disregard syntax.

Lisp has macros as a mechanism to create arbitrary control statements in the language.  Ruby has a subset of this functionality with its anonymous block syntax.  Python shuns anything like either.

6) Unless you address syntax and acknowledge its importance, I cannot assign much weight to your argument that "Everything that can be done with blocks can be done with functions".

&gt; Syntactically, it would be hard to implement anonymous blocks in Python

Hard?  Try putting a "print" statement inside the body of a "lambda".

7) You use the word "hard" as interchangable with "impossible", and it makes me doubt your commitment to truth and valid argumentation.

&gt; Ruby DSLs still look like Ruby. Lisp DSLs still look like Lisp. Python DSLs still look like Python, so I fail to see your point.

A "domain specific language" has to go beyond the implementation language, or else it is not a "domain specific language".  The example I gave, had work done to better match the language "SQL".

8) If you substitute "domain specific language" for DSL in some of your statements, they become trivially invalid or vacuous.  That make me doubt your commitment to truth and valid argumentation.

I grant you one point:

&gt; Second, you have a significant misunderstanding of the term "DSL" if you think that Python's standard library doesn't include any. The % formatting in strings is a DSL. The struct module uses a DSL for packing/unpacking values. DSLs are not always complex grammars.

I grant you this, and I remind you of another example, regular expressions.

But I cannot grant you:

&gt; that's a function of the community, not the language. Ruby isn't "more dynamic" because the community encourages DSLs.

The community is important.

9) You dismiss the importance of the language community several times.  I would have to demand your considered statement of the importance of a community to a language, or else I see no profit in considering your statements on the point.

10) Ruby has language syntax features that encourage DSLs.  Python does not.  Your inability to acknowledge the importance of this makes me doubt your commitment to truth and valid argumentation.

&gt; littering your post with useless vitriol.

It was hardly useless.  I enjoyed it greatly, and my personal joy is of great utility to me.

I submit that my post contained more "compelling arguments", since all your posts, in my opinion, could be demolished, point by point.I take this as the response to [my comment](http://programming.reddit.com/info/15cev/comments/c15k84). Since i've currently no parallel system at hand, i can't test your code myself. Thanks for the demonstration. :)That line doesn't offer the option decide if the string should be appended or prepended at runtime.
Ok, it only prepends.

&gt; As far as I know, this isn't a one liner in Haskell if you actually want the ability to set or clear the prepend flag at runtime.

Prove it.Damnit! I thought this was a clone of Final Fantasy I written in Haskell.It's odd that the author compares scaling Seaside to Rails, as the "shared nothing" approach of Rails couldn't be more different from Seaside's "shared everything".  The use of continuations in Seaside means that each session must have all of its requests served by a single instance, rather than directing them based on load.  It also means that, when an instance dies (which happens often enough that he wrote a script to clean them up), all of its users sessions are out the window too.  A downed Mongrel instance means I may get a timeout and have to refresh my browser.  A downed Seaside instance means I've lost everything.

Seaside is great and I hate to dump on it, but an article on scaling that doesn't even mention this is next to useless in my opinion.  Downvoted.Yes, thank you. This kind of problem is rampant in popular science books and magazine articles often written by qualified scientists who should know better.&gt; write a method that takes a list of integers, adds 10 to each element, converts the result to a String, prepends *** to it and returns the resulting list

How often do you have this kind of problem? I never seen it before and I thing it's a ridiculous solution to a problem the author did not understood correctly. I don't hate Haskell, but its advocates try to solve all problems by: 1. using a Fibonacci sequence (because it's all they show us in their blogs) and 2. making fun of object-oriented programming.map, filter and fold come up all the time in programming, regardless of language. Either your language supports this directly, or get this great "idea" to spend 400$ on a glorified macro expanding text editor.I'm a developer on my project with about 2 years of experience out of school. Our project is (hopefully) going to take off, and my experience with the system means I'm probably going to need to not only self-manage myself more, but also help keep new hires on task and assist others in their accomplishments. I'm going to be need to improve on 'team-lead' skills, and I'm wondering if anyone felt there was any set of books or any one book that helped them in a similar leap from developer to team-lead.[deleted]I can't lay my hands on it right now but I seem to remember hearing that Macintosh computers were good until ... some really insane date in the future.I believe most clients will also try to upload to people who are reciprocating rather than sending nothing. I can't imagine what reason there would be to not include that feature in a BT client, and it seems to be true from my experience (download speeds correlate strongly with upload speeds). So "BitTorrent relies on client altruism" is completely false for active swarms.How can I prove that I don't know if something is possible? Hire a licensed telepath to read my mind?As far as I know, VB.net doesn't support strings.I do that too, because although the article may suck, the title/article may be regarding a topic that I might not want eliminated from my recommended page.[This PDF](http://programming.reddit.com/goto?id=12ldk), see page 6.  I guess it's called eToys (or is part of something called eToys), so you could look that up.[deleted]Anything by Tom DeMarco, [Peopleware](http://www.amazon.com/gp/redirect.html?ie=UTF8&amp;location=http%3A%2F%2Fwww.amazon.com%2FPeopleware-Productive-Projects-Teams-2nd%2Fdp%2F0932633439%2Fsr%3D8-1%2Fqid%3D1172007843%3Fie%3DUTF8%26s%3Dbooks&amp;tag=davidrodgerbl-20&amp;linkCode=ur2&amp;camp=1789&amp;creative=9325) is probably the finest book ever written on the subject. [Slack](http://www.amazon.com/gp/redirect.html?ie=UTF8&amp;location=http%3A%2F%2Fwww.amazon.com%2FSlack-Getting-Burnout-Busywork-Efficiency%2Fdp%2F0767907698%2Fsr%3D1-1%2Fqid%3D1172007886%3Fie%3DUTF8%26s%3Dbooks&amp;tag=davidrodgerbl-20&amp;linkCode=ur2&amp;camp=1789&amp;creative=9325) if you can lay a hand on a copy of it.  [The Deadline](http://www.amazon.com/gp/redirect.html?ie=UTF8&amp;location=http%3A%2F%2Fwww.amazon.com%2FDeadline-Novel-About-Project-Management%2Fdp%2F0932633390%2Fsr%3D1-1%2Fqid%3D1172007955%3Fie%3DUTF8%26s%3Dbooks&amp;tag=davidrodgerbl-20&amp;linkCode=ur2&amp;camp=1789&amp;creative=9325) isn't very well-written, even though it's by Tom DeMarco I can't recommend it.

[Facts and Fallacies of Software Engineering](http://www.amazon.com/gp/redirect.html?ie=UTF8&amp;location=http%3A%2F%2Fwww.amazon.com%2FFacts-Fallacies-Software-Engineering-Robert%2Fdp%2F0321117425%2Fsr%3D1-1%2Fqid%3D1172007989%3Fie%3DUTF8%26s%3Dbooks&amp;tag=davidrodgerbl-20&amp;linkCode=ur2&amp;camp=1789&amp;creative=9325) by Robert L. Glass is pretty good, it covers all the high points of the classics.  Everyone raves about [The Mythical Man-Month](http://www.amazon.com/gp/redirect.html?ie=UTF8&amp;location=http%3A%2F%2Fwww.amazon.com%2FMythical-Man-Month-Software-Engineering-Anniversary%2Fdp%2F0201835959%2Fsr%3D1-1%2Fqid%3D1172008129%3Fie%3DUTF8%26s%3Dbooks&amp;tag=davidrodgerbl-20&amp;linkCode=ur2&amp;camp=1789&amp;creative=9325), but I've never been able to finish it.  It's very dry.

(Above are Amazon affiliate links.)

There's also a set of blogs you can't go wrong with:  [Joel on Software](http://www.joelonsoftware.com) (not to mention the [Joel reddit](http://joel.reddit.com)), [Coding Horror](http://www.codinghorror.com), [Creating Passionate Users](http://headrush.typepad.com), [Eric.Weblog()](http://software.ericsink.com/), and [Rands in Repose](http://www.randsinrepose.com) for a start.

Do read [Peopleware](http://www.amazon.com/gp/redirect.html?ie=UTF8&amp;location=http%3A%2F%2Fwww.amazon.com%2FPeopleware-Productive-Projects-Teams-2nd%2Fdp%2F0932633439%2Fsr%3D8-1%2Fqid%3D1172007843%3Fie%3DUTF8%26s%3Dbooks&amp;tag=davidrodgerbl-20&amp;linkCode=ur2&amp;camp=1789&amp;creative=9325), though.Will this ever be settled?&gt; Has everything to do with when the most people are online using Reddit.

Oh my!  Then they aren't malicious, but instead -inferior-.  Look at the awful articles they push to the top when the right-thinking Americans are sleeping!&gt; Dude - lighten up.

No.  Fuck off.&gt; It all seems a bit like being angry with dogs for crapping on the lawn

Maybe half of the reason is that this person is a rancher, then :-)Nope, they can't.

Linux kernel modules are linked into kernel address space. It is similar to dlopen-inig dynamic library by regular user space application. Just like distributing GPL application with binary-only library violates GPL, distributing kernel with binary-only module is GPL violation too.

Users who do not distribute anything can install binary-only modules and use them in any manner they like. This falls under privacy provisions of GPL.

For more information see LKML. This horse has been beaten to death there.
Too right. MeFi is the standard for a long running community content posting site.Huh. Those blogs are the ones I'm reading already. :)I agree. What he seems to be saying is: 

"As interface latency decreases, interface cognitive load increases." 

Huh? Warning: does not compute!hm ... the Bible, may be ?!&gt;What does that mean?
&gt;So if I ask $100 per licence for this new version, the end user is paying for the added value

Nothing is stopping you from charging for GPL software.  Feel free.  Just make sure to send back your new features to the community.

The difference between a *free* license like the GPL is that it can't be owned.  This means all new features have to remain free and the original never has to compete with closed source proprietary versions of itself.  BSD software is easy to make irrelevant this way, thus why Microsoft and its supporters like it so much.Does it have relativistic karma also?  Because then I wouldn't have to ever even see a comment by someone I disagreed with.And it is not like HTTP, FTP or any of the other protocols can magically create missing data out of thin air either. You always need at least one computer distributing it for every byte of the file.Maybe in 5 years. :-)

The first language that tries to do a thing like "Perl 6" might end up losing steam.

As I said in another thread, I think JavaScript based implementations will fight their ways into the spotlight, even for server side programming. In Java, there's the Rhino implementation. I think in .NET JScript might be available as well, and it's the default in WPF I think. Adobe has their focus on ActionScript, their own version of JavaScript.

JavaScript is the hugest threat to Python and Ruby, syntax and semantics wise.

Hence, in 5 years time, the scene might change. It's probable that Python and Ruby will stay true to their origins and will not try to adapt in a large scale like "Perl 6", but only time and Python 3000 will tell. :-)does anyone understand what bzip2's claim to fame is? I see too little improvement over gzip to be worth all the trouble, to be honest (especially since the pipe version used to have wrong exit statuses etc.)Ditto on Peopleware.  I never had a problem getting through the Mythical Man Month.If everybody loves Bittorrent, why noone uses it? I can't remember the last time I downloaded anything through torrents. Outsourcing file distribution to free upload services (Rapidshare, etc.) seems to be much better solution.A friend of mine is a hard core VB programmer.  To him everything can be done in VB (and he will prove it).  He one time cobbled together a VB based web server...just because.  I don't care what you say that's still impressive for one guy to do.

Super VB guy was really down about .NET.  No, he actually liked the change from old VB to VB.NET for the most part.  But as he worked with it he started to notice something: there were a lot of bugs in it.  That was version 1.0 of .NET.  As each version came out, he found more bugs.  Many old bugs stayed (and not for backwards compatibility) with no word on when, or if, they would be fixed.  Communication on .NET forums always resulted with people insisting he switch to C#.  It has only recently dawned on him the inevitable: VB.NET is dying.Looks like he just turned all features on at once. It is similar to that Firefox screenshot with all extensions installed. In other words, just a pointless exercise.&gt; No, the analogy is right, but it has everything to do with higher-order functions, and nothing with monads, as such.

You mean that every higher-order function is a macro? This is would be a contradiction to your later point that macros aren't composable.

&gt; This simply isn't true (as psycotic happened to point out very well recently,

It doesn't matter what mathematicians do if they are home along or under colleagues. It's about mathematicians teaching stuff to non-mathematicians or beginners. Psycotic's comment doesn't apply there.

&gt; No, they really are just plain functions.

A monad isn't a plain function, it's a *construct build out of plain functions*. A monads consists of at least three functions which have to obey certain laws or it's not a monad. Is a single 'map' function a monad? No, it's just a plain function.

&gt; This principle/similarity is what macros and higher-order functions have in common, not monads.

No. A High-order-function is a much less specific concept. If every HOF is a macro, why do we need macros? The article about composable macros stated the same (page 4) btw. And one HOF isn't enough for doing the work of a monad, the type-system alone won't allow that (talking about Haskell here). Or could you please create a monad using only a single plain function, maybe with the signature Int -&gt; Int.

&gt; Except those bells are not related to monads, but to higher-order functions, and creating the opposite impression just serves to confuse and mislead people trying to learn the difference.

I would say, that you created much of confusion instead of clearing things up. You told that functions are like macros (wrong), that monads are just plain functions (wrong), that they are 'a composable program evaluation strategy' (ok, sounds right but says nothing about the 'how' and is much to abstract).

You try to be as correct as possible and create lots of confusion in the end. I've read many 'tutorials' which were similar and left me more clueless than I was before I read them. So this method of teaching is obviously quite useless.

The reason why I 'got it' in the end were words like 'DSL' and 'code transformation' hidden inside big articles. And especially the 'awkward squad' article which described what happens behind the curtain and made it obvious that all the IO-monad does is simply code-transformation (and the IO monad was after all the starting point of the whole idea). And after looking at other monads with this idea in mind it's quite obvious that they do the same.
Karma is unrelated to comments.

However, the idea is you should be interested in different stories, so you won't be commenting on the same stories, thus you won't see many comments by people you don't like.

&gt;Because then I wouldn't have to ever even see a comment by someone I disagreed with.

Would you really like to hear only people you agree with? That's just sad.Has he been posting the bugs he found on MS Connect? I have found that MS really listens to bug reports, but only if you submit them there.

As for VB.Net dying, I'm afraid that I cannot disagree. Until now the VB team has been playing catch-up, and they admit it. VB9 will be the first release that actually offers a compelling reason to use VB over C#, but it may be too late.&gt; The real problem with TG, imo, was that it was ahead of its time. It wanted to do the 'best of breed approach', the problem with that was what was best of breed when TG was created was not the best option 6 months later.

Two things:

1- Not to beat a dead horse here, but Django was written before even TG 1.0's best-of-breed components.

2- More importantly, why do we assume that what is best-of-breed now won't change in the *next* six months?I'd recommend Rapid Development by McConnell.  It's more about dealing with projects instead of people, but I think you'd find it useful.Excellent advice from poeir. 

You also might consider that you'll be working with or as the PM, depending on your situation, in which case you need to understand the complex tradeoffs between functionality, delivery date, cost, and quality from a company perspective. Fergus O'Connell (How to Run Successful Projects II) and Steve McConnell (Code Complete and Software Project Survival Guide) come to mind. 

Another great book, although dated now, is Gordon Bell's High Tech Ventures. No matter what you've heard about him, this book discusses some amazing work about the factors that have contributed to project success and failure. A good complement to Peopleware and Glass's work IMO.[deleted]He's right of course. Anything can be done in VB. Only the Turing incomplete don't understand that.

As far as VB.NET is concerned: I saw this video of one of MS's languages R&amp;D team and they sure seemed to have developed "a vision" for VB.NET. So hang in there super VB guy. Some good stuff is coming.

L.

P.S. disclaimer IANVBP

?? It's obviously inverse, and he's pretty sure it's more than linear, so he's guessing it's geometric. Seems like a reasonably place to start. Now he just needs ways to test it.is there a 'rule' that NP complete problems are completely functionally and data independent when it comes to breaking them into component parts?Bruce Peren's put it best that, "they hope to get the benefits of Free Software without sharing those benefits with those who participate in creating them."

http://www.perens.com/Articles/StandTogether.html
I know what 'is not' does. You're 'see here' btw has no relevance. None == None -&gt; True, just as None is None -&gt; True. So what? How does that "explain" "pythonic"? BTW, I am a python programmer. I just don't see how this is better written "pythonic." Certainly there are certain constructs that are better expressed in a more "pythonic" (excuse my use of this word here) fashion. However, as "pythonic" starts meaning functional constructs in a non-traditional way (i.e. [x for x in range(1, 5) if odd(x)], instead of filter(odd, range(1,5))), I'm not buying this whole "pythonic" way. It's getting dumb, and it's killing interest for people who are coming from more traditional functional languages, or in my case someone who is learning traditional functional languages.Competition is a good thing. 

Competition between open source projects is encouraged (let the user decide which project is better). Competition between closed source products is a good thing also (hence our distaste for monopolies)

Competition between open source and closed source software is also seen as positive: open-office is partially where it is today because it had to compete with MS Office.

So why is competition of closed source BSD-licenced products with open source BSD-licenced products so bad?

BSD software is not easy to make irrelevant, you're joking right? The open source version stays free, so only if the original version is **so bad** compared to the new commercial version that nobody wants the free version anymore (and rather grabs for his wallet) the free version becomes irrelevant. Sounds fair to me.[removed]"uburbulous deprodication errebelously"

Is this some type of irony, or perhaps a nerd joke that has gone over my head???You don't need a BSD licence to do that.

Selling open source products for lots money is allowed by GPL (and any other open source licence).

If I wanted to, I could sell Ubuntu CDs to people who don't know it's also freely available for download.

In that case wouldn't you agree that I benefit from Free Software without sharing?[removed][Firebug for the win](http://getfirebug.com/)[deleted]this is a repostThere'll still be a slight bias towards 1, due to activity decreasing with time.

Just in case you're really anal, or planning to collect about 200 years' worth of bits.Machiavelli.Did you ever download a Linux distribution? I only checked a few (Ubuntu, Fedora, openSUSE) but most Linux dists offer download via Bittorrent.Absolutely. I'm reminded of this old story:

http://www.djangoproject.com/weblog/2006/dec/06/comparisons/no commentsThis license tries to prevent you from modifying the compiler, using
it to compile something you wrote, and then using the compiled program
to run your business.

I'd say that qualifies as BS.
No, I would not like to only hear from people I agree with.  I am making a comment on the shifting front page hiding elements it thinks I might not like.All good suggestions... I would add [Dynamics of Software Development](http://www.amazon.com/Dynamics-Software-Development-Best-Practices/dp/0735623198/sr=8-1/qid=1172016719/ref=pd_bbs_sr_1/002-0367231-5733624?ie=UTF8&amp;s=books)

It's one of the best books I've read on leading a software team.Yeah, I submitted the proof of why it's a quadratic speedup a little while ago:

http://science.reddit.com/goto?id=14uyz

Definitely interesting reading.Obedience to Authority by Stanley Milgram.I've seen plenty of good team leads!There's an even better graphic for that pseudo-math: `\{ x \in \mathbb N : x &gt; 4 \wedge x &lt; 10 \}` (generators go behind the colon or pipe or the "such that" symbol or whatever).lol

You're funny.The correct answer to Microsoft's question there is, "One time, my program solved an NP-complete problem in polynomial time. I pulled an all-nighter to solve it in constant time."[deleted]&gt; Just like distributing GPL application with binary-only library violates GPL

I think the status of shared/dynamically-linked libraries here is weird. What if I take the library and expose it as a service accessible by sockets? That doesn't seem like much of a change, and surely you're allowed to use a GPLed web browser to interact with non-GPLed web apps, for example?I find this blog entry woefully ignorant in some ways:

&gt;BitTorrent radically shifts the economics of distribution. It's one of the most miraculous ideas ever conceived on the internet. As far as I'm concerned, there should be a Nobel prize for computing, and the inventor of BitTorrent should be its first recipient.

First of all, that might be known as the Turing award, and secondly Bittorrent was not exactly the first implementation on the scene of such a scheme. 

Basically, I find it a little bit overselling and secondly an affront to those who did the preliminary distributed systems research.&gt;So why is competition of closed source BSD-licenced products with open source BSD-licenced products so bad?

Because there is no competition when one side can just copy the code of the other, but not vice versa.Yup, but you're blowing your laziness by using range. range is not lazy—it computes the list up front. You want [xrange](http://docs.python.org/lib/built-in-funcs.html).

(I understand that Python 3.0 will fix this, blowing away the old range and replacing it with what's currently xrange.)Voted down since its not an unbiased comparison.That just makes it unfair, not bad. Competition doesn't need to be fair to be effective.Reading the value *is* atomic but doesn't require an asm lock assertion.  However, that has nothing to do with invalidation -- the invalidation happens when another CPU modifies the value.
Way to misinterpret my comment completely, you uninformed paranoid jackass. I'm not trying to undercut or attack anything. If you had bothered to read my blog(Google my username) you would know that I love social networks like reddit, and I'm certainly not working for any media monopoly. 
My submissions on Digg(where I quickly became one of the top 100 users within a month) and here are part of a purely academic experiment in psychology to see what exactly makes people upvote stories on social networks, and why the same story can succeed or fail with different titles, descriptions, and submitters. I have never been dishonest about that or hid anything regarding my intentions. Certainly, this knowledge COULD potentially be used for commercial purposes, but I have no intent of using it like that. You say "confessed ex-Diggers" as if that's some sort of horrible stigma. Will you next be alleging that practicing Homosapiens are invading reddit?

Jackass.You aren't really getting #2.  Sure, *I* can fix a bug or add a feature, but I can't distribute my work, nor can I benefit from *your* bug fixes or added features.  That's almost certainly the primary benefit that most people get from open source stuff.

That being said, Microsoft has every right to assert these rights over their software.  Yes, it seems a little hypocritical to assert their rights to your work while denying your rights to...umm...your work, but I'm fairly pragmatic about things.  If it met a need of mine, I'd not hesitate to use it on political grounds.  Others would, and that's fine too.  To each his own.Now I think of it, it makes more sense for the name (ie noun) to be capitalised and the nationality (ie adjective) to be lowercased, as in: Karol the Pole is polish so he speaks Polish, Bob the Australian is australian so he speaks English.  So I might go with that in future; it makes more sense.

I don't have the AGPS Style Manual to hand, so I can't check the official word on the matter, but I find it tends to err on the side of Capitalisation that Samuel Pepys would consider a good Thing, some Times...

Interestingly Firefox 2.0's spell check doesn't like "australian" at all, though it's happy with "Australian".  Fascist spell check!  And anyhow, it also doesn't like "Firefox", so what does it know?[Pragmatic Programmer](http://www.amazon.com/Pragmatic-Programmer-Journeyman-Master/dp/020161622X)Perhaps [Influence](http://www.amazon.com/Influence-Psychology-Persuasion-Robert-Cialdini/dp/0688128165) by Cialdini would be useful.  You can use influence for good or evil, but it helps to know all the mechanics.

Also, [Leading Teams](http://www.leadingteams.org/open/index.htm), by Richard Hackman (awful professor, but a good book), talks a lot about issues specific to leadership roles, but not necessarily software."Sure, I can fix a bug or add a feature, but I can't distribute my work, nor can I benefit from your bug fixes or added features."

From what I gather you can do that, as long as it's not for commercial purposes.One big advantage of open source you're missing here is the community. I can exchange code snippets, discuss in large groups how to improve them, then commit them to the source repository.

Can't do that with shared source, because even if you were only to spread around patches, you'd still reveal trade secrets (read: code of Microsoft's) in public, something Microsoft very understandably isn't willing to permit.Your point is valid, although I don't really think a community is worth much. Talk much, code little.AVI with XviD and MP3, original size 183046144 Bytes.

* Compressed with LZMA (`7za a -t7z -m0=lzma -mx=9 -mfb=64 -md=32m -ms=on`): 181397549 Bytes, or a savings of 0.64%.
* Compressed with bzip2 (`bzip2 -9k`): 180946078 Bytes, or a savings of 1.15%.
* Compressed with rzip + bzip2 (`rzip -9kP`): 181016713 Bytes, or a savings of 1.11%.

Funnily enough, rzip (combined with bzip2, as is the default) actually has a slightly negative effect compared to 'pure' bzip2 in this particular case, and LZMA doesn't even reach bzip2's compression level.

I didn't get around to trying RAR, rzip+LZMA, etc. john_b's guess of 1-2% was right on the money.Awesome; cross-platform suck.

The CLR (CLI?) (and Mono's implementation) is actually pretty good, VB just isn't. Blecherous.
More like edit the wiki and give your opinion...it is a wiki after all (And the first wiki, at that)Highly recommended, although not really a PM book. Mythical Man-Month, as mentioned, is great. Death March (Yourdon) is entertaining. The McConnell MS books are quite good, but somewhat ironic. Herding Cats (Rainwater) wasn't bad.Thanks! Very informative :)Because we all know that electoral consumerism is an effective way to affect governments. It's why the USA is such a bastion of democracy.

If there's a problem with reddit then the first step is to complain about it, the second is to figure out what it is, the third is to complain about that, the fourth is to figure out how to fix it. This is how things get done in the real world of politics.Quantum computers - the cold fusion of the computer world.I was in a very similar situation, and read many of the books poeir wrote up. However, the one I found the most helpful was Steve McConnell's [Rapid Development](http://www.amazon.com/Rapid-Development-Steve-McConnell/dp/1556159005).

At least make sure you borrow it from your library and read the part about classic mistakes.&gt;More like edit the wiki and give your opinion.

The problem is that many programmers get very emotional about their pet programming language and will give false and/or misleading information in its favor.48 Laws of Power by Robert Greene.Lots of other good haskell stuff [is on his blog](http://scienceblogs.com/goodmath/goodmath/programming/haskell/), too.Not very helpful advice I know, but it's not what you read. It's simply how confident you sound, how loud your voice is, and how tall you are. In that order. 

Where's the original?&gt;That just makes it unfair, not bad. Competition doesn't need to be fair to be effective.

Thanks for clarifying why someone shouldn't use the BSD license on their OSS project.  I think I'll stick with GPL and the like so I can ensure a fair competition with my competitors.Why do in 1 line what you can do in 31?It was on the third take I realized that didn't say "problems".Nietzche's **Beyond Good and Evil**.Bollocks. Our team lead and tech lead are different people, and both are phenomenal. The team lead's technical background is fairly robust and this is a tremendous help, but he largely stays out of the tech side these days (and has done for a few years now).Or you could check out Data.Set in a Haskell standard library implementation.
1984[deleted]And one VBullet to shoot them all down.B.F.Skinner's "Beyond Freedom and Dignity""Ethel the Aardvark Goes Quantity Surveying"VBis one of the most functional languages ever, if not the most functional language. Certainly more functional than Haskell.My favorite one is ISNULL(field,NULL). wtf? i shout as i slam my head on my keyboardThese numbers basically confirm what we found on the [Great Language Shootout](http://shootout.alioth.debian.org/gp4/benchmark.php?test=all&amp;lang=all&amp;calc=Calculate&amp;xfullcpu=1&amp;xmem=1&amp;xloc=0):  GHc 6.6 is generally an improvement over GHC 6.4.2.

It is also interesting seeing the now 10-years-abandoned HBC still does well on simple h98 benchmarks.It's obvious! in fact it's SO obvoius that I haven't thought of it till now! xDD I love reddit, great article.Well, it's been a slow week.  But there's generally traffic on that list.   And there's lots of new stuff going on in CherryPy 3, Kid, and other components that will be in TurboGears 1.1, which we will be sprinting on next week. 

So, I think any claims of TurboGears death are entirely premature ;)Space leaks are something Haskell programmers attempt to avoid, they are not something we just accept as the norm.[removed](16:00 Sydney time) the benchmarks are currently updating. You're not seeing the full list.

Wait an hour or so for today's numbers.

Note also that small variations are statistically insignificant here (its only a single run of each compiler, with cache dirtying between each run).

*EDIT* 18:21 Sydney time. Run is complete.Right, lets not forget that anything BSD-licensed stays BSD-licensed.  I can't sell the un-adulterated FreeBSD source code as my own without the BSD license still intact.
Would you take a similar look at Tcl?  :-)Note to myself: Use the word "shootout" in the title instead of "benchmark" or "benching".
I like your "[VB is one of the most functional languages ever ... Certainly more functional than Haskell.](http://programming.reddit.com/info/15olk/comments/c15qct)"
comment better.

Keep trying, though!&gt; To him everything can be done in VB

I have a 5MB DLL that would disagree with him.

&gt; I don't care what you say that's still impressive for one guy to do.

Being a stubborn moron is not impressive at all.Can VB do GUIs? Not as far as I know.Ayrnieu, your command of the english language is truly remarkable, and the satisfaction you derive from lashing the ignorant with poetic attacks is clearly evident, but I feel the depth to which you infer from one line comments and the intricate scenarios you generate in response illuminate nothing about the original poster and serve only to surround the issue with a cloud of colorful fantasy whose connection to the actual situation ranges from approximate to less than tenuous.  It just saddens me to see such obvious talent squandered on vicious attempts of self-aggrandizement, and I wonder what life event caused you to turn to the dark side, and if the sweetness of its fruit is worth the toll upon your psyche.Hahaha, he changed his blog subtitle to "Quantum computers cannot solve NP-complete problems in polynomial time."

Note that Scott Aaronson did his PhD thesis on lower bounds and limitations of quantum computers.[removed]King Lear!&gt; If everybody loves Bittorrent, why noone uses it?

From [mininova](http://www.mininova.org/stats/):

&gt; Total torrents in database\t242,779

&gt; Total torrent downloads:\t1,267,314,388

&gt; Total seeds:\t3,338,001

&gt; Total leechers:\t4,283,566

&gt; Total number of trackers:\t2,325

&gt; Total searches today\t3,343,755

&gt; Average torrents added per day:\t772 (32.2 per hour)

&gt; Average torrents downloaded per day:\t1652847 (68869 per hour, 1148 per minute, 19.1 per second)[deleted][deleted][deleted][removed]This and much of the stuff filed under "programming" today have little to do with the subject. Here is a good rule of thumb: does the web page / video have any code? If so it might be about programming.your comment suggests that this view is wrong. but i'm not sure it is. after all, the qbits do evolve in parallel. i agree with you that they way they interact during this evolution is often misrepresented -- neglecting destructive interference. but i don't see why that would invalidate the notion of "parallel computers". the latter is just a bit limited.[removed]Maybe not but you got to admit that someone who comes commenting here and says "I don't care about what you have to say" is not really open minded either. Well I don't feel like he/she is.Yes, it's done like this in not-so-parallel systems, say &lt; 8x, or for not much contended locks.  On bigger systems (more CPUs) this approach appears too simplistic: if several CPUs are spinning, waiting for the lock, the lock is in SHARED state in the caches of all the CPUs and when the holder of the lock releases it (by writing to it) the cache line is invalidated and reloaded (i.e. causes memory bus traffic) on **all** the spinning CPUs.  This can be avoided with "queued spinlocks", basically each CPU spins on a CPU-private memory location and the lock holder releases the lock by handing it over to the "next" CPU, by a write to that CPU's private location.[removed]VB != VB.NET
People who say "VB sucks" when they clearly are talking about VB.NET needs to get a clue. 

Saying that VB.NET is bad but C# is good doesn't make much sense. VB.NET really is C# but with another syntax. Nothing more, nothing less. Sure there are some differences but they are rather minor.

The real difference are the developers behind those two languages. 

VB.NET developers tend to be former VB developers (the others VB.NET developers being the one whose management decided the use of VB.NET because their previous software where done in VB and they fell in the Microsoft trap that "VB.NET is just an updated version of VB".)

C# developers tend to be former Java or c++ programmers.

THAT make all the difference. VB.NET guys tend to be worse than C# guys. Just look at any website with a VB.NET and a C# forum. The dumbest question will always be asked on the VB forum... 


(Note that C# could be replaced by Java in my comment)While I like the idea, 2-5% speed-ups aren't that interesting (especially if they're not cumulative) and taw doesn't show the results of the most promising change (the Fixnum improvement).Download cheat sheets for Visual Studio keyboard shortcuts when developing in Visual Basic, C# or C++.[deleted]It won't matter how large you make them;  log files don't have the kind of redundancy (identical blocks) that `rzip` uses.It's not shared source isn't open source. Nobody said it is.

I looked at what the licence allows, and realized, 'That's not so bad, so why all the complaints?'

Judge the licence on its own merits, that's far more productive.I have two problems with BitTorrent:

1) "Tit-for-tat" *encourages* uploading, but I have no guarantee that I'll get value in return for the bits I'm uploading.
2) Rare content dies out.

I want to implement a micropayment system on top of BitTorrent, preferably with a made-up currency (not dollars).  When you upload, the recipient pays you.  You spend this money when you download.  This solves #1.

Clients negotiate prices automatically and compete on download rates with others that have the same content.  Rare content would be worth more, so people would be more inclined to continue offering it, if they have the disk space.  Share ratios and the obsession over them become irrelevant--a 0 or 0.1 share ratio is okay, because if you've downloaded, both you and the uploader profited from the transaction.

My big problem is, of course, figuring out how to do a secure payment system that doesn't rely on a centralized (*i.e.* corruptible) "bank" or authority of some sort, and that is anonymous.hey thats cool!http://swiss.csail.mit.edu/classes/symbolic/spring07/robust-systems.pdf turns out to have vanished into thin air. :( Now I can only reach its HTML version from Google cache... Where has the original version gone?&gt; But then you read about how complicated it gets to make such a system stable (error handling).

It's not actually that complex, and it makes for **extremely** stable systems. Not many languages can boast being crafted for 9-nines system uptimes, Erlang can.&gt; rzip is great for big log files [...]

`rzip` won't help with logs:  it depends on repeated blocks to give any compression.  What it's good at is archives/backups/images with identical parts/files scattered about.my IT shop is of a 4th variety not mentioned in the article: that anti-drivel variety&gt;Let me flip this metric on its head, and state that, if anything, for a certain domain of project, and a certain class of developer, a high rate of SLOC can actually indicate poor programming practices.

Oh such as cutting and pasting?  Gee thanks I'd never thought of that.I've spent so much time educating people about what load average actually means under Linux. The name sure doesn't help at all. People tend to cling to it as if it's some magic indicator of a server's overall health, when it's really anything but. I definitely wouldn't send people to *this* article, but I have seen other decent ones in the past.Interesting insight into the hopping onto a project and trying to just scratch an itch. 

Was the patch submitted to ruby, and where can one follow its progress?[deleted]I cannot agree with "piranha".. Not the Opensource system shouldnt be made as a payment system .. then many ppl wont get benefitted :( We can see many ppl using the free resource since , they werent able to pay for many things :). In the olden days, email service were under payment system , Only after making it a free service,everyone wished of transferrring messages to all and wished for the communication. Also many ppl are moving to Gmail for more Storage and Free Services . Think, if google make its service under the payment System ! No only can search anything !

Think , If reddit were under payment system ,Even , I wont comment anything :-) Pls avoid such a comment of making the things into a payment service system.

Free Survive ! :)
Just out of curiosity: Why wouldn't you want to send people to this article, and which article would you send them to?[deleted]&gt; Why wouldn't you want to send people to this article

Because it opens with *"In order to view the mathematical notations correctly, please check here before continuing."*

It's just far more technical than is necessary to get the important concepts across. I think it would put off many of the people I deal with.

&gt; which article would you send them to?

An internal wiki page I wrote that includes company-specific examples. I have seen more gentle articles in the past, but I didn't when I quickly Google'd.
One of the most important and most widely unknown issues with load average is that it contains the number of BLOCKED processes too. If you have, say, AFS or NFS filserver issues, you can easily have a significant number of blocked processes, and hence a very high load average - and yet still have basically 100% free CPU.2-5% speed-up required little code, not much familiarity with the source code, and it was done in just a single day. The things I attacked were easy targets in 2-5% range. The real bottleneck is rb_eval+rb_call+rb_call0, and optimizing it could easily double Ruby execution speed, but it'd require a few days of coding and some more familiarity with the source to make sure no border case behaviour gets accidentally broken.

I didn't show results of the Fixnum improvement, because I optimized only one method (Fixnum#equal). To get even 2-5% out of it, all common Fixnum methods would have to be optimized like that, and I wanted to post partial results before going to bed.[deleted]&gt; The inverse geometric relationship results from the compounding of two asserted inverse linear relationships. [...] If each of these inverse linear relationships is represented by the function 1/x then the product of the two is 1/x2 - an inverse geometric function.i downloaded a japanese movie once with a load average of like 40I didn't know what Scala is...  
[Scala](http://www.scala-lang.org/) is a general purpose programming language designed to express common programming patterns in a concise, elegant, and type-safe way. It smoothly integrates features of object-oriented and functional languages. It is also fully interoperable with Java.&gt; And some users may have their firewalls configured in such a way that they can't upload data, even if they wanted to.

I stopped reading at about that point.The tenth rule only applies if the complicated C or Fortran application was written by Lispers...Educating ppl depend on their insterest :) and our motivation level !. Better dont make ppl to read this article , just spread importance , they will help themselves to google around everything regardless the profession !

The Art of being Wise is the art of knowing what to Overlook
[deleted]If the universe was written in C, wizards would find the buffer overflows and enslave us all.Strange as the universe is, I think it's type safe, because I have never seen anyone summoning nameless horrors from the void.These examples aren't comparable to my proposal.

A comparison with reddit would be meaningful if it cost you one credit to post something, but you received credit whenever anyone up-modded your comments or submissions.  Then you could break even, or more--if you actually make a positive contribution to the site.

The bottom line of my proposal is that it extends "tit-for-tat" across the entire BitTorrent system, beyond the confines of one particular torrent.  You earn money whenever you upload, and you spend money whenever you download.  On the same system.  So it doesn't require "real-world" currency to use.

As it stands, BitTorrent users have absolutely no incentive to contribute (upload) after they've reached 100% on a download, except for warm fuzzy feelings.  While it would be great if everything was free with no requirement for anyone to contribute--like pennies from heaven, as it were--it is critical that BT users contribute in order for the system to work.Heh - love the Humpty Dumpty reference, although I don't think it's entirely fair.

Languages are used for communicating concepts - whether you're communicating between two humans or between a human and a computer.

You can define a programming language as "an interface to make a computer useful", but then you're taking into account the network effects of other software, user-expectation and historical flukes of the IT industry.  How then can you claim to be coming to a conclusion about the language alone?

Either tools or designs can have value aside from a specific purpose that you're putting them to, or they can't.

If they can't, it's impossible to say any tool is ever (generally) "better" than any other tool, because you can always construct a situation where the "worse" tool is more useful.

This means (for instance), that a top-of-the-range Ferrari is no better than a Robin Reliant... because IF you're stuck in the desert and IF you only have a pint of fuel and IF the nearest petrol station is several miles away, only the Robin Reliant would get there.

*In this contrived situation*, the Robin Reliant is more useful.  But try claiming "a Robin Reliant is a better car than a Ferrari" and see how many people agree with you.

If tools *can* have an intrinsic value aside from the use to which you're currently putting them, then you have to judge them on this when assessing their worth - Lisp is a better (more expressive) language than VB6.

So, are Ferraris "generally better cars" than Robin Reliants?  Or are Reliants equally as good because you *can* create construct situations where Ferraris aren't as useful?

But which car's designer do you have more respect for?  Why?You're confusing '[functional](http://en.wikipedia.org/wiki/Functional_programming)' with 'practical'.Am I the only one not to find this attractive?All the so-called B&amp;D languages allow various ways of unsafe escapes. Even Ada lets you read in arbitrary adresses as function pointers, as long as you are carefully declaring that this is what you in fact want. 
The only difference is that with these languages, safe is the default, and I think that is completely reasonable and productive.No.&gt; Even expressiveness is subjective.

Not really - you can have different amounts of expressiveness in different areas, but it's still the same attribute/quality/whatever.

For example, Lisp is expressive for parse trees or execution processes.  A Lisp-variant with support for set notation would be *more* expressive, and hence "better".

I never said Lisp was perfectly expressive, just that you can compare the expressiveness of different languages, and that this is a good guide to how "good" the language is.

&gt; VB was immediately popular, even without the influence of VBA. At the time of its release, Delphi didn't exist and writing Windows applications was just plain painful.

(Note the "**as** popular" ;-).

This was kind of my point, though - if there were more (and "better") languages around at the time, VB wouldn't have been as popular.  Therefore, when judging the "intrinsic worth" of a language, considering a historical accident a decade or more ago seems... irrelevant?

&gt; Actually I was asking from the position of someone who had to use an existing language and didn't have the luxury of creating something new.

If we're talking about any given *context*, then yes - the "best language" is frequently not the best *choice*... but this often says more about the problem domain or historical accidents than about the language itself.

(Kind of like how the very best calculator in the world computer in the world makes for a piss-poor draught excluder.)

If we're arguing the intrinsic merits of various languages, then the "best language" is best.

(Kind of like how, if we're arguing about the intrinsic value of a computer - *computational ability* - the computer beats the draught excluder hands down).C is bad, but to really make it horrible, you must try to implement the language features you miss with the preprocessor. 

I hear that was how C++ was born.&gt; But I'm expecting that it will graduate to a language I'll start using (much) more since &lt;a href="that&lt;/a&gt;.&gt;

Are you making a subtle point about Strings here? Like the language-aware strings they wrote about on planet-haskell the other day? :-PThe front page is what encourages people to register here and become part of the community.

When the front page is full of inane chain emails and cartoons, then the majority of people who join Reddit will be the type of people who submit yet more stuff of that nature.

If the stuff you value is on the recommended page but not the front page, then the type of articles you are interested in will slowly become a minority.  One by one, the people who are not interested in this stuff will get fed up with voting on the new page because they rarely see anything on there that they like.  Eventually there will be so few people with interests like you voting on the new page that nothing you like will get any votes and the recommendation engine will cease to function for you.

At which point, you'll look at the front page and realise that the Reddit you once knew has been completely replaced by... well, *Digg*.
&gt;It is also fully interoperable with Java.

Theres .NET interoperability too.There is no need to "save Unicode". The author of this somewhat inaccurate, sloppily written blog post is not a knight in shining armour helping the wounded standard regain its ground, valiantly hacking away at them evil misconceptions.

Unicode is just fine. Misconceptions exist, as they do about any other technology out there. Clueful people rid themselves of them by reading helpful material such as books, tutorials, specs, FAQs, Wikipedia articles etc. etc. with which Unicode is abundantly supplied. What's with the delusion of grandeur?There are two big claims in the article.

1.  Unicode is not a character set\*.  He seems to think this because he's mixed up character sets and encoding schemes.  At the very least, he thinks that a character set must have a language-specific ordering.  Huh?

2.  The idea that Unicode uses two bytes per character is a popular misconception.  Well, I've never met anybody that thinks this.

Wrong on both counts as far as I'm concerned.

*\* Well, technically, Unicode is the name of the standard, but the distinction isn't important here.*
Well if you take the trouble to understand relational theory (and if you are working with DBMSs you really should) then it will become apparent that the logical consistency of the language (which is where SQL falls down very badly) and the performance of the physical implementation are two very separate issues.

I see this implementation of D as a way of illustrating the strengths of the language. Will this particular physical implementation be scalable to large databases? Maybe not, but that isn't really the point. Eventually a native D DBMS will be developed and there is no reason why this should not match or exceed existing SQL DBMSs in terms of performance (and massively exceed in terms of productivity and consistency).The original list has been floating around for a while, but the additions I had not seen before.  Very interesting.The criticisms of SQL are entirely rational. SQL is often logically inconsistent and this leads to many tasks being far more difficult than they should be. One of the core principles of the relational model is that all data should be represented only as relations. You can break this rule with a single statement in SQL.

Obviously it is better currently to use an SQL-DBMS as opposed to a hierarchical DBMS, ISAM or flat files.

However SQL-DBMSs are really PRDBMSs (Pseudo-Relational Database Management Systems). 
&gt; From another perspective, Fig. 2 resembles the charging and discharging of a capacitive RC circuit.

wtf

Is this as much of a non-sequitur as it seems to me?This is a great article just because far too many people don't understand what load actually means.  For anyone interested in getting further into performance tuning and looking at these metrics for meaningful data, I'd recommend this article as a good starting point:
http://developers.sun.com/solaris/articles/tuning_solaris.htmlI originally saw these on uncyclopedia.Cool, thanks for posting.  His comments around Ruby and other dynamic languages are correct--you don't need to actually generate and store the output.  Just use the dynamicity of the language to create classes and objects on the fly.  That was something I had trouble groking coming from a .NET background, but it's very elegant.[removed]It is a good illustration, just the author obviously assumes a perspective that many people don't have.  It's an acceptable assumption that some geeks will have a bit of electronics background, but it's not a very good one.I'm sorry if you took offense to my post and I'm not trying to hide behind anonymity here either. 

I think I get your point, if you're saying that any large group of people will always attract a few "bad apples" that screw it up for everyone. However when you say:
&gt; lovingly reared by a group of devoted followers.

and then

&gt; A lot of people come and shit things up.

how am I not supposed to assume that you're not including yourself in the first group? and not somehow bashing new arrivals?I was ready to post essentially the same comment.  Well put.I agree with you. However, if people use the recommendation engine instead of complaining, they will be able to stay here instead of migrating. Thus, the resulting homepage should be better than on digg where all the good people have fled.

Also, recommendations should have a nice side effect of encouraging people to think more while voting: http://thejordianapathist.infogami.com/recommendation-side-effects

Now of course they are not going to solve all problems, but if reddit has a chance of staying the way it was, I think it's through the recommendation system. That's why it should be promoted as much as possible IMHO.

I gave a few tips on how to get the best out of reddit here: http://thejordianapathist.infogami.com/a_message_to_fellow_redditorsWhile not entirely unbiased, I do think there was some good discussion on there. Some healthy debate doesn't devolve into a flame war is about as productive as any way to compare languages.I usually use 18.18.18.18.media hype[deleted]&gt; However, if people use the recommendation engine instead of complaining, they will be able to stay here instead of migrating.

In order for the recommendation engine to work, there needs to be a critical mass of users voting up the articles that you enjoy.  That involves them reading the new page.

I believe once the majority of people on Reddit like inane articles, the majority of articles submitted to Reddit will be inane, and that will gradually erode the number of people who dislike inane articles and read the new page.

The recommendation engine doesn't solve this because it's a problem with the new page.  The recommendation engine can only hide the problem until it is too late.
&gt; The first language that tries to do a thing like "Perl 6" might end up losing steam.

Python's already doing that with Python 3000.  However they seem to be going about it in a much saner way than Perl.
Mmm, you're right about the new page. I often go on rampage on it and downmod the "lame" stories, but that's not going to work for long... Maybe there could be a "recommended new" page, with only stories submitted by people whose tastes are close to you?

My point is that all users are never going to agree on what should be on the "Front page". Thus there should be no front page, and the system should allow different communities to coexist and interract whithout disturbing eachother. I think recommendations are a good step towards that goal.[PyChecker](http://pychecker.sourceforge.net/).&gt; You can define a programming language as [...] How then can you claim to be coming to a conclusion about the language alone?

I agree that it makes the discussion more difficult. OTOH, it gives it a practical point. But what makes me stick to this definition is my conviction that this is the way it is understood by most people, when they talk about a "better language". If one plans to use this term with a different meaning, a preliminary definition seems necessary to me.

Actually, whatever non-trivial subject one discusses, defining the key terms to avoid misunderstanding should by the mandatory first step, (that's what's taught first in philosophy lectures)

&gt; *In this contrived situation*, the Robin Reliant is more useful.

I completely agree, and I think this answers you question:

* the efficiency of your car/language can only be defined relative to some usage conditions;

* if these conditions are not explicitly specified, one should fall back on the most typical conditions, because these are what most humans will spontaneously think of, and well, we're discussing with humans. However, it's always much better to be explicit.

* if there are no such default implicit conditions, the question is just silly.

For cars, the common usage conditions would probably include considerations about social aura, reliability, affordability (incl. oil/repairs/insurance), ability to run on "typical" roads (this is context sensitive), etc. 

I guess I'd rate Ferraris higher than Robins, but I'm not sure I'd consider them as better *cars* than a Honda Civic. I'd probably consider them as finer *pieces of engineering*, but I'm not even sure: if you take into account all the logistics involved into producing millions of reliable cars for a low price-tag, the Civic is quite awesome.

Anyway. To come back to languages: Lisp is more expressive than VB6 in the context of highly algorithmic problems, where no widespread library solves the issue, no teamwork is expected, deployment is not going to be tricky, interacting with a win32 GUI is not required, etc. I think these conditions can be described as "contrived", although I love it when I work under such assumptions.

And I know that `forall $X. "Lisp can $X"`, but this only brings me back to my original point: Lisp could but somehow doesn't. Neither in corporate nor in open-source contexts, i.e. in most non-contrived conditions; and the [smug lisp weenie](http://www.c2.com/cgi/wiki?SmugLispWeenie) attitude of accusing some mediocrity networking effects, without much more self-questioning, is absolutely sterile. I actually think it's one of the main roots of Lisp's relative failure.Neat. Should be built into the language implementation, though, right?

And I stand by my assertion: scripting languages should require explicit declarations of variables, so it's 100% clear where variables are coming from. PyChecker isn't going to find logic errors resulting from typos in variable names accidently creating a new variable, for example.&gt; Maybe there could be a "recommended new" page, with only stories submitted by people whose tastes are close to you?

Possibly, but doesn't that just move the problem one step away?  I actually like the "cloud view" Digg has, where the popular new stories get larger and larger the closer they get to the front page.  Perhaps Reddit could factor in your taste too.  In any case, I don't think the linear new page works very well.

&gt; My point is that all users are never going to agree on what should be on the "Front page". Thus there should be no front page, and the system should allow different communities to coexist

I think that would only drive away the casual visitor group that new participants come from.  Perhaps once the tagging is in place, the most popular tags can each have a section of the front page, so no individual theme can dominate it.  A bit like Google News.
Another humorless Haskell defender ?&gt;I actually like the "cloud view" Digg has, where the popular new stories get larger and larger the closer they get to the front page.

Isn't that what we have with the "rising" view? I'm genuinely asking, I don't use digg anymore since it nearly crashes my PC :)

&gt;I think that would only drive away the casual visitor group that new participants come from. Perhaps once the tagging is in place, the most popular tags can each have a section of the front page, so no individual theme can dominate it. A bit like Google News.

What you're describing is very similar to the current del.icio.us frontpage, which is actually pretty good. 

What I like about the reddit recommendations is their automated functionning. If I change my tastes, I don't have to reconfigure anything, just change my voting habbits and in a matter of days the recommendations will change. Of course that's not going to work for the new visitor. Maybe they could be asked to answer a few questions that would help the system classifying them? Like "Do you want to read GWB stories?" "Are you into science?" "Are you interested in Paris Hilton?"&gt; Should be built into the language implementation, though, right?

Debatable.  I kinda like having both the freedom to shoot myself in the foot and the tools to avoid shooting myself in the foot.  There's also more you can do with a standalone tool - PyChecker has all sorts of configuration options, so you can customize just what you want to be warned about.

I do think it should come with the standard distribution, and perhaps be used by default.  It's only an easy_install away, though.

And it should definitely get more publicity.

&gt; PyChecker isn't going to find logic errors resulting from typos in variable names accidently creating a new variable, for example.

It will, actually, as long as you don't typo the variable name *twice*.  It checks for variables that are initialized but never used, so if you accidentally create a new variable, it'll be flagged as "never used".QC first came out as a thought experiment to make some predictions about the "Many World" interpretation of Quantum Mechanics. Some people thought that perhaps this would mean we could make a computer that could split off an arbitrary number of universes at each step of the computation. They were wrong. But that thought ball, once rolled, was very hard to stop.

Today, any journalist can go on arxiv.org and download a few papers proving the lower bounds. We should not forgive them for neglecting to take 5 minutes to do so.Agreed. I did the same years ago.[deleted]looks like GHC and GHCI are the only reasonable implementations to use for real applications. all the others have too many outright failures.Ruby doesn't work with -fomit-frame-pointer, its GC needs the frame pointer to walk the stack correctly.

[google search](http://www.google.com/search?q=ruby%20fomit-frame-pointer)Not completely. Of course, the list is created and held in memory, which takes space. But for small n, range(n) is more efficient than xrange(n) and dict and set iterate over the iterator completely anyways.

Additionally, the lazyness is not completely lost; e.G.:
    g = (long_computation(i) for i in range(5))
    a = g.next() # long computation here
    b = g.next() # long computation hereI bailed on RedHat years ago.

But this rant seems curiously familiar. ESR started off with some very valid complaints about CUPS:

http://www.catb.org/~esr/writings/cups-horror.html

Then he tangled with the Fedora crew extensively, mostly over their decision to exclude proprietary codecs from Fedora:

http://lwn.net/Articles/178707/

And he's apparently had a run-in with the Subversion folks, too:

&gt;  The Subversion development community has been subject to Eric's drive-by flames, too. On several occasions. At least one of those times, we had to send Eric email off-list to ask him to stop posting and leave. By that point, everybody had flipped the bozo bit and begun ignoring him entirely. Thus, his continued rants were merely sucking energy from the community with zero potential for any positive outcome.
&gt;
&gt; Since this has happened several times at Subversion, and now I learn with Fedora, too, I can only conclude this is Eric's "typical" pattern of interaction with communities nowadays. Unfortunately, it is rather uncool, disrespectful, and unbecoming of him.

http://lwn.net/Articles/178934/

Obviously, ESR is very concerned with free software usability, and not without reason. But I'm not sure that ESR's "Goodbye, cruel world!" approach is actually influencing many people at this point.And here I was hoping to read that the memory footprint of `scala` and `scalac` was being reduced to something usable. But I suppose this is the 'ropes' when you're built *atop* the JVM in the first place. Scala seems a very interesting language and is quite usable on a large system, but a typical VPS or shared hosing environment which would be more than capable for a variety of other software languages (including Java) does not have the memory space available to compile or run Scala programs. This, when the target market for Scala is *expressly* web services and components.&gt; Isn't that what we have with the "rising" view?

I just took a look at Digg and apparently that feature is useless now.

Rising is *sort-of* the same thing, but I think the cloud view is a much better way of visualising it.  Here's a [good example](http://www.readwriteweb.com/archives/bill_gates_ces07_tag_cloud.php) of a cloud view.
[removed]Ah, nice.  I just got this in dead-tree form from Amazon yesterday :)

So far, it's definitely been worth the read.So what he calls "code generation" is what the rest of us call "macros".  Yes, he's technically correct about "code generation" but whenever people talk about it stinking they are referring to the crap spewed out by so-called "IDEs" which does get checked-in.

"Code generation" is usually a sign you're working in too low level a language.  He also mentions this, though, going on to put forth Ruby as the example to strive towards is well, amusing.
Suddenly I'm hungry for Wendy's.I'll take "I just learned to program" for $1000, Alex.

Seriously, this sounds like someone who just read a few chapters of Learning Perl and spent half an hour chasing down a typo.

Amen.That's funny, I gave up on ESR a long time ago. People still care about what the fetchmail author thinks?[deleted]Years ago, one of my clients did some consulting for IBM. Based on that experience, here are my definitions of "enterprise system":

1) An enterprise system is any system purchased by non-technical management based on a PowerPoint presentation.

2) Enterprise software starts at $40,000 to $75,000[1], often for something that Google or Yahoo would download for free from freshmeat.net.

3) With any luck, the huge price premium paid for enterprise software means that the vendor will have a huge incentive to keep the customer happy. Pay me $40,000/year for Apache, and I'll care about your problems, too.

Basically, enterprise software is generally about spending lots of money to make up for a lack of technical expertise. And this is a rational tradeoff for many organizations. But you can go in the other direction, too: A few retail organizations keep heavy technical staff in-house and blow off 95% of the enterprise vendors.

[1] For the pricing rationale behind enterprise software, scroll way down this page:

http://www.joelonsoftware.com/articles/CamelsandRubberDuckies.htmlI think that most of the time you're best off if you can generate the code you need on-the-fly, without resorting to creating and compiling files on the file system. Done with discipline, source file generation is perfectly valid, but more often than not programmers eventually start modifying the generated code, and this is the source of a lot of grief.

A few useful applications of code generation are: building a database API from a live database, generating a lexer and parser from a grammar specification, and creating bindings to C libraries based on header files. In all of these cases, the authoritative source of information is somewhere besides the source code of the program you're writing, so code generation can save you from duplicated effort.

It's easier to generate code on-the-fly with dynamically typed languages, so these techniques of generating lots of static files seem to be more common with languages like Java.&gt; To get your copy of repository in sync with master repository run svn up.

This updates the working tree from THE repository. There's no "your repository" and "master repository".  Working tree is most definitely NOT a repository, unlike in sane SCMs.

&gt; ... but most of the time I work on just the main branch and merge by plain Unix diff and patch.

Which says a lot about SVN merging capabities, or lack thereof.Translation: ESR screws up his system, has hissy fit, takes all his marbles and goes home.

Linear time-invariant first order differential equations usually have solutions that look like that. RC circuits are an example application. Really basic continuous time low pass filters also look like that (interestingly enough, I'm pretty sure that's why capacitance in CPU wires is such a problem, it attenuates higher frequencies, limiting clock speed).

I'm pretty sure that the load average wouldn't do a differential equation though (granted, I only took a cursory glance at the article) and instead just does a difference equation (discrete time analog of a differential equation). I'm too lazy to do so, but it might be nice to map his end equations from continuous time to discrete just so they're a little more accurate. (AFAICT, just CT DE solution -&gt; Laplace transform -&gt; substitute to go from s to z domain -&gt; inverse Z transform -&gt; DT DE solution)Evaling code is something a lot of languages can do and is nothing like real macros, not even C preprocessor macros. Building objects and classes on the fly is also something a lot of languages are beginning to pick up on, and it's been around since Smalltalk. That's not like macros at all either.You need to check the reverse implication as well; bfs could well be returning true in all cases.  You also didn't mention if you collected any statistics on the sizes of the generated trees.  They seem all right, but with randomly generated test data, you need to be sure -- it's all too easy to end up with trivial tests.


Edit: a) should've posted this to the blog, b) beaten.This is not the full copy of the real book.
man -k load | grep average 

getloadavg           (3)  - get system load averages

tload                (1)  - graphic representation of system load average

man getloadavg
[deleted]The book was based on his thesis which is linked here.

Highly recommended by the way.Just because ESR can come off a little, er, can I say nutty?, it doesn't means he's necessarily wrong.  Python and Ubuntu are both pretty cool.Ahh, Tcl. Forever in the legacy of "everything is a string" thinking. I'll always have a soft spot for Tcl... =)You put too much of yourself in when reading other people's posts.  Obviously you would rather be in the "devoted followers group" instead of the "group that shits things up".  So you automatically assign me to the latter group, feel indignation that I'd have the balls to claim that place and shoot off an angsty post.

ProTip: Be a bit more impartial on reddit.If he could give up filling the Jargon File with his nutjob neocon bullshit I think we'd all appreciate it.No wonder you creatures construct such ingenious traps -- your knowledge of mathemagics is astounding!
Forget all the goobledi gock, my understanding is the rule of thumb is this (this comes from a highly respected unix admin, I'm just an Oracle guy):
for each CPU: 
a load of 1 or less is lightly loaded.
a load of roughly 2 is "well utilized"
a load of 4 is maxed out.
For the mathematically challenged, a 4 CPU server would be lightly loaded at 4 or less, well utilized at 8 and maxed out at 16 or more.
There are several errors in the Ruby information.  Off the top of my head:

* routes are not a beta feature of Rails.  They've been available in the stable version of Rails for quite a while now
* rails supports multiple databases.  Models can even use things like ferret (Lucene for Ruby), memcached, or even _nothing_ as a backing store.
* You can retry exceptions in Ruby with the `retry` keyword
* All the discussion about passing scope revolves around `callcc` instead of `binding`.

The article also has a whole lot of debate about the C APIs in Ruby and Python, completely forgetting that they both kind of suck.  Neither is suitable for embedding (see the SpiderMonkey and Lua APIs for better examples of embeddable interpreters), and neither has a particularly consistent naming scheme.

I like the article overall, though.  Particularly the discussion about the merits of using blocks and about foo.bar syntax representing a reference to a method (Python) versus a method call (Ruby).[removed]Don't suppose anyone has experience with this and can say how well it works with py2exe and py2app?OK, it's very cool in a retro sort of way, but can someone explain to me what it's FOR?Excuse me, but what does this have to do with programming.reddit.com?Yeah, no shit, huh?  

I've got an idea -- how about we have a programming language where you can't have any variable names that aren't real English words?  That would cut down on typos -- and you could just use Microsoft Word or Open Office as your text editor. 

And if you're using global variables so much that the way python handles them is a major source of inconvenience, you're almost certainly doing it all wrong.  Global variables are evil, and often a sign of bad design.Solutions to this "problem":

- Unit test your code and get a code coverage tool which automatically tells you if any code isn't being called/tested.
- Don't use cheap, lying, no-good, rotten, four-flushing, low-life, snake-licking, dirt-eating, inbred, overstuffed, ignorant, blood-sucking, dog-kissing, brainless, dickless, hopeless, heartless, fat-ass, bug-eyed, stiff-legged, spotty-lipped, worm-headed sack of monkey shit languages like PHP. Hallelujah, holy shit. Where's the Tylenol?I bailed on linux years ago.  Redhat was always the crappiest distribution - by far.  It's just awful.People use a codebase from two kinds of incentives - commercial (I add some value and sell a product) or karma (I give back code and have impact). BSD codebases can act either as 'commons' infrastructure (for commercial incentives) or as communities for innovation (for karma incentives), while GPL codebases foster innovation either by community or commercially as services. Shared-source fosters innovation only by community; MS takes any commercial opportunities. The reduced incentives affect where value is added in the tree of code user-distributors. Consider each of these licenses from my perspective as a potential user of a codebase.

1. With a BSD license I can choose to make my modifications proprietary or open. Since I have plenty of incentives, I am willing to try out even a codebase that provides only minor functionality. Value can be added anywhere in the tree of providers and consumers (of source code). There's no limit on the value added; if I suddenly create something hugely valuable I can instantly choose to close my sources.

2. With a GPL license I am forced to keep modifications open. From a commercial perspective that makes me less willing to try out a codebase: it must provide significant functionality that I cannot easily reimplement. And if I suddenly find myself sitting on something of enormous value I will take the time to remove GPL dependences rather than release it openly. Like with BSD, value is added everywhere in the tree of providers and consumers. However, no single point is likely to add enormous value. The spirit of reciprocality allows for significant value to be added in gradual increments from user-distributors of code, and this value can be used commercially as a service.

3. Shared-source is of narrower interest than GPL in that it excludes *any* commercial use of the codebase by anybody except MS. The reliance on only karma incentives restricts the space of users -- academics are a more substantial fraction, especially because of MS-funded projects. It also raises the bar on the value created by a shared-source codebase before one is moved to add to it.

\---------------------------------------------------------------------------------Don't feed the trolls, right? Well, I can't resist. Actually, I've been programming professionally and for fun for many years. And this is a common theme I've seen in scripting languages. Maybe you're so smart that you never write bugs -- most programmers do make bugs, and spend more time debugging than writing code. 

Do you have any complaints about the points in the article, or is that all ya' got?
The funny thing is that I have seen the majority of the serious entries in actual papers or books.To me, ESR giving up on Fedora is almost an endorsement.&gt;Global variables are evil, and often a sign of bad design.

Or a sign of a relatively small program; in other words, exactly the kind you write in scripting languages.I've never understand why there are languages with implicit declaration of variables. It's just plain STUPID. Yes: S-T-U-P-I-D. It has no advantages but only disadvantages. 

It's absolutely unnecessary and only leads to errors. Once again: Stupid. 

The last time I created a scripting language (must've been about 15 years ago), I invented the following:

    x := 10    // declaration &amp; assignment
    x = 2      // assignment only

So if you only write

    y = 10

it's an 'variable not declared error'. And if you write

    y := 10
    y := 20

the second one creates an 'variable already declared error'. Simple as this. Still dynamic, still easy to type etc but the risk to typing errors is extremely reduced. Only 'disadvantage' is that you sometimes need dummy assignments.

Of course it only works with lexical scoping - general dynamic scoping is just as stupid as undeclared local variables. But sometimes it's useful (and has advantages over global variables), so my old language simply used a '$' prefix for variables with dynamic scoping. But even here: Nothing without declaration. So "$x = 1" was still an error, unless there was a "$x := ... " somewhere before.

Now I expect the whole "I want to make errors. I love to make errors. If I can't make errors, I'm not a real MAN!" sermon, which always happens is someone creates something making things more secure. Like safety-belts, hard-hats or variable declarations.
Yeah - I'm surprised yhc is so good, and hugs is so broken - I thought hugs was much older and more mature?[deleted]Pardon my ignorance, but what's a "scener"?Correct me if I'm wrong, but isn't there simple and elegant solution to prevent this type of error without requiring programmer to declare the variable ?

It should be fairly easy to check that a symbol name, appearing on the RIGHT SIDE of a statement, never was on the LEFT SIDE before. That gives you undeclared variable.
In this respect (implicit variable declarations) PHP is no worse off than more highfalutin' languages like Python or Ruby.

Unit testing or code coverage tools may solve this problem, but requiring explicit variable declarations would solve it more quickly and easily. And make the code easier to read, too. So why isn't this the norm?He's a couple of punched cards short of a program.Does it matter?  It's a big dollop of old-skool Amiga 1.3 goodness! :-)In the article, the first example I describe specifically is *not* caught by this check:

    PhaseInverterFound = nil
    foreach starship in Starships do
      if starship.IsPhaseInverted() then
        PhaseInvertedFound = starship  # TYPO!
        break
      end
    end
[deleted]I've been programming in Python and Perl for many years now, and I am yet to need to generate code. Where I'd "generate code" in C++ or something, I can use some combination of dynamically created classes and closures to do what I need. In both languages there are little corner cases that are still a bit klutzy, but it's better than code generation.

So, survey: Has anybody found something in Python, Perl, or Ruby that they _absolutely had_ to implement as a string which was then either dumped to disk and read in, or eval'ed? (We might also add Javascript, since the only time I ever "eval" anything in Javascript is when reading in a JSON-style data file from an XMLHttpRequest.)

(One exception: Perl has support for "autoloading" modules if you push an object/function onto @INC. This requires you to actually return a file, though it can be a string-based file, so I'm actually generating a string for that. Still, I generate a minimal "package" specification for the autobuilder, then fill the package out with code that makes heavy use of closures and stuff. (It's an ORM that is generating a class based off of the database.))

At least in terms of generating code for programming languages, I think we're going to see that idea die. (Generating SQL from your programming language to make tables that match your model, or dumping out constants for some other environment, probably isn't going away.)Which distribution were you using?  I'm not a fan of linux but there are some distributions that get package management right.  I wouldn't give up on Linux entirely if it was something specific to RPM.  As an aside, FreeBSD I think is a terrific replacement for Linux.  

My reasons for abadoning Linux go far beyond package management though, and I understand that while Windows has its problems it (well, AD) has a lot to offer, especially in the enterprise environment.Unless you do some neat meta-programming tricks that magically introduce new local, global or instance variables.Duh! Yoг are right. Option Strict is the only way.They were originally planning that for Python 3000, then Guido saw what was happening with Perl 6 and backpedaled a bit on the wishlist.[deleted]Okay Dijkstra, programmers frequently use computers, and these  computers are frequently network enabled.  In particular, many are connected to the internet.[removed]Unfortunately the .Net backend has kind of been abandoned. Martin Odersky feels the maintenance isn't really worth it and  a lot of Scala users are migrating Java people who have no interest in the CLR. However he's willing to hand over maintenance to a suitably qualified person.

If I wasn't such a wuss and a compiler ignoramus I'd volunteer.It's generally agreed that no natural languages are significantly more "Primitive" than others. They have complexity in different areas. Conversely, there are great differences between the complexity and level of abstraction in programming languages.[removed]"Only AJAX can make it possible."

Mmm. No. A java applet would work a lot better, though the load up time is worse. Example:

http://www.gagaplay.com/jemu2/applet/index.htmlSo? It still doesn't belong in the programming section. This isn't sysadmin.reddit.com or some such.Actually MS changed the name from VB.NET back to VB in version 8.

&gt; Saying that VB.NET is bad but C# is good doesn't make much sense. VB.NET really is C# but with another syntax. Nothing more, nothing less. Sure there are some differences but they are rather minor.

That really depends on what you are doing. If you have to do late bound code, there is a significant difference. Likewise if you don't like explicit type casts or even declaring variables. The next version should see even more divergence as VB is expected to become a more dynamic language while C# is remaining static.

&gt; C# developers tend to be former Java or c++ programmers.

That doesn't mean they are any good. There are countless Java programmers that don't understand the difference between by-reference and by-value parameters.

Likewise, the only person I have ever worked with who didn't understand what a static variable was or why you cannot use it to store user information in ASP.NET was a C# programmer. For weeks we had a major security problem because context switches would randomly give a user another users permissions.

Finally, a lot of C#/ASP.NET programmers come from a ASP/VBScript background. 

&gt; VB.NET guys tend to be worse than C# guys. Just look at any website with a VB.NET and a C# forum. The dumbest question will always be asked on the VB forum...

That isn't a fair assessment. The dumbest questions will tend to be asked by the more novice programmers, and VB claims to be a better langauge for beginners and thus should be seeing more novice programmers.

The influence of VBA should also result in more non-professional programmers trying out VB over C#.
Those are formulas, not numbers. A number would be something like "the cognitive load caused by a two button dialog is 12.7 braincells/second".This is some sort of veiled drug reference, right?[removed]See: [Demoscene](http://en.wikipedia.org/wiki/Demoscene), [Chiptune](http://en.wikipedia.org/wiki/Chiptune), [Amiga](http://en.wikipedia.org/wiki/Amiga) [Workbench](http://en.wikipedia.org/wiki/Workbench_%28AmigaOS%29)That pretty much sums it up.I’ve noticed this on some blogs. When a widget (recent posts, categories, archives, etc.) gets too long and you have to scroll down seemingly like forever, that widget either gets its own page or gets trimmed in some way. I’m here to offer an alternative, but it’s not for the code queasy. You’ll have to edit your theme by hand.I'm forced to admit that I like Perl's my; it's non-intrusive in my opinion and it makes sure you don't have misspelled variables.  Smalltalk has the vertical pipes to do that.[deleted]And the fun continues in C++ with templates tricks :)
yeah, his shock that "man average" didn't return anything... that was sort of weird. 

If he'd used "apropos average" he could have found the getloadavg man page pretty quickly. With that man page, he could have found that:

"The  getloadavg  function returns the number of processes in the system run queue averaged over various periods of time", namely the last 1, 5, and 15 minutes.There looks like a big win available here: a development methodology that combines formal specification, quickcheck and test driven development.

Paul.The point is, macros (especially Lisp macros) do the same thing better; it's only in retarded languages that you have to call it code generation and use special extra-lingual programs to do it.I'm pumped.  The DataTable parts sound cool, but I'm mostly just glad that Yahoo is continuing to develop this library.A common theme is to not create throwaway variables (ones used for over a short span of code) with *massive* names, use pychecker, or in his case, just:

     StarshipFound = [x for x in Starships if x.IsPhaseInverted()] and True&gt;Question:
&gt;How many parameters should a function have?
&gt;Answer:
&gt;The minimum necessary to achieve the function’s goal(s).

Now that's enlightenment !And how many of these are legal? I'm talking about legitimate uses.To Lisp programmers, any use of "eval" is an automatic code stink.  This is something that was realized decades ago.

"Building objects and classes on the fly" is only a startling result when you think classes are a compile-time-only construct.

I did neglect to mention "higher-order functions" mostly because I didn't want to confuse.  Macros cover code generation at compile-time, and functions can express everything else.  Of course, in real languages, macros are just functions which happen to run at compile-time and on the domain of source code.

What I found amusing about Ruby is not that it's a terrible example of a dynamic language, but rather, there are much better and more mature examples out there.  For example, languages with robust and efficient implementations and much more thoughtful design (like, say, Smalltalk).
No. But then if you believe that everyone uses Linux, you may as well believe that everyone uses Bittorrent. In reality, both of those are not used by 90% of users.[removed]You've lost me at *bam!*, young grasshopper.

Variables are far more complicated than they look like and as your experience increases you will learn to treat them appropriately. If you still forget your variable names or make too many typos you are probably over-worked and need to take a rest.


There will be times when you are able to challenge design decisions. This is not one of them.&gt; To Lisp programmers, any use of "eval" is an automatic code stink. This is something that was realized decades ago.
"Building objects and classes on the fly" is only a startling result when you think classes are a compile-time-only construct.

Yup. My point was that these techniques don't try to be like macos. Perhaps macros solved the problem long ago, but Ruby programmers like the richer syntax that can be used if you don't have to make macros easy (yes, I know Dylan and Nemerle and probably other infix languages have macros. The thing is, they're hard).No, PHP is significantly worse off than Python, Ruby, etc., because of two tiny PHP design decisions that interact horribly.

Decision #1: Global variables are not visible from within a function unless specifically declared as global _within_ that particular function.

Decision #2: Reading an undefined variable in PHP will return a null value.

Therefore, every time I use global variables in PHP, I spend 40 minutes trying to figure out why they're null.

(Oh, and for what it's worth--I haven't verified that the latest version of PHP still works this way. They do change a lot of stuff between major revisions.)

Many other scripting languages use lexical scope (making global variables automatically visible from within functions) and raise an error if the program tries to access an undefined variable.And so your point is?That is pretty funny, because a similar event what made me drop Windows and finally switch to UBuntu for good. THe Wireless drivers for my Intel2200BG never worked quite right on windows, after the 2nd or 3rd (can't remember) update (which always meant installing huge megabytes of additional software) and ticking the wrong checkbox, I was left unable to log into the system. While I could go into safe-mode, setup would not let me uninstall the wireless software. After a whole wasted on this nonsense without making any progress and nobody able to help me, I installed Dapper - at first only meant temporary, but never switched back. Never again had a problem with wireless either. 
Things have gotten so much easier and so many more possibilities that I can't imagine going back to Windows either. (Note I've been tinkering with Dos/Windows since '93 and with Linux [on and off] since '96).everthing in IT is...[deleted]I use M4 all the time primarily for specific-purpose code generation. Do I think there are better ways to affect the same functionality? Maybe, but I don't always care. Sometimes the best is the enemy of the merely good.Good.  Now let's give up on Stallman, too, and we'll finally be able to stop worshiping open-source and actually address all the problems with it.  Maybe even approach it with a down-to-earth, unbiased point of view...But I want to make errors. If I can't make.... I mean, hey, that's a neat idea.As a long-time Linux user I have caused the system to break on quite a few occasions. Most times I have managed to claw it back (for some reason I have found bringing myself back from the brink of a re-install to be easiest under Gentoo).

While it can be annoying when this happens, I like to accept it as part of using Linux; indeed some of my best learning experiences have been a result of shooting myself in the foot.

To conclude, I am at least grateful that most of the time I am able to get myself going again. A lot of people who develop problems under Windows, for example, either limp on with the problem or have to re-install. And I'm not just talking about the not-so-technically savvy.&gt; But while [currying] interesting mathematically, and perhaps from a theoretical point of view, in everyday code it simply seems like a fantastic way to obfuscate your code. Take a look at Wikipedia’s JavaScript example of Schönfinkelisation - which should be just about understandable to most programmers, even non-JavaScript programmers - but demonstrates the point perfectly.

Why not take a look at a programming language that actually suppots currying natively before passing judgment on this one?WowYeah, who would ever want to know what something actually measures and how?  Just use magic numbers and all will be well.
[deleted]Shoot the messenger, but the message is sound: Dependency management in FC is weak. Ubuntu has their act together in that regard.True, but he asked for a team-lead book, not necessarily a PM book.  Nonetheless, [The Art of Project Management](http://www.amazon.com/Art-Project-Management-Scott-Berkun/dp/0596007868) is another good read.Got an example?  I've written hundreds of python and jython scripts; I currently have about 200 on my computer that I've bothered to save over the years.  I just grepped thru them and I only found one -- with some really poorly thought out multithreading I wrote while I was first learning the language -- that uses global at all.  

I really think that whatever you're using global variables for, there is an easier, cleaner way to do the same thing in Python or any other scripting language.  Or maybe I just have a really idiosyncratic style of scripting.I found atomic coding to be fairly annoying with subversion due to having to hit the network for every single change. Perhaps svk would be better in this regard, but these days I've been using git and it's usually so fast that you wonder if anything actually happened. It makes dashing off an atomic change a breeze, and then you just push a bunch of these to the server when you're done. Other distributed systems also have this advantage of course.And your point is...?Hum. Eric Raymond has recently entered into [an association with Linspire](http://www.linspire.com/linspire_letter_archives.php?id=37). He should at *least* have mentioned this in the letter.Imagine for a moment that your text editor had no undo feature. Anything you type is typed; anything you delete is gone forever. Someone comes along and says "hey, maybe the editor should have a way to revert changes you've made, so that if I accidentally screw up my text, it doesn't wreck me". 

And you reply, "someday, you will learn to treat text with the reverence and respect it so richly deserves".  

I disagree. If the computer can help you save time with no drawbacks, why shouldn't it?

Also: I've actually been programming kind of a long time. When I started using C, for instance, you weren't required to declare prototypes for functions; it was up to the caller to make sure the right types were used in the call, and if not, your code crashed. 

So  I don't think my mind is going to change a whole lot about this. I can see there's definitely two points of view, which surprises me, but don't expect me to swing around to the view that catching typos is always the programmer's responsibility.

Why exactly is this a design decision that can't be challenged? perl, for example, implements a solution that lets people who want to always explicitly declare variables do so, and people who never want to don't have to. What's bad about this?Everybody writes bugs, it's just that most bugs are more complex than a spelling error.  It's kind of like talking about how the biggest thing keeping you from being a successful novelist is the fact that you aren't a very good speller.I would not consider this a win for Ubuntu as ESR will eventually find something to complain about.

I thought all the so called linux gurus used slackware or gentoo anyways.  ESR uses Ubuntu?You might want to try using keyboard shortcuts in your editor of choice (I'm sure even JEdit has them), if only for the speed boost it gives you.  Note that this takes time, and you have to learn new habits, but it's worth it.

I've made myself much faster in Vim by learning a bunch of tricks (macros, especially), and one of the Vim gurus I know practically flies over a file when editing.

Bram (Vim developer) gave a talk at Google about [effective text editing](http://video.google.com/videoplay?docid=2538831956647446078 "Seven Habits of Effective Text Editing") that you might want to look over.  The idea is not to be an encyclopedia of shortcuts, but to use the right tool for the job.Who said it was the biggest thing? It's just "a thing", ok? A thing that wastes time, for no good reason. 

Programmers, even very good programmers, make typos. Why exactly should the development environment be forbidden from helping them find those errors? Why?What point are you trying to make here?Also, if you've got a problem misspelling variable names, you're not using completion enough. M-/ will solve this problem in Emacs.Well, he was a friendly neighborhood libertarian nutjob until 9/11 and then he became a frothing-at-the-mouth neocon-endorsing libertarian nutjob, which basically reduces down to neocon nutjob.

For example, in his blog "Armed and Dangerous" there are numerous neoconnish posts, such as these numbers on Iraq:

* ["Media Analysts Sound Pessimistic as Iraq Civil War Fails to Materialize"](http://esr.ibiblio.org/?p=269)

* ["Is that victory I smell?"](http://esr.ibiblio.org/?p=243)

* ["The 'Bush Lied' lie"](http://esr.ibiblio.org/?p=198)

* ["How the Left betrayed Iraq"](http://esr.ibiblio.org/?p=183)

Edit: Summary: I think esr jumped the shark, long, long ago. I agree with schwarzwald that the only thing of value to the F/OSS/computing community that he has his hands in is the Jargon File. More and more I wish it had a new maintainer.Unit testing finds this problem, and many other problems that standard typing does not catch.   I use standard typing, because there are claims that Haskel's type is strong enough to catch most of the errors I would catch with unit tests.   C/C++/Java/C# type systems are not that strong.Haha.  It even traps Javascript errors and throws up the guru meditation error screen with the flashing red border.

I can get to happen but I don't know what exactly causes it.. I'm just opening up Zalsa's chiptune folder and making the window real tall.I would say that currying is the only reason for a function to have more than one parameter. If you are going to pass all of your parameters to a function at the same time, then they probably deserve to be wrapped in a structure.

EDIT: Of course, the functions that build your structures also need to have multiple parameters.One problem I could see with this approach is that the revision history would become a mess.

When you review a history, you care about feature changes, bug fixes, etc.  You don't care about minor changes in comments, or small test adjustments.[ESR is nuts](http://www.catb.org/~esr/writings/dancing.html).This is what you get when algorithms researchers have too much time on their hands.

Divide and conquer turns into multiply and surrender.  It's quite funny actually.Thanks for letting the world know, ESR. Last week I switched from one-ply to two-ply toilet paper. The difference is amazing!I gave up on Fedora years ago. Of course, I also gave up on ESR years ago. 

He's a writer and tireless self-promoter, not a hacker of any description.I can't wait for Everybody loves Eric Raymond's take on this.Well, you actually got my reply quite right there ;) I hardly use undo for text because I concentrate on what I'm writing and how my fingers are moving across the keyboard; when I catch a typo I backspace it away.

As typist this is my responsibility. As programmer it's my responsibility to know what is declared and how and in which scope etc.

Some interpreters / compilers give you warnings about typos but that is *not* what they are supposed to do.

And about challenging design decisions: I didn't mean it cannot be challenged. You're free to write to the python developer's mailing list to tell them that you accidently hit yourself with a hammer and want that hammer to be fixed.Eric Raymond doesn't play by the rules.

Eric Raymond doesn't even know what the rules are.

Eric Raymond.

Uh-oh.  He just turned up in my building holding a copy of Guns &amp; Ammo.  Wait a second, that's not dangerous.

God damn it!  Can't someone suck all the oxygen out of this blowhard so all we're left with is a sack of meat that flips about once in a while.Named parameters make this a non-issue.Because even strong static typing with required declaration of every variable, every function/method prototype and all the types of variables and return values won't catch every error. This is why we have unit testing and other such things, and why many programmers like the tradeoff of occasionally having to debug a bit in return for not having to write every variable twice and (often) every variable's type multiple times (as in the parody Java-style statement `SomeLongTypeName SomeLongTypeNameInstance = (SomeLongTypeName)SomeLongTypeNameManager.getSomeLongTypeNameInstance()`. Yes, I've seen code in the wild written like that, and seen it explained as being for "safety").

If you don't like that tradeoff, no-one's forcing you to accept it -- there are languages which require the sorts of things you want, and you're quite free to use them :)&gt; where he worked on advanced scripting languages such as Haskell

No comment necessary.This is more than just spelling. I had a nasty bug once where someone introduced a local variable called "records" or something like that into some shared code.

The problem was that some of the code using that function also had a page-level variable with the same name. Since the local variable wasn't explicitly declared, it bound to the page-level variable and fragged it.

thank you for this informative reply. 

i had always thought about interference as something "extra" rather than something "less" in the context of quantum computing. thanks for [teaching me otherwise](http://sigfpe.blogspot.com/2007/02/essence-of-quantum-computing.html) ;-)[removed]How many lines of code should a program have?The funniest thing about this article is the fact that a "Code Generation Network" even exists. I mean, it's cool to have a web site that focuses on programming techniques, but it strikes me as nearly comical to be so specific.In case you don't want to trudge through the whole thing, here's the money quote:

&gt; I was climbing the stair to the fourth floor in one of the quad houses one day, to visit some friends. A girl stepped onto the landing, saw me, and turned white as a sheet. I said something obvious along the lines of "Why are you looking at me like that?"

&gt; She started to babble something about last week when she was studying, looking outside at the quad and seeing me walk by "...and the leaves were following you!" And it was like I was the Spring and the life in the grass. And a whole bunch of other stuff that made me wonder what drugs she'd been doing (this was 1976 or early '77; every second dorm room had a bong and blotter acid was easier to score than good music). So I shook my head dubiously and rolled on upstairs and visited my buddies.

&gt; I was walking home, idly puzzling over this peculiar incident, and damn near fell over when I finally got it. That girl had been trying to cope with a theophany; **she had looked at me and seen a god. A particular god. And I knew, suddenly, with utter shattering certainty, which one it was.** And that it probably was not the first time I had inadvertently triggered such an experience, and would almost certainly not be the last.

Kid and Genshi compile XML templates to Python bytecode.Whatever, my post was constructive. How many people are going to remember all the math?  Compare that with how many people might remember 1 - light, 2 - well utilized, 4 maxed out.
Your post - what is your post anyways?  Just spew for spew's sake?[removed][deleted]Stallman is nuts; ESR is insane.  The difference is that Stallman can be endearing while ESR is just scary.Does the easier, cleaner way involve passing the same array to nearly every function in the program, because they all need to use it? Because I'm not convinced that's easier or cleaner. 

I'm talking about scripts here. Often stuff you throw together in a couple hours, and it does a simple job, and does it well. The stuff people generally use scripting languages for.On a side note, I've been using Slackware on some machines (still do) for eons. This year, I've been using Kubuntu, which pleases me a lot (ant my Intel 2200 also works!). Until the day I had to reconfigure the console to, you know, really use the console. I've had given up the console configuration, and came back to use xterms-under-X exclusively.Looks like they're Verizon now.

    dfranke@laurelin:~$ host 4.2.2.2
    2.2.2.4.in-addr.arpa domain name pointer vnsc-bak.sys.gtei.net.
    
    dfranke@laurelin:~$ whois gtei.net
    ---snip---
    Administrative Contact:
        Domain Administrator (NIC-14530206)  Verizon Trademark Services LLC
        1320 North Court House Road
        Arlington VA 22201
        US
        domainlegalcontact@verizon.com
        +1.9727187621
        Fax- -
    ---snip---For less BS and graphical frills than than that, I heartily recommend [ratpoison](http://www.nongnu.org/ratpoison/).Having had similar issues with RPM fucked-upness, I quite sympathize with the guy.

(Now a happy Ubuntu user.)

(Still uses OS X as primary system.)&gt;As programmer it's my responsibility to know what is declared and how and in which scope etc.

And presumably this is information you can keep in your head until the end of time, then? You can look at code you wrote five years ago, or even six months ago, and immediately figure out where variables are "declared", when a declaration consists of the first call to:

    x = y + 3

You can look at someone else's code and figure this out at a glance, too?

I see why you disagree with my proposal, then.Screw the idols, I agree. But the down-to-earth view I've gotten over the years is that no matter how much things are broken in FLOSS land, I cannot force myself to lease what is effectively an extension of my mind - the thing that stores and processes my data, enables me to communicate over considerable distances - from a fucking corporation. So, FLOSS may be broken in places, it's still the only thing that there is.I agree, and this is one of the things I complain about in the article.How do you declare global variables in Python? A variables scope is never outside the file it is in. Declaring it at top level means it's "global" within this file/module.This one gets a thumb down. It's a spoiler of the upcoming ELER ( http://geekz.co.uk/lovesraymond/archive/gun-linux ) and not much more. ESR whining is not relevant to anything.I've had exactly the opposite experience.  YUM does the right thing in every instance, apt-get screws up horribly fairly regularly.   It seems maybe I'm the only one, but I went back to Fedora after fighting Ubuntu for days just to get basic web server functionality going.This was neo-paganism *plus* pantheism, as far as I recall from reading it.The preprocessor in C++ is a code generator.  The output is transient; you don't ever see it.ESR: blah blah blah blah blah blah blah blah 

Rest of the world: \*yawn\*Ah, Gentoo, because 17 hours of emerge output is better than 5 minutes apt-getting the same file. ;) I'm in the same boat as you, except I'd dual booted it, so I could just blow the Gentoo away and go back to Windows. My mate tells me Kubuntu is worth a look though.Well, often times you don't absolutely have to implement code generation, but it makes things much nicer if you do.  I'm actually preparing to write some operator inference stuff for C++ (given some human-coded operators and functions, right out all the deriveables).  Yeah, sure, it's C++, an inelegant language rife with repetition, however, no language can truely avoid such issues.

This is why, I think that the idea will not die, it will be integrated into languages, like lisp macros.  The truely elegant language will have implementations primarily consisting of such transformations.Yes, all media will hype it as Doomsday, but nothing much will happen.More like this i think:

    for s in Starships:
        if s.IsPhaseInverted():
            return s

I think, if you get problems with typos in variables, you have a bad codeing style. His ruby version looks like C with the wrong syntax.Web (n+1) point oh to you.So wait - this guy JUST FIGURED OUT that rpm sucks?

... a little slow there.If you need a little more possibilities try [ion](http://modeemi.fi/~tuomov/ion/) (Lua) or if you like shell scripting (and Plan9) [wmii](http://www.suckless.org/wiki/wmii).The support of the presentation (powerpoint) mode alone made me upvote this!Web^{i \pi} - 1Some of the worst email modules of the Python standard library, he had a hand in writing. (I'm thinking of imaplib, which is basically a glorified tcp/ip connection wrapper through which one sends strings as per the RFC... compared to the Ruby imap module, it's a crying shame, here's one of his comments:

&gt;Note: to use this module, you must read the RFCs pertaining
to the IMAP4 protocol, as the semantics of the arguments to
each IMAP4 command are left to the invoker, not to mention
the results.)I'm sorry, did you say *20* programming jobs??_shudder_

It's not the neo-whatever-pagan spiritual kick that's worrisome. It ain't _my_ kick, but I've got friends who get off on this kind of thing, and more power to 'em, I suppose. It's the thread of psychotic arrogance that gets me.Yesterday at work I've seen a method with 28 parameters. This is what happens when you let Math PhDs write C++ code :p&gt; Global variables are evil, and often a sign of bad design.

Global variables, or at least the problems associated with them, are unavoidable in many OOP languages. Essentially a member variable is global to the class, so it lends itself to the smae kinds of mistakes that a true global does.

And really, some globals cannot be avoided. I have seen some poeple "remove" globals by threading context object throughout an application, but the only net effect is a global that clutters you function calls.
F# is a clone of Objective Caml and C# an enhanced version of Java some people would say. I don't see Microsoft driving anything here.How I yearn for an Amiga key on my keyboard.I've never had to use global in Python. Not even sure how it works.[removed]Or, for lispers, [stumpwm](http://www.nongnu.org/stumpwm/)As opposed to full blown apps you mean?A right click seems to produce the Guru.  Remember the good ole days when the guru would occasionally pop up on cable tv stations?Here's a simpler way: the load average is the number of CPUs that would be used at the same time.

If your load is 4, then if you had 4 cpus, all of them would be used.
The weird part is that "advanced scripting languages" phrase seems to come from Meijer's Microsoft Research homepage

http://research.microsoft.com/%7Eemeijer/

I guess a comment from Meijer would be interesting :)&gt; This is why we have unit testing and other such things,

Honestly, do you really think it takes less time to write a comprehensive unit test than it does to declare a variable? 

&gt; Because even strong static typing with required declaration of every variable, every function/method prototype and all the types of variables and return values won't catch every error.

No, but it EMLIMINATE a class of bugs. Not just reduce, but totally eliminate bugs caused by spelling error induced variables and unexpected scope binding.

In short, unit tests shouldn't be used to verify things the compiler can take care of for free. Unit tests are for things that cannot be as easily tested any other way.
Editing in fundamental mode? That's gangsta.

It's certainly not the pinnacle of "no-bullshit" setups, though ... he's using a color terminal, and a color theme in Emacs at that. Not to mention that there is a ... *gasp* ... scroll bar in his terminal window!I agree - sure, using C# means I avoid some errors that occur in more dynamic environments, but the half an hour of extra debugging a dynamic environment incurs is about equivalent to the extra half an hour of coding that using a static environment tends to occur.

And nothing shields you from logic errors, which are the bugs that bite you hardest.Uh, no.  Unit tests are to verify that your program actually works when run, so you'll be needing them no matter what.  It's the compiler typechecking that's useless and redundant.  Why add extra junk to the code when you're going to need a *real* test anyway?  ;)wow, this brings back memories. Thanks :)
I spent years trying to be productive using environments like these.  I experimented with fvwm, fluxbox, sawfish, and a number of others.  Maybe if I only ever used the system as a development environment I could pull it off, but for me it just isn't worth it.   KDE just gets more done.I personally like atomic coding.  But I think the main issue is the partial commit.  So, if I checkout revision 123 and then I do a build of the system, it doesnt build because you didnt do commit 124.

I dont know, for larger teams; doing commits on the branch of your code such that you dont break the entire nightly build or unit tests seems like a more thought out approach.

But, I like atomic coding for my personal projects.I think a big part of the problem is that people are fascinated with unit tests, but don't really know how or when to use them. Thus they randomly apply them to every and all situations.

This kind of problem often cannot be found with unit tests. In the error only occured when the function in question was used by another script that happened to have a global with the same name. This is an intergration issue, so it would never be caught by a traditional unit test.

And even if it did catch this kind of bug, it wouldn't really tell you the underlying cause. Unlike a compiler error that gives specific line numbers, all it could do is say something is wrong.

I hate to call schwarzwald a liar, but I honestly do not think schwarzwald has ever really fully unit tested even a small program specifically against this issue. Considering the difficulty in doing so, I don't think schwarzwald has even tried."After thirteen years as a loyal Red Hat and Fedora user, I reached my limit today, when an attempt to upgrade one (1) package pitched me into a four-hour marathon of dependency chasing"

L-O-Freaking-L.  Welcome to the world of Crystal Reports.  Welcome to the world of InstallShield.  Welcome to the world of Visual Studio...  If I gave up on this kind of thing I'd be unemployed.  Welcome to the real world.
[deleted]&gt; Unit tests are to verify that your program actually works when run, so you'll be needing them no matter what.

All tests are to verify that your program actually works when run. You are aware there is more than one type of test, aren't you?

&gt; Why add extra junk to the code when you're going to need a real test anyway?

1. Compiler verification is a real test.
2. Compiler verification is the most accurate test for misspelled variables and unexpected scope binding caused by duplicate variable names.
3. Unit tests are for testing isolated parts of a program. The unexpected scope binding issues I refer to happen at integration time.
4. I have a finite amount of time for testing, and I have more important things to test for.
From his [config file](http://www-cs-faculty.stanford.edu/~knuth/programs/.fvwm2rc):

&gt; (By the way, it's interesting to try "while(1) ;  xeyes &amp; ; end".)

Teehee...

Programmers frequently use a lot of things which aren't programming.AFAIK, ESR has had very little to do with that module (from what I can tell, all he did was converting it from 1.5.2-style string module calls to 2.0 string methods).  But yes, the module is embarrassingly low-level; combine that with the fact that IMAP is probably the worst network protocol anyone has ever designed, and you end up with something that's almost entirely unusable for a non-expert.

Contributions are welcome, as usual.I'm perplexed why this submission is rated highly, while another submission I made the same day, 
[Software testing cheat sheet](http://programming.reddit.com/info/15ny4/comments), which I thought was just as useful, is rated 0.  I'm interested to hear from people who think the cheat sheet is much less interesting about why they see it it that way.It actually makes some sense for novice users and one-off scripts. But I don't like it for anything I want to use more than once.Just for the record, you need no interest in shell scripting for wmii. You can script wmii using any programming language, Ruby being quite popular. Just need to write to the 9P2000 filesystem, either directly using some ixp library, or using wmiir.At first I was curious why there was an article about the founder of Wendy's in the programming subreddit.  The more I thought about it, I realized that the Junior Bacon Cheeseburger has contributed a great deal to the field of programming.  Where else can I find an affordable high-sodium and fat treat at midnight to fuel my all-night coding session?&gt; I'm not expecting Ubuntu to be perfect...

He'd better not.  I tried Ubuntu briefly, but it destroyed itself when I tried to get it to upgrade to the new version.  (I went back to Gentoo, which is not perfect either, but I'm familiar with it.)&gt; Honestly, do you really think it takes less time to write a comprehensive unit test than it does to declare a variable?

Honestly, did you read the sentence _before_ the one you quoted out of context, about how even explicit static typing with declarations of all variables can't catch every possible error?

You need to write tests regardless of variable declarations, so why not have your tests cover that stuff too?I'm using BitTorrent to download the HP Abelson and Sussman SICP lecture videos: http://www.swiss.ai.mit.edu/classes/6.001/abelson-sussman-lectures/

This content is popular enough that their web server is overloaded for requests to the videos themselves.  There's several BitTorrent seeds for each of the DivX videos listed.Hehe.... my version of Mathematica won't work in graphical mode either. I feel his pain. I have to use the command line version too.Press F11 to switch to presentation mode if you use Opera.Really, people. Don't downmod me unless you can explain why javascript in a browser is more suitable for emulation than java in an applet plugin. Really.Which mode should he use to edit an rc file otherwise? And he's using Tex mode in the lower buffer.Is this a joke?Good post.  You basically described Python.  :)Try JMath, http://robotics.caltech.edu/~radford/jmath/Are you using nVidia's binary drivers?  If so, I have a pretty good guess what your problem is.  Try turning off Compositing.I use Pocket Scheme on my Sprint Windows Mobile phone. It is wonderful as well as free. 
That's funny, because I've been using Gentoo for years and have rarely run into such problems. But when I did, they were with X and Xorg. 
I frequently run into RPM dependency hell with Red Hat EL 3 &amp; 4 at work. We use Red Hat here, unfortunately. The last time I had a serious circlejerk clusterfuck was also with X. Seems that if you do a "minimal" install, and then try to add X &amp; Gnome on a RHEL 4 box, it will hopelessly footfuck itself no matter how many RPMs you throw at it.

RPM is a pain in the ass. Gentoo's Portage is worlds better. And for those that are uninformed, no, you don't need to compile everything. Many many many packages have a binary that you can use. I think that Xorg is one of them now. :)Nice, I may need to use this.[deleted][fvwm-mode](http://www.lair.be/projects_fvwm-mode.php), of course!Oh, for sure, Portage is superior to everything else I've used, don't get me wrong. As for using a binary, I was and still am a Gentoo newbie, can that be done via Portage? Would you have a link to a HOWTO or similar? Cheers.Seems to be a download site for those chiptune thingies. Click on the "Chipbench" icon around the top right.Does anyone have a welder's mask? Because the goggles, they do nothing to stop that retina-burning yellow.So he's religious.  At least he's not talking about nailing his god to a tree.

('cause that line never gets old)Yes it's possible with py2exe. I have done it with roids:

http://roids.slowchop.com/

It works just like any other compiled Python extension.http://www.stanford.edu/class/cs140/projects/pintos/pintos_7.html#SEC128

Ugh, I wish that I DIDN'T know how the load average really works...[deleted]&gt;The idea that Unicode uses two bytes per character is a popular misconception. Well, I've never met anybody that thinks this.

I've seen this stated as fact in recent editions of computer science text books.This is actually the first time I see a website using Google Analytics actually bother to check if the Google Analytics script is working. (Which, thanks to NoScript, is usually not the case.)If ESR had ever had any credibility whatsoever (and he didn't), he lost it when he posted that psychotic scrawl.An excellent choice indeed -- conf-mode is a good all-purpose mode for such things also. Pretty much anything that will enable font locking for comments would be an improvement over fundamental mode.&gt; As typist this is my responsibility.

Why are you so keen to take responsibility?

Computers are there to do work for us, not the other way around.  Do you allocate your own memory?  Do you decide which registers to use?  Do you write your own implementations of linked lists etc?

Everybody who uses a computer hands off responsibility at some level.  Declared variables seem like a perfectly reasonable instance of this; they don't incur any runtime performance loss, they aren't an onerous requirement for the programmer, and they catch some hard to detect bugs.  And the people who like it want it to be *optional*.  What's so unreasonable about that?&gt; You need to write tests regardless of variable declarations, so why not have your tests cover that stuff too?

It is a different type of test, you don't get it for free when you do isolated tests.

For exmaple, let say you have these tests...

Test 1: 

    MyClass.Value = 6
    X = MyClass.Value
    Assert (X = 6)

Test 2: 

    X = MyClass.Multiply(5, 10)
    Assert (X = 50)

Internal code:

    int Multiply(a, b) {
        value = a * b
        return value
     }

Both unit tests prove what they intended to prove, but neither catches the scope error in the Multiply method.

It gets worse in scripting langauges that use literal file includes. You not only have to check unit test MyClass by itself, you also have to run integration tests against every script that includes MyClass as any of them may hide a global that changes the scope of MyClass's local variables.

The only reasonably way to be certain you have not introduced this sort of error is to use compiler checks. It simply isn't feasible to write the kind of white-box unit tests the account for each and every variable.That is something people should know about him: Eric follows the money.  He is not interested in any project which does not have the potential of resulting in free airplane tickets and public speaking.Well, I would say that most window managers have a pretty normal life. The only reason why the lacked certain features in the beginning was for lack of development time. There are a few exceptions and baroque accretions, but if one looks at fvwm 1 and then at the current version, one wouldn't be that surprised. Heck, even looking at a recent enough version of twm lets you see the similarities.

Window managers actually seem to encourage forks if the feature set is getting either too low or too high for some tastes. Ratpoison -&gt; stumpwm, fvwm -&gt; bowman -&gt; AfterStep -&gt; WindowMaker, wmii -&gt; dwm, twm -&gt; vtwm -&gt; ctwm/fvwm...
What scares me more is that he doesn't even question what kinds of drugs he was doing back in '76 and '77!The problem with X is that it's a horrendous gigantic codebase centered around the server which is amazingly poor at adapting to changes. This creates compound problems because it's hard to organize large scale changes (like a version update or a change that breaks the server-driver ABI) and even when you get that right you run the risk of breaking people's configurations because the server freaks out if you change your video card without telling it.

I think after a year's practice with the modularized version of Xorg, the distributions all pretty much have it down, so there've been very few problems since then. But when it was shiny and new there were definitely problems in just about every distro. Welcome to the joys of new code. As for the latter problem, it's starting to be addressed in Xorg 7.2, which adds improved autoconfig support. Look for major improvements in the 7.3 release, which will support input hotplugging as well as randr 1.2 for better monitor hotplugging for drivers that support it (currently ati, intel-modesetting, and nouveau).

There's other problems too, but X.org is working to shake off a lot of cruft in a very old codebase. The number of people actually working on it is surprisingly small, so don't expect all the big changes to happen overnight.If your code is good, it should be easy to understand what variables are for and what is their scope. If you can't understand your code after few month, you have much bigger problems than misspelling variables names.

Generally this is not a real problem and the design choice made by most scripting languages is good.Ah, thanks for the correction. And, I've always had it in the back of my mind to try and improve imaplib when my skills were sufficient, perhaps I should give it a go. :)staunch - I'm working on a similar system and here's a question for you: are the product site's themselves something that you want to brand or are they relatively boring/not-memorable? I'm working with branded site names (and it sounds like you are as well) and I *want* my users to have to authenticate to each site individually but I want them to only have to put in their password. I was thinking of having a central authentication web service with a GUID that recycles every so often but I can't figure out how to manage it when someone *types* in the domain name; I've got the cross site links part in mind. Any ideas?Does not compute.http://research.microsoft.com/terminator/collatz.htm

&gt; Proving the validity of the Collatz conjecture is equivalent to proving that the following C code fragment always terminates:

    while (x &gt;= 1) {
        if (x % 2 == 0) {
            x = x / 2;
        } else {
            x = (3 * x) + 1;
        }
    }

uh, shouldn't that be
 
    while(x &gt; 1)

otherwise, x=2 never terminates

EDIT: collatz fractal: http://upload.wikimedia.org/wikipedia/en/1/1c/CollatzFractal.pngReally?  Wow.  Which ones?No, we need *more* idols. The coders of Terminal and XFCE need some praise too!yikes - did a search for "SQL Server 2005" and found 0 results...I can't find zealotry endearingwhat nirs said, also comments.

Now either think of some real problems to discuss or better yet write another blog about how bad other *tools work against you*.&gt; He is not interested in any project which does not have the potential of resulting in free airplane tickets and public speaking.

That's an exaggeration. But yes, he hasn't been above freebie-grubbing in the past either.I love how they call him an open source guru, because...he doesn't have an ACTUAL title&gt; That would be because we believe in Free Software and doing the right thing (a practice you appear to have given up on). Maybe it is time the term "open source" also did the decent thing and died out with you.

Ouch.

Free software folks need to get over themselves, and that goes for Alan, too.  The ideological purity of your operating system is little comfort to those of us who want our computers to make our lives *easier* rather than *harder*.

Explain to me why I should give a shit if my copy of Adobe Flash is "free software" if it works and I never plan on modifying its code.  I don't care if some hypothetical free software developer out there might theoretically modify the code to make it better - if it works, then I just want to use it.

The free software movement's rigidity is ludicrous.  Making the perfect the enemy of the good, etc.  The OSS movement is much more palatable and realistic.My programming (and that of my coworkers) occurs in 3 environments: (1) Subversion repository; (2) Workstation; (3) The development server. 

We use subversion to move the code around. We do atomic commits continuously: we edit code in our workstation; then, to try the new code out, we commit it to the repository, update the working copy in the development server, and run the new code. 

The commits are easy: we hit the up arrow in the command-line interface, then Enter (unless we need to change the commit message because it's a major change). This works whether we're using a Windows or a Linux workstation. Updating the server is just as easy: up arrow then enter. We're usually connected to the server via SSH.

Using subversion like that, as the mechanism to edit and move code around, allows us to work on the same files at the same time (though we usually work in different areas of the file or on different files), have a history of commits, backup every single change, no matter how minor, and use our favorite workstation environment and editor. 

We don't worry about keeping the development trunk working at all times because we have a staging area that QA uses for testing. When we want to promote a new release to the test environment, we all agree to get the main development trunk working and we create the test branch. 
Back in the 1990s, when very few companies supported Linux, he claimed he was putting together a list of such companies with the intent of showing the world that Linux had support.

I had a corporation that had been supporting Linux for about five years at that point so I submitted our information.

He replied that he didn't know why I had contacted him with that information.  Once I had explained, he replied that he had meant "real companies, those with over $10M/year revenues".  It was at that point I realized what a whore he is.[deleted]arrrrggghghhh... downmod for windows or upmod for lisp?!? my brain just segfaulted.&gt; Why are you so keen to take responsibility?

Because I know who to blame when something goes wrong.

&gt; Do you allocate your own memory? Do you decide which registers to use? Do you write your own implementations of linked lists etc?

Depends.

&gt; Declared variables seem like a perfectly reasonable instance of this; they don't incur any runtime performance loss, they aren't an onerous requirement for the programmer, and they catch some hard to detect bugs.

You're right they seem to be such an instance. But just for a change try to make them an onerous requirement and you will soon find that bugs are fewer in number and easier to detect.

And maybe then you will learn that taking responsibility can be a good thing.In spite of his semi-God reputation, I can't help but think that his .emacs file must have less than 10 lines.  Editing a configuration file in fundamental-mode?  No syntax highlighting?  Poor guy.And that's even *with* nutbags like ESR in it.  By gods, imagine how much more sane it would sound if he would only shoot himself accidentally through the head and put us out of his misery...At 2400 baud and below, those extra colors aren't worth the wait, and once you get used to no colors, you don't feel like you are missing much.
Okay, I guess if the platform was part of the product, then it makes sense, in that context. "cool" is relative, though; applets are really, really uncool. 

I thought it was pretty good and tried to show some people, but it wouldn't work in their browser--javascript gets more and more wanky as you move from the dev system.Used to use 4.2.2.2 but it got spotty.  Wonder if that had to do with the wikipedia entry?  Now tend to use these addresses for non-critical stuff:

208.67.222.222
208.67.220.220

(they're from www.opendns.com)

// Sorry, -1 on the headline; it's not programming.
[deleted]fluxbox has everything useful that kde does.L# - Extension of LISP (Arc) written to the Common Language Runtime.  They missed it because the author did not look beyond Microsoft's offerings.Dude, I can't tell you the easier, cleaner way if you're not going to bother posting an example of where you think you need global variables.  Like I said, I'm talking about scripts too, hundreds of scripts that I've written over many years.  I don't mean to wave my dick around, but python scripting has been paying my bills for years and the need has just never come up.  

I *really* have no idea what you're talking about when you say passing the same array to nearly every function in the program.  Please give an example.What about using the [Borg pattern](http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/66531]) or a singleton?&gt; One of the big advantages of applications over the web is that everyone has always got the most recent version.

If only that were the case.[deleted]So I take it you would be in favor of a programming language that only allowed valid English words as variable names?  

I never said that the development environment should be forbidden from helping in such a case.  My point is that everyone shouldn't chuck dynamic languages out the window just because you occasionally make a typo.Looks good!  [The example](http://pyraknet.slowchop.com/pyraknet/wiki/BasicExample)  makes it look a bit easier than digging through [Twisted](http://twistedmatrix.com) `:)`Is there really any difference between globally shared data, a globally shared pointer to an object holding globally shared data, and numerous objects all using the same globally shared data?

Dress it up anyway you want, a global is still a global.&gt; Because I know who to blame when something goes wrong.

What can go wrong with explicit declaration?

&gt; Depends.

Fine, go up or down an abstraction.  At some point you *do* hand off responsibility for various things.

&gt; But just for a change try to make them an onerous requirement and you will soon find that bugs are fewer in number and easier to detect.

I can't parse this sentence, at least not in any way that makes sense in context.  Can you restate your point another way?

&gt; And maybe then you will learn that taking responsibility can be a good thing.

It's not commendable to take responsibility when the alternative is avoiding the problem altogether.
My code? I don't work on my code. I work on the code of about a dozen people, some of which are no longer in the country. And this is a small company, my previous one had about a hundred developers touching the same code base.You sure? For a short time last week, your servers seemed down.*ADJECTIVES ARE FUN*What makes it so great is that it's actually all solid work under the veneer.Worst title ever.It's amusing that he made such a point about having used RedHat/Fedora for thirteen years.  *It hasn't existed that long.*  The first release of RedHat was at the end of October 1994.See http://www.dourish.com/goodies/jargon.htmlI agree with dotrob that he himself is a nutjob, but how is this reflected in the Jargon File?I say that while ESR can come off as being a bit mad, at least he has contributed, in whatever way.I agree that an applet in a plugin would be more suitable, just so long as it's not using java.

What's wrong with java?
* Windowing (swing+awt) looks really crap
* PITA to install - version confusion etc.
* Annoying tray icons and self updaters
* Javascript is a much more capable programming language&gt; all we're left with is a sack of meat that flips about once in a while.

I think that's pretty much the situation right now...L: "You've really been useful as a PR icon here at Linspire, Eric."

E: "Thanks. I'll take any chance I can get to affirm my position as the most influential figure of hacker's thoughts and desires."

L: "Er... yeah. Anyway, there's a problem. People still see you as a Red Hat user, and we think that'll cause a problem if you're promoting Linspire."

E: "I see... what can I do?"

L: "Well, what if there happened to be a small... "accident" to occur during the installation of an RPM?"

E: "I see what you're getting at. Don't worry about a thing.
I'm esr, I can damn well destroy any free software entity I want. In no time I'll have every hacker on the internet lining up to torch Fedora to the ground. They won't know what hit 'em."

::types furious and vague letter about RPM installation::See: http://catb.org/jargon/html/A/anti-idiotarianism.htmlwhat yellow? my eyes cannot discern yellow anymore.Never mind that Stallman practically created the concept of free/libre software (from which open source, it's biz-friendly offspring, came). Without the GPL, open source would never have expanded the way it did.Or: if the load is smaller than or equal to the number of CPUs, having additional CPUs would not significantly speed up finishing the load. If the load is larger than the number of CPUs, however, additional CPUs would make a noticeable performance difference.What bizarro world are you living in? esr has written an app no one uses, and written a book describing the FOSS system of development. Stallman started GNU, wrote Emacs and GCC, and *started* the free software movement.

Plus, Stallman is cute when he demands nobody sell proprietary software. esr just sounds mean when he talks about killing Muslims.[dwm](http://www.suckless.org/wiki/dwm) is the new wmii!

From the author of wmii:

&gt; In contrast to ion, larswm, and wmii, dwm is much smaller, faster and simpler.

Not for the weak of constitution though.&gt; The ideological purity of your operating system is little comfort to those of us who want our computers to make our lives *easier* rather than *harder*.

Then use something else.  Freedom is an explicit goal of the Fedora project.  If that's something you dislike, then perhaps you should use another operating system.

&gt; Explain to me why I should give a shit if my copy of Adobe Flash is "free software" if it works and I never plan on modifying its code. I don't care if some hypothetical free software developer out there might theoretically modify the code to make it better

Nice dodge.  Given that Flash *isn't* Free Software, absolutely *any* explanation would be hypothetical.

&gt; if it works, then I just want to use it.

"Works" is not a black and white issue.  No software is perfect.

&gt; The free software movement's rigidity is ludicrous.

The Fedora project doesn't want to get sued for infringing on patents.  I don't think that's something you can blame *them* for.
Funny, the use of optional/named parameters was one of the things most disliked about VB.But how many of you who call down ESR have actually contributed? I have met many ESR-bashers who have done absolutely nothing in the way of contribution. 

In my mind it is better to write an app and a book that no-one uses, as long as you've given it a go. 

I don't agree with a lot (most?) of what ESR says, but at least he has tried to give something back. AS for his views on handguns, summoning Pagan gods, etc. I couldn't give a damn, as long as he is helping with the F/OSS community.I didn't mean that you had to stop respecting the man.  But not all of his actions have to be seen in the glorious halo of his previous efforts in open source.Hello mum!Only at first glance (I thought the same thing when I first read this article).  Compare figures 3 and 4, look at some of the math (the exponents and inversion), and consider what a capacitor does.  I think it basically just measures lag, but I could be wrong.  Mine's at 0.03, 0.06, 0.04, so I guess I'm ok :DMost of the hugs errors are due to insufficient heap and stack space. I've increase the limits there (though Hugs' defaults should probably be set higher than they are).While I guess "magic pen" might be one way to describe the behaviour of blocks, it implies that there is "magic" at work here.

Believing in magic in code (ie: I have no idea how or why it works like that, it just kind of does...) is how you end up with bugs and crap awful code. You need to understand what's going on.

It's NOT "handing you the magic pen", and it's NOT always an iterator. the 'iterator?' method is an alias for 'block_given?' and is badly named. All it means was that a block was passed to this function.

Think of a block as defining a function. A function is an object in ruby, you can assign variables to them, copy them around, stick them in arrays, just like you can with anything else.
The 'yield' keyword is a nice short way of saying "I expect that a block was passed as a function to me, call it now".

In summary. blocks/yield are just "create a function"/"call it", no magic and hopefully less bugs for you.CATB was a good book, and ESR was once an important figure. But these days, he seems to be [hurting the community](http://www.theregister.co.uk/2006/08/17/eric_raymond_linux_compromise/)  more than helping it.I agree. All that I am trying to say is to give respect where it has been earned, regardless of weird beliefs. Sometimes I think that bashing ESR is almost as fashionable as bashing Bill Gates and Microsoft indiscriminately.In case anyone wants a link: [dwm](http://www.suckless.org/wiki/dwm).Entry is now marked for deletion. Feelin' the reddit love...Is this computer-generated nonsense?Distributed Systems: Concepts and Design (4th Edition) 
by Coulouris (Author), Jean Dollimore (Author), Tim Kindberg (Author).Windows CE is a lot cleaner than XP/Vista. :)No-one uses vc-mode?&gt; But for small n, range(n) is more efficient than xrange(n) …

How so? (Aside from membership testing—xrange had efficient membership testing before 2.2, when it was removed due to “disuse”. I know because I wrote a patch to add it, and it was rejected for that reason.)

&gt; and dict and set iterate over the iterator completely anyways.

Truth.

----
In any case (notwithstanding any efficiency disadvantages compared to range), I maintain that using xrange is a good habit to get into.He didn't exactly write fetchmail either.Oh. Was that meant to be a joke? I'm sorry then. By the way, I'm an ex-VB programmer, and though enthusiastic in Haskell I've never wrote a line in it, so maybe you're missing something here.Nice signature Alan.Point taken. I stand corrected, but AFAIK most theories normally start from formulas. In some cases the number to prove them may not be available for some decades....I ping Google :/[deleted]Use EJB.And global/local scope conflicts are a reason to introduce variable declarations and compiler scoping and type checks? So perhaps a safe programming language would have to write it as

    local method class=(local class filename=srcfile MyClass) filename=srcfile int Multiply(local argument filename=srcfile int a, local argument filename=srcfile int b) {
        local return_value int belongs_to=(local method Multiply of local class MyClass filename=srcfile) filename=srcfile value = local argument int a * local argument int b
        return local return_value int value
    }

Except nobody requires such an extreme level of verbosity to deal with these issues; typically, a couple of simple built-in rules for scoping and occasionally an explicit scope identifier (like Perl's `my`, JavaScript's `var`, etc.). And even what I've provided here isn't truly safe because somebody might overload the multiplication operator -- you'd need a `builtin operation` keyword set, too...

Requiring the programmer to get lots of boilerplate correct is not a remedy for programmers who can't be trusted to get the actual business of the program right.

And "scripting languages" (no, really, they're called "dynamically typed") don't have the problems you keep suggesting _if_ the programmer is aware of the language and the applicable best practices.So the only problem with reddit is its population. And if you took the reddit software to a different domain then the population you would get from a second instance of reddit would be radically different from the first instance. For some reason. Unspecified. Perhaps relying on magic. Or psionics.Then you didn't read the other descriptions, not to mention the project web pages, of Spec#, Sing# and X#.[How about theorem proving, too?](http://www.coverproject.org/)In many cases, it's just "-bin" after what you want to emerge.

Like 'emerge seamonkey-bin'

I think open-office is openoffice-bin

Look on www.gentoo.org and read up on the GRP installs too. They use binary packages.I'm not sure what you mean by "did not look beyond Microsoft's offerings", but the table in the middle of the article mentions Gtk#, which is *certainly* not from Microsoft.No, I wouldn't be in favor of that. 

If you think implicit variable declarations are the core feature that makes a language dynamic, I'm not sure there's much point in further discussion.

I didn't realize the misguided malice this article would unleash. It seems I've struck a nerve, and I can't even guess why.No bullshit? What about those window borders?&gt; Then use something else. Freedom is an explicit goal of the Fedora project. If that's something you dislike, then perhaps you should use another operating system.

And I don't use Fedora.  But it is Alan who made this less about "Just go use another OS" and more about "you don't care about doing the RIGHT THING".  So free software becomes identified with the "Right Thing" in his mind.  His attitude is that if I or others use a less pure OS, then clearly we are bad people.

&gt; Nice dodge. Given that Flash isn't Free Software, absolutely any explanation would be hypothetical.

It wasn't a dodge, that was my *point*.  But the hypothetical developers are not realizable constructs.  Not supporting Flash just means that I can't use Flash, it doesn't mean that Adobe will realize the errors of their ways and switch to the GPL.

&gt; The Fedora project doesn't want to get sued for infringing on patents. I don't think that's something you can blame them for.

I call bullshit.  The Fedora project isn't worried about being sued, they just don't want to deal with it - plenty of the proprietary software that is being discussed is trivially licensable for distribution.  And that's fine, if they don't want to deal with it then they shouldn't have to.  But they should expect to lose users who require that proprietary software.  Maybe that stands up to *their* cost-benefit analysis, but it wouldn't in mine.testFor ESR's more detailed explanation: [https://www.redhat.com/archives/fedora-devel-list/2007-February/msg01082.html](https://www.redhat.com/archives/fedora-devel-list/2007-February/msg01082.html)goodWhat happens when this tube fills?[deleted][deleted]Take this quiz to answer some of the most interesting PHP questions.
note to Ubuntu team: invest in kevlar jackets.&gt; But it is Alan who made this less about "Just go use another OS" and more about "you don't care about doing the RIGHT THING".

He was responding to Raymond &amp;mdash; *co-founder of the Open-Source movement* &amp;mdash; who was complaining that Fedora wasn't including code that happens to be patent-encumbered.

&gt; It wasn't a dodge, that was my *point*.

No, you asked somebody to explain the advantage freedom would bring to a specific example &amp;mdash; Flash &amp;mdash; while rejecting hypotheticals.  It's a hypothetical question in the first place!

&gt; Not supporting Flash just means that I can't use Flash

No it doesn't.  It means that it doesn't come pre-installed.

&gt; it doesn't mean that Adobe will realize the errors of their ways and switch to the GPL.

[\*cough\*](http://www.mozilla.org/projects/tamarin/faq.html#details)

This isn't about Adobe anyway.  It's about Fedora.  Convincing Adobe to open up Flash is not one of Fedora's core goals.  Shipping Free Software is.

&gt; I call bullshit. The Fedora project isn't worried about being sued, they just don't want to deal with it - plenty of the proprietary software that is being discussed is trivially licensable for distribution.

No, you're mixing up two different issues here.

Fedora won't ship proprietary software like Adobe's flash player.  They aren't worried about being sued for things like that, it's simply against the core goals of the project.

They *are* worried about being sued for distributing free software that is covered by patents, such as the Fraunhofer patents.  They don't have to obtain a license for *Fedora* to distribute, they have to obtain a license for *everybody* to distribute *royalty-free*.  And that is *not* trivial.  And if they don't do it in the case of GPLed software, they are committing copyright infringement.

&gt; But they should expect to lose users who require that proprietary software. Maybe that stands up to *their* cost-benefit analysis, but it wouldn't in mine.

Well, with all due respect, who cares?  They started this project with clear goals.  Freedom was one of them.  They are merely following through on that.  You clearly disagree with their core goals, so *obviously* your cost-benefit analysis will be different.
... she was indistinguishable from a human?

Oh wait, that's the Turing *Test*, my bad.See inkieminstrel's [post](http://programming.reddit.com/info/15tes/comments/c15w7n) below.Agreed. As a CVS replacement, SVN does a good job, but that's not striving for much. Nowadays, there are more powerful version control systems available (if you care about decent merging, changeset tracking, flexible development model, etc.)hooray for girl nerds!&gt; I *really* have no idea what you're talking about when you say passing the same array to nearly every function in the program. Please give an example.

A configuration file.  Typically in a standard location, but specifiable via command-line argument.  It usually needs to be consulted in many different parts of your program.  It doesn't make sense to parse it over and over again, so the usual approach would be to either have a global variable of some sort, or pass the configuration around all over the place.
[removed]you can create a file named globals.py and initialize all your global variables in it that you import in all your files.

You can then call globals.foo or global dot whatever you put in there. And you can assign new variables as you please:

globals.bar = x

That's what the Python FAQ suggests.[removed]What does 2400 baud have to do with anything?  He can run fvwm, he surely has the bandwidth to display colored fonts.Impressive, but I don't see how this is any better than a more traditional approach.Amen -- like define and set![deleted]ESR does not speak for or promote [free software](http://www.gnu.org/philosophy/free-sw.html). He has his own movement called Open Source.Ion rocks so hard it hurts. There is no better development environment, once properly configured.stfu, it's the future.[removed]The Fixnum#equal part [already got applied](http://svn.ruby-lang.org/cgi-bin/viewvc.cgi?view=rev&amp;revision=11807) and it was followed by [other similar optimizations](http://svn.ruby-lang.org/cgi-bin/viewvc.cgi?view=rev&amp;revision=11808).

The easiest way of following progress of the rest is blog's RSS.I've always liked the := for declaration. It was the only positive feature of PL/SQL back in the bad old days.Agreed. There are many more interesting people and projects in the FOSS world.or to have a "config" module, giving:

    from config import config

    def myDeeplyNestedFunction():
        doSomethingWith(config["important_url"])

And, if important_url doesn't exist, it's an error. Et voila!

This is a python pattern you will see in many, many python programs.

(And, on a side note, I too have hundreds of python programs lying around my computer, from scripts to change id3 headers to a ~5000 line go GUI and AI, and 0 uses of "global")morselsrule - 

just saw your post on this topic and am very interested in learning more about your suggestion.

you mentioned "via HTTP, Rest interface". I'm unfamiliar with the term Rest interface, and while I was able to Google a bunch of info, it's still unclear to me as to how this is accomplished.

I'm working on a project right now that requires single sign-on from a different domain, and while your solution sounds like it could be the needle in a haystack i've been searching for, I was wondering if you were available for some  consulting work.

Our main issue is that we have very little control over the master domain (the site the user will be coming from) but that they will be able to send some of the user information via POST.

Your token suggestion sounds very clever, but again I'm not sure the owners of the master domain will be willing to develop a routine that contacts our server behind the scenes and requests a time-sensitive token.

Anyways, I'd be very curious to know if you are available for consulting work on  our project, and if you had any additional details/suggestions.

many thanks!It may get more customers, but it doesn't achieve the goal that many people want: freedom.

See, before free software, there were all kinds of hurdles you had to go through. There still are. We're still seeing that with proprietary file formats, DRM, and closed source voting machines. The idea is that free software makes everyone play fair. Businesses get free (as in beer) software and community support, the community gets the businesses to play more openly. But OSS hurts that goal, because it only works one way: businesses get free software, but aren't obligated (and in fact encouraged) to keep up proprietary practices that hurt the community.&gt; And global/local scope conflicts are a reason to introduce variable declarations and compiler scoping and type checks?

No, just variable declarations and compiler scoping. Compiler type checking doesn't address this issue.

It may not matter for short scripts, but when you are writing classic ASP code with literally dozens of include files this is a serious concern.
Maybe ESR is beginning to learn Free Software &gt; Open Software.That coding costs the same amount no matter how many places you use the code.

On the other hand, the loss of encapuslation caused by not declaring variables results in hald an hour of debugging for every place that uses the code.

Undeclaired variables just don't scale to large code bases.
[cache](http://reddit-links.infogami.com/15x4n)Agreed!  I for one welcome our new brilliant female computing overlords!  Seriously, it's great to see a woman win, and for such an amazing body of work.  I hope this is another step toward the end of the male gender dominance in the field.  I don't know why there aren't more women doing it already.His copy of Mathematica is copyright 1999.  I had broadband circa 2001. 2400 baud?!Sounds made up.&gt; It may not matter for short scripts, but when you are writing classic ASP code with literally dozens of include files this is a serious concern.

See, there's your real problem. Variable declarations are unnecessary in an otherwise well-designed language. And classic ASP doesn't come anywhere near that description. So you're telling other languages that they should do something to make up for faults they don't have?

And again, variable declarations, scope declarations, type declarations and many other suggested routes to "safety" are really being proposed to make up for the assumed incompetence of the programmer -- why not spend that time and effort improving the part of the system that actually has the problem?Animated gifs power this thing (the window manager is impressive.. but I thought they were raytracing for a minute there =P)

http://www.chiptune.com/gfx/jga.gifWell, no, actually you didn't say anything about that.  Certainly the other "download services" that you implied comparison with are no less flooded with copyright violation.

In any case, there are many trackers that deal exclusively in legal content.  The [Azureus Wiki](http://www.azureuswiki.com/index.php/Legal_torrent_sites) maintains a page dedicated to these.  Also, there are numerous private trackers such as BitMe, of which I am a member, that explicitly and carefully exclude music, movies, etc. that are likely to bring down the wrath of the (RI|MP)AA.
I was hoping the FemBot had finally been invented too.[deleted]&gt; And again, variable declarations, scope declarations, type declarations and many other suggested routes to "safety" are really being proposed to make up for the assumed incompetence of the programmer

It isn't an issue of incompetence, it is an issue of scale. At my last company they had over 5,000 ASP source files. Individual pages, with includes, could easily be 500+ KB in size. No programmer could deal with that.
&gt; Explain to me why I should give a shit if my copy of Adobe Flash is "free software" if it works and I never plan on modifying its code.

Perhaps because Adobe Flash Player is [licensed under terms](http://www.adobe.com/products/eulas/players/flash/) that force you to --

* give up important consumer rights;
* give up the right to downgrade to earlier versions of the player software, even if newer versions prove to be less capable, buggy, laced with DRM and/or spyware, or otherwise unacceptable;
* give up legitimate fair-use and free-speech rights that may be infringed upon by DRM restrictions imposed by the player software; and
* "fully document and certify" compliance w/ Adobe's licenses within 30 days of a request to your business from Adobe or its "authorized representatives" such as the Business Software Alliance.

Maybe you don't care about those restrictions and burdens, but lots of people do.  And, as far as license agreements go, Adobe's license for Flash Player is pretty lightweight.

The terms under which Free Software is licensed, in contrast, are designed to guarantee that you don't have to accept the kind of restrictions and obligations you'll find in the Flash Player license.

That's reason enough for me to stick with Free Software.

Update: fixed typo.[removed]You do realize you're doing the equivalent of showing up at a human-powered-vehicles meet and asking a bunch of mechanics why they don't strap engines to their creations since it would make them go faster and you'd be more likely to buy one...
 
If you want to run a full fledged linux with commercial apps, Suse, Ubuntu or RHEL are all worthy choices. Fedora is not aimed there.For OS X users, [Think](http://www.freeverse.com/think/) works pretty good for cutting down the distractions when getting work done.If you check the last sentence of the comment I was replying to, you will see that you're preaching to the choir - or, perhaps, to a congregation that's already heard what you're preaching.  That is to say: yes, I know.Because women aren't that interested in the field compared to the men who are, it's pretty simple.  Do you wring your hands wondering why more men  don't enter female dominated fields?&gt; Fatal Error
&gt; Error Number: 2002
&gt; Unable to connect to database: Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (2)

Really funny! :)

Alan Cox and the Fedora people should be more careful when provoking a God.

[Dancing With The Gods](http://www.catb.org/~esr/writings/dancing.html)

"The Gods taught me these things while I was being some of them. Usually I've been the Horned Lord (Pan/Cernunnos/Freyr/Krishna). Occasionally I've been the Trickster (Coyote/Mercury/Loki/Eris) or the Sage (Thoth/Merlin). Just once, by accident in a martial-arts class, I have been the Warrior (Thor/Indra/Cuchulain)." -- Eric S. Raymond, lunatic
[deleted]Lame.[deleted][removed]Red Hat turned into the Microsoft of Linuxes years ago. You pay almost as much for support, and you get the same amount of brush off.

&gt; The idea is that free software makes everyone play fair.

And here is precisely where people like ESR and I part ways with the FSF - we balk at *making* anyone do *anything*.

OSS doesn't hurt the goal.  If you care if people make money off of modifying your software and not releasing the changes, then you can feel free to license it under the GPL or whatever.  Have a great time.  Personally, I don't really care what people do with my code.  I gave it out for free - if they want to help me improve it, all the better; if not, I don't really care.[deleted]Your article was fatuous and poorly reasoned and your comments in this thread are even worse, but I don't think there's any malice.[deleted]There's a huge difference as far as maintainability, reusability, etc.  There's a reason why people use design patterns.  If you don't know that reason, maybe when you have learned it we can continue this conversation.Thanks for the example.  I have to agree with llimllib below, though.  Put it in a module -- it takes no more time.  Some day you may decide to get your configuration from a database or a web service or something.  Or decide you need to do a little pre-processing on that configuration file, or provide a default value if one does not exist.  You could also do it as a singleton.I agree. Lame. Let's stop posting EVERY comic from killnine, shall we?Well look at the relative impact of male dominated versus female dominated fields. No contest (men win).What's the workspace-switcher called?&gt; It isn't an issue of incompetence, it is an issue of scale. At my last company they had over 5,000 ASP source files. Individual pages, with includes, could easily be 500+ KB in size. No programmer could deal with that.

I hate to say it, but this sounds like a project that went off the rails for reasons having nothing whatsoever to do with whether the language requires variable declarations. I've worked on and with projects written in "scripting languages" which ran to thousands of source files, all importing bits from each other, and found no great difficulty tracking where things come from. A little bit of organization and discipline goes a long, long way.Have you looked at the nethack source? ESR helped with that too.Nowadays it's only that boring "The application bshdfg.exe has caused an access violation at address 0x0000FED". Ah, well, at least we get to see the icons on their desktops when that happens.Well, it all depends on what the goal is. The FSF's goal is to make non-free software unprofitable and unpalatable for the entire computing community, and the argument really lies in whether or not this will be a net positive.

If your goal is to be charitable and give stuff away, then OSS is the way to go. Sure, technically the FSF's way results in a loss of freedom for those who want to take OSS code and make it non-free, but the argument is that not allowing those  people to do that results in greater freedom for the community.

I don't want this to turn into an argument about OSS vs FOSS, so in that regard, I'll just ATD. But at the very least, for distributions to support non-free drivers is encouraging that behavior. If ESR wants to play tough (and you know he does), we should play hardball instead of compromising to get "market share". The reason Linux doesn't have market share has nothing to do with his iPod or binary video drivers; the reason Linux doesn't have market share is because the OSS programmers haven't made usable systems yet &amp;mdash; which brings us around full circle to ESR's point with this article: Fedora isn't usable. He may be right, but 1) he's not being constructive; 2) he's not helping by complaining about a cutting-edge system (he might as well complain that Gentoo is confusing); and 3) he decided to bring the old OSS/FOSS debate into this, where it has no use.

Crazy guy...the War on Terror is reduced to taking off our shoes in airports, hoping we can bomb Muslims into loving America and chasing journalists around the bayou.Yes, apparently they weren't content with _one_ turing complete way to mangle your language before passing it on to compilation .http://geekz.co.uk/lovesraymond/archive/nop-nop-nop-nop-nop

(Everybody loves Raymond) - a bit offtopicWhere did Guy Steele's presentation go?Agreed. And she's an American. I studied Computer Science with a number of highly intelligent women, but at least in my case, they were mostly from Eastern Europe.

Edit: I meant a number of women, who were highly intelligent. They mostly were.In the same genre:
 [Does Visual Studio Rot the Mind?](http://www.charlespetzold.com/etc/DoesVisualStudioRotTheMind.html)I'd be willing to switch places with someone at a P.R. firm. I went to one of their Christmas parties once. Imagine the Park West in Chicago full of beautiful people, 3 women to every guy, live music, open bar, and tons of free prizes, including a car. Then again, this is an industry where the execs fly to the Bahamas to decide to lay-off a 1/3 of the workforce when sales are down.Which would be what?Fedora / Red Hat has sort of been a joke for years as far as I'm concerned.  Back in the day Debian was whipping RedHat's ass all over the Linuxverse, now it's Ubuntu.  RedHat just had some lofty ideals to break into the business world.  That IPO sure did wonders for investors, I tell you what.  RedHat / Fedora is schmucko Linux.Has anybody here fiddled much with XForms? Are you as excited about it as Pemberton?[removed][deleted][Alan Cox:](https://www.redhat.com/archives/fedora-devel-list/2007-February/msg01021.html)
&gt;On Wed, Feb 21, 2007 at 03:03:50AM -0500, Eric S. Raymond wrote:
&gt;&gt; * Failure to address the problem of proprietary multimedia formats with
&gt;&gt;   any attitude other than blank denial.
&gt;
&gt;That would be because we believe in Free Software and doing the right thing
&gt;(a practice you appear to have given up on). Maybe it is time the term 
&gt;"open source" also did the decent thing and died out with you.
&gt;
&gt;&gt; I'm not expecting Ubuntu to be perfect, but I am now certain it will
&gt;&gt; be enough better to compensate me for the fact that I need to learn
&gt;&gt; a new set of administration tools.
&gt;
&gt;I'm sure they will be delighted to have you

Oh snap!Agreed.*I'm sure they will be delighted to have you*

Tedious male cunts like ESR should just all be moved over to Debian, so they won't contaminate the rest of the FOSS-world.And [quoting one of ESR's FAQs](https://www.redhat.com/archives/fedora-devel-list/2007-February/msg01070.html)     really has to hurt.It's not only about making it "better". The problem is that there is still no x64 version of flash (or FreeBSD native version, I guess, etc) and Adobe does not care. Running 32-bit Firefox instead of 64-bit Konqueror just because of Flash is quite f'd up. The reason - Flash is not open source.If Fedora developers are in the habit of responding to honest criticism with sardonic crap like this, then ESR is probably right that they're screwed.No, next.Sceners are so old in tech years they are seniors.[removed]It's mentioned in the [announcement](http://groups.google.com/group/comp.lang.scheme/msg/5f88fbb15f5b6f73) anyway...  Could it be in the video labeled "Introduction by David Wise"?  Can't check right now...Erm what ESR has to say here, pretty much makes sense. Fedora core does suck ass (relatovely speaking), and has chucked away an unassailable lead over other distros.

ESR owned? I dont think so.I don't like windows either and only use it at my custom's site.  I don't even have such a mobile phone.  But I found the fact that you can have all this on your mobile phone just amazing.Because, by and large, they aren't very good at it.

Now commence downmodding me, instead of producing evidence to the contrary. Because we all know that popularity influences truth-value.Pretend as strongly that you can that it is, in fact, entirely made up.

Do you feel harmed?Those clean and fullscreen window manager are really good for coding. I've yet to find a good wm for graphic design, where i need to rapidly switch between Gimp, Inkscape, terminal and image browser.The essential difference is for every block: font-size: 300%; height: 605px;Exactly.That's very true, however, before the almost-eighties, there were (almost) only "free software", by definition.&gt; This isn't sysadmin.reddit.com or some such.

It also isn't crypto.reddit.com , or webdev.reddit.com , or programming-education.reddit.com , or hacker-webcomics.reddit.com , or are-lisp-and-haskell-not-great.reddit.com , or open-source.reddit.com , or have-you-noticed-how-poorly-RMS-dresses.reddit.com , or IT-businesses.reddit.com , or desktop-environments.reddit.com , or emacs-and-vim.reddit.com

How do you excuse these?  I suspect that at least as many programmers find themselves doing system administration than find themselves plotting a startup, or care very much that [treewm](http://treewm.sf.net) is obviously the best WM that currently exists, or find themselves always interested in learning a new language.&gt; Windows CE is a lot cleaner than XP/Vista. :)

Naturally.  What phone could sustain Vista's DRM?In that point, I can understand Slackware: you install, it recognizes your mouse, keyboard, network, video and sound cards, and you're ready to go. 

I can't really understand Gentoo.&gt; I don't know why there aren't more women doing it already.

Because, whether it's due to societal pressure or inherent genetic differences, most women simply *aren't* as interested in technical fields, no matter what? This is *not* a topic where "gender equality" makes sense. If a woman wants to get into the field, then by all means, she should be encouraged and supported. Artificially boosting female numbers in IT, though, is simply a misplaced idea.That's FvwmButtons, included with the window manager.Good link. There's a few other places that combine QuickCheck with theorem proving (Galois, for example).

Its useful to check your code first with QuickCheck, then once you know its good, try to prove things about it. Rather than going down the (expensively) wrong path when trying to prove the wrong thing.Maybe you should have busted out the xterms and coded rather than fiddling around with your env so much.ESR wasn't exactly mature in his initial post ... to bazillions of other people.&gt; ESR is the has-been George W. Bush

Yes, except I think he'll be around for another couple of terms yet.&gt; That would be because we believe in Free Software and doing the right thing (a practice you appear to have given up on).

"The right thing" *for who*? Unless you truly believe that idealism comes first and actually using your computer for something useful comes second, deliberately restricting the amount of available video codecs because of their licensing (which is of little concern to the user) doesn't do anyone any good.Its seems like repetitive whining more than clear criticism and as Alan Cox pointed out the comments about proprietary formats leave out the most important details of the discussion. 

Like the CUPS thing, he seems to like to come in like royalty, making proclamations and promises and then exits till the next rant session. Not a strong contributer since the papers about how to bang multiple chicks at a time and how tingly he felt when he got rich and the value to society of those two literary works seems questionable.Nope, that one is only 5 minutes. The Steele video was one I really wanted to watch, so it's a pity that it isn't available.&gt;Somebody take Eric Raymond's hubris-laden peapod testicles and chomp them down like so much edamame and be done with the man, Jesus Christ what a fucking bore.

This is poetry.If you like this, read [Bruce Eckel's love for Flex](http://www.artima.com/weblogs/viewpost.jsp?thread=193593) too.
['use strict'; does more than this.](http://perldoc.perl.org/strict.html)Looks cool but...

&gt; dwm has no multihead support

Oh well. I'm doing fine with Ion3&gt; If you want to run a full fledged linux with commercial apps, Suse, Ubuntu or RHEL are all worthy choices. Fedora is not aimed there.

ESR posted an inflammatory, juvenile and ideological letter to the mailing list. Alan Cox responded in a far more mature, but equally ideological manner. academician, finally, is responding to this ideology with what he or she regards as pragmatism.

It has nothing to do with whether or not academician would like to use Fedora (which apparently is not the case), and everything with personal opinion. The entire "thread", started by ESR, is full of opinion. There is no right or wrong to this.I think you don't get why other people don't understand monads. Monads are easy to understand in the terms you describe them, *after* you *already* understand Monads. However, the terms you use to explain them are *not* an easy path to understanding monads.

"In a phrase: a program evaluation strategy that's composable."

This essence does not contain enough information to e.g. try and implement the monad concept in an otherwise imperative language. In fact, it's so general that it's almost meaningless. One could say that 'functions' or 'objects' are equally good fits for this essence.&gt; Nice dodge. Given that Flash isn't Free Software, absolutely any explanation would be hypothetical.

Please read up on the meaning of the word "if". In this context, is is perfectly synonymical to "whether". There is, therefore, nothing "hypothetical" to the explanation, as academician did not create a hypothesis, nor a "dodge" for that matter.Any idea what wmii's support for dual screens is like now?It measures process not threads, does it?Bloody hell! Next thing we know is that they'll be getting the vote....

[Women, know your place!](http://www.youtube.com/watch?v=MMb8Csll9Ws)Aha. According to a guy on LtU it was a mistake not to include it on the conference page. Here's the direct link to the video:

http://video.google.com/videoplay?docid=-4633168320660258097&amp;hl=en
The examples you list are pretty much programming related, except RMS, IT-business and desktop-environments if you want me to nitpick. 
Oh and programmers use cars sometimes, should that warrant the posting of automobile-related stuff in programming.reddit.com? Programmers even eat food, perhaps we should put food related posts here as well? 

My main concern here is that I don't want the programming.reddit.com subreddit to be watered down by "amazing DNS IP-addresses", "coolest *** ever!" posts (see digg).fantastic critique. Now THATS sarcasm.[removed]&gt; This process is required to turn programming code into the binary zeros and ones actually read by a computer's colossal array of minuscule switches

Gah!![removed]&gt;If I wasn't such a wuss and a compiler ignoramus I'd volunteer.

There is a "cheesy" way of getting this done too:

http://www.mono-project.com/Java[deleted][deleted]Dĳkſtra argues that moſt analyſts aren’t ƿell trained, and that programmers uſually screƿ up analiſys, for example creating bad databaſe models.Soooo, that means this is the first time that a woman has been able to not be discernable from a human when tested in conversation?

/ducksIf only I saw your comment sooner.  UPVOTE!@malcontent - 

That about sums it up - ESR's acting like a spoilt child (as always).

Don't get me wrong, I don't use Fedora (always had problems with it, and now I prefer debian and now ubuntu), but there are plenty of constructive ways of criticising it (bug reports for one). Trying to stir up a sh*tstorm on a dev. mailing list is not a productive method.garbageIf you want a summary: 

ESR should have booted from a recovery CD, and re-installed the missing library from there.

Unfortunately, he panicked, and didn't think of that.  Happens to a lot of people in a lot of situations, so nothing new there.

ESR then goes off into a rant about (perceived) problems with Fedora and Redhat, in a more bombastic tone than was really needed.  Community responds.  Again. nothing new there.
Despite your downmodding I don't think anyone would disagree that men are more successful than women in these kinds of fields. However, as Larry Summers wondered, the question is whether they aren't good at it because of nature or nurture. 

Anyway, this debate will probably change as the younger generations enter the workforce, because i's just the current generation of women that isn't very good at it. [Men only make up 42% of college students](http://www.nytimes.com/2006/07/09/education/09college.html?ex=1310097600&amp;en=cd9efba2e9595dec&amp;ei=5088&amp;partner=rssnyt&amp;emc=rss), and women at Harvard, UCLA, and others are earning a disproportionate share of the honors degrees.
.actually thanks only to the confusion the link went so up ./&gt; You don't have to learn Lua/sh/ruby or some weird configuration file format (like X resource files), beside C, to customize it for your needs: you only have to learn C (at least editing header files).

C *is* a weird configuration file formatBefore you tell someone "they can take a hike if they don't like it" - I'd hazard a guess that Eric Raymond has probably contributed a little bit more towards Fedora than the average poster who makes this kind of comment.&gt; Does $_SERVER['REMOTE_ADDR'] always return the real IP address of the client?

Answer: Yes. If someone is using a proxy, the proxy is your client.I didn't know **he was/is on the board of Linspire**, which makes his attacks on FC for not rushing to include non-free components seem even more suspect.I spent way too much time today watching some of these presentations. The best is Steele's, but Sussman's has some memorable quotes, e.g.

&gt; When trying to learn all this stuff (advanced classical mechanics), back when I was an undergraduate, you used to read stuff like Arnold's Mathematical Methods of Classical Mechanics--you've seen that book?--and, he's a son of a bitch, okay?Utterly brilliant link!I think Haskell is to Reddit what Apple is to Digg.[deleted]It's hard to encourage women to get into this field when math and science classes are dominated by men from a much earlier age than when she is looking for a job. I'm not talking about college or even high school, really; as far back as I can remember, girls who were smart overall were never encouraged by teachers to focus all their intelligence on math and science. 

Case in point: recently, my mom told me that when I was in 7th grade I got the highest score on a standardized science test out of everyone in my entire class, including the nerdy boys who would go on to win multiple national math awards every year of high school. The teachers were blown away because my grades in science were so utterly average, even bad. Because of those test scores, I got to take Advanced Earth Science in 8th grade -- I just thought that I got in because my previous teacher had said I could handle the material. The class was the hardest and most interesting science course I've ever taken. I don't remember a single class I took in 8th grade except for that one.

Then I got to high school, dropped out of all of the advanced science and math classes and put all my energy into English. I haven't gotten less than a perfect score on a standardized English test (including SATs, APs, etc) since 8th grade. On the other hand, I never got above a B- in a math or science class again, and I stopped taking those classes entirely two years ago.

This isn't a boo-hoo, poor me thing: I dropped out of those math and science classes because they were boring as fuck and I wasn't willing to work my ass off in a class to memorize loads of information, as opposed to working my ass off to get my mind around a challenging concept. Maybe if my school had bothered to hire more than 5 talented math and science teachers for a school of 1000 (and those teachers focused mainly on the AP classes), I wouldn't think that. And this wasn't at a shitty public school; my high school was considered one of the best in Massachusetts.

The brain is like a muscle, and if you don't establish and consistently reinforce the pathways used to do math/science then they wither away and die and all you're left with is a Journalism major who had to check on Google to make sure the brain wasn't *actually* a muscle.
I have experienced the exact same problems with Fedora/RPM as ESR did.

While I'm not as experienced as ESR at such things, I do have very experienced friends who have stepped in to help and thrown up their hands after wasting time in library hell.

Ubuntu, on the other hand, has worked like a dream most of the time.Timbuk 3
The future is so bright, I gotta wear shades.[deleted]Links for the above literary works:

* http://www.catb.org/~esr/writings/sextips/

* http://lwn.net/1999/1216/a/esr-rich.html
[deleted]From the picture she's at least indistinguishable from a *man*.
Looks like you missed the "once you get used to" part. 
For all the arrogant diatribe about mysticism, his conclusion is that 'spiritual enlightment' is an individual pursuit. Nobody can really help you 'channel' it, and to an extent this is correct - it's a pillar of Buddhism, Gnostic Christianity and other mystic beleifs.

Very strange essay though. I'm not sure I would feel comfortable standing near him on a bus.I agree with you, but don't leave it to the w3c. They couldn't specify their way out of a brown paper bag.&gt; Debatable. I kinda like having both the freedom to shoot myself in the foot and the tools to avoid shooting myself in the foot.

This is kind of the anti-python philosophy, isn't it?Yes, but it's the most flexible.Good thing he has all that [VA Linux](http://www.linuxtoday.com/news_story.php3?ltsn=1999-12-10-001-05-NW-LF) money to keep him going...The picture in the article is horrifying...Kinda, yeah.  Although practicality beats purity. ;-)History in progress!
We must have more girls in science and engineering field.
RH is dead to me, too.  I used CentOS for awhile and then hopped to Ubuntu.  I'm hoping JBoss doesn't start to suck too much (more, hehe).&gt; It's hard to encourage women to get into this field when math and science classes are dominated by men from a much earlier age than when she is looking for a job.

That's something I never really understood. If you were given an opportunity to participate in something that's not only interesting but also primarily attracts members of the opposite sex, wouldn't you be ecstatic about it?[removed]I love that and I found it funny that, about half way through, someone tried to re-name the thread "[The Long Goodbye](https://www.redhat.com/archives/fedora-devel-list/2007-February/msg01088.html)"By and large, *men* aren't very good at advanced math and cs either. 
However, if 0.1% of men reach a given level of skill in the practice, and 0.001% of women reach the same level of skill, this results in a profound disparity. Steven Pinker presents a similar statistical argument, which results from the width of the skill distributions and the consequent likelihood of outliers.

Whether this disparity actually results from nature or nurture is of course up for debate, but this is one way it could happen.http://geekz.co.uk/lovesraymond/archive/show-them-the-codeESR is an insufferable little annoyance, but he's right that the rpm executable should be statically linked.  yum's, too.

I maintain both kinds of systems, and I always cross my fingers when yum runs.
Curious. I still use fvwm precisely because it makes me far more productive than the current desktop environment of the week. KDE and GNOME just get in the way of getting real work done.IE doesn't seem to like this URL, is there another?I'm sure I read somewhere, that range was encouraged for small n. But I made some quick tests, and it seemed to me as if xrange is always the better choice.

So until proven otherwise, I'm wrong. =)I don't get no joke.You wouldn't, trust me.  I used to date someone that frequented Balticon, a science fiction convention.  ESR would show up every year, and the man *seriously* could use a shower and mouthwash.I, for one, value functionality more than anything.  I figured open source was supposed to deliver better functionality...

But, I guess, as this guy put it best (from the same thread):

&gt;I've about had it up to here with this New Slavery crap, this pseudo-socialist movement in the Linux world being pushed by self styled self aggrandizing arrogant ivory tower elitists intent on a fascist enforcement of the licensing policies of THEIR choice dictated from inside a closed circle-jerk where everybody pats themselves on the back and laughs about how stupid the "outsiders" are.

In the end, it's what open source software can do, not under what license it can do it is what's really important.  The open source licenses are supposed to free developers and help users, but it's often used to complicate the lives of both.This debate goes at least as far back as Plato, who suggested that writing hurt memory by creating a reliance on external media. Aristotle, of course, thought differently.
I think people are switching to the programming subreddit because they're fed up with the quality of the posts, moderations and comments on the main reddit. Of course, it's only a matter of time until the Digg refugees notice the pattern and move here (by then, the subreddit will have nothing to do with programming). That's just the way it works.ESR experienced "ego death" ?? Truly a miracle :Dor plain-old generic-mode.here's a reason that most redditters will hate.  CS males are the most obnoxious people on earth.  I know because I was in a somewhat prestigious engineering school for a year and even though I had a 3.8 average I still dropped out.  The arrogance/annoyingness of people who suck but think they're goods gift to the world of computing is just not worth handling.  Combine that with the long hours and hard work needed to accomplish anything important in CS and you'll find it no wonder that smart women have focused on fields like law or medicine (fields they now dominate).

I'm not saying everyone in the field sucks at life (actually the best programmers tend to also be more humble and better people) just the obvious fact that programmers need to work on their social skills if you want to see more women in the field.I don't think its fair to say that women aren't very good at it.  I do think, however, that women were not encouraged to pursue such studies because it wasn't seen as being 'feminine' enough.  Case in point:  my parents bought a PC in 1982.  It was popped in my younger brother's room, not in a common area (of which we had plenty of space).  A few years later I managed to get to 3rd top in computing in my highschool (better than my brother's results later on) in spite of the fact that I was told that computing was for 'nerdy boys' and that there was obviously no career in the industry for me (1986).  Instead, I was pushed to do nursing/librarian or occupational therapy.  All lovely little female-dominant fields that won't frighten off a potential husband, as far as my parents were concerned.

I believe that more women would enter the industry, IF they were encouraged to do so from highschool or earlier.What about the people who list 384310 different frameworks on their CV, as if reading API docs is _hard_?Ahhh... and now the anti-haskell philosophy :)Bastard - beat me to it. ;-)Not to sound blasphemous or anything, but JPEG is a poor choice when screendumping a screen full of xterms and no fancy background.

Tutorial about installing GIMPshop in Ubuntu.[removed]He certainly has a right to express his opinion, and I'm sure these are honest, heart-felt feelings, but he comes off as arrogant.

[deleted]&gt; Maybe you don't care about those restrictions and burdens, but lots of people do.

The guy you responded to used pretty forceful language ("why should I give a shit?"), but most people may very well care about those things to some degree and still choose to use Adobe Flash Player. That is, no matter how much one might object to the terms of the license, one does stand to gain something from accepting it by using the software. You have to weigh the gain against the pain and compare with competing options.

&gt; That's reason enough for me to stick with Free Software.

It's almost never that black and white an issue except for the hardcore ideologues.

Sure, if presented with two otherwise identical options, one free, the other not, then I would choose the free one in every case, at least for my personal use. The reality is that in many instances there are no feasible free alternatives available. When the non-free license is reasonable enough (a subjective judgment), then pragmatism trumps idealism for a lot of people.&gt; Variable declarations are unnecessary in an otherwise well-designed language. And classic ASP doesn't come anywhere near that description.

Er, do you even know what classic ASP *is*?  It's not a language *at all*.  It's a language-independent framework.Free software is like free speech or any other freedom -- it benefits you, even if you don't use the freedom personally. One benefit: It guarantees that the software can never become hostile. They can't remove features from a gratis program only to charge you for them. They can't start serving ads. They can't phone home. They won't make you type in a 13 digit code. They can't pull DRM shit. If they did any of that stuff, someone else would come in and fix it. Just as free speech helps keep the political system honest, free software keeps your software suppliers honest. It even helps keep non-free software suppliers honest, by competition. 

Other than the lack of hostile software, there's a swarm of other benefits that effect you, even if you don't modify the code. 

Running all free software is a personal, ethical decision, but if we all do it, we'll all be better off. It's just like not accepting a totalitarian government (even if accepting it benefits you in some way) -- an ethical decision for the common good. We're only talking about software, so it's obviously not as big a deal, and both the benefits and sacrifices scale down accordingly, but it's the same type of thing.Tempest in a teapotPeople put TLAs (Three-letter acronyms) on their CV because of the hype, all right, but also sometimes because that's what recruiters want to see. They set these keywords on their CV searchs bots.Aren't you just using config as a global variable?  Sure, the initial parsing is hidden away, but it's an object that's in the global scope.
You should really submit that to reddit.  I think a lot of people would get a laugh out of it, even though it's kinda old.i've seen research(actually performed by female scientist) that claim:
left half of brain(logic,math,language) is dominant with women,right half(intuition,emoions) with men
my theory:men show off with math, women by appearing emotional...You haven't produced any evidence to *support* your theory.  For more information on the logical fallacy here, please see: http://www.nizkor.org/features/fallacies/burden-of-proof.htmlAlthough it is quite enjoyable to see people flaming each other about subjects that really do not matter at all, I find it more entertaining when it is about religion, sex and other subjects that normal people can relate to.

There is no way that I will tastefully retell this story on some party or in the pub. People fighting about the number of angels on the tip of a needle are a lot funnier.Heed, for this man speaks much truth.It's in response to ads looking for Web developers who "must work with LINUX, SQLServer, PERL, ColdFusion, PHP, Struts, J2EE.  Knowledge of JAVA a plus."I'll go out on a limb here and say that in a parallel universe, Stallman didn't start the FSF and the GPL and no one worked on stuff for free - like people generally don't do in other professions. So now, in that universe, there is no need for hairsplitting discussions of free vs not free and licensing because the whole thing never happened. 

Free and Open Source can deliver more because it exists and if it weren't for having to fight the most monied and powerful corporate interests in the world and their allies in governments (who pressure all less powerful governements to tow the line) maybe everything would "just work" because we'd all be playing music in FLAC, not mp3.

What would the state of the computer world be if the US had a spine and a desire to make MS stop its anti-competitive activities? OEMs selling Linux, BSD, BeOS on any machine - small innovative startups pollinating other ventures because they made money because they weren't crushed by MS taking their tech with or without payment? The Alpha chip version 12 with 128 cores? 

OS licensing seems to be a grain of sand on the beach of things that are holding us back in the world of computing.See, this is what happens when you develop a sub-culture of people who scream "BLOAT!" at the drop of a hat: Anorexic software. Complete with the pride and group-affirmation you get with anorexics.

I'm only surprised that there isn't something on that page to the effect of seeing how many more lines of code they can lose.

(I do not believe this is the same as anorexia. That would be stupid. I do think the pattern of forces in play have a certain similarity. However, since nobody is going to _die_ by making their programs small (at least not without straining the point), it's not a problem, just an interesting parallel.)

Upshot is, dwm, honey, you can afford to write a key/value parser. It's OK, it won't make you "bloated". If you don't want to, fine, but seriously, editing header files? If that's the way you want it...&gt; somewhat __prestigious__ engineering school

Consider the possibility that it was the "prestigious" part, which seems to be a magnet for such people. 

I skipped "prestigious" for that very reason. I just went to a school with a rock-solid CS department, even though I probably could have gone to a "prestigious" one, because just because I'm male doesn't mean I can stand such people either. What women were in my courses didn't seem to complain, even when complaints were solicited.

The advantage of this theory is that it also explains similar experiences from people going to prestigious law schools or prestigious medical schools, where you get the exact same complaint.Yeah! I really showed that guy!

nerds.&gt; I'm totally not interested in revision control software 
&gt; unless it's Open Source, prepackaged for Ubuntu, and 
&gt; supports importing repositories from Subversion with full history

That's a hefty set of requirements, but I'll do my best:  [Bazaar](http://bazaar-vcs.org/) is a distributed version control system, in that you can basically fork and clone repositories to one or more machines.  You don't need a server (but they have written one for users needing higher performance), so if you are on a plane or in a coffee shop with no internet, you can commit, branch, diff, revert, etc. "offline".  

The big advantage to me over SVN, since I'm almost always online, is being able to branch a repository, make a bunch of checkins, and then have it intelligently merge my changes back to the repository I branched from.  In SVN, you would need to cherry-pick revision numbers to merge, since it's basically a manual process.

Also, it's written in Python, and has a plugin-model, so if you want to add a new command to it, like "bzr my-cool-command", it's relatively easy.

They have an svn2bzr tool, but I've never used it, so YMMV.
DWM is great; I've been using it for months (from either &lt;v1 or &lt;v2) and really enjoy it when combined with xbindkeys. I don't mind having to edit the header file, though it's a bit weird having to recompile every time you change config. The upshot of having such a small WM is that recompiling is an instantaneous affair.

However, I do think a config parser would be a much nicer alternative to the current setup. I recognise that the author is trying to place barriers in the way of newbies pestering him with silly questions, but really, having a WM where everything is maximized and windows arrange themselves is probably enough of a barrier.

By the way, the system works beautifuly and I strongly reccomend this WM. Especially on a laptop.Evil Windows API! Microsoft claims it is so great, but it really sucks to use it because each function has too many unrememberable parameters. I like POSIX functions a lot better. They are simple and usually take one or two parameters.Is this the functional equivalent of the [God class](http://en.wikipedia.org/wiki/God_object)?Why gah? It's not a bad description.And as for the Open Source, Ubuntu requirement, Bazaar's development is largely driven by Canonical.In my imagination, it's a global constant - I'd make config a read-only dictionary (whether by convention or force depends on the context).

(By "constant" here, I mean something like "read-only value". It may change, for example, if config reloads the config file.)Depends on the platform of course.  Back when I was a .NET guy I used SourceGear's Vault and I really liked it.  I actually got to the point of recommending it to my clients and helped sell hundreds of license for them.

Microsoft now has Team Foundation Server and it's a good product, but it's geared towards larger shops.

Coming over to open source and Python though I've really liked Subversion.  Simple and effective.  Free.

Well, maybe when you're an adult, but most girls in middle school want to be with their friends and don't like to be singled out for any reason, including for harder math/science classes. 

Plus, what kind of members of the opposite sex are we talkin' about here? I'm not sure I would be ecstatic to be the only woman in a room full of computer programmers...Of course you should look at them all.   The only question is if you can find the time to do so.   Wisely using the time you have (before you die) is important.   Using bad version control will cost you more time than trying a bunch of choices, but since you are using subversion which isn't bad, the question is can you save even more time with something else?   

There are two types of version control (loosely), central and distributed.   Since you are happy with subversion, you should experiment with git and/or darcs, because they are distributed, and thus are very different.    The entire development models they support is different, so you should learn to use one to see if the development model works better for you.

Don't start with your current projects.   Start with a tiny "helo wrold" program.  Pretend to be two different developers and have several trivial versions to see how merging the spelling fix works across several versions, and all that. 

There are tools to convert from one version control to another.  Don't worry about them until AFTER you have decided that switching is worthwhile.   You can write/fix the tools yourself if needed.

P.S.  there are plenty of good source control packages that are not open source.   Don't be blind to their advantages.   I don't think they are worth the cost for open source  projects, but they are interesting.   Some of them will give out free licenses from time to time - if you can get one try it.  It might be worth having two version controls, one official for the project, and one that everyone actually uses.  (FreeBSD is mostly developed with perforce, and only stable things are brought into CVS - this works well for them, but in part because most developers have a perforce license)One-ply?  You might as well have been wiping your ass with pine cones.

Welcome to 2007. :DBut it isn't unique to this window manager.  If you really want to, you can edit the C (or perhaps C++) source of almost any (free software) window manager...  Editing the source code to make a config change is just, well, silly, though.So, there are relatively few women in physical science, engineering, and math compared to medicine and law, because the males are more obnoxious?   Met any doctors or lawyers recently??  Do we really believe that software developers are more arrogant than surgeons or partners in law firms?

&gt;I recognise that the author is trying to place barriers in
&gt;the way of newbies pestering him with silly questions, but
&gt;really, having a WM where everything is maximized and windows
&gt;arrange themselves is probably enough of a barrier.

Actually, this just isn't the case. The amount of pestering we receive from newbies about wmii, dwm's "big brother", is astounding. At any rate, the primary reason that configuration is entirely via C is that the author has decided that that suits him best. He wants to hack code with no superfluity and he wants to configure his code directly in his code. It's that simple.the c2 wiki, as usual, has [an interesting page](http://c2.com/cgi/wiki?MonostatePattern) on the topic of singletons. Alex Martelli has a [great article](http://www.aleax.it/Python/5ep.html) on the topic as well.You miss the point.

dwm is designed to be simple and easy to understand and hack. Most open source window managers have code bases which take ages to learn. dwm can be understood and edited in a matter of hours.I work with both SVN and Darcs on a regular basis. SVN scores points over Darcs in that it doesn't get too slow on huge codebases, and has nice things like easy Trac integration.
Two Darcs features that might be eye-openers to SVN users are firstly that every checkout is also a repository. This makes it trivially easy to do branching and offline committing (as with Bazaar), which is really quite a win (it lowers the mental barrier for committing something). Secondly, the way Darcs handles patches makes it much easier to exchange them between different branches, so you can have a divergent branch that you are hacking on, but still push some of your changes to a main branch (though it still isn't hard to get into a mess that only be solved by manual merging, unfortunately.)Sorry, man. [Not the first.](http://programming.reddit.com/info/15x4n/comments/c15x7f)I'm behind Fedora for *not* including proprietary formats and closed binaries.  Those things are available via Livna for those who want them, leaving the distro itself untainted, and with a clear message: only Open Source is good enough.Could be fixed? Yes.

Could be fixed easily? Probably not.Sure ! Male Scientist and engineers need to get laid too !http://geekz.co.uk/lovesraymond/archive/dead-parrotYeah, I realized that contradiction between good/bad and successful/unsuccessful, but I had to leave for breakfast so I didn't bother to edit it. 

Anyway, it seems kind of apologist to say that women's lack of success in these fields has nothing whatsoever to do with their "not being very good" at them. Call me naive, but I think that sexism in American society is more about a woman having to work twice as hard as a man for the same rewards, rather than women being shut out from the fields they want to pursue even if they are 100% qualified. When women are smart and willing to work much harder than men, they are capable of achieving the same goals. 

Anyway, I think the important question is not whether women are unsuccessful because they are not good at these fields, but what happens in educational institutions that causes so many women to think that they are not good at it, or to have no interest in being good at it.Duh?

I've always understood that the [stop] button stops processing / downloading on the *client* side.  AFAIK there is no HTTP mechanism to inform the *server* to cancel a requested operation.Ugh.  Talk about wrong priorities.  People complain that Flex breaks stuff, and he says "well you don't need it anyway!"  How user-friendly.

Sure, traditional applications don't have back buttons, but who cares about traditional applications?  The back button is *useful*.  If I go into a particular section of a web application, and then decide to go back, I *should* be able to just hit the back button.  *That's what it's for.*

And that's not even considering the fact that many different parts of a web application would be implemented by dialog boxes in traditional applications, meaning that there *is* an equivalent of hitting the back button &amp;mdash; closing the dialog box.

Addressability too.  Your bookmarks and links don't work?  *"They shouldn't!"*  Bollocks.  If I want to bookmark an important email, why shouldn't I?  If I want to link to a particular bug report, why shouldn't I?

He's wrong when he says:

&gt; Applications aren’t made up of pages, they’re made up of states.

User interfaces are typically made up of *views*.  And views are a pretty good analogue for pages.

But let's pretend that he's right; that web applications should follow the lead of traditional desktop applications.  Let's follow, oh, say, *Apple's* lead.  What do they focus on?  Document-centric computing and search.  Oh.
&gt; Er, do you even know what classic ASP _is_? It's not a language at _all_. It's a language-independent framework.

I'd bet large sums of money that the project he's talking about was done using VBScript. For all the "language-independence" ASP was supposed to offer, very few projects were ever done in anything other than VBScript.First, dwm is not intended being configured. It should be usable out of the box (by the builtin defaults) - so only if someone has got enough freetime, he can go configure it.

Second, it's easier to edit a header file, than reading a man page or some obfuscated document covering all options which can be set in a configuration file. Even if it's a key/value parser, you still need to know all keys and possible values. This costs time. Besides this, you need to learn yet a different language for configuring the tool in use. dwm can be compiled in lesser than 2s on a normal computer (and it doesn't depend on autohell).

Third, dwm is intended to be minimalist. Writing a config file parser, or reusing the X resource database will add tons of complexity - if the tool processes input, it has to verify the validity of the input (==configuration). If you look at the dwm config.h file, you will notice that there are several parts which would need additional mediation layers for the effort that a user has to read a bunch of additional documents, that the source code gets more messy and that there is less flexibility in setting dwm up.

At least, it has been tradition in Unix, that you can edit the source code to fit your needs. dwm is *not* intended to be the KDE replacement for the masses, it is intended to be a tool for me - and for other developers if they like it. Nothing else...

So I hope you understand my intention of its config-approach...Dead on with that. Ubuntu here. Thanks.[deleted]How do you figure that?&gt; It's amusing that he made such a point about having used RedHat/Fedora for thirteen years. It hasn't existed that long. The first release of RedHat was at the end of October 1994.

Then he could be in his 13th year of using RedHat, yes?

Oct 94 - Oct 95: 1st year
Oct 95 - Oct 96: 2nd year
...
...
Oct 05 - Oct 06: 12th year
Oct 06 - Oct 07: 13th yearif you replace "flash" for "images", it's the same argument made against images on the web years ago. they're binary blobs... bandwidth heavy... inconsistent (png 24 on i.e)...not accessible and so forth.

with time, the 'web' meaning designers, developers and browsers have learned to deal with images. 

I understand that a portion of the web is text only. great. reddit is a good example of this. but the web is more than one thing, in fact, it's greatest strength is that is has been able to adapt and include different contents / ways to interact.

I use flickr (uh, images are evil). I have fun using youtube (uh, flash should die)

there is a (big) place for text content on the web, but there's also a demand for images, sounds, video. flash is the most powerful way to deal with these.

yes, heavy flash ads are annoying, yes most flash sites suck , but it's not the technology per se.

let's make an effort to educate developers / content producers: make flash content more accessible, use progressive enhancement where possible, but saying flash is evil simply won't work.

the average joe likes 'wow': animation, special effects. images filled with superfluous images and video (else tv ads would be just text). people are drawn to images / sound. no matter how much you hate it, a more "multimedia" web will also exist for a long time.Well, to do some reasonable window manager customisation you need a little bit more than a simple properties file. Then someone defines functions for it and you basically need a whole scripting language. The same results can be done with some rather simple C functions.

Simple config files are fine if the customisation will stay simple. If the rest of the code would be sufficienetly large, embedding Lua might be a good option, but if the scripting language is bigger than the main program...

Having said that, I kinda agree with the anorexia statements in general. Just being small for smallness sake is no big use. Debugging 100 lines *is* easier than debugging 1000. But using a 2000 line library might be even better...

And to completely stray off-topic: Almost every alternative window manager has a screenshot where vim is used. I find that both interesting and amusing.So, for the people who are bashing ESR - which of his points are actually wrong?

Personally, I write Linux drivers for a living and I think he's right - redhat/fedora have destroyed themselves as viable innovators.

Quick question: How many of you Linux users are still using RH or Fedora at home? How many are using Ubuntu? Why?
Well, if you mostly use Emacs, configuring the window manager is a bit superfluous. That seems to be the target for Knuths setup, don't get in the way of the One True Editor...Fuck yeah.Precisely the opposite actually. ESR's complaints were directed at the "inward thinking 'free software' ideology" and "lack of support for proprietary multimedia" among others.Hahaha, I will take care that I run sam when I make the next screenshot ;)[removed]When I get results the exact opposite of what I'm expecting, I go back and check the calculations for a sign error...

But seriously, I'm curious what the research entailed. Have you got a link to where you saw it?Definitely bookmarking this.Many good comments exists to this article:
http://blogs.csoonline.com/node/151[removed]I've been thinking about this for a little while now - it seems to be time to replace C and C++ as the languages we use to write low-level software in, but it's not clear what the replacement should be.
This was my understanding too.  The author claims that other platforms have this stopping action though.  I wonder how they do it?I wanted to use bzr (because I wanted a DVCS and I like what Canonical are doing with ubuntu, so I wanted to participate with bzr), but it lacks tags.

That's a fairly fundamental feature, in my opinion. How can you release software without being able to tag a release? I must be missing something...Who died and made you the arbiter of acceptable web technology?

Flash adds and poorly built Flash-based sites annoy me, too. Flash is best of breed for media-rich sites, though, and works well for games.

Just because Flash, or any other tech for that matter, can be used poorly doesn't mean it can't be used well.I've had problems with svk seemingly getting confused. I was never able to reproduce it reliably, but I ended up manually pushing patches to the upstream svn repo.

It didn't give me confidence, which is something you want from a VCS. Sorry if this seems like FUD without hard evidence and also sorry not to be able to make bug report, but it was an occasional error (happened to me twice).Do I have to pay for flex? Bah.I'm not really arguing, after reading what arg said I can fully imagine the headache of maintaining a config parser. I find it delightful that dwm has been put out there for use by anyone who thinks it's good enough to use on a day to day basis. I'm one of those people, and I couldn't care less that other people might not find it easy to configure.It's not really the licensing part of the open source culture that is holding back open source.  Part of the problem is that consensus can often be difficult to reach, especially on projects that have no clear leadership.  This is made harder by the fact that a lot of open source project participants are extremely intelligent and are used to being right and getting their way.  

Also, great projects often lose momentum.  And, between projects, interfaces can be wildly different, confusing lots of potential users.  Somehow, the audience gets pared down  to those who know what they're doing, the requirements from the projects become more about advanced features and power rather than presentation which is equally important among users (and to a small minority of programmers like me, it seems).  

There's a lot of problems.  Why do open source advocates miss the big flaws and talk about the tiny ones? Are they afraid of taking an honest look at the naked emperor that is open source?  

ESR might not know exactly what it is he's upset about (and therefore sounds like a complete ass), but maybe he's seeing something there.  There is something wrong here.  No one can quite put their finger on it and give it a name, but it's there and it's huge.http://rali.iro.umontreal.ca/Publications/urls/LapalmeLispComp.pdf is a paper from 1991 about implementing list comprehensions in Lisp.
Why Falcon iſn’t intended to replace a real DBMS.IMHO, the killer features of the distributed version control systems (DVCS) over a centralised-server system like SVN are:

- disconnected working (laptop on train etc). Using any modern VCS, you want your commits to be in logical units (changesets). Once you understand this, you realise that you can't work unless you can commit, which means you have to be able to access your repository all the time when developing. With a DCVS, your working tree is a repo, so you're always OK.

- the ability for colloborators to push patches "sideways" without going via the main repo. You generally want this ability in any non-trivial setup (code review, emergency fix, etc. It's really useful).


You may want to check out the 'tailor' command if you're playing around with different VCS. It's not quite an 'any-to-any' converter, but it's a good start.

I haven't done a big evaluation, but I initially looked at bzr and dropped it due to lack of ability to tag.

I'm currently moving to darcs and like it a lot so far. The other free DVCS which seems interesting to me is git, but I don't know much about monotone or mercury (are there other major contenders?)

The biggest issue for me with darcs is the lack of an API. A couple of projects (e.g. TortoiseDarcs) are driving the command line tool and parsing the command responses, but this is fairly brittle I think. Since it's written in haskell, a C-linkage library seems unlikely, so bindings to different languages are likely to remain thin on the ground.

I'm in the process of writing my own svn -&gt; darcs migration tool which can cope with the naive version control atrocities I committed in my early days with svn. If you have a sane setup then I think 'tailor' will do the job, but I didn't and I'd still like to keep my history. If it goes well I'll end up with a set of perl bindings to darcs, which may also come in handy.There is, if the request is still being processed (i.e. the server hasn't completed output and closed the HTTP connection yet).  I know this is PHP (boo hiss, and all that), but here's  the way it handles it: [PHP: Connection handling - Manual](http://www.php.net/manual/en/features.connection-handling.php)

But there are others of those, too. For example, [PyWM](http://pywm.sourceforge.net/), which has the advantage of being devoted to the notion of simple/easy/hackable, rather than splitting that concern with the notion of tiny/mouseless/l33t.&gt; It's almost never that black and white an issue except for the hardcore ideologues. [...] The reality is that in many instances there are no feasible free alternatives available. When the non-free license is reasonable enough (a subjective judgment), then pragmatism trumps idealism for a lot of people.

What I believe is that almost everybody who uses non-Free-licensed software has no idea what is really in the licensing agreements to which they have "agreed."  If they knew, they would know how egregiously one-sided most of those agreements are, and, if they took those agreements seriously, they wouldn't have "agreed" to them in the first place.  And, if that were the case, the software companies would have lost some sales.

So the software companies do everything they can to make sure you *don't* take the agreements seriously.  In most cases, they can discourage you from even reading the terms.

Did you ever wonder why software EULAs are presented to you, the would-be "agreer," in a tiny, monospace font inside of a tiny, Post-It-note-sized text area that's accompanied by a big, easy-to-click "I Agree" button?  Why, it's almost as if the software companies want to discourage you from reading and carefully considering the terms of the contracts you're about to "agree" to.  "Why bother" their UIs suggest, "when you can just click on the big easy button and be done with it?"

But, even if the software companies don't want *you* to take the agreements seriously (because, if you did, you wouldn't agree to them), *they* do take them seriously.  If you "agreed" to let the software phone home, for example, you can be sure it will be reporting back to the mothership daily.  If you "agreed" to allow your software usage to be monitored for license compliance, you can be sure that kernel-level facehuggerware will end up on your computer to "monitor" your computer use (and if it steals a few cycles, so what?).  If you "agreed" to let the software companies change the terms of their agreements, you can be sure that over time the agreements will tilt more and more in their favor.

To be clear, I prefer Free Software because I have actually read some of the license agreements that most software companies try to foist on their customers.  And, I believe that many (if not most) of the people who choose to use non-Free software do so only because they don't take the terms under which non-Free software is licensed seriously.  Most of those people, I'd wager, didn't even bother to read the EULAs before clicking on the big, easy button.

Thus, I don't agree that "pragmatism trumps idealism for a lot of people."  What I believe is that if you look at that hypothetical "lot of people," you would find that for many, if not most, what allows them to use proprietary software is not pragmatism but ignorance of what they have "agreed" to.

Cheers!  &amp;#8212;Tom

Update: edits for clarity.&gt; And here is precisely where people like ESR and I part ways with the FSF - we balk at *making* anyone do *anything*.

On the contrary, read the thread.  One of the things that upsets Raymond the most is that he wants Linux to get enough market share to throw its weight around, while Fedora isn't willing to sacrifice freedom to get market share.  [Quote](https://www.redhat.com/archives/fedora-devel-list/2007-February/msg01103.html):

&gt; To get these things, we certainly need market share over 30% and probably need market share over 50%.  We need politicians and vendors to fear our wrath.

Raymond doesn't want to make anybody do anything?  Pull the other one.
Just make a clone.

    bzr clone /path/to/frob/dev/mainline /path/to/frob/releases/frob-0.98.4
"Senior C# Architect (8 years+)"Ad hominem much?

He's still got some good points in the article. 
[removed]I hadn't looked at that page in some time; it seems that someone went through an earlier versions and tried to clean up/rewrite the whole thing.  But it is littered with numerous not-quite-right descriptions of various Ruby features, and overall strikes me as somewhat biased towards Python.


the author mentions using his technique on double (64 bit floating point). There is a problem with using a swap routine for floats and doubles that uses return by value that doesn't occur with integral data types. 

if your function looks something like this:

double swap(double)
{
...do whatever casting and pointing to swap the bytes around
}

After the swap, since you are returning as a float/double, the compiler probably will put the swapped bytes back into a floating point register at some point in the chain. But a byte swapped float or double might no longer be a valid floating point number and you can get floating point exceptions such as denormalized value. Or if exceptions are suppressed the floating point unit might normalize your value, changing all the bytes. When the swapped double eventually gets unswapped and reused at the other end, the value will be garbage. Of course this problem only occurs occasionally, so  you are then subject to Murphy's law on when you finally get an error.

so to swap floats or doubles reliably you generally have to use a pointer cast or union to overlay an integral type and once swapped don't use the value again as a float or double.I would recommend either Bazaar (www.bazaar-vcs.org) or Mercurial (http://www.selenic.com/mercurial/wiki).   

I prefer Mercurial more for a more convenient (for me) approach to merging and also for being much faster (or was when I switched from Bazaar several months ago).&gt; Fedora / Red Hat has sort of been a joke for years as far as I'm concerned.

ESR has sort of been a joke for years.He appears to be a [gigantic douche](http://en.wikipedia.org/wiki/Eric_S._Raymond).  I think RedHat would be better off with the [other Eric Raymond](http://en.wikipedia.org/wiki/Eric_Raymond_%28villain%29) mentioned on wikipedia.I don't get it either.  Just...  huh?I think he's actually correct.  RIAs run in a browser purely out of convenience.  A web browser provides a (more or less) ubiquitous platform-independent container for an RIA.  An RIA often doesn't care about "Back" or "Bookmarking" and for a given application these concepts may even be meaningless.  

Consider Apollo which allows Flex apps to run in a Flash runtime on the desktop - no browser needed.  If an RIA were running under Apollo would you expect it to provide "Back" button or "Bookmarking" features?  

I really don't care if Flex ends up in the scrapyard along with Java Applets.  I would be interested in *why* it fails if it does though.  Why did Java Applets not take off?  Was it because of poor performance &amp; a fairly crap GUI toolkit or because you couldn't use "Back" and Bookmarks?

I'm no advocate of Flex at all.  However, I don't believe that "Back" and Bookmarking are fundamental components in all uses of the intertubes.[deleted]Yeah, I was pleasantly surprised that AP so ably described a compiler to a non-technical audience.I like mercurial. Before that my favorite was arch, but its development stopped and it was too low-level and unintuitive (UI and messages).[deleted]So you're very upset that female models get paid much more than the men and become household names?  This is a horrible injustice of some sort that must be addressed?[deleted][removed][removed][removed]Ok, I'll play. 

How did you keep page-level variables from colliding with local variables without explicit variable declaration?
Submissions like this make me love reddit. :)Another feature i like about darcs is that you can commit parts of file changes.

For example you changed a file at line 10 and line 20, but you only want to put the change in line 20 into the commit. With subversion you'll have to copy the file and manually remove the change in line 10 or something. Darcs will ask about both changes seperatly.It got my attention when I read that applications with _"multiple top-level windows that affect a single document are also not particularly well suited to the Ion model and should be fixed."_

Not that I necessarily think these are particularly well-design applications, but, uh... isn't bad to assume a particular UI design is simply a bug?I've never looked at PyWM, but, regardless of whether there are other WMs designed to be easily hacked, dwm is one such window manager, and it fulfills the purpose well. The fact that it's designed to work well without the mouse or that the author claims an 'elite' (not l33t) userbase doesn't detract from this.

The fact that it's designed to be tiny certainly contributes to this. It's easy to make desired changes to the dwm code, and it can be done without looking through hundreds of lines of code. Further, the code is quite well organized and readable. There are very few other WMs that I can say that about (windowlab is a good example, however).+1 mercurial
see slides on http://indico.cern.ch/contributionDisplay.py?contribId=29&amp;sessionId=49&amp;confId=44
 for some comparison.

One thing that usually gets overlooked is the size/quality of the codebase and dependencies. Night and day comparing svn and mercurial.Seems like a great sign that we now even have Haskell WTFs.  World domination, here we come!Or take a tarball of the release, which is broadly equivalent.

This sort of thing is the job of the vcs. Otherwise we could go back to tarballs and patch files.
Correct.  His checklist is very good and very appropriate.  I like his analogies to other professions--we need a lot more of that kind of message spread around the software-for-hire world.  A few wording changes would eliminate the hard-nosed impression.Yes, that is pretty cool. It's nice behaviour and helps to catch the "oh I added debug there and didn't mean to check it in" self-review step.
If there was such a thing as assclown poetry, GP would be it.I agree.  I've been using nothing but fvwm for almost 12
years now.  Once I tried KDE and I found it bloated, and
much slower.Two great jokes that go great together?Now if we could get back to having this kind of thing on reddit, the dark side of the force (political reddits) might be defeated! +1But, but, but [Everybody Loves Eric Raymond](http://geekz.co.uk/lovesraymond/archive/nop-nop-nop-nop-nop)!
We had a recent conversation internally.  People were talking about subversion when I mentioned darcs.

Overall, I believe that darcs is better than subversion.  It handles merges better, stores the repository in an easier to browse way and overall is just easier to use (but that last point is my opinion).  

Like bazaar (which I personally don't know) It's distributed.  This means that tagging works by forking the repository into a copy.  Now I noticed before that someone said "why don't just tarbz the thing then, this is lame, this should be task of the VCS".  Perhaps, but then you're missing a big point.  One of the advantages of a distributed VCS is the ease with which you can move patches inter-repositorily.  This means that branching is just as easy cloning/getting a copy of an existing repo.  And then if you have backwards patches that you want to get into your main trunk, it's as easy as pulling that patch from the other repo (or alternatively, pushing it from the development repo to your main trunk repo).

The people at work were very convinced.  Finally, the choice went to subversion for two specific reasons, though they are still open to the talk.

1) They knew subversion better.  I told them, and they agreed, that the important part is setting up a flow-document as to how different steps are done.  Translating this flow document from subversion to darcs is then purely syntactical

2) And this is a valid point, there are less front-end tools.  Now personally I do not really use these but I can definitely see the validity of this claim.  Personally I do not need this, but then again, I'm not working in a big team.  The things they were looking for but do not yet exist (and which are orthogonal to the technical merits of darcs and can thus be developed independently) are a) a GUI to show the diffs more graphically and b) a GUI (or same GUI) to see the history of a file in patches.  Now both of these informations can be retrieved from darcs, it's more about having a frontend to display it clearly.

And notice that 2 is the reason they 'currently' are going with subversion, because the agreed that if it were not for that, darcs would be the choice as it is superior, technically speaking.

I hope that gave some clarity :)&gt;And to completely stray off-topic: Almost every alternative window manager has a screenshot where vim is used. I find that both interesting and amusing.

That's a good point (though I've seen a few with emacs, instead. Especially the lisp ones), but I've posted at least a few screenshots of wmii with sam and/or acme. The default one has vim, but it's a contrived screenshot, anyway. I'll update it to use sam instead.Yeah, I am quite surprised at all these comments. "I work in the aviation industry..." "I have flown over 100k miles this year..." "I work in automobile manufacturing..." He's got a regular Who's Who of anonymous comments going on here.The two biggest advantages of distributed revision control systems over centralized revision controls systems are _disconnected operation_, or the ability to commit without network access to a central repository, _cheap branching_, or the ability to clone an existing branch quickly, and _no central repository_, or the ability to push (or pull) sets of changes  between multiple independent repositories.

I do a lot of work on my laptop, so it's nice to be able to commit small, atomic changes to my local repository, then push the changes en masse when I have access to one of my networked repositories.  

Multiple independent repositories are useful if you want to, say, maintain a private and public repository.  Technically you can emulate this behavior with `svn-mirror` or some combination of dump and restore, but Subversion and other centralized version control systems aren't designed for this kind of behavior, which means using them this way is cumbersome at best.

There are other side benefits to each of these features as well; `stat` and `commit` operations don't require a network round-trip, which makes them blazing fast compared to Subversion.

I spent some time with Monotone and Git, and read through the documentation for Darcs, Bazaar, and SVK.  Monotone is too slow for practical use (I hear it's faster now) and is (or was, anyway) missing some important features.  Git extremely Linux-centric (Windows users need not apply, at least not unless they want to use Cygwin), and the command-line interface is a convoluted mess; it took me several hours to get the hang of   Git, and less than an hour for Monotone and Mercurial.

I can't really see the benefit of SVK;  It's slower and has less features than the alternatives. Not to mention an unnecessary dependency on Subversion.

I haven't used Darcs or Bazaar, so I can't comment on either directly, but reading the documentation for both and some comparisons online was enough for me to discard them in favor of my final choice.

I finally settled on [Mercurial][] because it's fast, portable, and simple.  

  [Mercurial]: http://selenic.com/mercurial&gt; There's a huge difference as far as maintainability, reusability, etc. 

I agree with you there. It is just the "Global variables are evil" mantra that get on my nerves. I would rather people say, "global variables are dangerous". The former suggests one should never use them, while the latter suggests they should only be used with care.

On the same vein, I don't see things like singletons or the borg pattern to be functionally any different that a raw global on their face. 

I think it would be far more productive to talk about how to protect globals. For example, globals that are pseudo-constant or globals that can only be changed by calling a specific method that ensures the new value is valid. 
&gt; yes, and for those people who were actually trying to make -entire applications- based on images (using image maps), i offer the same criticism.

You extended the analogy past where it made senseJust great stuff - thank you for sharing. 

TIP FOR THOSE ON IIS: to install this on IIS, you need to download Active Python, install it and then configure web service extensions in IIS to allow python. I found a tutorial that is actuall a pain in the ass but it worked for me: http://python.markrowsoft.com/tutorial/default.aspxOpen Source CAN do the Intertubes including all major protocols and most presentation layers (Web, P2P, VoIP), almost all financial transactions not stuck in legacy COBOL, and Operating Systems and software and that powers Google, Amazon, and nearly every other top 100 site. Oh, and it also is the heart of OS X and, up until Windows XP, powered Microsoft's TCP/IP stack.

But, I forgot, it doesn't do games as well as Vista. I guess it loses :([removed][Future generations will recognize that women are naturally suited for computer programming.](http://dynamic.ropine.com/yesh/article/the-future-of-women-in-computer-programming)Yeah, ESR seems like a difficult person. But a lot of shit gets done because difficult people whine, complain, step on toes, and generally force reality to conform to their expectations of it.

Look at any great person in history who got a lot done and made the world a better place, and I guarantee you that they were probably not the easiest person to get along with.

ESR made a huge difference in the open/free software movement by explaining how it worked in a way that made it comprehensible to people outside the geek community - including people with money and influence in the corporate world.I haven't looked through most of these, but the logic book strikes me as being a little weird. For example, he uses _induction_ but never defines either it or formal recursion (in the constructive sense). This seems sloppy.

If you're interested in getting a good introduction to mathematical logic, you'd probably be better served grabbing the classic [A Mathematical Introduction to Logic](http://www.math.ucla.edu/~hbe/amil/index.html) by Enderton. You can find copies for nearly nothing on bookfinder.com.

Also strangely missing is the brilliant introduction to semantics, [Semantics with Applications](http://www.daimi.au.dk/~bra8130/Wiley_book/wiley.html) by Nielson &amp; Nielson. Great book, free PDF.

It's surprising a woman won the Turing award? I don't get it.That's pretty "fantastic".For those that find git a bit daunting, Cogito is a really nice wrapper around git.  It's just shell scripts around the raw git commands, which gives a sort of "best of both worlds" feel.  You can treat it like a really simple CVS replacement, but because it uses plain git repositories, you have all that hard-to-use power if you ever need it.Exactly.

Circa 1992 or 1993 I had 2400 baud at home and the terminals at school were all black and white, I didn't use colors in my editors until maybe 2000.  I still know some reasonably proficient programmers (not including Knuth) that eschew colors, and sometimes I still don't use them, I barely notice the difference unless someone points it out while looking over my shoulder.

And I come to think of it, I probably use less colors than your average person, and no IDE.  I generally stare at stuff like this all day: www.jqrsoftware.com/editing_file.png

Sadly I have to run cygwin at work, but you get the picture.
I don't think it's curious: it's a matter of taste or training, or both. And yes, I still love fvwm after all this years.Curious about the down votes - are security problems in the PHP language not of interest, or do people think this is a lousy article about them?I have to admit that I lean more from WTFs than from tutorials. With WTFs, you get see to why a best practice exists and what happens when you don't obey it. I trust these more than ones created from thin air with questionable theoritical support.&gt; Microsoft now has Team Foundation Server and it's a good product, but it's geared towards larger shops.

We tried using TFS, but it was a stupid. The requirements for getting it setup were way too much, and if you botched a single step in the lengthy install process there was no guidance for fixing it.

Of all the ones I tried, Vault was the one best suited for us. (MS shop, ~10 developers)
As a speed comment,

Bazaar is slower than just about every other drcs.  I don't know why you would use it, when the point of distributed is committing often, and what's the advantage when the commit time is longer than a fraction of a second?

git, monotone, mercurial all have very quick commit times.  I would actually say that mercurial is the fastest drcs, but I haven't finished testing things out.

Monotone has horrifically slow pull times, just try pulling monotone's source.Same with "hide"Definitely a written proof is a work of art, a helpful, explicative narrative for the user that hides all the dirt used to produce it.[removed]It exists, though I heard that it isn't perfectly reliable. ASP.NET lets you access this via the HttpResponse.IsClientConnected Property.

&gt; The IsClientConnected property returns false under the following conditions:

&gt; The connection to the client was terminated. This can occur if the Close method was invoked or the client stopped execution of the Web page or browsed to another page.

&gt; The HttpWorkerRequest object that is handling the request is a null reference (Nothing in Visual Basic) or the HttpWorkerRequest.IsClientConnected method returns false. If a custom HttpWorkerRequest object handles the request, then the HttpWorkerRequest.IsClientConnected method might be set based on custom criteria. For example, the custom worker request might force a time-out after a period of time.

http://msdn2.microsoft.com/en-us/library/system.web.httpresponse.isclientconnected.aspx
think of it as a useful simplification.

When you're using gimp, you want to be moving windows around, resizing things, and moving palettes all the time. When I want to use it, I log out of ion and log back in with xfce.

When you're using ion, what you want is everything to be where you expect it to be, accessible by a known set of a few keystrokes, and taking the largest area possible. My setup is a full-height Vim window on the left with two half-height terminals (ipython + misc) on the right and no wasted space *anywhere*.The easy way would be to expose a IsConnected property that developers can check during long scripts like ASP.NET.

The hard way would be to actually abort like PHP, but even that is doable if they already support script timeouts. By the way, do they?A better question is how should they do it? Should it be a external abort like PHP or something the developer has to check for like ASP.NET? Or do you have a third idea?Version control is something I'm pretty conservative about.  I'll let other people experiment, and play it safe with subversion.svk is hard to get used to.  your svn fundamentals need to be really good, and you need to play with it before using it in productions.  the documentation is not really very good.

once you've gotten over learning it, though, you won't use anything else :)Has Lanier ever contributed anything useful besides overly verbose articles completely devoid of meaning? You know unlike that fraud Turing...If you've got operations this expensive that you aren't spinning off to a background process, you've got bigger problems to deal with anyway. Sure it might be nice to be able to cancel an action via HTTP, but it's not a significant problem in any realistic production environment.[deleted][deleted]I [just transferred](http://billmill.org/ss2svn) my shop (MS, 6 devs) to subversion from sourcesafe. While we liked Vault in principle, it was hard to justify the cost when [AnkhSvn](http://ankhsvn.tigris.org/) works so well as a Visual Studio plugin and [Trac](http://trac.edgewall.org/) is so super sweet.Bazaar supports tags in a plugin, which works very well. You can also branch to a separate directory, similarly to Subversion.

In my opinion, the best thing about Bazaar is the brain-dead easy branching. Want to add a new feature? Branch to a local directory, make your changes and commit often, then merge it back in. A full history is kept of all changes, and you don't have to bother figuring out version numbers like in Subversion.

The second best feature is offline committing. For example, if somebody cuts a fibre line and you're without a network connection, you can still work on whatever - just push your changes like normal when you have a connection again.I'll have to take a look at that next time we consider leaving SourceSafe again. Last time we didn't actually switch to vault because the pain level wasn't high enough yet.Definitely check out darcs. It's distributed, like other people here have mentioned, but it's also the easiest for me to learn. I've used mercurial too, and I like a lot of things about it.

One thing I like better about darcs is that it lets you correct mistakes easier. Mercurial has one level of undo. Darcs has unrecord, unpull, unrevert, and amend. Unpull and Unrecord can be done at any time, since they do not depend on an undo history. Amend is for adding something to a change thats already been committed. None of the other RCSs seem to have that level of recovery for mistakes.

Mercurial is cooler when it comes to branches. If your code needs to maintain several branches simultaneously, mercurial may be better.

Both mercurial and darcs don't need anything special to set up, unlike cvs and subversion. The best thing to do is just

    apt-get install darcs mercurial

and try them out.

go into a directory, then

    darcs init
    darcs add &lt;file&gt;
    darcs record &lt;file&gt;
    darcs changes

That's it. Mercurial (hg) is similar:

    hg init
    hg add
    hg commit
    hg log

-DavidMercurial is great. :)  It's lightweight and easy to pick up, too;  if anyone wants to give it a try, here are the starting points:

* the minimal [quick-start](http://www.selenic.com/mercurial/wiki/index.cgi/QuickStart) guide
* the [basic concepts](http://www.selenic.com/mercurial/wiki/index.cgi/UnderstandingMercurial)
* the full [tutorial](http://www.selenic.com/mercurial/wiki/index.cgi/Tutorial)Strange, the only time I've had serious issues with apt-get is linking up to the unstable branch, but even then, waiting a few days usually solves the apt-get dependency breaks.  When rpm breaks, it just never ends...Dude, you cheat.  Your string literals, comments, and function names are all colored.  Pretender!

;+)

(This is coming from a guy who thinks Visual Studio doesn't have enough color options though.)[removed]Discusses the 10 reasons for migrating to logbackHave you read any of the guy's political screeds? He's a raging asshole, and his continued insistence that he, personally, speaks for the hacker community is both pathetic and offensive.I have uploaded a modded version of colkassad's reddit-lite here: http://www.learnsqlserver.com/redditlite.zip

There are five files - my filterfile.txt and four sub-reddits (1 each for front page, new, programming front page, and programming new). 

I have also made it pass the HTML validator test and it looks nearly identical to the reddit site. The only problem? No way to upvote, downvote or hide :(  Someone please show me how to do that!!!!

Download file [here](http://www.learnsqlserver.com/redditlite.zip)By the way, this is my first time every f-in with Python so, if I've made some goofy mistakes, please tell me.Yes, those features you list of bazaar are common to all the DVCS's (bzr, darcs, git, mercury, monotone and the others) and are the reasons to switch away from subversion (and they're great).

It's good to know about the bzr tags plugin, I don't think that existed when I was looking around before and doesn't seem to be widely known about (I've asked this question a few times and this is the first response I've got which mentions it).

[NB: I don't dislike subversion. Quite the opposite, it's a great centralised VCS. But it isn't a DVCS and doesn't try to be.]

We just need the current glut of DVCSs to settle down a bit into one or two 'clear winners' so we (developers) can avoid having to learn 6 similar tools instead of 1 or 2.

(Edit: or get some seamless real-time conversion between the main players, so people can use their DVCS of choice and still contribute to projects easily)Interesting. I find them to be quite handy in Python. I don't use them for everything, but in cases where there are more than one or two args, I like to use them -- it makes the code more clear.

What didn't you like about them? Was it specific to VB's implementation, or a more general dislike?&gt; Joseffer responded yes, that's the US' position, but no, that's not AT&amp;T's position. "It's the physical embodiment of the software which in some instances is manifested by -- by those electrons," said Joseffer, perhaps broaching for the first time in history the topic of whether electrons are patentable. "Now AT&amp;T's contrary view is that the abstract code in the abstract is the component. The reason that can't be, is that object code in the abstract is just a series of 1's and 0's. In theory I could memorize in my head or write down on a piece of paper. But that's not going to combine with other, with other parts to make a patented invention."
You mean that wasn't some pirate zen station?ObScheme: [SRFI 42: Eager Comprehensions](http://srfi.schemers.org/srfi-42/srfi-42.html)what notation was used to produce this diagram?Heh that's funny because Red Hat does more open source development than any other entity. The majority of work done on the kernel (including the virtual memory manager and CPU scheduler), GCC, glibc, glib, gtk+, cairo, selinux, and many many other important open source projects are worked on and maintained by Red Hat.

Get informed: 
http://fedoraproject.org/wiki/RedHatContributions
http://www.sourceware.org/projects.html

That's great that your favorite distribution can take other's work and redistribute it, but someone needs to code that stuff and if Red Hat went away you'd see a ton of very important open source development slow to a crawl overnight.How does the external abort work in PHP?Okay, I'll stop thinking of Flex.I personally don't have a problem with them, it is the MS community in general that seems to hate them.

There are two 'problems' commonly cited with optional parameters in VB.

1. VB didn't support overloads until 7.0. While I do find optional parameters useful at times, quite often I find overloads to be more appropriate. For example, when writing a method that takes a string or an integer, but not both. I think this has caused a backlash over their use.

2. The second problem is that of versioning. In VB, you can specify a default value for an optional parameter. When you do so, the parameter value is compiled with the application, not the library.

If you later change the library to use a different default value, the application won't pick it up unless it is also recompiled.

On the other hand, if you use an overload the application will effectively use the new default immediately.

I don't think this is necessarily a problem, as I can argue that either design is problematic and in the end the library writer shouldn't be making breaking changes.

3 . A new problem is that C# has zero support for them. In libraries that heavily use optional parameters like MS Office Automation, you end up having to specify 30+ parameters all with the value "missing".

Oh, and if they are ByRef parameters, you need to create dummy variables to hold them of the correct type.

C# really sucks when it comes to COM.
Sigh. If you're making this argument in the headline, don't restrict it to software -- non-software patents are just patents on unusual arrangements of preexisting molecules.I don't know anymore than what I could gleam from DerelictMan's post in this thread.

http://programming.reddit.com/info/15y35/comments/c160sg
Info about the KernelMapper can be found here: http://lug.oregonstate.edu/projects/kernelmap/about.php

It's based on the BCG/OSDN KernelMapper ( http://kernelmapper.osdn.com/ ) using the Free Code Graphing Project ( http://fcgp.sourceforge.net/ )You should really check-out the following video lecture about maths. In only a few minutes, you'll learn more than you'll ever learn from reading those books.

[Maths](http://www.youtube.com/watch?v=MiMWJ1xBo8w&amp;mode=related&amp;search=)

And if somebody tells you they don't understand maths, just show them the video. They'll understand it in no-time.It's hard to describe certain benefits of moving to a totally distributed system because it's really radically different from using a centralized system. The offline commits and cheap branching are a big deal, but the savings in the day to day workflow that result from these sorts of things are sort of difficult to talk about. 

Learning to use them effectively seems to be similar to learning functional programming when all you know is imperative: it'll make you understand how to collaboratively manage and develop code in new ways. After moving away from Subversion recently (which I do quite like) I really have no desire to go back because there's no way I could work in as productive a manner as I can now.The complete list of words that (when used in a story title) get more than 500 diggs per story on average. 

How it was developed? We analyzed 17,713 article titles and extracted individual words. Those words that were used in less than 30 article titles or got less than 500 diggs per article were removed. 

Answers to your questions: 

all words from the following list were found in at least 30 story titles on digg.com; 
there are no words that were used in only a few story titles, there are only words that were used in at least 30 article titles; 
there are words like "a", "the", "those", and we will not remove them in order to keep the research clean; 
numbers in the right column are "diggs per story": if a word were used in 100 story titles with 100000 diggs total, its "diggs per story" will be 100000/100=1000; 
all words are in the lowercase - this way it's easier to build statistics, however we can change letter cases on request (like we did with iPod); 
Here is an experiment (more funny, than real): Nano-mario wants his amazing RIAA photos! got: 

7 diggs in 10 minutes; 
10 diggs in 20 minutes; 
15 diggs in 30 minutes; 
21 diggs in 40 minutes; 
30 diggs in 50 minutes; 
58 diggs in 1 hour; 
This story is on the front page of digg.com now! 
157 diggs in 1 hour and 10 minutes; 
208 diggs in 1 hour and 20 minutes; 
260 diggs in 1 hour and 30 minutes; 
300 diggs in 1 hour and 40 minutes; 
Removed from the front page because its title doesn't match its content; 
330 diggs in 1 hour and 50 minutes; 
350 diggs in 2 hours; 
400 diggs in 2 hours and 10 minutes; 
440 diggs in 2 hours and 20 minutes; 
470 diggs in 2 hours and 30 minutes; 
Hit 500 in 2 hours and 37 minutes! 
509 diggs in 2 hours and 40 minutes; 
533 diggs in 2 hours and 50 minutes; 
553 diggs in 3 hours; 
570 diggs in 3 hours and 20 minutes; 
590 diggs in 3 hours and 30 minutes; 
610 diggs in 3 hours and 40 minutes; 
620 diggs in 3 hours and 50 minutes; 
640 diggs in 4 hours and 20 minutes; 
664 diggs in 4 hours and 40 minutes; 
678 diggs in 5 hours; 
685 diggs in 5 hours and 10 minutes; 
700 diggs in 5 hours and 30 minutes; 
707 diggs in 5 hours and 50 minutes; 
718 diggs in 6 hours and 10 minutes; 
765 diggs in 10 hours; 
771 diggs in 10 hours and 30 minutes; 
It's not us who posted it - somebody else decided to start an experiment. We have already stopped to track stats and this page will no longer be updated. 

Here is a report with the detailed stats and conclusions. 

It seems to us that every 5th new story on digg.com uses these words now. People, please don't write crazy titles! It's humans who will read them, not robots. Digg.com moderators, sorry and thank you for your hard work. 

Word Diggs/Story 
riaa 1,320 
amazing 1,258 
photos 1,159 
nano 1,132 
want 1,118 
mario 1,071 
his 1,051 
who 1,042 
ever 1,008 
vista 996 
page 995 
people 967 
they 960 
digg 950 
made 944 
my 932 
awesome 931 
$100 930 
our 928 
billion 927 
-- 921 
watch 910 
super 902 
geek 901 
shows 901 
boot 898 
water 896 
officially 896 
i 894 
lego 889 
any 887 
does 886 
man 884 
1.5 878 
huge 876 
last 869 
wikipedia 864 
story 854 
photo 852 
10 851 
image 849 
black 844 
keyboard 842 
it's 831 
physics 831 
so 828 
store 828 
china 828 
that 828 
drm 824 
revolution 822 
can't 818 
20 813 
most 811 
city 811 
if 809 
every 809 
really 808 
display 807 
nes 805 
finally 803 
2006 802 
makes 802 
earth 801 
pro 799 
top 799 
behind 795 
ultimate 792 
ajax 788 
years 785 
steve 780 
screenshots 780 
things 780 
to: 779 
don't 778 
best 777 
minutes 775 
turns 774 
password 767 
their 767 
kevin 765 
javascript 762 
css 762 
know 761 
mpaa 755 
just 754 
ten 753 
it 751 
school 750 
revealed 746 
paper 745 
100 740 
end 739 
from 737 
what 733 
gates 733 
xp 731 
when 730 
+ 730 
faster 728 
2.0 728 
map 726 
wants 726 
camera 725 
have 724 
only 724 
look 724 
very 722 
playstation 721 
this 721 
bill 720 
video 720 
play 719 
released! 715 
net 715 
should 714 
record 714 
office 714 
by 713 
complete 712 
science 710 
own 710 
photoshop 709 
design 708 
mit 708 
you 708 
machine 707 
music 706 
list 705 
other 705 
guy 704 
simple 702 
game 702 
an 701 
why 700 
3d 700 
comparison 700 
inside 700 
of 699 
user 697 
ds 694 
being 694 
time 693 
need 693 
has 693 
brain 692 
jobs 691 
access 690 
light 690 
storage 690 
without 689 
gets 688 
call 688 
download 686 
moon 686 
no 686 
buy 684 
become 683 
explorer 681 
usb 680 
patent 679 
full 679 
iPod 679 
can 678 
= 678 
do 678 
family 677 
data 677 
first 675 
google 674 
nintendo 672 
in 672 
laptop 672 
live 671 
color 671 
than 671 
about 670 
how 670 
drives 670 
getting 669 
tools 669 
final 669 
tutorial 669 
under 668 
ubuntu 666 
industry 666 
not 665 
a 664 
space 664 
add 663 
internet 661 
new 660 
is 659 
was 659 
official 658 
real 658 
after 658 
at 658 
we 657 
year 655 
one 655 
apple's 655 
all 654 
world 654 
the 653 
program 653 
using 652 
opera 651 
human 650 
more 650 
now 650 
like 648 
apple 647 
on 646 
3 645 
better 644 
ps3 643 
running 643 
- 643 
red 642 
news 642 
big 642 
pictures 641 
50 639 
interface 638 
price 638 
sued 638 
make 638 
japan 637 
gmail 636 
million 636 
secret 635 
working 634 
tv 634 
to 633 
go 633 
install 632 
be 632 
will 632 
gaming 631 
flash 629 
are 629 
get 628 
into 626 
php 625 
its 624 
with 624 
windows 623 
code 623 
high 623 
talk 623 
found 623 
over 622 
web 622 
sites 621 
works 620 
as 620 
and 620 
back 619 
much 619 
build 618 
dell 615 
against 614 
out 613 
u.s. 612 
interview 612 
down 609 
center 609 
 609 
itunes 608 
online 608 
torrent 607 
through 607 
videos 607 
vs 606 
been 606 
intel 606 
satellite 605 
great 605 
ebay 605 
release 602 
developers 602 
7 601 
version 601 
speed 601 
take 601 
your 601 
students 599 
launch 598 
games 597 
case 596 
pack 596 
up 595 
collection 593 
scientists 592 
see 592 
hit 592 
two 592 
movie 591 
coming 590 
360 589 
based 587 
says 587 
or 587 
trailer 585 
hack 584 
old 583 
easy 583 
dead 582 
home 581 
lcd 581 
art 581 
dual 581 
find 581 
computer 581 
create 580 
episode 580 
day 579 
use 579 
car 578 
ready 578 
learn 577 
cool 575 
controller 574 
website 574 
still 573 
vs. 573 
sony 572 
us 570 
making 570 
generator 569 
drive 569 
8 568 
microsoft's 568 
technology 567 
system 567 
aol 566 
life 566 
images 566 
1 565 
but 565 
hardware 564 
test 564 
mod 564 
show 559 
help 558 
portable 557 
player 557 
run 557 
file 556 
macs 553 
movies 553 
adds 550 
off 549 
robot 548 
virus 546 
power 545 
&amp; 545 
x 544 
free 544 
cards 543 
audio 543 
5 542 
core 541 
dvd 541 
mac 539 
tutorials 539 
pics 537 
via 536 
good 535 
market 535 
2005 535 
could 534 
software 534 
takes 533 
say 533 
project 533 
for 533 
xbox 533 
work 530 
desktop 529 
users 529 
future 529 
blog 528 
tech 528 
computers 528 
next 528 
firefox 528 
may 527 
open 527 
programming 527 
mars 525 
unveils 524 
world's 523 
set 522 
cd 521 
hd 519 
site 519 
microsoft 518 
hard 518 
/ 518 
ipods 517 
battery 517 
phone 516 
released 514 
4 513 
hacker 513 
engine 512 
available 511 
releases 510 
part 510 
screen 509 
legal 509 
mini 509 
update 508 
nasa 505 
bittorrent 505 
pc 505 
2 505 
tips 502 
extension 501 
downloads 501 
Max: 1,320 

&gt; He seems to think this because he's mixed up character sets and encoding schemes.

Where did you get that crazy idea from? One of the points of my article was to expel the mixing of those two!

&gt; At the very least, he thinks that a character set must have a language-specific ordering. Huh?

Oh, come on! Now you're just trolling. Characters sets are *all* about ordering. For example, Unicode code point 0x20AC (Euro Sign) is located at 0x80 in Central European character set, and at 0x88 in Cyrillic character set. Wouldn't that qualify as language specific ordering?

&gt; Well, I've never met anybody that thinks this.

Then you're either very lucky, or you meet very little people...Upmodded for being totally in Russiansourcesafe is a train wreck of a product, as I'm sure you're aware. 

It is a source control system which does not lock files when it writes to them. Thus, if you and I commit to a file at the same time, the results are unspecified.

That and the loss of file history for moving a file (sometimes!) gave me enough ammo to get us to switch, and it's been worth it lots of times over.[deleted]&gt; Misconceptions exist, as they do about any other technology out there. Clueful people rid themselves of them by reading helpful material such as books, tutorials, specs, FAQs, Wikipedia articles etc.

The problem is that people don't tend to read from reliable sources. They seek simple definitions, that they can understand. And there are tons of those on the web. In the article, I was trying to address the false ones.

&gt; What's with the delusion of grandeur?

Huh? There is none. Have you even read *past* the title of the article?What about [SICP](http://mitpress.mit.edu/sicp/)?This guy writes as if C++ and VB are the only languages in the world. I think he forgot about Java and COBOL... well I guess those 4 sum it up. Are there any other programming languages other than these 4?Timeouts are part of the ruby standard library.  It takes three lines or so to add timeouts to an app.You're welcome...I'm glad you like it :) Just to elaborate, your keyword file should be one keyword per line. The script stops reading once it reaches a blank line.Thanks. :)Yea, it's like when O'Reilly (and now Stephen Colbert) makes some outrageous statement, then adds: "Prove me wrong."I happen to have spent a good deal of time with Jaron in the late '70s so have been mildly interested in his subsequent career. 

He is an extraordinarily bright guy, and indeed a bona fide ex prodigy (if there is such a thing). Moreover, he grew up without ever having had to adapt to conventional mores and without having had to adopt any conventional modes of thought beyond the mathematical. (Translation -- he was raised by a warped genius artist father on a goat farm in New Mexico and blew away his math profs in college). 

What has he done? Well, he did some work at Atari that you probably haven't heard about, and built-and-burned a virtual reality startup that maybe you have. He likes to call himself a musician, but I would characterize his work more as metamusic. If you had seen his amazing presentation/performance at Siggraph in the early '90s (VR music wheel thing) you would know what I mean and might not be asking this question. 

Not to say I'm dissing you for asking it. If I only had access to his recent writings and utterances I'd be wondering as well. I too have problems with some of his blanket statements. However, I cut him some slack because I know (or knew) him. Yes, he seems arrogant and disrepectful, possibly because he is. But that's not necessarily bad for someone like him, just like it's not bad for someone like Wolfram. Both are looking at the big picture and trying to stay out ahead. Both want to change the world in a big way and to be known for changing it. 

But one needs a certain amount of introspection to understand why one does and wants certain things, and here is where I think Jaron is weak. Just the same, I pay attention to what he says. 

I've met some of the smartest and creative people in the world (Feynman, Freeman Dyson, Eugene Wigner, the aformentioned Wolfram, and so on). I'm just smart enough to know that these people are seriously smarter and more creative than I. Jaron, IMO, is in the same league, but not yet quite over his need for attention.You know I'm going to start handing examples of where, say, Python is used in large codebases right? Suffice to say, there's a bunch of people who would disagree that it doesn't scale.

Unless by large you mean "obscenely large".I want to vote for darcs too. The big win for me is that the 1-page HTML version of the manual weighs in at about 150K, whereas the 1-page HTML version of the svn manual (the O'Reilley one) weighs in at almost 10x that.

If the entire system can be explained in 150K (basically 30 minutes of reading, if that), then that seriously can't be bad.

Also: http://darcs.net/DarcsWiki/MigratingFromSubversion  is a good place to start if you want to know how to migrate a repository.
&gt; &gt; He seems to think this because he's mixed up character sets and encoding schemes.

&gt; Where did you get that crazy idea from?

You think that the *particular* code points are a defining quality of a character set, rather than a detail, right?  That sounds awfully like a character encoding.

&gt; Now you're just trolling.

Why are you accusing me of deliberately antagonising people?  Or did you just want to call me a name because I didn't like your article?

&gt; For example, Unicode code point 0x20AC (Euro Sign) is located at 0x80 in Central European character set, and at 0x88 in Cyrillic character set. Wouldn't that qualify as language specific ordering?

Of course not.  The most obvious reason is that neither "Central European" nor "Cyrillic" are languages.  But at a more general level, just because these two happen to be in a different order, it doesn't mean that this is a defining property of a character set or that it's intrinsically linked to language.

&gt; Then you're either very lucky, or you meet very little people...

Well the first thing I learnt, and the first thing any Unicode material I've ever read has said, is that Unicode has multiple, variable-length character encodings.  I'm very surprised if this really is a popular misconception, but I don't have numbers and I doubt you have either, so I guess we'll just have to differ on this point.
Err, can somone point out the difference between this and map/filter?while most redditors are thinking,
"""
dude java is soooo lame!! in awesome dynamic languages like python, lisp and ruby, you don't need dependency injection in the first place, thus proving how lame java is.
"""
if you're stuck using java professionally, this is a nice toolFor non-trivial projects where frequent branching and merging are a necessity, SVN will definitely feel inadequate. [Even the Subversion Book admits that Subversion is lacking in this department](http://svnbook.red-bean.com/nightly/en/svn.branchmerge.copychanges.html#svn.branchmerge.copychanges.bestprac.track):

&gt;Ideally, your version control system should prevent the double-application of changes to a branch. It should automatically remember which changes a branch has already received, and be able to list them for you. It should use this information to help automate merges as much as possible.
&gt;
&gt;Unfortunately, Subversion is not such a system;

Also, you can still use a distributed revision control system in a centralized fashion using a "master repository", [without losing any flexibility](http://www.selenic.com/mercurial/wiki/index.cgi/UnderstandingMercurial?action=AttachFile&amp;do=get&amp;target=autogenerated-d52da1b336fe07da3a8a6688c1bb2c7082ecc148.png).

Finally, the preferred choices for a DVCS seem to be:

* [Darcs](http://darcs.net/)
* [Mercurial](http://www.selenic.com/mercurial/wiki/)
* [Bazaar](http://bazaar-vcs.org/)
* [Git](http://git.or.cz/)
Actually, you'd be surprised how many Emacs users use wmii. It astounds me. In my eyes, Emacs and wmii are polar opposites, and I don't see why an operating system (Emacs) needs to run in a window manager, but they come in droves.

Ruby... bleh. There's a ruby configuration driver for wmii. It creeps me out. It's probably got nearly as many lines as the wm itself.The builtin LOOP macros is "list comprehensions for Common Lisp" and more. His first example could have been expressed using LOOP:

    (loop for x from 1 to 8 collecting (* x x))
    ==&gt; (1 4 9 16 25 36 49 64)

for example.It's not "conservativeness", it's called "lack of openness".Just before the above gets downmodded below the default threshold, I'd be grateful if anyone who thinks taking a clone of the repo as a release tag *isn't* like taking a copy (whether as tree or tarball) could let me know their thinking?

In both cases you're just taking a copy of the repo at a point in time. You aren't adding information to the repo to allow comparisons etc.

You're taking up disk space with another entire copy.

If someone takes a copy of your repo, they don't get the other copies you've made for release tracking. They would get tags which lived in the repo.

It's good to hear (from a different response) that bzr now has a semi-official tags plugin (the description page suggests it is a temporary solution, but is likely to have a migration path to the eventual official solution).The new star wars Euphoria engine uses the same principle.
Video: http://www.youtube.com/watch?v=3bKphYfUk-M

We could really use an open source physics engine like that, I hope this goes places.Thanks for the link grauenwolfWhy not use CVS then? It's been out for over 20 years...&lt;/sarcasm&gt;Yes. Every minute spent in OS X, kills 3 GNU/Linux brain cells.Git 1.5 has been released recently, with much improvement in the user-friendliness area.&gt; There's a lot of problems. Why do open source advocates miss the big flaws and talk about the tiny ones?

All of the flaws you mention apply to software development ***teams***, generally -- both proprietary *and* open source:

* Difficult to reach team consensus
* No clear leadership
* Stubborn yet intelligent developers
* Possibility for loss of momentum
* Non-standard or unnecessarily unique interfaces

There are many examples of strong teams on both sides that have solved or mitigated these issues, but you're kidding yourself if you think these issues are not widespread among in-house development teams and software shops.

To wit, most DailyWTFs that I have read have come from closed source shops.  I am not drawing any conclusions here about one method's superiority -- just pointing out that these problems are not unique to open source.&gt; You think that the particular code points are a defining quality of a character set, rather than a detail, right?

If you read the article properly, you'd know what I consider defining properties of a character set:

1. which code points it includes, and
2. in what order.

&gt; did you just want to call me a name because I didn't like your article?

No, that's not it. I obviously don't know you, but I have read some of your responses to other threads: http://programming.reddit.com/user/Bogtha/. From there, I got a distinct feeling, that you really like giving out negative opinions. Hence, a troll.

&gt; Why are you accusing me of deliberately antagonising people?

Need I say more?

&gt; neither "Central European" nor "Cyrillic" are languages

True. They are character sets, used by two *groups* of languages. Nowhere did I say there's a 1 to 1 mapping between languages and character sets.

&gt; just because these two happen to be in a different order, it doesn't mean that this is a defining property of a character set or that it's intrinsically linked to language.

Then what *does* it mean, according to you?SICP is an (extremely good) introductory text. All of the texts that I recognized from this list are at a more advanced level.Just saying. Publish something that somehow involves Ruby and all the hipsters will come out from their hiding places...

And "polar opposites" isn't quite right, too. You've got an extendable software kernel and get lots of stuff that uses it. That's true for any operating system, for Emacs, for quite some computer games and one might even extend the metaphor to computer languages themselves ('though I guess that the whole body of code for some functional languages are smaller than the compiler themselves...).

The editor core of Emacs is pretty small. The rest is the lisp interpreter and the scripts who make up most of the actual functionality. It only seems elephantine because the modules can't be separated that easily -- by design, probably. 

If you've got software that has more supporting code around it than the code of the actual thing, you've done nothing wrong...Any idea how we can sort the links based on submit date? The $10 wok is still at the top... I can't figure out the logic of sorting but it seems to be that, the more numbers or special characters in the title, the higher up it is displayed. 

I want it just like reddit :)Sure. How 'bout [this](http://www.mozilla.com/en-US/) one?

&lt;grin&gt;The most interesting online math book I've found is http://www.cis.upenn.edu/~wilf/AeqB.html
It's about hypergeometric identities (and I didn't know what a hypergeometric function was when I started the book - it's very accessible).[removed]So what happens when you have more than one variable? Nested LOOPs.

EDIT: What I meant is that when you use more than one variable in LOOP, it does not behave like a list comprehension.I didn't pretend anything, I just said it sounded like a made up story.  I didn't say it *was* a made up story.&gt;This keeps its userbase small and elitist. No novices asking stupid questions.

Priceless.[Generatingfunctionology](http://www.math.upenn.edu/~wilf/DownldGF.html) is also terrific.&gt;I am a happy user of Subversion.....So - should I take a look at the alternatives ? 

No.  If you are happy with it, why would you look elsewhere for alternatives?  I can think of a ton of different programs I hate that would take higher priority to looking for alternatives.  I guess if you have absolutely nothing better to do with your time and have no systems that you have issues with, it doesn't hurt.I prefer Ubuntu myself. But as I read your comment, I cringed and thought: I hope that when I tell someone why I like FC less, I don't sound like that much of a pathetic, uninformed, flame-baiting nincompoop whose idea of a coherent argument is throwing words like "schmucko" and "whipping ass" around.

So, if any RH/FC contributor is reading this: thanks for working hard on a rock-solid distribution that did more than any other to make Linux a reality in large corporate environments. Thanks for contributing so much to the kernel, gcc, and many other parts of the system. Thanks for introducing the RPM repositories, back when the usual way to install anything was to hunt around for a release tarball, download, configure, utter a silent prayer, make, make install.

Thanks for all the code.
&gt; I have read some of your responses to other threads

Not good enough.  You accused me of trolling specifically in my original comment to this story.  What is trollish about that comment?

&gt; From there, I got a distinct feeling, that you really like giving out negative opinions. Hence, a troll.

There's nothing wrong with negative opinions.  Like Sturgeon said, 90% of everything is crud.  Consequently, 90% of opinions *should* be negative.

Having a negative opinion of something is not trolling.  [Trolling](http://en.wikipedia.org/wiki/Troll_%28Internet%29) is deliberately antagonising people to disrupt a discussion forum.

&gt; Nowhere did I say there's a 1 to 1 mapping between languages and character sets.

That's what "language-specific" means.  Specific to a language.

And I think it's quite dishonest of you to go back and modify what your article says on this matter without noting it like you did for your other update.  [Here's Google's cache of your article](http://66.102.9.104/search?q=cache:7S_-g3Tj7tUJ:developersoven.blogspot.com/2007/02/saving-unicode.html).  I quote:

&gt; Compared to Unicode, a character set is a very limited set of characters which appear in a specific order, **as used by a specific language** or script.

Here's the article as it appears right now:

&gt; Compared to Unicode, a character set is a very limited set of characters which appear in a specific order, **as used by a specific group of languages** or scripts.

Can you see how dishonest that looks?

&gt; &gt; just because these two happen to be in a different order, it doesn't mean that this is a defining property of a character set or that it's intrinsically linked to language.

&gt; Then what does it mean, according to you?

Why does it have to mean *anything*?  So the code points don't match up.  So what?

Let me clarify: it's of course quite probable that a given character set has an order chosen based at least partly on an alphabet.  But that doesn't mean that a character set *must* be ordered in such a fashion in order to qualify as a character set, and it doesn't mean that a set of characters that is not ordered according to an alphabet isn't a character set.
Well, there is the holy trinity:

* α-conversion
* β-reduction
* η-conversionThis is $%&amp;* insane! This sounds like a movie but it's really happening in real life?Except it's not an outrageous statement. It's only viewed as one, because it says something not-so-nice about women, which is a social taboo on par with criticizing Israel, cute puppies and kittens, motherhood, etc.

Evidence? How about the fact that there *are* so few women in the field? The simplest explanation for this is that they're not very good at it and/or don't enjoy it very much. 

How about my personal experience as an engineer? Especially in school, we had a fair amount of women in the program. Now, in any tough engineering program, you know who around you is good and who isn't, who has the chops and who doesn't. And since this was a tough major at a tough school of engineering at a well known university, a lot of the men struggled, too. Only a relatively small percentage of us were top performers. 
But the women just stunk. Almost all of them were being carried by their lab partners and classmates; they seemed to have more trouble understanding, and by and large, they just weren't willing to work as hard (this was the sort of program where you spent all weekend in the lab, going home only to sleep). 
In all my years of school, and of working as an engineer, I've only met one woman whose capabilities as a a scientist and engineer I respected. The rest were just kind of so-so, at best. There are women who are gifted at this sort of thing, but they are several orders of magnitude rarer than men of equal ability. 

I am afraid you just have to accept that there are inherent biological differences between the genders, even if saying so isn't politically orthodox, and doesn't give us warm fuzzy feelings. The Emperor really *is* naked.

Would it be so "outrageous" if I suggested that men are stronger than women? Or that women have better colour vision than men? Neither of these facts are very fair, or nice. But they're true. Nature has never felt compelled to be fair or nice. Don't let what you *want* cloud your perception of what *is*. 

If we're afraid to face the truth because we don't like it, then what are we? (Aside from the obvious answer, "Dick Cheney".)

&gt; So what happens when you have more than one variable? Nested LOOPs.

Or maybe you could just use LOOP's built-in support for multiple variables.
[deleted]&gt;There is a difference between "successful," and "by and large not very good at it." 

Not as much as you might think, given that we are talking about engineering, a field where results tend to speak for themselves. 

&gt;There may be many reasons for the lack of women who are successful, but not being "very good at it," is not necessarily one of them.

Results are the way we measure skill. If there aren't many results, lack of skill isn't the *only* explanation, but it's certainly on the table, despite everyone's politically orthodox attempts to shove it off or hide it under a napkin.First, this:

&gt; Applications aren’t made up of pages, they’re made up of states. Inherently, states aren’t made for the bookmarking model of the browser.

Then, this:

&gt; There is no back button in Microsoft Word.

That which we call a Save or an Undo by any other name would work as well... (sorry, Juliet!)Yes KDE is a resource hog, but all the computers I use are more than sufficient to handle it.  Bloat is always something that developers ought to address, because smaller system requirements mean that their software is useful to a larger number of people.  But from the user's perspective, why should I care that the software I use is bloated when the alternative is simply for my RAM and CPU to sit idle?[removed][removed]&gt; (Edit: or get some seamless real-time conversion between the main players, so people can use their DVCS of choice and still contribute to projects easily)

http://www.darcs.net/DarcsWiki/Tailor

I think Tailor is probably the closest you're likely to get. Converts to/from Darcs, Subversion, Monotone, CVS, Bazaar, Mercurial and Git (other systems supported just as source or just as destination).

I liked the Tailor dev's note re: Darcs - "The more systems I add support for, the more I love Darcs" :)This made me laugh:

&gt;(Each element of the list (1 2 3 4 5 6 7 8)**)** squared.
&gt; [...] whether it's due to societal pressure or inherent genetic differences [...]

This is really mostly my point.  There is likely some "nature" involved, but I firmly believe that capable women are not going into CS and other technical fields *because* they are so male dominated.  A stigma is often attached to such fields and young girls learn that "only boys want to do that", and they pursue other things.

Other fields (such as medicine) have seen massive increases in female participation in the last decades.  Today, the ratio of men to women in many technical fields is drifting toward being the same as the ratio in the general population.  I think CS is lagging, and too few capable women are going into it because of the stigma.  Hopefully another prominent female in the field will help.Thanks for the info.The reason that a lot of RIA stuff ultimately fails is not because it doesn't implement web features such as bookmarks and the Back button.  Rather it fails because the web, as a platform, out competes just about everything else.

The web is an open, organic, flexible, adaptive, ever-improving, ubiquitous, distributed medium that just so happens to solve a lot of the same problems that Java applets, Flash applets, and ActiveX applets can solve.  And if a problem *can* be solved by both standard Web stuff and RIA stuff, I'll put my money on the Web version every time.

The only place where RIA stuff has an edge is for solving problems that you _cannot_ solve using the standard Web stuff.  And, as the Web evolves and improves, there will be fewer and fewer problems outside the reach of the standard Web stuff. 

In sum, if you're selling RIA tools, you've got a tough road ahead.
&gt; (FreeBSD is mostly developed with perforce, and only stable things are brought into CVS - this works well for them, but in part because most developers have a perforce license)

Perforce apparently offers no-cost licenses for genuine open-source projects. As far as I understand, that's what FreeBSD uses (I know one FreeBSD developer and I've heard him babbling about Perforce :-)).

http://www.perforce.com/perforce/opensource-faq.htmlPosting examples in Python won't change my mind. I know from my ASP experience that you *can* write even obscenely large code bases without declaring variables. But I also know that turning on Option Explicit can drastical reduce the bug count from even modest (&lt;200 files) sites.

And the bugs are often so subtle that the program works 'correctly' even though you are unintenionally resuing globals all over the place.
Yep. Like I said, I'm just waiting for the pain level to be high enough to justify the change. We just added two new devs, so maybe we have enough ammo now. I certainly would like to go to a system that doesn't require exclusive checkouts.In my opinion IE got the quotes right and the w3c standard is wrong. I wrote [a more elaborate rant in my blog](http://computerroriginaliascience.blogspot.com/2007/02/q-incident.html).

So fixing IE to support the standards is actually a workaround for a faulty standard.&gt; This is an obvious dupe, of course.

[This is only bad if the submitter is malicious.][1] Given that a significant majority of reddit have expressed concerns about a syndrome, an analogy to Usenet isn't all that bad. By itself, it's still an interesting piece of history. We should give the submitter the benefit of doubt.

&gt; On a related note, is there a statute of limitations on dupes?

Yes, self-judgment. Again, reddiquette.

&gt; Feel free to post something again if you feel that the earlier posting didn't get the attention it deserved and you think you can do better.

[1]: http://reddit.com/help/reddiquetteThe average user doesn't care (and shouldn't even have to
care) if the software used if bloated.
However, I'm not the average user and **I** care.
I want all software on my system to be as small and
efficient as possible, even if I have plenty of resoucres.
It's more elegant to run efficient software, even if this
means that the RAM and CPU sit idle.
That's the difference between me (and other geeks)
and the average user.REST is just 90% buzz and hype, 10% common sense to control mess with data from afar over HTTP. (See Flickr's API for a good example.) This ties into malcontent's suggestion for an authentication web service.&gt; What is trollish about that comment?

See the end of my reply.

&gt; And I think it's quite dishonest of you to go back and modify what your article says

You're certainly entitled to that opinion. From where I stand, it's a minor modification of the statement, which doesn't change its point in any way: even in the original form, it doesn't say anything about language-to-character-set mapping, which seems to be what you're on about.

You may view my policy as dishonest, but I promise you, had we been conversing in blog comments, I'd certainly have marked the change, even if it were only a misspelling. That's the reason I marked the first one - because of a commenter. You're free to believe otherwise, of course. I don't care about your views on honesty.

I do care about the following, though:

&gt; Why does it have to mean anything?

Because otherwise, you're just twisting[*] my words, without providing a good counter-argument. To me, that's trolling.

[*] - You're the one who used the word "must" in the first place (in your very first comment and again now, in your last reply). You're practically basing your bashing on something that I didn't even say! I can take a constructive criticism any day, but I expect from the one giving it not to twist my words, and then preach to me about dishonesty.

Interesting as it is, I feel this is going nowhere. If you genuinely feel I've said anything misleading in the article, you're more than welcome to provide me with what you believe to be a correct version. I'll consider revising the article, and even giving you credit for it! Otherwise, I have nothing more to say to you.&lt;sentence&gt;you naysayers should just wait until we get the semantic web, where markup strips out the *meaning* of the text, not the layout&lt;/sentence&gt;

&lt;parenthetical&gt;&lt;sentence&gt;that wasn't a typo; the fullstop gets inserted automatically&lt;/sentence&gt;&lt;/parenthetical&gt;What kind of hex is the submitter trying to put on us?[removed]Wow, there's a  [penguin](http://lug.oregonstate.edu/projects/kernelmap/map.php?x=19200&amp;y=18400&amp;zoom=64) in the kernel?!list comprehensions are an alternative syntax for map/filter, that can be more concise for some uses, and simpler to reason about.

They can also subsume zip (with parallel comprehensions), and monadic operations (mapM), if you allow for monad comprehensions in general. (A list is just a particular monad).Uh, no. Here's what [an expert](http://www.freedom-to-tinker.com/?p=1121) has to say about the news:

&gt; [The] usefulness will probably diminish as AACS implementers adapt. [...] This power will not last long. For future discs, [this approach will work] only on a single disc or a few discs at most.

To summarize the in-depth analysis that the Princeton University team led by crypto expert Ed Felten has published on that site:

* AACS will continue to be an arms race
* Ultimately, the fair-user hacker heroes will win and playing AACS media will be as easy as cracking the CSS used to protect DVDs
* We're not there yet&gt; Another feature i like about darcs is that you can commit parts of file changes.

I use that feature all the time.  Tonight, for example, I made a batch change across a bunch of files.  When I went to record the change, darcs detected that I had also made some other changes to certain of the batch-changed files.  Those other changes were minor edits, and I didn't want to intertwine them with the batch changes, so I told darcs to leave them out of the recording.  Later, I recorded the minor edits separately.

Now I have two independent patches, one for each logical change:

    $ darcs changes | head -5

    Thu Feb 22 17:26:07 EST 2007  Tom Moertel &lt;tom@moertel.com&gt;
      * Minor edits for better reading and clarity

    Thu Feb 22 17:24:32 EST 2007  Tom Moertel &lt;tom@moertel.com&gt;
      * Switched a lot of old code blocks over to syntax-highlighted blocks

If I want to roll-back the batch change (the second one) I can do so without losing the minor edits &amp;#8211; and vice versa.

Being able to say *exactly* what goes into a patch is one feature I wouldn't want to code without.

Cheers!  &amp;#8212;TomWhat kind of pestering do you get?  I've been using wmii for a few months now, and haven't run into any huge snags (except when gentoo installed wmii-2 instead of 3 and none of the documentation worked, but that's gentoo for you).to me it's quite obvious that your expectations have a "sign error"...
logical thinking=&gt;high average(low risk)
intuitive thinking=&gt;high maximum,low minimum(high risk)
Yeah, I shoulda taken the time to dig back for that one.
I'm familiar with where the burden of proof lies. 

The question, however, whenever one discusses the burden of proof, is just who is making the assertion here? 

That's a more complicated question than one might assume. You see, if I say "The sun is hot.", and "There is a diamond the size of a pickup truck buried in my front yard.", technically, I am making two assertions, and the 'burden of proof' rests with me. However, someone who says "No, the sun isn't hot at all.", and "There is no diamond of any sort buried in your yard.", will probably be asked for evidence of the first assertion, despite the fact that it is a negative, and not the second. 

That is because the burden of proof *really* rests with someone who makes a statement that disagrees with anecdotal evidence or casual observation. The reason that the statement "The sun is hot." doesn't carry a burden of proof is that the sun certainly feels like it's radiating heat. 

So perhaps a better way to articulate this standard would be:

"Extraordinary claims require extraordinary evidence."

I don't think anyone would have a problem with that statement. It's one of the basic principles of skepticism, and of science. But you will notice that the standard is not:

"Unpopular claims require extraordinary evidence." 

That's because the truth is not determined by vote, or by what people believe. It's discovered via evidence. 

The claim I made was not at all extraordinary. It was merely unpopular. All the evidence I need is the fact that so few of the great achievers in math and CS are women, and that so few female students do well in math and CS. 

If someone else wishes to assert that this piece of evidence is caused by something else, then their claim may be more *popular* than mine, but it also more *extraordinary*, and hence requires more evidence.Well stated and informative!  Voted +1!My favourite query (not in the article):

    mysql_query.*\$_GET

Nice and creamy wad of SQL-injection for PHP-"developers".[removed]How about [ODE](http://www.ode.org) - does that have fewer features? Or maybe ODE is only rigid body...You sunk my battleship!That's the stupidest combination I've ever heard in my life!I'm surprised such a blog post could go without mentioning [cyc](http://www.cyc.com). An AI based on the information available via web was precisely cyc's goal. They've been at it forever, but it's still only decent in special cases or with domain restriction.

Google has more resources, but my guess is that they'll stick to practical AI for the most part rather than try to capture some notion of a sentient machine. Google has shown (at least openly) more of an affinity for statistical approaches over attempts complex semantic analysis, for instance. We're probably still a long ways off from a general implementation of something that would have the understanding of being able connect, say, the temperature of the freezing point of water in degrees Celsius and the freezing point in degrees Fahrenheit.Warning: main(/home/tillmanj/public_html/jontillman/wp-content/advanced-cache.php) 
    [function.main]: failed to open stream: Permission denied in /home/tillmanj/public_html/jontillman/wp-settings.php on line 84

    Warning: main(/home/tillmanj/public_html/jontillman/wp-content/advanced-cache.php) 
    [function.main]: failed to open stream: Permission denied in /home/tillmanj/public_html/jontillman/wp-settings.php on line 84

    Fatal error: main() [function.require]: Failed opening required 
    '/home/tillmanj/public_html/jontillman/wp-content/advanced-cache.php' 
    (include_path='.:/usr/lib/php:/usr/local/lib/php') in /home/tillmanj/public_html/jontillman/wp-settings.php on line 84By never using page-level includes, and staying away from languages which encourage them ;)

Seriously, though, organization and discipline do far, far more than any boilerplate declarations.[deleted]your luggage has hex on the lock?  I think I'd buy those locks...Yes, I've gotten pigeonholed as a short term contractor and that tends to have a snowball effect: companies looking for long-term or permanent won't consider me, leaving me only short-term opportunities to choose from.Okay, I'm going on break. Someone else type in the numbers in 108 minutes so we can keep watching the movie.Other implementations:
http://www.tfeb.org/lisp/toys.html#LIST-COMPREHENSIONS
http://www.cliki.net/COLLECTING[deleted]He also left off:
Internetworking with TCP/IP, Volume I:
Principles, Protocols and Architecture
Douglas E. Comer

The C Programming Language&gt; Another Haskell blog that I totally don't understand

Why comment then? It's already clear from [your previous comments](http://programming.reddit.com/user/radioactive/) that you don't use or understand Haskell, though you sure seem to like Visual Basic.

How are you contributing to the discussion?If you want to use a high performance and pretty gravity simulator try Gravit

http://gravit.slowchop.com/A long time CVS user, and then after several years on subversion, I've switched to bazaar. I had been looking around to try a new VCS for a while, and slowly started focusing more and on bazaar. Circumstances arose where I had to move my business SVN repository quickly from where it was hosted, and i decided that at that moment I'd take the plunge since i was making changes anyhow, even though i really hadn't tested bazaar enough yet... 

Overall? It's been great. Really great. 

Without realising it ahead of time (and this is a feature I never hear anyone mention), I found that i could set up bzr to work functionally the same as a centralised VCS (ie. just like subversion). This made transition a lot easier on the brain. You can use it exactly like a centralised VCS with the old familiar checkout, update, etc (over ssh). Yet at the same time, on any whim, have all of the power of decentralization available: you can branch locally off of your remote checkout. You can detach your "bound" remote checkout and work locally, and then rebind to the central repository later. Yep, it's been great. (Note: you can do "light-weight" checkouts from a repo that do not include full version history, just like a SVN/CVS checkout, or "heavy-weight" checkout which behaves mostly like a SVN/CVS checkout, but includes full version history so it can be unbound from the repo, or branched from locally.)

It's not perfect though. As some have noted, it is pretty slow when doing checkins and updates to a remote server. The development team seems very active though, and are working on these problems. I fully believe they will overcome the speed problem in the not too distant future (check the mailing list for evidence). Another annoyance to me is that the "update" command doesn't show me the files it updates as it updates them. Ah well.

On the plus side, I've peaked at the code a bit... it's really convenient that it's python. It seems very well organized, and not at all scary. Looks pretty easy to jump into the extensive API if I ever feel the need to. I like this flexibility. And paid development is being sponsored by Canonical, which makes the thing look more legitimate to manager-types (which gives me at least a little help justifying the switch to my boss). 

All this being said, SVN conversion didn't go smoothly. I tried many tools. The problem was with my subversion repository: we'd done some pretty heavy re-organization using svn "mv" commands in the past. This confused the heck out of Tailor. The best tool i found was [svndumpfilter3 ](http://furius.ca/pubcode/pub/conf/common/bin/svndumpfilter3.html), combined with Tailor... but even that couldn't do all I needed to do, and i finally gave up on parts of the SVN repository... started fresh bzr repos, archiving the SVN until the future when maybe the native svn dump will be smarter... 

Anyhow, I'm really loving bazaar now... continuing to use it 75% centralized workflow, but slowly getting more comfortable with the decentralised methods... pealing a new branch off my *working* copy is sooooo easy.... then push back to it, and commit to the repo... such freedom.



[SCWM](http://scwm.sf.net) works again now too.

It has a few rough edges, but I should be done getting *everything* to work in a few weeks (delays due to my family falling ill).have to say, gravit kicks arse over thisWell done, the next lesson after cut and paste is arranging icons on your desktop.It isn't that good programmers never make bugs, or even that they never make typos like this.  There are two things that make this seem like it isn't an issue:

1. It doesn't happen often (I think this was Therac25's point).  I mean, you must have hundreds of variables for each time you misspell one long enough to get it run (not just misspelling and backspacing over it; I do that several times per LOC).  Given that it's so uncommon an error, forcing the programmer to add an extra thing to (mis)type for each variable declaration just seems wasteful.

2.  Everyone seems to be doing lots of testing these days, and even those that don't write tests first will be testing each function or class as they write it, possibly for several iterations of improvement before moving on.  Any such bug would likely be caught very quickly in that cycle before there was any need to search for the first time it occurs.

3.  If you're using nicely descriptive names like this, grepping for "myVariableName =" works nicely, as there won't **be** any for the typo.

All that said, it does seem, as kawa points out in a comment here, that conflating initial assignment and mutation is a bad idea.  I just don't want to have to type a lot more for it. :)&gt; For those who don't do development on Windows XP, everyone runs as admin since ordinary users don't have enough rights to make any changes in their environment. It can be (painfully) made to work for simple Word users, but for programmers it's impossible.

Why is it that game developers (notoriously unprincipled software developers) can come up with a maxim such as "all the developers shall have the 'Minimum Requirements'-specified PCs so they can guarantee the game works smoothly on it," and yet things like this still exist? It's possible to install Java as an unprivileged user, right?After months of looking at arch/tla/baz/bzr, and skimming some of the other dVCS websites, I finally looked at the Mercurial website.  After just skimming the quickstart, I tried using it, and lo and behold, everything just...worked.  Simply.  Easily.  Just about every command was instinctual, where I wanted to do something, typed "hg _whatever_", and it worked without having to lookup the command.  For some of the more complicated things I tried, I did look in the manpage or on the website, and it was still simple.

I used to use CVS sparingly, as trying to do checkins while sitting on a Metro train, on a plane, or deliberately off the network on vacation wasn't going to happen, and mirroring painful.  Now, I commit almost everything regularly, and push/pull to other machines whenever I happen to have a network connection.  Instant multiple mirrors of all my work, both home and work stuff...(Published December 30th, 2004)[removed]I couldn't quit fast enough.

I can't imagine why the author stuck around for 3 months or more.You need a WEP key to get into your luggage?

PS: WEP - Worst Ever ProtocolUh, huh. And how do you enforce this discipline on several dozen over-worked coders on a code-base that has to be actively worked on for at least a decade? 

It seems to me your are going out of your way to avoid a trivial amount of work.Microsoft has posted an open letter discussing the conflict around ODF and Open XML.  Does IBM fear Open XMl for the wrong reasons?I can't say that I'm a fan of this limitation, but it's not as bad as you make it out to be. You can restart your window manager without restarting X, and, since dwm does the work of positioning your windows for you, it's really not much of an ordeal to restart it.Mostly just the standard "RTFM" type stuff. People don't read the manual, or they don't understand it, and they ask many questions whose answers are covered in detail in the docs.

There's also a good amount of trolling and feature requests which clash with the wmii philosophy.We seem to have broken it. :PI don't think so. At least I can't think of any.Raytracing seems very much more intuitive than rasterization. I hope a raytracing video card comes out soon.It now says: “I Broke it Back soon”.[Google cache of this article](http://www.google.com/search?q=cache:sjFAsOXGfCoJ:jontillman.com/2007/02/13/09-f9-11-02-9d-74-e3-5b-d8-41-56-c5-63-56-88-c0/&amp;hl=en&amp;ct=clnk&amp;cd=1&amp;gl=us&amp;lr=lang_en).Google cached [link](http://64.233.167.104/search?q=cache:sjFAsOXGfCoJ:jontillman.com/2007/02/13/09-f9-11-02-9d-74-e3-5b-d8-41-56-c5-63-56-88-c0/+09+F9+11+02+9D+74+E3+5B+D8+41+56+C5+63+56+88+C0&amp;hl=en&amp;ct=clnk&amp;cd=1&amp;gl=us&amp;client=firefox-a)Can someone explain what was that about? Link is dead :(Except that, unlike the rest of the English language's punctuation, quotation marks are actually a semantic wrapper around a section of text, the exact kind of thing markup excels at. &lt;q&gt; is just &lt;blockquote&gt; without the block.

Also, it's less work to just use &lt;q&gt; than to write what a given HTML author wants it parsed into (smart quotes, that alternate between single- and double-quote marks), either as HTML entities, or as unicode characters (which the author must then ensure pass correctly-encoded to the viewer.)It's hex for 'week old news'.Before he was at Google.Depends on how many windows you have open.

I find vi vs emacs users have very different kinds of editor and window manager usage. If I'm in emacs I never exit it, and I never exit my wm either. The workspaces and windows tend to pile up, and some of them will have something stateful I care about.

Luckily I'm a vim user myself. I seldom seem to need more than 2 workspaces, 2 windows per workspace, 2 windows per vim session..

A session save/restore mechanism would be another way to fix this. But I'm sure it's been hashed out in the dwm mailing list ;) I don't mean to make it out to be too bad.&gt;What kind of hex is the submitter trying to put on us?

I put orange up arrow beside your name.The link i posted works for me -- take another look.4 8 15 16 23 42
You're right, evil was too strong of a word.That is awesome.1 1 2 3 5 8 13 21PAIP, On Lisp, and http://research.microsoft.com/~simonpj/Papers/pj-lester-book/Now if it can just write and direct better than Lucas, we'll be all set for Episodes VII-IX!yes, unfortunately ODE is rigid body only.What game developers are those? Not exceptionally successful ones, I'd be willing to bet.

* "Minimum requirements" are generally not decided upon until the game is nearing completion.
* Debug-build software can be an order of magnitude slower than Release-build software, so it's not even an effective test.

I could see such a machine being made available TO the developers, but forcing them all to develop on minimum requirements seems unlikely in any company that wants to be competitive.It crashed my machine. :(430 forbidden.  maybe you could have spent 5 minutes saving it and uploading it to a damn website instead of being snide.

sorry, i'm having a rough lonely night :-/[removed]I'm a geek and I consider elegance to be matching my hardware to my needs. If my new computer allows me to run something that's a hog, but that software provides something desirable in return, I am certainly going to use it. Otherwise I wouldn't have bothered getting said new computer in the first place.I don't know what to tell you -- it's either your browser or ISP. You can always copy the URL in my comment into Google and read the cache.&gt; It's more difficult than I thought to built a Lisp CPU. Perhaps the Verilog language is not so good, because some nice standard language featuers (forever-loop etc.) are missing in the Xilinx-Tools. But it is possible, the code looks only a bit more complicated.

Verilog is actually two languages in some sense. It's both a full programming language and a hardware description language, but the subset of Verilog that can actually be synthesized into hardware and should be used when using Verilog as an HDL is *very* restricted.

That's also nowhere near a real processor yet. The simplest of pipelined processors (a classic 5 stage pipeline) is easily over 1000 lines, and it only gets more complex when you add on stuff like caching.COBOL?  I think you mean C!  Nobody calls it by its full name!  Hahahaha!I UPMOD YOU!!!A conversion function which does the multiplication - or just a straight multiplication.It's likely that your browser cache is allowing you to see it.  I've tried with two systems running different OSes and multiple browsers on two separate networks and get a 403 in all cases.

Here's the Google cache for those interested:

http://72.14.203.104/search?q=cache:sjFAsOXGfCoJ:jontillman.com/2007/02/13/09-f9-11-02-9d-74-e3-5b-d8-41-56-c5-63-56-88-c0/+http://jontillman.com/2007/02/13/09-f9-11-02-9d-74-e3-5b-d8-41-56-c5-63-56-88-c0/&amp;hl=en&amp;ct=clnk&amp;cd=1&amp;gl=us
Dear lord man, to say good-day just when your opponent gets to their point:

&gt; But that doesn't mean that a character set must be ordered in such a fashion in order to qualify as a character set, and it doesn't mean that a set of characters that is not ordered according to an alphabet isn't a character set.

Is the *epitome* of trolling. Anyway, I'd really love to hear the rebuttal of this specific part: Unicode is a character set, that is, a set of characters. It has an ordering; that ordering need not be specified by a specific alphabet, but rather is completely up to the character set designer. For example, it would be completely valid to have a character set that was ordered by "median example-glyph surface area"--and likely just as useful, once all the subset re-encodings take place.The hard drive space or the memory isn't even the worst part. The worst has always been that kde and all of it's hundreds of  supporting apps all form one giant dependency web.

Want to install just parts of KDE? Not going to happen. I know this has been slowly changing but it's still terrible.Programming for the sake of programming. Who cares if it's been done before. This is a true coding badass.I made you a website, but I broked it.Oh, you guys are talking about the jontillman.com link!

Since this entire thread is in reply to a post i made with a link to freedom-to-tinker.com, i assumed (rightfully so) that you were saying my freedom-to-tinker.com link was no good.I do this all the time and I've been doing it since 1999.WHY DO I GET THIS RECOMMENDATIONS FOR GEEKS?WHY DO I GET THIS RECOMMENDATIONS FOR GEEKS?[deleted]Didn't we do a round of Yegge's old posts just six months ago?
a lot of... chutzpah?
Yes yes, but try installing some kde app on a new empty system and it will pull in far more deps than a non kde/gnome app.

Maybe this only matters to those who compile everything..[removed]Dependencies? I use Slackware.I use SVK for that.  It's pretty easy to make the switch, once you get your head around how SVK works.You think putting up your body as an exhibit gives the same sort of status as being a lawyer or a doctor? No, models are enjoyed, but held in contempt, and seen as trash.
 
A model was invited to speak about modelling at a school I attended. She started off lightly, then after about two sentences she broke down crying, saying that she wished she never had made a career out of it, and telling us that after a while you feel like the brainless slut that society (not least of all the photographers, agencies, other models etc) assume you to be. And that was a "decent" fashion model, you'd have to wonder how the borderline porn stars feel. 

Probably they keep the mask a little better, but how do they feel?

The bits and pieces of the late Anna Nicole's life that has been exposed in the media after her death has convinced me more than ever that living off your looks is one of the most miserable careers in existence.Wow that look much better than every other crappy shooter out there.  I'm glad so much money is being spent in the right place.[removed]Uriel complained to my first point in the #dwm channel tonight. So, to be fair there are two configurations in vanilla dwm - one minimalist which has no dependencies and my own config, which depends on dmenu and lsx (both tools available from the same site and also written by me and others) - in the future it will also depend on st. So it's up to the user to decide if he wants to use a minimal config which works out of the box or a slightly extended config which depends on some convenience tools.Clearly, using Java is the *real* problem here ;)Take a look in the mirror buddy.You could always monitor is with [l8tr](http://l8tr.org).

John.
&gt; something desirable in return

What is so desirable about KDE (or GNOME)?
This is not a joke, I really want to know.
Maybe I'm too old-fashioned or too used to fvwm,
I just don't see the advantages.
He writes Perl, but you could substitute any one of  Python/Tcl/Ruby/PHP.

Java has a little bit of the turtle syndrome, but it's not too bad, and what with the money they had and have, they can get away with it more than your average language team.CONDENSED VERSION: Some other person niftily observed that Perl 'is extroverted' whereas Lisp and Smalltalk 'are introverted'.  I like Groovy.  Some drifting-out-of-orbit-into-deep-space rambling about how my workplace doesn't use CPAN.&gt; Finally, the preferred choices for a DVCS seem to be:

This isn't a summary: this a listing of NotSvk.&gt; and what's the advantage when the commit time is longer than a fraction of a second?

I don't know how bazaar commands actually go, but work with me:

    bzr commit &amp;  # OMFG LESS THAN A FRACTION OF SECOND I'M GETTING BACK TO CODING NOW"Nothing was hacked, cracked or even reverse engineered btw: I only had to watch the “show” in my own memory. No debugger was used, no binaries changed."Wow. Would you like content with your advertisements?&gt; Not to mention an unnecessary dependency on Subversion.

Seems reasonable enough to me: svk focuses on high-level repo management that subversion either ignores or gets wrong, and gets -- for a good value of 'free' -- a solidly maintained versioned filesystem and associated underlayer.  Darcs can't do this because it has its own special idea of versioning; Git and co-spawn can't do this because they've their own very different idea of repo representation.  For any other VCS, I'd lean towards NIH as the reason for their failure to have this unnecessary dependency.

Morever, svk allows hip svk-use-y people to simply use it and still have friendly interactions with people who are comfortable with svn or simply want to try out the bleeding-edge of a project without installing e.g. GHC.  Whereas I've had to politely ask people for snapshot tarballs, who use darcs.  Although FWIW, the pugs project has a simultaneous svk/darcs repo.

Penultimately, I like Perl and like that I can hack svk without difficulty if I want to contribute or work around a problem (or help someone else work around a problem -- which I've done twice, idling on #svk).  Python likes to pretend that its inexpressiveness makes its programs magically always more readable than those in Perl, but I've only happened to observe that the opposite is the case.  Anyway, if you like Python, you'll probably get the same joyous feeling of 'wow, it is all Python down here!' when your not-depending-on-subversion VCS breaks at the version-controlled-filesystem level.[Thank you Arnezami](http://en.wikipedia.org/wiki/Arnezami#Processing_key_retrieval_and_Arnezami_contribution):

http://forum.doom9.org/showthread.php?s=d9da68c11a88904641ba24b96ff95a12&amp;p=953036#post953036

&gt; "As I can understand some of you are interested in how I retrieved the Media and Processing Keys. I will tell what i did."I think most developers there were holding onto the hope that their managers were being listened or would be listened to eventually and that the head of network operations would not be seen as a god.

That head of network operations should have been taken out at night and hung, drawn and quartered so he can get an idea of the cumulative pain he has inflicted on the programming division and the long term profitability of the company as a whole. Same goes for the rest of the board who deified him.&gt; Clearly, using Java is the real problem here ;)

Clearly, using *Windows* is the real problem here.

Write in C/C++ and OpenGL on OS X or Linux, write portable code, port to Windows after the product is complete.&gt; In other words can I use subversion as a back end to a better tool?

You can most obviously use svk and get this benefit, but other VCSen have facilities to interwork with subversion.  Someone mentions a 'Tailor' of darcs, for instance.It would have been nice if he described succinctly what "playing nice" means.  FFI?  getenv, system and co?

Yes guys.  Nothing to see here, keep moving.Raytracing is computationally **very** expensive. You won't be seeing high quality, realtime ray tracing any time soon.

However, it is quite possible to increase the speed of ray tracing using the GPU if you have one of the newer video cards with a programmable GPUs (9700pro and above, more or less).[removed]Sounds like an infomercial. 

"You know how your blender is never powerful enough. The magic bullet is capable of blending not one, not two, but THREE vegetables at a time."

"That's great, John, because you can never blend enough vegetables."Which is great because the Death Star was almost entirely made out of.. err.. wood....  The first attempt we never got to see that is, which was all mock-tudor dark-wood beams, wattle &amp; daub and a thatched exhaust port leading to the main reactor. 

The emperor has really bad taste in architecture ;)Since I started checking Haskell, I also started checking `darcs`.

It's a very good DVCS but it has opne huge (to me) inconvenience:

**Updating to a past version of the project is a fucking pain**. While svn only needs `svn up -rPastRevision` darcs will require a combination of `darcs get` (to create a copy of your current repository), the setup of a new environment (web server) if you need configuration, and (potentially several) `darcs obliterate` (which is equivalent to `darcs unrecord` followed by `darcs revert` btw) to remove the patches you don't want anymore. Now you can do your checks. Oh, and you can't update your repository back, you've got to destroy it and `get` a new one. It's my biggest problem with darcs by far.He was obviously waiting for his notice of termination to compile.What do you use as your main reddit page? Currently it parses the reddit "hot" page ($10 wok is still at #5 currently on reddit, #3 at reddit-lite). So it's just like reddit except no links containing key words. It doesn't do any kind of sorting that I know of.

Oh..and I'm not affiliated with reddit in any way...just in case you think that :)

Firstly: You make an excellent point regarding the web out-competing challenges from other technologies.  

&gt; The web is an open, organic, flexible, adaptive, ever-improving, ubiquitous, distributed medium  

Yes, "The Web" is great... flexible, adaptive and ever-improving just as you say.  So adaptive, that one day it may adopt a new presentation technology such as those touted by RIA technologies.  "The Web" is great, but HTML/JavaScript may not be the best way of creating a presentation layer for a web application.  

How would you feel if Flash was open source and Firefox adopted it as an in-built layout engine for SWF sites?  From a browser perspective SWF is equivalent to HTML and Flash Player is equivalent to Gecko.  

You can solve most problems with existing web-based technologies, but you can also build Microsoft Office from assembly language.  Flexible, adaptive and ever-improving - that *doesn't* make me think that HTML/Javascript will be around forever.This is a [duplicate](http://www.frank-buss.de/lispcpu/) ([programming](http://programming.reddit.com/info/rm8c/comments)) ([main](http://reddit.com/info/c4sd/comments), with comments).You can try Mercurial + MQ (quilt like tool for Mercurial). Use it to work with a local pile of patches applied on top of another revision control working directory. Then you can work offline while preserving your change history, doctoring your not-yet-committed history (reduce load on code reviewer), and try whatever you want without impacting the official repository (at the cost of patch rebasing, which cannot be avoided whatever the branching scheme you use).

And when you are happy, commit your patches.Have very good write informatic-program, now just need learn Englishs, wa wa wee wa!This stuff if why I started graphics and demo programming in my teens - it's so very rewarding to code for an evening and end up with something cool and beatiful.Ray tracing has much better asymptotic scalability than rasterization and has the advantage of being embarrassingly parallelizable, always a good thing for hardware. Probably the biggest implementation issue for a hardware ray tracer is how to deal with the scene database. For flexibility you'd probably need to be able to upload microcode that takes care of traversal, so that you can use whatever acceleration structure (hierarchical uniform grid, kd-tree, whatever) is appropriate, rather than having it be set in stone by the hardware designer.[deleted]Or, develop Java under Linux or OS X. I'd say under OpenBSD, but I've only tried that once, and it was really quite painful... still, that was before Java went open source...
Now that AMD has opened HyperTransport, doing an FPGA CPU or coprocessor for a "real" PC should be possible. Link dump:

* [(proprietary) development package for HyperTransport coprocessors](http://edageek.com/2006/10/23/celoxica-fpga-hypertransport/)

* [someone else's gear](http://www.drccomputer.com/pages/modules.html)

* Articles in the [Inquirer](http://www.theinquirer.net/default.aspx?article=30539) and [Register](http://www.theregister.co.uk/2006/04/21/drc_fpga_module/) .

Taking wild uninformed guesses, other langugages worth targeting might include [Strongtalk Smalltalk](http://www.strongtalk.org/), [Coke](http://piumarta.com/pepsi/coke.html) or [E](http://www.erights.org/), while other interesting starting points for the hardware design might include the [Burroughs B5000](http://www.ajwm.net/amayer/papers/B5000.html) and the processors PARC wanted to have for the [Alto III or NoteTaker](http://portal.acm.org/citation.cfm?id=155364&amp;dl=ACM&amp;coll=portal). Of course just working on free tools for generating coprocesssors might be a big project in itself. I expect there's a very interesting project somewhere in all this for the right people.Would be nice to know what company it is... guesses anyone?Oh I see you're ironic heh? wink wink nudge nudge! You forgot on purpose to mention assembly language!&gt; I'd really love to hear the rebuttal of this specific part: Unicode is a character set, that is, a set of characters.

Bravo! Now this is something I can finally understand! If this is indeed the same case that bogtha was trying to make, then I can reply to both of you here...

I see a difference between an arbitrary "set of characters" and a "character set". To me, a character set is what has traditionally been called a "code page", or also "encoding" in standards like XML. A set of characters, on the other hand, can be *any* set of characters, used for a specific purpose, for example "0123456789abcdef". But that's not what I'd consider a character set!

By that definition, Unicode is neither a character set nor a set of characters. It's a set (or a list) of *code points*! The term was invented specifically to separate Unicode from the notion of characters, because "character" implies encoding, and that's not what code points are about. Unicode defines two special encoding schemes for that - UCS and UTF.

Maybe I should've made this more clear in the article...FPGA FTW!+/- same thing in C++ using MFC and one small class from Stingray Objective Studio:

    #include "stdafx.h"
    #include &lt;FileSys.h&gt; // Stingray
    using namespace std;

    CWinApp theApp;
    CString Heading(int Depth)
    {
      CString Result;
      if (Depth)
        for (int i=0; i&lt;Depth; i++) Result += (i == Depth-1) ? "|--" : "|  ";
      return Result;
    }

    void RecurseDir(const CString&amp; Dir, int Depth)
    {
      cout &lt;&lt; Heading(Depth) + SECFileSystem().GetBaseFileName(Dir) &lt;&lt; endl;
      CFileFind ff;
      BOOL bWorking = ff.FindFile(Dir+"\\*");
      while (bWorking) 
      {
        bWorking = ff.FindNextFile();
        if (!ff.IsDirectory() || ff.GetFileName() == ".." || ff.GetFileName() == ".") continue;
        RecurseDir(Dir + "\\" + ff.GetFileName(), Depth+1);
      }
    }

    int _tmain(int argc, TCHAR* argv[], TCHAR* envp[])
    {
      try
      {
        if (!AfxWinInit(::GetModuleHandle(NULL), NULL, ::GetCommandLine(), 0)) return 1;
        RecurseDir(argc &gt; 1 ? argv[1] : ".", 0);
      }
      catch(CException* e)
      {
        e-&gt;ReportError();
        e-&gt;Delete();
      }
      return 0;
    }

Same amount of source code as the Haskell version, linking to 9k exe that depends on MFC, C and C++ runtime dlls.

My point? TFA does not show that Haskell is particularly good for this particular task. It shouldn't be on reddit.Two more things that I wish we had: (1) the ability to have the reddit frame at the top of anything we link to (I love that thing) and (2) the ability to filter based on previously liked/disliked/hidden items. 

In case you missed it, I've uploaded a 'Looks like reddit' version of colkassad's scripts to http://www.learnsqlserver.com/redditlite.zip but it doesn't feature the aforementioned items. 

I'm not a python developer but is there a way to sift through our browsing history (firefox) and ask it to ignore any urls found in our history? I looked on Google and someone mentioned "IUrlHistoryStg" but it looks like an IE thing...Cool, I actually made an image to ascii converter in C last year, see http://www.asciiconvert.com
I don't get it. They had admin rights, but they couldn't change the settings in their antivirus?Nitpick: Ray tracing only has better scalability if the scene database is sorted into some sort of spatial hierarchy. Otherwise it's O(n) in geometry. If you have fancy acceleration structures rasterization tends to scale pretty well with geometric complexity as well. It's much more complicated to implement though.

Ray tracing also tends to scale badly with higher levels of AA, which is arguably more important than correct reflection and refraction for high image quality, at least in the near term.This is the paper that made me look more closely in FP. I'm not an expert by any measure, but now at least I know what FP means.

A must read for every self-respecting programmer, I think.&gt; Nitpick: Ray tracing only has better scalability if the scene database is sorted into some sort of spatial hierarchy. Otherwise it's O(n) in geometry.

Clearly. One nice thing about ray tracing (yes, with proper data structures) is that each ray test is independent of the depth complexity along that ray. The ray test complexity is roughly logarithmic in the scene complexity and often much less than that if you use nested uniform grids or something along those lines.

&gt; Ray tracing also tends to scale badly with higher levels of AA, which is arguably more important than correct reflection and refraction for high image quality, at least in the near term.

Anti-aliasing on current consumer graphics hardware is done by multi-sampling, and that technique can be carried over to ray tracing without modification. In fact it's easier to do selective multi-sampling when ray tracing, because you can look for discontinuities and only sub-sample when the estimated benefit compares favorably with the sub-sampling cost. You can do edge anti-aliasing with rasterization too, but it tends to work less well, as it only deals with geometry and not shading or lighting.You brokeded my website?   

Just let them create their fabulous engine. Someone else will use it to create a fantastic game.I forgot to reply to this:

&gt; ordering need not be specified by a specific alphabet, but rather is completely up to the character set designer

I see this as only being true in the beginning, when character set is being designed. Once it's out there, its design *becomes* the specific of the language (or group of them) that uses it. I live in Central Europe and I *expect* to be able to press one of the specially assigned keys on my keyboard to get that special character from the Central European character set. If the keyboard or the character set were designed differently, then my expectation would differ accordingly, but it'd still be there and just as strong.Um, no. I've worked in game development, and you generally need more resources for development than you do to play a game. Now things like graphics cards, yeah, maybe.

Of course you need to test on low-end machines. But not develop.This is sadly a management (and managing up) problem.

If you keep escalating up the message: "We (our entire department) can't get work done," you will eventually get heard, except with truly pathological management, which may be the case.

When you can point to one option, or one version, that will solve the problem, I really see this as a mixture of stupidity and a failure to communicate.You should give it a try. It doesn't take a coding bad ass to write a ray tracer like that in a few days. It's probably the easiest thing you can possibly do in 3d graphics programming. Even writing a simple triangle rasterizer that observes proper fill conventions, and so on--that's a lot trickier than this, difficulty-wise. So, give it a shot![deleted]I believe such technology was invented 5 years ago.Correct but they spent the entire article talking about the MS experimental languages.  Gtk#, Eiffel.Net, L# and the other third-party languages targeting the CLR got short to no coverage which is really too bad.  The work targetting the CLR beyond MS is really quite interesting.oh man... I lost that battle. We're using subversion with exclusive checkouts (d'oh!), though the pain is lessened by the fact that subversion lets you steal locks when necessary.I wonder if all software looks like a spirograph with this technique, and it does, what's the point?broken link #1....great job redditorsIt's not people like you that ruin the reputation of Visual Basic, it's the "script kiddies" that do it. The problem is that any language easy enough for most 13 year olds to pick up and pwn the world with are looked down upon simply because of the image of that section of the community. As with any computer programmer, I always recommend broadening your horizons for fun, but stick to the language in which you have the most fun and can make a living doing so.I'd like to see an Andy Kaufman deviating from the scripted banter.

&gt; "That jelly deformation's pretty cool. But isn't it weird that there are hungry people even in the US, yet our workforce is expending much effort and resources on things like this?"

or from [the comments here](http://programming.reddit.com/info/163r5/comments/c164xp):

&gt; "That breaking glass will definitely add a little extra realism to future games. Now if it can just write and direct better than Lucas, we'll be all set for Episodes VII-IX!"

;)
obligatory link: http://www.ibiblio.org/Dave/Dr-Fun/df200002/df20000210.jpgHow common are restrictions like these in companies today?  I've worked at mostly small companies where I had complete control over the machines I've worked on.

I couldn't even imagine having to submit requests to some other department to install software.  Of course we do that to the non technical people, but only because they are retarded and will ruin their computers if left to their own devices.They always seem to have text-based apps running, too. I've never seen a tiling WM screenshot with KDE and eclipse and OpenOffice and a couple of firefoxes.

Edit: having just played a game of "try the various funky WMs", I can see why not, too. wmii and ion just crash. dwm works, but looks terrible with any graphical app and treats the KDE taskbar as a tiled window. larswm refuses to answer the keyboard, and floats the KDE desktop as if it were a window! Blech. Only openbox plays nice and steps on nobody's toes.I'm not downmodding you, but you do realize that [most of the planet does not speak english fluently, as a first language](http://en.wikipedia.org/wiki/English_language) and that, yet, it's the best way to get yourself understood by a large audience on the 'net ? Don't blame them for trying.[deleted]What the hell is this "girl"/"woman" thing you all are prattling on about? Are they useful?I only have so much time every day, and I prefer to be 'open' about things with big upsides, or things that are new/interesting/entertaining/thought provoking.  Version control, even the cool experiment ones, just doesn't fit into that category for me.&gt; Evidence? How about the fact that there are so few women in the field? 

How on earth is that evidence they're "not very good at it"?

&gt; The simplest explanation for this is that they're not very good at it and/or don't enjoy it very much.

A possible *reason*, but not evidence of inferiority at a particular task.

In order for something to be *evidence* you would need to demonstrate that women are not as good at engineering as men are, not that there are fewer women in engineering, which is an entirely different thing.

Your anecdotal evidence of your experience with female students and engineers (besides being just that, anecdotal) also neglects such trivialities as sample size and is, again, not *evidence* of a sweeping statement as "women are not as good as men are at this particular task". It is evidence that *those* women (might) not be, and that is all.Thanks for that daddyc001.  I hadn't seen that before and it gave me a chuckle.  I've got DK's 'The Art of Computer Programming' but disappointingly it didn't come with any free TrueType fonts.*Why don't Visual Basic programmers get any respect?*

For the same reason that we laugh about Ruby-"developers" who think their language supports functional programming.No no, that gives VB programmers far too much respect ("Yeah, I can write a function, what?")&gt;why should I care that the software I use is bloated when the alternative is simply for my RAM and CPU to sit idle?

Electricity costs?Great interview on the philosophy and practical use of Lean software development.Also, efficiently maintaining that spatial hierarchy in animated scenes with large numbers of arbitrarily moving objects is hard. In fact, it's an open research problem and one of the biggest hurdles facing real-time ray tracing today. If you can solve that, you get a free PhD.Killed it to death.obviously he converts it to decimal firstApparently it's either so secure the article is not displayed, or it's so insecure the site got pwn3d.(loop
       for x in (list 1 2 3)
       and y in (list 4 5 6)
       collect (cons x y))
    =&gt; ((1 . 4) (2 . 5) (3 . 6))

is not the same thing as (in Erlang)

    [{X, Y} || X &lt;- [1,2,3], Y &lt;- [4,5,6]].
    =&gt; [{1,4},{1,5},{1,6},{2,4},{2,5},{2,6},{3,4},{3,5},{3,6}]Upmodded because that's what I was thinking when I followed the link.&gt; The web systems folks were also in pain, as builds took
&gt; all day (and often they turned off the virus checker,
&gt; despite warnings of firing).

Sounds like they _could_ change the settings, if they wanted to be fired.As a java programmer, most of the people I have worked with on  java projects who were previously VB programmers have been next to retarded when it comes to knowing anything about actual programming. That's why they don't get any respect.

I also think that most people who code in VB are not geeks, but 9-5 programmers, they don't code anything in their off time...they're a different breed.1 1 1 2 1 1 2 1 1 1 1 1 2 2 1 3 1 2 2 1 1VB.net is looking cooler all the time, what with getting lambda and language integrated query, it eventually might not be so bad.

Most of the lack of respect for VB comes from the fact that there are just lots of developers using VB who don't really know what they're doing. Hence, you'll run across lots of VB projects which are just insanely messy and lack any sense of design. You'll find them with any language, but VB has at least historically had more than its fair share of them.This should really help with those Ewok attack scenes...if I recall correctly, blasters never did this type of damage.

They really need to find people that are more eloquent for their demos.It's art! ART!Hey... Don't knock RCS too hard.

The nice thing about RCS is that it's almost always available (on a UNIX machine) and that it doesn't need a fancy setup for you to start using it. The commands are similar enough to svn (how can that be?) that you don't have to remember anything too arcane to get quick version control.

While I wouldn't use it for any *programming* project of more than three files, it's invaluable when you're configuring or maintaining system files for a UNIX machine, especially an unfamiliar one. You can also use the RCS engine to get quick version control for web based things as well (yes, you will want to replace this for production ;-).Somewhat more practical: `ls -R | grep ":$" | sed -e 's/:$//' -e 's/[^-][^\/]*\//--/g' -e 's/^/   /' -e 's/-/|/'`Agreed. When I worked at a game development shop, the development cycle was something like 18 - 24 months. So what you'd do is start development on blazing fast machines, with the idea that machines of that caliber would be mainstream by the time you released the game.I know this looks pretty sick, and I had the same reaction when I first saw it, but how does this differ from the new Crysis engine? Weren't they touting the same "features" for that game?[removed]Yeah, that's the worst reddit title for that link.  I wish I could edit it.  I should have just used "Serving YUI Files from Yahoo! Servers."If it's given that most of them already know some Lisp... then I would choose Haskell, Smalltalk and either APL or J. The goal should be not just to present the different languages/paradigms, but explore each deeply enough for the students to be able to understand clearly, in their own experience and not just on the teacher's authority, how
those different languages encourage ways of approaching problems that are essentially different from the usual procedural fare, and not just a set of convenient shortcuts and idioms. 

Both Haskell and Smalltalk are as ideologically pure, in their respective ideologies, as possible, presenting clearly the power of their approaches. With Haskell, I would expect only a few of the brightest students to be able to internalise, during the time available the understanding of monads well enough to use them creatively. But everyone can be shown the standard uses of IO, State etc.; and more importantly, they should learn to appreciate algebratic datatypes, type inferencing, closures, currying, and understand how usefully strong really strong static typing is, compared to C++. Actually, it may even be a good idea to look into Clean instead of Haskell and forgo monads altogether. 

With Smalltalk, the goals should be to understand the power of a completely dynamic evolving object hierarchy; to explore object-oriented programming in its most consistent setting; and to realize that a language doesn't have to be full of warts and exceptions to its primary paradigm in order to be useful "in the real world". The students should appreciate the beauty of the consistent application of the same abstraction - all the way down to primitives - and how that beauty frees the mind from incident clutter of ad hoc constructions, and leads to wonderfully productive clarity.

As for APL/J, I know so little of either that it's just enough to understand that I would (and will) benefit from a better understanding of their approach. Perhaps so would your students.

If all of these taken together still leave some time, I would try to take the students deeper into Lisp and show them what code=data really can do, and what macros really can do. Nevermind Scheme, they'll get enough lambda beauty from Haskell; Common Lisp will serve better.

I don't like leaving Forth out, but I think that with the time constraints given, there just won't be enough time to show its real power - only to explain the basic ideas and leave some vague impressions that won't stick.
This is off-topic, but I get sick of seeing old pronouns misused.

 * thou - second-person singular informal nominative
 * thy - second-person singular informal posessive
 * thee - second-person singular informal objective
 * thine - second-person singular informal posessive noun

The conjugation for sentences with *thou* as a subject in general is "-st" (e.g., "thou readst thy book quickly"), but *shall* is a special case; it's "thou shalt." So, your sentence should be:

 &gt; thou shalt not insult the magic bullet

(Sorry, too many years of linguistics and English.)I'm not sure I would call it free...

Practically speaking, I think we'll first start to see results when stream processors like the Nvidia G80 become mainstream. If you have a processor that can raytrace hundreds of pixels at the same time, then for interactive scene, building the scenegraph quickly becomes the limiting factor, which means that it becomes practical to use an acceleration structure which is easier to build, even if it takes a bit longer to traverse.

Of course, if you could raytrace millions of pixels, you don't need an acceleration structure at all. But somehow, I don't see that happening. :P

Personally, I think that the first real-time interactive raytracer will use some kind of object hierarchy, rather than a spatial hierarchy. Something like this: http://en.wikipedia.org/wiki/Bounding_interval_hierarchy , perhaps, or even just a regular BVH. Object hierarchies are far simpler to construct than kd-trees, and for huge scenes, that matters a ton.
Good demonstration of Haskell and probabilities.

For a political comment, I appreciated your observation:

"Similar constraints apply to any population-wide surveillance: If you’re searching for something sufficiently rare (criminals, terrorists, strange diseases), it doesn’t matter how good your tests are. If you test everyone, you’ll drown under thousands of false positives."&gt; it doesn't say anything about language-to-character-set mapping

I quote from the article:

&gt; Compared to Unicode, a character set is a very limited set of characters which appear in a specific order, as used by a specific group of languages or scripts. Unicode maintains no such language-specific order.

The distinction you are drawing between Unicode and character sets is that of language-specific ordering.  I'm not making this up out of thin air, it's *what you said*.

&gt; You're the one who used the word "must" in the first place (in your very first comment and again now, in your last reply). You're practically basing your bashing on something that I didn't even say!

Your article wasn't very clear, I attempted to sum up your reasoning in my first comment.  I said:

&gt; At the very least, he thinks that a character set must have a language-specific ordering.

You replied:

&gt; Oh, come on! Now you're just trolling. Characters sets are all about ordering. For example, Unicode code point 0x20AC (Euro Sign) is located at 0x80 in Central European character set, and at 0x88 in Cyrillic character set. Wouldn't that qualify as language specific ordering?

Now if I was twisting your words, if my characterisation wasn't what you believe, then why did you defend the statement "a character set must have a language-specific ordering"?

&gt; If you genuinely feel I've said anything misleading in the article, you're more than welcome to provide me with what you believe to be a correct version.

I disagree that one is a misconception.  I'm willing to give you the benefit of the doubt on the popularity of the other.  Correcting the article would involve deleting the majority of it, leaving you with very little.
Me:

&gt; He seems to think this because he's mixed up character sets and encoding schemes.

You:

&gt; Where did you get that crazy idea from?

You:

&gt; To me, a character set is what has traditionally been called a "code page", or also "encoding" in standards like XML.

Yes, where on earth did I get the crazy idea that you've mixed up character sets and character encodings?  I guess I must be "twisting your words" again, huh?
The [Intel iAPX 432](http://en.wikipedia.org/wiki/IAPX_432) might be another interesting starting point for message-passing languages like Smalltalk/Coke/E.Well, there's nothing like LOOP in Erlang. One would have to `zip` the lists and then do the list comprehension (or fold, or whatever)

    [{X,Y} || {X,Y} &lt;- lists:zip([1,2,3],[4,5,6])].
    =&gt; [{1,4},{2,5},{3,6}]

Of course, this is a silly example but it shows what I mean.Oftentimes, pointing to one specific option that will solve the problem makes it *harder* to be heard, as management thinks you're just making excuses for your personal (or organizational) productivity problems.

Unfortunately, the notion that productivity is tied to *developers* instead of *development practices* is still widespread.  It'd be nice if more people would read Kent Beck: "[I'm not a great programmer, I'm a pretty good programmer with great habits.](http://c2.com/cgi/wiki?GoodProgrammerGreatHabits)"Indeed, this is a wonderful paper. I suspect it's more effective on those who already mostly know why functional programming matters, but it's quite good nonetheless.

Another good name for it would be "Why Nonstrict Semantics Matter", but that's not as catchy.I think it's interesting to look a program written in haskell that should be way *out* of the language's wheelhouse. It's impressive that a Haskell program consisting mostly of IO is just as short as a C++ program, and is arguably clearer.Crimea super!!!Crimea no war!!!Thanks for your comment.

For the sake of people who can't try out both versions or who don't have time to study the code sufficiently, I would like to point out that the C++ version presented above does less than Haskell version.  In particular, it &amp;#8212;

* doesn't process multiple command-line targets;
* doesn't hold to the "highest traditions" of glorious ASCII art when drawing the tree (e.g., it doesn't round off the edges of sibling-connecting arms when they terminate); and
* doesn't prune dead-end connecting lines (e.g., when the final entry in a directory is a directory that has children).

For example, given the following input structure:

    INPUT HIERARCHY

    dir1
      file1
      subdir1
        file11
        file12
        subdir13
          file131
      zfile

The two versions produce much different output:

    HASKELL VERSION'S OUTPUT

    dir1
    |-- file1
    |-- subdir1
    |   |-- file11
    |   |-- file12
    |   `-- subdir13
    |       `-- file131
    `-- zfile


    C++ VERSION'S OUTPUT (simulated)

    dir1
    |--file1
    |--subdir1
    |  |--file11
    |  |--file12
    |  |--subdir13
    |  |  |--file131
    |--zfile

Note that in the C++ version's output, the file "zfile" appears to be connected to "subdir1" and even "subdir13" when really it isn't.

Cheers! &amp;#8212;Tom
I up-voted this only so that the dissenting comments keep the definition of the web and REST style in the forefront.

The web itself is a generic application.  Many applications work fine with HTTP's GET and POST.  Throw in a little Ajax style and you can handle just about anything.  No need for Flash apps, applets, SmartClient (fat client), etc.

The 3270 "dumb" terminal standard worked for many years because it was a generic and simple API.  It took a while, but now with HTTP/REST/Ajax browser we finally have a high-functioning, standardized, interoperable, interconnected platform.

Now, let's just use it.&gt;embarrassingly parallelizable

Holy stretchable english, batman.A few years ago this was being sold as a video card, but now it's a separate box.  It's interesting in any case to see that dedicated ray tracing hardware exists.If you are interested in the students discovering code=data and you regret the absence of Forth, you could recommend Factor over Lisp and Forth and kill two birds with one stone ;)&gt; Yep. Like I said, I'm just waiting for the pain level to be high enough to justify the change.

You shouldn't (wait), because that will probably mean that SourceSafe will have corrupted your whole repository (again?) or forgotten a whole file's history. Plus when it really starts itching it may be sign of the gangrene trying to set in which can be prevented by a few antibiotics, while the excruciating pain later will mean that the whole leg has to go.

If you want ammo in your "let's get away from SourceWreck" battle, refer to the fairly well known [Visual SourceSafe Version Control: Unsafe at any Speed?](http://www.developsense.com/testing/VSSDefects.html) from Michael Bolton and [Visual SourceSafe: Microsoft's Source Destruction System](http://www.highprogrammer.com/alan/windev/sourcesafe.html) from Alan De Smet, as well as the fairly less known [Source Control: Anything But SourceSafe](http://www.codinghorror.com/blog/archives/000660.html) by our all time friend Jeff Atwood from CodingHorror.

And remember: real friends don't let their friends use VSSAAAAGGGHHHHHHH!!!!!!!! I DO NOT HAVE A MIRROR!!!! I had to take a picture of myself with my Cybershot cellphone then bluetooth it to my lifedrive. Somebody save me please!"Throw in a little Ajax style and you can handle just about anything. No need for Flash apps, applets, SmartClient (fat client), etc."

Myopic much?

So, using nothing but those technologies, how would you implement a graphical dungeon crawl? A shooter? Virtually any other sort of game or simulation?

What about web-based monitoring and control of complex systems? Wouldn't that require Flash, or something like it, if you want reasonable graphical feedback?

What about interactive photo and video applications, perhaps with collaborative editing?

Seems to me that HTTP/REST/Ajax fits everything *you* think the web is good for. Some of us don't have such limited vision.&gt; Holy stretchable english, batman.

"Embarrassingly parallelizable" is a stock CS phrase that describes problems which can easily be split into a whole bunch of independently solvable subproblems. Google it if you don't believe me. :)"In a purely functional language like Haskell the question of "ordering" is simply not meaningful."

Maybe it is just me, but I find this widely repeated in the FP blogosphere statement misleading. Ordering is still very meaningful. We don't have an algebra of programming precisely because programming is largely ordered/non-commutative and we have no idea how to capture that in a reasonable well-behaved algebra. The plus with FP is just that what needs to be ordered is made explicit instead of implicit.C, Java or C#, and Python.  Teach the three and you are done.  Explanation for those missing:

Haskell, Ocaml, D, Lisp, and most other languages mentioned on Reddit regularly: Semi popular in the academic world, but not used widely (if at all) in the work world.  

Ruby: Probably more popular than Python, but Python is still more common in the work world.  This could be because Ruby hasn't proved popular outside of web sites using Rails.

C++: Popular, and it was hard to exclude.  But if you have a good grasp of C and either C# or Java you should be able to easily handle C++.

There is an age old question: should we teach students to understand things at their best or give them the skills they will inevitably need for their future?  Sadly, too many CS students come out of school lacking the later and wonder why the hell they had their time wasted studying language X.This is interesting, but shouldn't be on programming reddit.[deleted]That was an exclamation of admiration, not of condemnation.  Huzzah for english!&gt; It's both a full programming language and a hardware description language, but the subset of Verilog that can actually be synthesized into hardware and should be used when using Verilog as an HDL is very restricted.

Which sort of sucks. And you have to be very conscious of what code is generated (e.g. is the synthesizer going to infer latches here), so in reality you end up studying closely the synthesis manuals from your compiler vendor rather than really coding to Verilog's semantics. And usually there are all kinds of synthesizer-specific pragmas that deal with how to synthesize, say, switches to deal efficiently with different signal arrival times--so you might want to hint that a certain signal arrives late, which means it should be near the bottom/output of the tree so its influence doesn't have to propagate through too many layers of logic. It's like you roughly know what you want the synthesizer to spit out, but you're left trying to manipulate it through the Verilog front-end.[Brainfuck](http://en.wikipedia.org/wiki/Brainfuck).[deleted]Looks like a useful project, but it's a bit premature to announce it, only 3 days ago [he said he was writing it in OCaml](http://groups.google.com/group/algokit/tree/browse_frm/thread/fcab4bed583ad5ee/6a28207956dc9dbf?rnum=1&amp;_done=%2Fgroup%2Falgokit%2Fbrowse_frm%2Fthread%2Ffcab4bed583ad5ee%3F#doc_ca889f4b9ac89387)...Unfortunately, they're not very much interested in anything but the Windows platform. All the other platforms are just lagging behind.none of those links even work.
edit: the javascript/css files in the page return a 404

             
edit 2: ok. now they're working again.The point of a computer science course should not be to teach popular programming languages, but to provide the student a strong grounding in the theoretical workings of computers and algorithms.

Once this groundwork has been laid, learning languages such as Java, C# or Python is a relatively trivial task. The hard part is giving the student a good understanding of programming, and learning Java won't help with that as much as Lisp or another more 'academic' language would.&gt; That's standard. It is assumed that if you are competent enough to start a book on mathematical logic then you've had a grounding in elementary logic, including induction.

Well, yes... I guess I wasn't clear. Recursion is important constructively when building first-order languages. Proof by induction is a useful result that falls out of recursively constructed first-order languages.

It's worthwhile to define both of them formally in this specific context because there are important properties of first-order languages that cannot be proved without those definitions. Moreover, you use the recursiveness of the language in the proofs of the soundness and completeness theorems.

On another note, if you leave the recursiveness of first-order logic out of your course, then you miss out on the wonderful opportunity to tie in Church's Thesis.

It may be standard practice, but it wasn't in my logic program. And, it seems... uninspired.

&gt; "Semantics with Applications" is computer science rather than mathematics so it's no surprise that isn't on the list.

Well, I think lots of logicians would disagree. But, I stipulate that most other mathematicians would view it as CS.That depends: are they attending a University, or a trade school?
No Dijkstra monographs?

???

[E.W.D. Archive](http://www.cs.utexas.edu/users/EWD/)
Why the hell would they have a -f command in the first place?I bet all parent's downmods are for saying 'it shouldn't be on reddit.'

a) TFA made no claim that haskell was better than another language; it's just sample code. Haskell is hard to learn, sample code is valuable.

b) The whole point of a decentralized link economy like reddit is that the value of each link emerges gradually. We allow everyone in, and we don't constrain their judgement to a specific time period and then lock it in. So if some new programming language implements this more elegantly in the future the C++ implementation will still be here, showing posterity the wayposts in getting there.

c) The haskell solution is easier to refactor. Focussing on a single metric like line count and being quick to exclude other options will narrow your view.At a MS shop? Man, that's gotta hurt. I swear that one of these days there is going to be a brawl over the one-true project file for our main web site.thanks! so based on Flickr's brief explanation of REST, it's simply attaching auth values via GET or POST, or is there more to it?

Would it be secure enough to pass the user's login (email address) and say 3 "tokens" which are predefined and are each 20 character "random" strings from one site via POST to another using SSL?

In other words the 2 sites would agree on 3 different 20 character strings that would be fairly random at a glance, and as long as all 3 strings match and a user exists with the email address they would bypass the login screen.

Not as elegant as morselsrule suggestion in that the tokens don't expire after 5 minutes.

Does this sound reasonable or am I way off base?

thanks in advance!&gt;The point of a computer science course should not be to teach popular programming languages

Then computer science is not in line with what the vast majority of students are looking for or what colleges were intended for.  That disconnect is probably the main reason why less and less students in the U.S. take computer science.

&gt;but to provide the student a strong grounding in the theoretical workings of computers and algorithms.

You can't do that in any of those three languages given?  I think this is a deficiency on the teachers part.I was trying to follow along, but I got:
&gt;     ERROR file:.\Bayes.hs:7 - Undefined class "Dist"

in Hugs.Yes, yes I would. I had read the article three times before I was convienced that the author had nothing to say and I jsut didn't miss part of it..&gt;That depends: are they attending a University, or a trade school?

["College or University"](http://en.wikipedia.org/wiki/College) or "Undergraduate or Graduate".  I have no problem with graduate level teachings in languages outside the three I gave.How strange that on Dec 19 they announced [Clean 2.2 available for Windows, PowerMacintosh, Linux and Solaris](http://mailman.science.ru.nl/pipermail/clean-announce/2006-December/000027.html)867 - 5309The key paragraph:

&gt; Since the main operation of computation in functional languages (function application) is strongly typed, there's simply less room for errors to sneak in. So the difference, from a static checking standpoint, is that in an imperative language only part of the method of computation is actually checked, whereas for functional languages the entire method of computation can be type checked.

That's a pretty interesting observation.

Personally, I find static typing the most helpful when I'm dealing with crazy higher-order abstractions.

For really tricky Haskell code, I write down the type that I have and the type that I want, and then I use Hoogle to search for library functions which map between the two types. I bet this could be automated!With a managed norton you can lock it down hard from the server (most options in the client are greyed out). You could possibly terminate the scanning engine, but that would just throw up an alert on the server which would have someone at your desk within a day. Id imagine mcafee has similar structure.RSpec definitely [needs more publicity](http://www.personal-api.com/train/2007/feb/23/rspec-and-readable-testing/ "My own article on unit testing.") as a testing framework.I don't understand what you mean. Do you mean that you think all algebras are commutative ?

We do have an [algebra of programming](http://www.amazon.com/Algebra-Programming-Richard-Bird/dp/013507245X/sr=8-2/qid=1172251431/ref=sr_1_2/102-5205206-0979327?ie=UTF8&amp;s=books)You can find most of the missing pieces in the earlier articles:

http://www.randomhacks.net/articles/2007/02/21/refactoring-probability-distributions
http://www.randomhacks.net/articles/2007/02/21/randomly-sampled-distributions

You'll also need MaybeT, a version of which is here (you might need to adjust it a bit):

http://haskell.org/haskellwiki/New_monads

Note that I'm refactoring my probability library with each article, which makes it harder to follow along. :-( I'm hoping to get some source code into Darcs ASAP, so that people don't need to dig through the entire series to get something running.Layman alert: 
Any idea/insight on why two locations/hashes are needed and not just one? How are the new hash functions chosen; can we get an infinite loop here?
&gt; Unfortunately, the notion that productivity is tied to developers instead of development practices is still widespread.

I don't think this is an all or nothing propositionOh yeah, it sucks. On the upside, you can just ignore subversion's locks and work as usual, or steal a lock if you need it. One of my coworkers just ignores the locks in protest of the locking-required policy; I would like to, but since I took on responsibility for the whole svn shebang, I don't think it's right to.According to the article:

&gt; This code is based our [FDist](http://www.randomhacks.net/articles/2007/02/21/randomly-sampled-distributions) monad,

Which seems to be where Dist is defined.[another helpful link](http://www.joelpm.com/articles/2006/09/12/visual-source-safe-woes)Hey, all you Haskell promoters out there, here is a chance to prove your monads: do the same in Haskell and show us the code. Maybe Haskell can be useful after all.Languages have different strong suits, and I don't think it's inappropriate for teachers to use more "academic" languages to teach students.  If a professor was teaching an advanced compilers class, he'd probably want to choose a language that could support CPS a bit better than Java.I do have a example-compliant version, it's not much longer ;-) (one more line in "Heading" and "main")

It seemed to me that example is not really good for functional programming style (which I wanted proven by C++ example).

Less baroque imperative language than C++ could probably do even better.
&gt; I bet this could be automated!

Yup, sort of.  If you look at Djinn (e.g. http://lambda-the-ultimate.org/node/1178) it's actually a slightly different approach, but the same general idea.

However, according to the guy who did the maths, there's a related but more powerful algorithm that ... is better [0] (I'm afraid the logic is a little beyond me, so I lack the ability to express why).

Anyway, take that sort of algorithm, and add in the results of a Hoogle, and see what it produces.  Aught to be able to produce a decent stab at most things, might even turn out options that are surprising.

[0] Roy Dyckhoff, personal communication, 2006

For use by systems that use an authentication method other than normal UNIX authentication.damn man I just.... never mind![deleted][removed]&gt;A university CS program shouldn't be Java

I said college, not university.  I prefer to say "graduate" or "undergrad" since its less confusing (especially to non-U.S. citizens)

&gt;or, more accurately, they're too soft.

The students are the problem.

&gt;All the hard sciences and engineering are suffering as students head to less math-intensive courses. 

Math has to be the problem.

&gt;Modern public high schools in the US are all about leaving the student no choice but to pass the state exit exam 

The schools have to be the problem.

&gt;Which would be fine if the test were the SAT or ACT of 30 years ago

The tests have to be the problem.

&gt;and sell my soul piecemeal every time.

Because it can't be your fault.

Sorry, I'm not big on the blame game.  I could care less who is to blame.  None of this excuses students coming out of school unable to handle a job.I'd be interested in a more detailed explanation of how it was cracked ... but job well done.[deleted]I'm pretty sure that they had all the computers on a domain. You can set up policies that prevent user administrators from changing options (for example, all workstations have windows firewall disabled).[deleted]So I guess Linux forked from an [earlier project](http://www.lost-civilizations.net/images/inca/inca.calendar.jpg)
developed by [the Inca](http://www.lost-civilizations.net/inca-civilization-page-5.html) ?But as a command line switch? That seems rather dodgy to me.&gt; Then computer science is not in line with what the vast majority of students are looking for or what colleges were intended for.

One would not join a biology course in order to learn veterinary medicine. If you want to learn a popular programming language, there are many college courses available. Computer science courses should be teaching computer science, not how to program in the most popular programming languages.

&gt; You can't do that in any of those three languages given? I think this is a deficiency on the teachers part.

How would you go about teaching syntactic macros in a language that doesn't support them? Or teaching type inference? Monads? Arrows? Recursive type definitions? Lazy evaluation? Argument pattern matching? Function composition? Logic programming?

Aside from the features not supported by the languages you list, some languages are more suited to teaching certain programming paradigms more than others. The relationship between ASTs and code is inherently more obvious in Lisp than it is in Java. Functional programming is clearly best taught with a functional language. Static typing is best taught in a language that actually has a type system that incorporates ideas after 1970.

So given that there are better languages around for teaching, why bother teaching C#, Java or whatever in a computer science course? Once you have a good grounding in programming, learning new programming languages becomes relatively trivial, especially for languages with extremely limited syntactical features, such as Java. The hard part is getting that grounding, and some programming languages are more suited to that than others.I did this in college. While my scenery wasn't as interesting, it's not difficult to write a raytracer that does mirrors and shadows. I didn't do it all in one day, but it probably took the same number of hours...
Ah OK, I missed that. Thanks.Agreed, this was one of many assignments in the first level graphics course back in undergrad, but the output really makes you feel like a coding champ.Haskell would be somewhat slow for this.

I think OCaml would rock though, it can be faster than C/C++.ahah, I know the guy in the last picture! :)[deleted]RTFA. He's specifically charged with setting up a "fringe language" class for masters students who already have experience with C, C++ and Java.Having just started looking at Haskell (just a couple of evening's study) one of the things I have found most difficult is understanding error messages given out by ghc. So I have a lot of trouble understanding why my code doesn't compile. I agreed once it does it tends to be fairly bug freeThis turns, again, in a side discussion. Main point being order is pervasive to programming, FP or stateful.

Perhaps I misused the term 'algebra'. I'm looking for some algebraic rules to operate on the program semantics.
Commutative rules -&gt; Nice compositionability, nice small formulas. Your high-school 'algebra'.
Non-commutative rules -&gt; Nobody has a clue how to deal with them in the general case without collapsing under sheer complexity of the resulting formulas.
The larger point is that you would like to prove your programs manipulating some (simple) rules. If the rules are not commutative, then manipulating them becomes a nightmare. That's why (almost) everybody shies away of proving program properties, whether in the FP community or in the state community.

PS. Yeah, and lambda calculus is a rule based transformational system, etc. There is your 'algebra'. But you haven't even started to capture the semantics of the objects you manipulate, so I'm speaking of a different 'algebra'. Or I am speaking nonsense, I am largely an innocent bystander that tries to make sense of the obvious impasse we are in :)Does she really deserve the award? What did she do? I think they just wanted to have a woman winner this year. How could someone win the Turing Award for "optimizing the performance of compilers?" I've never heard of her before, I've never heard of the Francis Allen compiler before. This is a total fraud. She had done nothing for advancement of computer science. I shall boycott further Turing Award ceremonies, and when I win it, I shall say that she has to give hers up for me to accept.Are you trying to imply that programming languages are the problem?

The most important computer science classes involve absolutely no programming whatsoever.
[removed]&gt;One would not join a biology course in order to learn veterinary medicine.

Should doctors in school learn from equipment and tools that only exist there?

"Teach a man to fish and he will eat forever
Teach him only about fish and he will go hungry"Forbidden
You don't have permission to access /2007/02/13/09-f9-11-02-9d-74-e3-5b-d8-41-56-c5-63-56-88-c0/ on this server.

Additionally, a 403 Forbidden error was encountered while trying to use an ErrorDocument to handle the request.
Apache/1.3.37 Server at jontillman.com Port 80No.Or perhaps students uninterested in CS aren't taking it anymore because they now realize it's not a quick ticket to a high-paying job as it once was?

Giving under-qualified students degrees is not a good way to "improve" CS.
Yes, that in fact is very strange. Particularly if you consider the fact that I've known about Clean for over 4 years now, and kept visiting their page, and they never seemed to get their act together regarding non-Windows platforms. Is their video-game and IDE working OK on POSIX now? Nope. Just checked. CleanIDE is Windows-only. 
Oh, wait, it says they support Mac OS X PowerPC...still...We use Intel, ok, since Apple moved to Intel...Next time you have brainfart out of your mouth, you check first, ok? I don't consider what they present as a serious commitment to non-Windows platform. There are quite a few FP languages teams that are far ahead of Clean. You can't expect a language to be takes seriously with that kind of "support" (unless you're Microsoft).[removed]You could just use one 'good' hash function and rebuild with a new hash when you have conflicts. Why two functions/tables is better than one?

The other question is how do you pick new hash functions if you do get conflicts? Or do I just grow the table and use the same functions? Especially since those functions don't appear to grow on trees.[deleted]Pretty URLs look like the names of objects, whereas URLs with query parameters look like you're calling a function.  Pretty URLS are supposed to be stable and guessable.  If your URL is based on query parameters, it's anyone's guess just which query parameters are allowed or required, and it may be that multiple URLs with the query parameters specified in a different order are actually equivalent.Unreadable.By that argument, courses on marine biology should revolve primarily around learning how to use fishing rods, nets, and how best to cook a freshly caught salmon.Sure, but thats not at all what I'm bitching about... I'm all for the transition from a path that points to code towards a path that points to data. However, the real pretty url fanatics are denying any use of query parameters at all... thats whats bothering me.I think sbrowne is British. In Britain, colleges are [these](http://en.wikipedia.org/wiki/Sixth_form_college)."down votes 4"; I guess there are stupid questions.You don't love linux?&gt; The solution is simply to rebuild the hash table with a new hash function. But (if you work through the theory) this happens infrequently enough that on average you're no worse of than the rebuilds you have to do with an ordinary hash table.

That's not completely true, is it? My understanding is, some hashing algorithms will basically keep producing new potential spots (dumbest way, try the next slot, then the next). So you never have to rebuild the hash table (if it's big enough); the drawback is you lose the guarantee of constant time access (although with a not-too-full hash table, you should still have it for the average case).

Regarding why two hashes vs. one, well it seems with good hash functions you reduce the chance of an irresolvable hash collision at least quadratically. (Actually, the more I think about it, the more complicated it gets -- as the table fills, the chance of a loop rises rapidly, as does  the chance of a collision).Hey, my comment knocked CVS, not RCS. :)

You'll find `{,/usr/local}/etc/RCS` directories on most of my machines.[deleted][deleted]C++ is just one ALGOL dialect. What if you wanted to port this code to another? There would be a lot of work involved. ALGOL is too fragmented. I find there are many languages, such as Haskell, which are much easier to read than ALGOL, too.They don't want to stay back behind Apple.
As you say, it's pretty simple but it's rather clever.
Thanks for linking it.stop ad bastardIsn't that open addressing?  Each key has a permutation of the table entries that it looks at, and new data is inserted into the first available slot in that permutation.

Unless I'm confused.  I always understood chained hashing to be having a list hanging off each table entry where the data are stored.  Linear time lookup, but with a small constant.&gt;thou - second-person singular **informal** nominative

What's the formal version for those pronouns?[deleted]&gt; You can get infinite loops. The solution is simply to rebuild the hash table with a new hash function.

And, uh, [how do you know when you're in an infinite loop?](http://en.wikipedia.org/wiki/Halting_problem)[deleted]you.

http://en.wikipedia.org/wiki/T-V_distinction

thou being informal makes sense if you think about your Shakespeare. "Shall I compare thee to a summer's day? Thou art more lovely and more temperate..."You notice that you're processing the same element more than once.When I was an MIT undergraduate in Political Science (long story), there was an upper-level CS class in object-oriented program design that used [CLU](http://en.wikipedia.org/wiki/CLU_(programming_language%29), a language designed by one of the class's professors.  When, years later, I embraced my inner geek and took the same class as a special student, they had switched to using Java.

I asked the professor why they had changed, and he said that the people who looked to hire MIT students for summer internships wanted those students to have experience with a language that was used commercially.

If MIT can't hold fast to the principle of "we teach fundamental principles and our students can pick up any language once they know those principles", what hope is there for any other school?Okay.

&gt; So they said (to everyone) we know of many Java terrorists in the wild.

Oooh scary!In a normal programming language you are correct, but haskell is not normal, so in haskell something like this is legal and works, while in C it is not:

y = foo(x)     // x first appears here.

x = bar()  // x is declared/assigned here

Haskell compilers know to re-order the execution.I find this in OCaml as well -- the programs are often correct, and they provide a lot of help if my ideas aren't terribly well formed in terms of refusing to compile something would end up as a run-time bug in another language.

I agree, though, that the error messages can be seriously cryptic.I thought floating point was OCaml's Achilles heel? Of course the proof would be in the testing.Not exactly.   I have been using Unix desktops (mostly kde) for years. However they have almost never been Linux - I use FreeBSD at home, and most of my work time was on Solaris.   The last version of Windows on my personal computers was 3.1.  

Personally I find windows very limiting any annoying.   No focus follows mouse.  (People keep pointing me to tools that allow that, but they all raise the window with the focus, completely missing the whole point of focus follows mouse)    Windows seems like it was designed for people who run exactly one program.   There are many little things that just work wrong.

My work has placed mostly Windows in front of me for the past 3 years, and I still have not got used to Windows.The answer to that's pretty obvious if you try and do something in Lisp (can't speak to smalltalk) that you can easily do in Perl.

I sat down and tried to get the most useful implementation of lisp at the time (SBCL) playing nice in Unix.  I simply wanted to pipe stuff to it and have it write output.

You basically have to implement that behavior in your program -- fortunately, they tell you how in the manual:

http://www.sbcl.org/manual/Unix_002dstyle-Command-Line-Protocol.html#Unix_002dstyle-Command-Line-Protocol

But that should give you a good idea of what it means to "not play nicely" with the outside world.&gt; It seemed to me that example is not really good for functional programming style ...

That's what makes the example problem interesting.  The problem is all about side effects &amp;#8211; directory scanning and printing  &amp;#8211; the exact opposite of what functional programming is all about (and supposed to be good at).

I trust you can see, then, why solving this kind of problem in a purely functional programming language would be interesting.

Cheers! &amp;#8212;Tom
[deleted]Even knowing Haskell and understanding type inference I had to admit that this article is naive.  It's tremendously easy to make algorithmic mistakes that result in a program that compiles but doesn't work.

Imagine a graphics program where you gave a coordinate as "y" when you meant "1024 - y".  Or passing coordinates in reverse.  Or using x0,y0,y1,y1 when a rectangle routine takes x,y,w,h.  Or getting the split wrong in a binary search.  Or getting the comparison backward in a sort.  Or forgetting to put spaces around a value in an output string.  Or using forgetting to prefix special characters in a generated HTML page.  Or forgetting to close a tag.  Or adding the wrong value to a score in a game.  Or reversing a list when you shouldn't have.  Or dividing by zero.  Or forgetting to convert degrees to radians.  Or typing "x*x + y*x" when you meant "x*x + y*y".  Or getting the order wrong in a matrix multiply.  Or getting the first element of an array by indexing with 1 instead of 0.  None of these cause compile errors.Heh, no, I just don't use Windows. :) I've been using Linux pretty much exclusively for about 6 years now. (Debian for about 5 years, then Ubuntu.)

Ubuntu seems much more user friendly than Windows as a desktop these days anyway. I recently installed it on my father's machine. He had no prior experience with Linux, and he liked it better right off the bat. It's even easier to install it than Windows XP. In the XP installation, it was an awful pain downloading sets of patches and rebooting XP over and over, and even once you have XP set up initially, you still have to install all of your software, like Office, etc.

He found Ubuntu's package management to be rather impressive by comparison. He also found the UI to be cleaner and generally work better. Now he only uses Windows for specific software where the Linux equivalents aren't quite there yet. (Specifically Cakewalk Sonar.) For basically everything else, he prefers Linux.

That's not even mentioning myself. As far as I'm concerned, Windows is horribly crippled when it comes to basic desktop features. Things like point-to-focus and multiple desktops are a pain to configure and generally require locating and downloading additional software. There's no builtin equivalent of half the things I have on my Gnome panel. Little niceties are missing from the file manager, like automatic preview of audio when hovering the mouse over a music file. Admittedly, I've hardly used Windows at all, but there are just lots of little things that I miss whenever I sit down at a Windows machine, and looking through the options didn't seem to produce results very quickly.

Then there's the commandline. This new Powershell thing might do better, but historically, I don't even really have to describe to you the difference there.Funny story, the Mormon church told it's members to use thou and thy and such in prayers, then realized that they had it backwards.  Then they said to do it anyway because [God said so](http://library.lds.org/nxt/gateway.dll/Magazines/Ensign/1993.htm/ensign%20may%201993.htm/the%20language%20of%20prayer.htm).You do realize that this is a Master's degree class?Code Monkey like Tab???Well, there's already been some work done on things like [BVHs for changing scenes.](http://www.sci.utah.edu/publications/SCITechReports/UUSCI-2006-015.pdf)  Interestingly, some of the techniques for building quality kd-trees (SAH), apply just as well to BVHs with a little care.  And last year's SIGGRAPH had a [paper on using grids for ray tracing animated scenes.](http://www.sci.utah.edu/~wald/Publications/2006/Grid/download/grid.pdf)  Also, for kd-trees, there's been some work on spatial kd-trees, a.k.a., skd-trees that have a pair of overlapping spatial partitions and [applying them to ray tracing](http://www.cgg.cvut.cz/~havran/ARTICLES/rt06havran.pdf).

Some interesting hybrid acceleration structures are coming out these days, and handling animated scenes was a big topic in the field last year.I like the idea of learning from tests, but RSpec looks hideous. Didn't we ditch cobol already so that we didn't have to try and write in stilted natural language?Without programming, those courses would exist in the Mathematics dept.  Sometimes they do anyhow.  Nonetheless, the important groundwork for computer science and the theory of computation was done prior to the invention of electronic computers.

Hilbert's tenth problem, presented at the beginning of the 20th century, could be said to mark the beginning of the field; although he did not realize the implications at the time.  Of course, there are important precursors such as Cantor's set theory, Frege's idea of a formal logical language, and the paradoxes of these which led to plenty of consternation by many mathematicians until their hopes were mostly dashed by Goedel, Turing, Church, and others in the 30s.  Notions of formal languages, incompleteness, and undecidability now existed by the time the first primitive electronic computers were constructed.  The tenth problem itself derived from a two-millenia old question about the so-called "Diophantine" equations, after an ancient Greek.
(squeezing all this into a short paragraph was rather difficult, sorry, I hope it made sense)

What's programming?  A programming language is a formal language.  What's it describing?  A model of computation, which can also be described as a formal language.  What's a formal language?  A set of strings contained within the Kleene closure of some alphabet.  How do you decide which strings are in the language, and which are not?  That's where computability comes from.
That is trivial to detect.

The halting problem is about static code analysis that has to examin every possible path, not runtime analysis examining the current path.[removed]1. How to create a new hash function on the fly?
2. How to you prevent the new hash function from causing an infininte loop itself?
There is a Google Tech Talk about [Cuckoo hashing](http://video.google.com/videoplay?docid=-2473927019456727523&amp;vi_action=large).  The relevant part starts at about minute 20.  The proof about the performance is based on the probability of embedding full binary trees of fixed size in random graphs.  Pretty cool, IMHO.&gt;By that argument, courses on marine biology 

The "teach a man to fish" is an old Chinese proverb.  It goes as such:

"Give a man a fish and you feed him for a day. Teach a man to fish and you feed him for a lifetime."What I found missing in the OCaml setting was a way to interactively navigate the expression contexts that generate the type mismatches. When the compiler tells you 'type abc expected but xyz found', I would love to be able to use up/down /left/right arrows and move between highlighted expressions/contexts.I see now. I can't arbitrarily 'improve' a single hash function, because conflict resolution is context-dependent. Thanks.WHOO! JELLY!Dan is quite the technical writer.

[removed]"Ubuntu seems much more user friendly than Windows as a desktop these days anyway" How did I know a reddit user would say that.  

I won't disagree with your opinion.  I have used Redhat/Ubuntu/Gentoo; redhat since the 98 years.  There are some things that are better and some that aren't.  But, I still feel that Windows 'feels' a little bit more usable.  And there some millions that might be more inclined to agree with me.  "At the 2004 IDC Directions conference, IDC Vice President Avneesh Saxena stated that Windows had approximately 90% of the client operating system market.[1]
This is not a traditional SDK in the sense that it doesn't arrive on CD-ROM, and it isn't one monolithic download. Instead, it is a gathering of 3rd party contributions from many of the leaders in the community. In some cases the information and downloads are available directly from the SDK on opengl.org. In other cases, you'll find links to the original materials elsewhere on the web. In all cases, the contributions have been hand selected and represent the best of what's out there.&gt; Yes, where on earth did I get the crazy idea that you've mixed up character sets and character encodings?

Look, I haven't been mixing anything up, OK? Character sets and encodings are traditionally the same thing: http://en.wikipedia.org/wiki/Character_encoding

In my article, I stated that Unicode is a different beast and is therefore neither a character set nor encoding. You seem to disagree, so fine.I recommend using additional compilers/interpreters. I think hugs tends to produce more understandable error messages than ghc, or at least I did when I was first learning.Djinn is really sweet:

&gt; I've written a small program that takes a (Haskell) type
and gives you back a function of that type if one exists.
It's kind of fun, so I thought I'd share it.

It uses the Curry-Howard correspondence to turn types into theorems, and then runs a standard theorem-prover over them. Then the proof is mapped back to a program.

That's a seriously nice hack.[removed]In the 2000 ICFP-Contest to write a raytracer, the Ocaml teams beat the haskell teams.
The raytracers written in C and C++ were all too slow or produced incorrect results.

http://www.cs.cornell.edu/icfp/contest_results.htm
&gt; "Give a man a fish and you feed him for a day. Teach a man to fish and you feed him for a lifetime."

Give a man a fish and you'll "have" to feed him for a lifetime.  Teach a man to fish and you'll only "have" to feed him in the meantime.  (Tells you something about business, doesn't it?)

Not that the proverb says too much about your argument.  Teaching sufficiently similar languages is teaching how to fish sufficiently similar fish, whereas teaching a variety of different languages is teaching how to fish a variety of different fish.

The languages taught should vary in the concepts and styles they support, so that students learn not only many concepts and styles, but also exercise learning new concepts and styles.At minimum, I think his code examples are horrible. I don't see how the 2nd version shows a switch from imperative to functional programming, it just expresses the algorithm more verbosely.

This obssession with statelessness (is this even a word) or side-effect free (or side-effectlessness) programming is just that: an obssession. It's just a bad approach to disallow changing states. Just because I wrote my code in NY doesn't mean it won't work in CA, or even HI. OK maybe HI is too far, but CA ? Definitely yes !

Now this makes me wonder, if I program in C++ with only const variables, is my code closer to state-free functional programming ? Maybe, but it would come with a horrible cost.
Well, there may still be other reasons for that as well. There's an incredible amount of inertia there, remember. Most ordinary people I talk to don't even really know about Linux, or if they do, they have opinions about it which are more reflective of the state of Linux back around 2000 rather than the current distributions.The class-modification version seems that it produces code that is easier to read.  

I'm also a fan of "tell, don't ask", and prefer not to ask an object for data to pass to some function if the object can just perform that operation itself.&gt; if I was twisting your words, if my characterisation wasn't what you believe, then why did you defend the statement "a character set must have a language-specific ordering"?

I defended my original statement - the one without the "must". In it, I was stating a simple fact, *not* a requirement, which is how *you* interpreted it. I already explained my reasoning: http://programming.reddit.com/info/15r8w/comments/c166cs

&gt; Correcting the article would involve deleting the majority of it, leaving you with very little.

Oh-kay then. You're obviously not being constructive, so we're done here.

But thanks for the conversation anyway! Regardless of what you may think, I've learnt a valuable lesson here. My article could have been clearer, and I'm going to try harder in the future. I'll still defend anything I write until someone clearly proves me wrong, which so far, you haven't managed to accomplish. Mere opinions are not enough - after all, everyone has one.[removed]&gt; The "teach a man to fish" is an old Chinese proverb.

I know this, and you seem to have missed my point entirely. Perhaps I should be less subtle:

**Computer Science != Learning a programming language**

Does that express my point clearly enough? I have nothing against programming courses on Java or C or even Python, but Computer Science is not about learning a programming language, or even several programming languages. It's not even really about programming, but more about the theories and algorithms behind computing.

Hence the allusion to marine biology; in the course of studying for marine biology you'll probably handle a net or two to inspect specimens, but that clearly isn't the ultimate point of the course. Likewise, the point of Computer Science isn't to teach the student the latest and most popular programming languages, but to teach computing theory. The popularity of a language is largely irrelevant; what matters is how easy it makes teaching the students the concepts of computer science.Why is it a bad approach?

In C++, you would be fighting the language itself (in addition to all the fighting you have to do just to write C++ code).  That's silly.

Eliminating implicit state eliminates a whole class of bugs of which imperative programmers typically spend most of their time chasing around.

One could sensibly argue that this just causes bugs in a different way; but keep in mind, the flow of data should be much more comprehensible overall, and statelessness keeps bugs localized.
For shame! They used the deprecated &lt;center&gt;&lt;/center&gt; tag.[deleted]Meanwhile, over in XML land ...

"Make that an attribute!"

"No, it should be an element!"

"Attributes are expressive!"

"All-element markup is easier to manage!"


While, at the same time, in comp.lang.flamewars ...

"Significant indentation is The One True Way!"

"No; a developer must be free to express intent using the most appropriate format."

"But mandatory whitespace ensures beautiful, well-structured code!"

"Code beauty is more than robot precision of text placement!"



Arguments based on aesthetics are fatally flawed, no matter which side you choose.  If there are reasons one or another URL structure lends itself to less fragility, or ease of development, or *something*, then that's the  way to go.You can make more or less use of the type system for those purposes however.

Ideally, one would want a type system which could express all program properties and verify program correctness completely.  That's an impossible goal.  So, a decidable type system has to express weaker constraints on a program.  It's a fact of life.  But there's still plenty of opportunity to give verifiable structure to your programs.

A lot of the mistakes you mentioned in that list are ones that should not be an issue.

* Forgetting to put spaces around something in an output string?  Perhaps you should be writing a higher-order function to do that for you instead of explicitly writing it out each time.  Pretty-printer libraries typically have these combinators.

* Similarly, closing tags.  Why are you crafting XML with strings?

* Degrees to radians.  You can easily create proper types for your units.  In a way, dimensional analysis is a "type system" for arithmetic expressions.

* Research into dependent-type systems is actually driven by interest in performing static analysis on some of the other problems you name, such as array-bounds checking, division by zero, distinguishing an empty list, etc.

C, Java, and Python are not University level material. Everyone should be required to learn basic programming before they graduate highschool. Suggesting that Computer Science courses need to focus more on those is like complaining that the Math department focuses too much on Differential and Integral Calculus, and not enough on multiplication tables.&gt; The point of a computer science course should not be to teach popular programming languages, but to provide the student a strong grounding in the theoretical workings of computers and algorithms.

Why can't it do both at the same time?

C is enough for teaching low-level concepts like memory management and common data types. High level langauges like C# or VB are great for GUI design or a follow-up course on relational databases.

I'm not saying we shouldn't offer academic langauges like LISP, but they shouldn't be an emphais either.Not only that, but you can define mutually recursive values:

    evens = 0 : map (+1) odds
    odds = map (+1) evens

This will define two lists, named evens and odds such that evens is the list consisting of 0 followed by the list of odd numbers with 1 added to each, and odds as the list of even numbers with 1 added to each.

When you go to find the 5th element of odds for the first time, evaluation will bounce back and forth between the two lists in order to determine the result. Thereafter, it will be stored (until it's no longer needed, and reclaimed by garbage collection).[deleted]That's true, but I believe the article makes (an oblique) reference to the style of programming Haskell is supposed to encourage, typeful programming (for a definition, see &lt;http://en.wikipedia.org/wiki/Typeful_programming&gt;). 

Eg, it says:
"If you've misunderstood the algorithm, it is very likely that this will result in some type error when expressing it in a functional style, whereas if it is expressed in an imperative style the misunderstanding might take the form of an improper ordering of commands which cannot (in general) be caught. Haskell programmers will often testify that while working on a particularly complicated problem the type system helped them spot where their logic was flawed, and once the type errors were fixed so was their understanding."

But you can use types to avoid a lot of those semantic errors. To address some of your possible mistakes, you could've defined degrees and radians to be of different types (I believe you are allowed to define constraints on types using stuff like GADTs such that Degree would be the rationals between 0 and 360, and Radian would be an expression such that "the angle subtended at the center of a circle by an arc of circumference that is equal in length to the radius of the circle" or whatever); obviously then trying to use radians where you meant degrees would cause an error. Your HTML example is easily solved through lots and lots of types - I remember reading on Reddit a Haskell library for XML which could only generate well-formed XML, so I would not be surprised if there is already a HTML library which would not allow you to forget to prefix those special characters or not close the tag. As for division by zero, there are "Safe" libraries supporting total functions where division is a total operation and is defined when dividing by zero. (Similar considerations could apply to xx + yx and probably most of the others mentioned but I do not know enough about them.)I wonder if Photography professors argue about whether to use Canon, Nikon, Pentax, Hasselblad, Mamiya, Leica, etc.?Flying Frog has a bunch of ray tracing examples.  Here's the comparison of C++ and ocaml. Performance is about the same, but look at the differences in the code.

http://www.ffconsultancy.com/languages/ray_tracer/comparison.htmlIn practical terms, there are lots of specific laws which the components of functional programs obey.

One of the more important ones is map fusion. It states that:

    map f . map g = map (f . g)

where (.) is function composition. This is a simple example of a bidirectional rule which can be used practically to reduce the number of passes over the list structure.

There are actually quite a large number of such rules governing most of the standard library functions in Haskell. The number is greatly increased by the fact that Haskell is referentially transparent -- functions are evaluated only for their results and not for side effects.

Side effecting computations do actually obey algebraic laws, though they are considerably weaker in general. The category-theoretic notion of a monad formalises the basic laws which computations of this sort obey, though more can be proven about specific monads. (For example, presence/absence of continuations makes a fairly big difference.)I'm not even secret about it. There are just too many keystroke combinations I use in Windows - and have engraved in my muscle memory - that have no equivalents in the Unix desktop systems I've tried. (Mostly KDE and Gnome.)

And these go beyond just Ctrl-Esc to pop up the Start menu or Ctrl-F4 to close a window in an MDI program. How about arrow-keying through the tree view in Windows Explorer, hitting right-arrow to open directory, and then wanting to scroll the contents of that tree pane leftward to see the newly revealed stuff? Ctrl-arrows does it in Windows, but there is *no corresponding event to bind any keys to* in Nautilus.

Unfortunately, I think Vista is going to make me scream. I really don't look forward to having to choose between Vista and KDE.
I found the reverse, myself, though it differs on a case-by-case basis.

It's true that the error messages can be a bit bewildering at first, and many could use improvement. Helium is a cut-down implementation of Haskell which tries to give really thorough error messages. It's too bad that some of that isn't worked into GHC.

The first thing to pay attention to is the line/column number. This will at least get you looking at the right part of your code. Sometimes you'll know what the problem is right away, and that'll help you interpret the message for next time.

If that doesn't work, looking up the words used is a good bet, or better yet, come online and ask someone to help. :) On irc.freenode.net, #haskell is a great IRC channel, and there are always friendly people hanging around willing to help explain errors and help get something to compile. The mailing lists also work quite well.See http://www.cpjava.net for similar project.
(29 billion photons)That's a great idea. There's a lot of cool stuff one could do with a type-directed IDE.&gt;Give a man a fish and you'll "have" to feed him for a lifetime. Teach a man to fish and you'll only "have" to feed him in the meantime.  Not that the proverb says too much about your argument. 

The proverb actually isn't a part of my argument.&gt;Perhaps I should be less subtle:  Computer Science != Learning a programming language

Learning without application?  Why would you want to do things the hard way when there is no advantage in doing so?

&gt;in the course of studying for marine biology you'll probably handle a net or two to inspect specimens, but that clearly isn't the ultimate point of the course.

Why not?

&gt;The popularity of a language is largely irrelevant; what matters is how easy it makes teaching the students the concepts of computer science.

I thought you just said that computer science has nothing to do with learning a programming language.  Can I use an academic language without learning it first?It would be more important to have a system that could understand the difference between "water" and "rocks" than it would to have a system that could understand the difference between freezing in Fahrenheit and Celsius. (Just like that would be more important for a human child.) One is a cognitive model and the other is a relation of metrics on the cognitive model.

As an aside, here is my prediction. Google will not be the first with true AI as long as they require their programmers to try to build it in C++. And Google culture seems completely unwilling to try anything but C++. Which is good news for the rest of the people trying to get real AI before Google. ;)

I don't think your suggestion is tenable. Consider four objects and hases...

    A: 1 2
    B: 2 1
    C: 1 3
    D: 3 1

While unlikely, if it does occur you will never be able to store it in a hash table no matter the size. For this to work you must be able to generate new hash functions.
The KDE project announces the availability of the third development snapshot of the upcoming KDE 4. This snapshot is meant as a reference for developers who want to play with parts of the new technology KDE 4 will provide, those who want to start porting their applications to the new KDE 4 platform and for those that want to start to develop applications based on KDE 4. This snapshot is not for end users, there is no guarantee that it will be stable, the interfaces are subject to changes at any time.[deleted]I'd be willing to make the claim that mutable state is the source of most bugs. At the very least, the lack of highly-principled ways of dealing with state. Imperative mutation is a really dangerous model. (There are more declarative systems of specifying how state should change reactively, but these would be considered fairly fringe techniques by most programmers today.)

The reason that it's dangerous is because when you write imperative code which is manipulating a piece of state, you tend to think about how each piece of the code should transform a valid state into another valid state. However, you have to do this thinking at a very large number of places in the code, and it's not necessarily trivial. It's often easy to get things off by one, or in general to miss a case where valid state gets turned into unexpectedly invalid state. It's also often easy to get things which have side effects in the wrong order. It's easy to just plain forget to think much about it at all at some points, since there are so many points where it matters.

So you end up with lots of bugs. Moreover, they're hard to debug, because determining what's wrong involves recreating an often-complicated program state, one which is usually invalid in the sense that if your reasoning had been correct, it would never have occurred. Then you have to determine why it's incorrect if it is, or why the code isn't dealing with it correctly otherwise. Transformations after that which invalidated your state can cause it to be much harder to determine where the error really occurred in the first place, which might be far away from the first place that it can be detected.

The reason why global variables are bad is the same reason why state in general is bad.

That said, there are places where state is the right model. It's very good for things like simulation, and this is something which OO programming does a really good job of making obvious. However, it's not good for everything, and you really want ways to be guaranteed that it's not around when you don't need it.

This decreases your workload of thinking when trying to ensure that your code is correct. It lets you reason algebraically about your code in more powerful ways than you otherwise could, or barring that, just makes thinking about what's going on a whole lot simpler. It reduces the number of pieces of data which could possibly influence the behaviour of each piece of code, and makes those data explicit.

If you do need state for your application, there are some pretty good modern views of how to deal with it more declaratively, one of the most promising of which is something called functional reactive programming. This is an approach taken by the [Flapjax](http://www.flapjax-lang.org/) system you might have heard about, and research about it has resulted in the production of many Haskell libraries, [Yampa](http://www.haskell.org/yampa/) likely being the foremost of those, which you can see in use in [Frag](http://www.haskell.org/haskellwiki/Frag), a proof of concept first-person-shooter.
why dont they have a link to http://nehe.gamedev.net/? i find it a better site to get openGL tutorials from.Awesome. If I had performed the following substitutions, the comment would be at -10 by now:

- C++ --&gt; Scheme
- ALGOL --&gt; LISP (note: not Lisp)
- Haskell --&gt; Java[removed]GADTs don't solve that specific problem you refer to, however, construction of an abstract type which ensured values were in range by modding out by the appropriate bounds would. Even just newtyping a numeric type and deriving all the operations on it would be enough to keep uses of degrees and radians in the program separate.

There are still lots of things which aren't caught by the type system, but they are considerably fewer and harder to make mistakes than they'd be without it. Sure, there are still bugs in Haskell programs, but the article is trying to describe why Haskell programmers feel like their programs, to an almost unreasonable extent, "just work". This is an experience which almost every Haskell programmer notices at some point -- large programs will come together, and after a bit of fighting with the typechecker, the whole thing will just work on the first shot. While this sort of good-feeling moment happens in every language, it's something that happens almost unreasonably often in Haskell.&gt; I thought you just said that computer science has nothing to do with learning a programming language. Can I use an academic language without learning it first?

I said that computer science does not equate to learning programming languages, not that it has nothing to do with them.

When studying marine biology, I suspect the course would cover topics such as aquatic respiration, fish migration patterns, predator-prey relationships, and a great deal more things that I am completely ignorant of. During the course, the students will likely have to catch fish in nets in order to study them. So, catching fish is necessary to the course, but only as a means to an end.

What you're proposing, in essence, is that marine biology courses should focus not on studying the biology of marine life, but on the mechanisms of fishing rods, nets and all the tools of the professional angler. What I'm saying, is that whilst angling is a perfectly fine sport, it is not something a marine biology course should be primarily focused upon. Leave that to the courses on fishing.

Likewise, programming languages are merely a means to an end in computer science. Like fishing nets, they allow the student examine the practical effects of theoretical concepts. A computer science course should not be concerned about teaching popular programming languages, if they do not help the process of learning computer science. In this respect, Java is less useful than Lisp.

Finally, I wouldn't be worried about a lack of practical skills. For a skilled programmer, most programming languages are trivial to learn; it's getting the skill in the first place that is the hard part. A student with a solid grounding in languages like Haskell, Smalltalk and Lisp will find languages like Java and C# a walk in the park in comparison.Why would you want to edit code that your vcs is attempting to commit?  That doesn't make sense.[Done.](http://haskell.org/haskellwiki/Libraries_and_tools/Graphics#Ray_tracing)Having a "show source" button on the computer is one of the coolest things I've ever heard of.&gt;It would be more important to have a system that could understand the difference between "water" and "rocks"

The problem with this is the notion of "understanding." The description I give is less fundamental or profound, but it's also better defined: can the system take "32F" and also note that documents that mention "0C" may also be relevant and discover such facts on its own? This problem is particularly tricky because the most simple statistical correlations don't work so well: people generally use one temperature unit or the other in a document and not both. On the other hand, one may notice a spike in the words "freezing" and "ice" and "standard temperature and pressure" and chain through that, but then it gets muddy.If you just want to be a code monkey working at Grauenwolf, Inc. why get a degree at all?&gt; Everyone should be required to learn basic programming before they graduate highschool. 

Yes!

&gt; Suggesting that Computer Science courses need to focus more on those is like complaining that the Math department focuses too much on Differential and Integral Calculus, and not enough on multiplication tables.

YES! Although its nice when students have a basic grasp of differentiation and integration by the time they finish high school as well :)

Dude, you win reddit.f . g != g . f
This looks to me like a very meaningful ordering.Python is hardly used in the "real world" either.

Anyway, fewer students taking CS is a good thing. Less idiots, the courses can be taught at a higher level, and aimed at people who actually want to do research and meaningful stuff, not just crank out inane database-backed web apps. Any high school kid can do that.I'M GONNA GET DOWNMODDED AND I DON'T CARE!!Shoot, accidentally hit enter and now have no way of fixing the title!Nice article, shame about the viewer. If you click anywhere it takes you back to the first page!

Perhaps, but Factor is changing so fast so what students learn may or may not work in a month's time. If this was a seminar with a smaller number of students, perhaps they could cook up some library and contribute it.You can't expect every pair of operations to commute. You can't expect that in mathematics, so I wouldn't expect it here either.

However, you might note that the map fusion equation above is invalid in a language with side effects, as the side effects will occur in a different order. Thus, it expresses a certain kind of commutativity of application of functions to different values.

Similarly,  first f . second g = second g . first f, where

    first f (x,y) = (f x, y)
    second g (x,y) = (x, g y)

Operations acting on different parts of a data structure will reliably commute. This could not be guaranteed if the evaluation of f and g could produce side effects.

The point isn't that everything you could possibly want to be true actually is, just that you get more nice algebraic properties than you otherwise might have."In a purely functional language like Haskell the question of "ordering" is simply not meaningful."[removed]Well, that "ordering" is supposed to mean order of evaluation, not order of composition.

Of course, order of evaluation does mean a little bit with regard to performance of Haskell programs, but so long as it's non-strict, the semantics will be the same. An implementation is fairly free to choose any order that it would like, so long as the ordering gives the same termination behaviour (it's not allowed to stupidly evaluate infinite loops forever when the results aren't needed).The [Io programming language](http://www.iolanguage.com/) recently [switched to cuckoo hashing](http://www.iolanguage.com/blog/blog.cgi?do=item&amp;id=95) for slot and symbol lookup.When you bump into the same element you're busy inserting.[deleted]Computer science would certainly be a lot more abstract if computers didn't exist.

Nonetheless, acoustics students aren't required to learn how to play the piano.But doesn't the order of composition dictate the order of evaluation? Isn't a fold evaluation order dictated by the compositions that define it?Here's a PDF version:

http://kilana.unibe.ch:8080/SCG/uploads/330/haskellTutorial.pdfSeems like a useful algorithm to use when you want mostly fast insert but you don't care if the worst case really sucks. 

I wonder if you could handle (prevent) the infinite loop by extending the size of the hash table by n elements when the 1st and 2nd locations are filled, then take the modulo of the hash and n to get a position in the extended portion. You could repeat that indefinitely, (but then I guess you'd gradually loose fast inserts).*Lazy* evaluation, on its own, dictates an evaluation order, together with order of composition. However, the Haskell standard actually doesn't say that lazy evaluation is what takes place. It only guarantees non-strict semantics -- that is, that you end up with the same nontermination behaviour and results as with lazy evaluation. Due to referential transparency, any evaluation order which terminates produces the same result.

Compilers actually do change evaluation order around quite a bit from what's written by things like CSE, inlining, and application of algebraic transformation rules.

However, the important point in this discussion is that you as a programmer can safely rewrite things in a way which changes evaluation order without changing values, which gives you a lot more freedom in deciding how to express things, and vastly increases the number of algebraic properties that your code will satisfy.[removed]This tutorial uses OpenGL 2.0 and shows how to use framebuffer objects to achieve a bloom effect. (A bloom effect spreads out a light source so it brightens an image and gives it a “hazy” look ). A two-pass Gaussian filter is implemented. It also explains how to exploit hardware filtering to reduce the number of texture lookups.I don't think that analogy is quite correct. Arguing about which camera to use for a photography class is equivalent to arguing about which text editor you should use (which is certainly done, but not at the course planning level, I hope). I don't know of a proper photography analogy, but you can compare computer scientists arguing about programming languages to mathematicians arguing over which notation to use.If only it was still alive...&gt; Although its nice when students have a basic grasp of differentiation and integration by the time they finish high school as well :)

Those same high school students would probably love to have an honors course based on SICP, or HTDP. Something where they can get a taste of Computer Science. This would be the CS equivalent of calc 1: something that works well for both college freshmen and motivated high school seniors.Hatcher's Algebraic Topology is quite good.

I like the way he does homology with Δ-complexes, it simplifies the practical computation of homology groups quite a bit from the approach I see more often with simplicial complexes, by allowing for much simpler triangulations of spaces.

Some of the approach to homotopy theory would be improved by leaning a little harder on category theory though. Particularly in things like the Seifert-van Kampen theorem, which I think is most elegantly expressed by saying that the fundamental groupoid functor preserves pushouts. That approach gives one a profound sense of what pushouts mean in general, while making the theorem quite memorable. Of course, you still need to look at it on a practical level, understanding what a pushout is in the category of topological spaces, and in the category of groupoids, and how to compute them, but this isn't hard. (The specialisation to the fundamental group functor and pointed topological spaces is also very important, but again easily taken care of.)Ah, I see. I mentioned GADTs because I knew that some sort of abstract typing allows the needed constraints (because I've seen code that does that), but I couldn't remember the name! Well, I was kind of close. :)&gt; As we saw earlier, this type of mistake can quite often slide through the compiler's checks in an imperative language since for imperative code the important property of ordering is not checked. 

I think a lot more can be done with code analysis in this respect than we are currently doing.I think replacing raw numbers like integers and doubles with concrete types like lenght, volumne, etc. would go a long way to reducing those kinds of errors. But ultimately I have to agree with you, there are too many bugs that can never be caught by a language.&gt; Forgetting to put spaces around something in an output string? Perhaps you should be writing a higher-order function to do that for you instead of explicitly writing it out each time. Pretty-printer libraries typically have these combinators.

I agree with everything you said except this. There is no way any langauge tricks can prevent you from forgetting to include whitespace in a string.You could litter any imperative-style program (with a reasonably powerful type system) with scads of types to get the same benefits, but I've found that Haskell's syntax makes defining and using your own types so convenient you'd have to be crazy *not* to do it.

At the very least, starting with type synonyms and changing to more specific types later, then generalizing common operations across typeclasses.[deleted]I think it's the precursor, but not the future yet.

What I've noticed about the way I naturally use windows is that there are modes and they're quite tied to tasks. I often have a few big apps maximized and overlapping - I don't want to see one when I'm seeing another (eclipse). Some smaller apps, I never maximize, but often force on top above a maximized app, pasting to and fro (nedit). Some stuff I want several of, sometimes maximized, sometimes not (konsole). Some stuff I never maximize and avoid overlapping, because I'm dragging to and fro (gimp, konqueror file panes).

I suppose what I'm getting at, is that there's room for a *smart* WM. One built around both learning ("he always pulls nedit to the top", "one konsole is maximized, two are floating") and heuristics ("windows are tiled or floating or maximized, but they never overlap except when maximized; overlapping windows are useless when you want to see both", "the app that usually gets the attention starts in the big tile").

Edit: another way to put it... everything a user does to a window in a conventional WM is a hint that it ought to be *learning from*. And yet, most WMs immediately forget that precious training information! What a waste.&gt; I'd be willing to make the claim that mutable state is the source of most bugs.

I find that laughable. While it is true that most bugs are found in code that contains mutable state, it is like saying most bugs are found in code that is executed.

If you could really write a stateless program with no side effects, why would you ever run it? A highly optimized compiler would just reduce it to a constant.

The hardest bugs are not really about internal state anyways, they are about the state of the world. Things that are hard to code around deal with external interfaces like file systems, network communication, database access, and user interfaces. 
Python is a good choice, IMHO, but there is no need to use an interpreted language.  After all, the Python VM interprets bytecode, not the actual source._"..why do I have to see all samples on reddit? ;-)"_

The more information is easily available out there, the more we each have to filter it to find what is relevant to us. Myself, I think this is progress, that I can spend my efforts judging the usefulness/correctness of information from multiple sources rather than actually having to go searching or paying for the information itself.

Refactoring. I started out in C++ and have been using haskell for a bit now, and it's constantly amazing how much refactoring is simply moving some set of words from one place to another. Type inference means I don't have to repeat argument types. Being [pointfree](http://programming.reddit.com/info/12ew8/comments) means I don't even have to repeat argument *names*.Of all the things I pointed out, I thought this was the least controversial.  What I am basically saying is that assembling structured strings through basic unstructured operations is not the way to go.  With the standard Haskell pretty-printer library I might write something like:

someForm = parens ("body" $$ hsep ["a","b","c"])

to get say, the output of

    (body
      a b c)

and the library takes care of handling nested indentation and such.  But I don't have to worry that I malformed the basic structure because that's encoded in the combinators I used, which are typed.  Of course, this comes out as a huge win in a real situation.  Recently I wrote a parser and pretty-printer for a particular language specification (proprietary) in Haskell.  I made extensive use of Parsec and Text.PrettyPrint and it worked out great.  Everyone using Java just writes horrible grammars which are inflexible, and their "pretty" printed output does not deserve the name.  It's awful to read manually, not to mention the stupid bugs I've found due to mis-ordering of println statements.
Structure editors haven't been forgotten, they've just been conveniently misplaced until a suitably snappy buzzwordy name is conceived -- like "refactoring".
Refactoring is orthogonal to structure editing. It's just that in an environment where you directly edit text, the refactoring browser has to parse your source into an AST first, and unparse it later.

In fact pretty much anything you can do with a structure editor can be done with text, just more awkwardly. Structure editing appeals to me because the user cannot introduce syntax errors, and it becomes easier to implement syntax-aware editing operations.[deleted]You are thinking about the wrong class of strings.

Print "You're order was processed on" + orderDate + " and will be shipped within three days"

There are three bugs in that one line, can you spot them?
If you can teach with an language you love, in a subject matter you love, everyone usually benefits.  A few teachers can fake that enthusiasm, but it's always better to have it genuine.  

In other words, consider using Factor.  I haven't looked at it yet.&gt; Why can't it do both at the same time?

Because popular programming languages tend to cater for the lowest common denominator, and in terms of programming theory, are still stuck in the 1970s. They are also rather homogeneous, and ill-suited for teaching concepts outside their scope.

For instance, if you wanted to teach a student about ASTs, or functional programming, I think you'd struggle if you were using C#, whilst Lisp would be a far more apt choice.Perhaps you didn't intend to address this reply to me? Its worded in a confusing way:

&gt; If you can teach with an language you love, in a subject matter you love, everyone usually benefits.

Hey, I like Factor, I am the lead developer after all.

&gt; In other words, consider using Factor. I haven't looked at it yet.

Samuel Tardieu is the one who wrote the article we are discussing; I'm not Samuel Tardieu and I'm not a CS teacher. :-)&gt; It depends on what you're rendering, but if you're rendering for movies, say, then the scene database (an abstract concept because it refers to widely distributed data) will typically be larger than fits in RAM.

To clarify, my remarks were made in the context of a hypothetical hardware ray tracing architecture for real-time graphics, as in video games (my field). Even streaming all the geometry data from the CPU to the GPU on non-unified memory architectures, like the current PC's, would totally destroy performance; typically you can only afford to have a fairly small handful of dynamic vertex data uploaded each frame.

I do agree about the cache characteristics of ray tracing as compared to rasterization. In a rasterization architecture, when rendering a set of triangles submitted by a draw call, you tend to have good coherence: a small amount of textures being sampled in a locally predictable way, spatial locality in the frame buffer, etc. It's hard to beat.C# is obviously not suited for teaching functional programming, but I have doubts about ASTs. In fact I think LISP may be a poor choice because the raw code in LISP essentially is the AST, which prevents the students from considering design questions about how to represent the code.
I'm aware of structure editing, I'm just pointing out that the Java crowd suffers from a strong case of NIH.
The grammatical mistake, the missing space, and the missing period?  Assuming orderDate is correctly typed.  That's still missing my point.  Technically the grammatical mistake could be eliminated, after all, natural language grammar is quite formalizable; it's just usually considered too much of a burden to do so.  The fact that you talk about a "class of strings" leads me to find the underlying generalization used to generate them.  Constructing these strings like this, concatenating willy-nilly, is not a very good idea.  At the very least, think of i18n.

If you must put strings together like this, I definitely prefer a printf or format (even better) style control string.
I'm curious if you have benchmarked this in practice against a decent open-addressed hash table implementation, like [Sean B's](http://www.nothings.org/computer/judy/hash.c). The Wikipedia page says cuckoo hashing beats chaining, but that's not saying very much--what kind of performance-conscious programmer would use an externally-chained hash table?

In any case, very cool algorithm!Mozart/Oz, because it will have multiple paradigms, like OOP, functional, logic, constaint and concurrent, and the language will be built formally, starting with kernel level semantics, so that students will be able to reason about the programming.

Or Scheme, with and approach towards concurrency. The same flexibility wrt paradigms is available, except that students will built an interpreter instead of reasoning around a kernel language.

Additionaly, those 2 language communities  have accumulated a huge amount of pedagogical experience and material (such as the classics, CTM, SICP and EPL.) Also, they have environments that are very mature and multiplatform.Add in internationalization, it doesn't change the problem.

en:OrderMessage "You're order was processed on{0:d} and will be shipped within three days"

Print OrderMessage, orderDate

I know stuff like this isn't sexy, but end users care. Hell, some end users care about tables widths being off by a single pixel. I have lost count of the number of hours I have spent digging through code, database tables, and resource files in an effort to find and correct minor cosmetic issues.
What a fantastic find!IE under 50% in several countries? I doubt it. Many people used Firefox in the first place because of the tab browsing feature and the popup blocker. Now that IE7 has these and is pushed to the clients via Windows Update, my guess is that it will be harder to for Firefox and Opera to gain a lot more market share. What could an average user possibly see as convincing enough to switch? Security, maybe? Web standards aren't that sexy to most people...What would be *really* cool is if he put up the vc repository for the weekend, so we could see the evolution of the program for each of those images.This is because VB is a very functional language and IDE (more functional than Haskell :) ), and very easy to get into. So you get a lot of people who don't know much about programming or computers coding in VB and they manage to produce lots of functional software.
So the real reason VB programmers don't get any respect, is that VB is such a great, functional language and IDE.[deleted]That is not correct. The halting problem means that you cannot write a program which takes an _arbitrary_ program as input, and determines if it halts.

Most people misunderstand the halting problem and its implications, talking about "code paths" and the like.Muddy indeed! ;)

I don't think that way of doing AI will ever work. You are welcome to try but I think you will be wasting your time. Emulating "understanding" has to serve as the base of any real AI system, even if "understanding" is somewhat poorly defined.The point is that code which has side effects should be kept to a *minimum*, not that it should be eliminated altogether.

I'm counting pure functions in "programs without side effects". In some sense, a pure function is a constant, but it's a rather interesting constant. Applying it to various values can produce results which solve problems.

Armed with a REPL, you have all the side effects you need to solve simple problems with pure functions and values. It may not be something you'd give to a customer, but it might be able to do all of the real important work of your application. You can usually write the UI separately once you solve the real computational problem.

You should note that I'm including the state of the world in my sense of the term "state". Things which depend on and have side-effects in the state of the world are just as bad as those which mutate program-local state parameters.

Being able to write code which you know is not affected by that world makes a huge difference in terms of your ability to reason about its behaviour and correctness. You no longer have to consider the whole world, only a comparatively "small" space of inputs and outputs. This of course extends to other functions and values which you use in building it up. (Or else you really would have to worry.) 

Debugging/testing is comparatively simple, because you don't have to recreate a program state, you just have to know which inputs you'd like to test, and you're guaranteed that nothing else could have anything to do with the results. For the same reason, just thinking about what the code is doing is made simpler. It might still do something complicated, but at least there are some limitations on where it's getting things.

Your whole program might not be possible to write without consideration for side effects, but the larger you can make that pure part, the simpler it is to manage your program, and the smaller chance you have of writing bugs.You're missing the point, I think, grauenwolf. Your example is unstructured, and simply typed. So as you've not encoded the problem you're trying to spot in the type, there's no way to try to detect the problem as a type error.

Simply put: you're not leaning on the type system enough.

It might be surprising, but at least the problem of the missing space and missing full stop are routinely prevented by construction (in Haskell, anyway), using pretty printer combinators to build up an abstract structure representing well-formed text. Here's a complete program:

    process t = sentence
        [text "You're order was processed on"
        ,date t
        ,text "and will be shipped within three days"]

Which when run, produces:

    *Main&gt; main
    You're order was processed on 2007-02-24 and will be shipped within three days.

The key is to embed the problem in the type. Here, using a Doc type to describe sentences with enough structure.

Detecting a spelling or grammatical error is harder, of course. You could build an AST representing a natural language, and apply similar techniques as above. More likely, you'd use a spelling checker. If it was really mission critical, it might actually be worth using a set of grammar combinators.

The lesson is: don't be satisified with bugs! For those you really care about, use your type checker.

----

grauenwolf's example problem:

&gt; There is no way any language tricks can prevent you from forgetting to include whitespace in a string.

makes me think that the techniques for preventing these kinds of bugs on construction need to be more widely publicised.

The above techniques are *really* quite well understood, so  its surprising that there are people who still think this is a problem that type systems won't catch.

The problem is really that  people don't utilise the type system *enough* (or know when they can use the type system for this kind of problem).

For another good, practical example of leaning on the type system to prevent a common class of errors in unstructured, simply typed data, I recommend Tom Moertel's [string escaping/injection solution](http://blog.moertel.com/articles/2006/10/18/a-type-based-solution-to-the-strings-problem).Yeah, but some languages (eg, VB) are more prone to bugs than others.I think you meant the opposite of what you said. (?)I was wondering about that comment too. makefiles never really seemed to be *that* big of a space or complication issue. Python is a good choice if you want something that is relatively easy to learn and study from though.Yeah, fixed. You know I only threw VB in there to boil grauenwolf's blood. I've never even used VB, just like grauenwolf has never used Lisp but likes to talk about it.&gt; There is no way any langauge tricks can prevent you from forgetting to include whitespace in a string.

Yes there is. Think outside the (basic) box.[deleted]Those bugs are caught by unit testing and similar techniques.

In Haskell, you have QuickCheck -- it uses static type information to generate unit tests.&gt; Everyone should be required to learn basic programming before they graduate highschool.

What in world *for*?  

Please.  Teach kids to balance a check book (by hand), fix a flat tire, change a diaper, and speak to adults without using the words 'cool' or 'awesome', before teaching programming.Slava, I find it odd that you can know the formal definitions of so many things and yet still have a complete mental break down when the concepts are presented using different terms.

I am well aware of what the halting problem is. I spoke of code paths because Blaidd_Dwrg, who despite having a link to the definition, didn't seem to understand what it really meant.

Teaching often involves dropping down to simpler, less correct definitions in order to get the point across. How can anyone understand the deeper implications of the halting problem before they understand that it is about one program analyzing another?

On retrospect I realize that I probably should have been a bit clearer in pointing out that, unlike the halting problem, this problem has the advantage of a constrained domain.
For problems where you've decided not to encode the problem domain in the type, the preferred technique is to use QuickCheck, which will generate tests based on the type.[removed]Yes. That's exactly the technique: encode more information about the problem domain in the type. Then you can lean on the type checker.

A recent example I used at work was to represent compressed gzipped strings in memory, with a new type (so not String, but GZString).

The type checker then found for me all points in the program that required a call to unzip, before displaying the string.
If I had not encoded GZString as a new type, those strings would have been displayed as binary garbage.

*When in doubt, and if it matters to you, encode the problem domain in the type*Well, at least it's getting proper lambda expressions and LINQ. VB.net seems to be turning things around a little bit.

Erik Meijer and friends are sneaking functional features into Microsoft's languages, which I'll take as a good thing. I wish he'd talk about Haskell a bit more positively though. You'd think someone with their name on the Report would be a little more enthusiastic about it. I sometimes wonder if he's contractually obligated to downplay Haskell in public. ;)It's not a secret :p

Right now I am trying to install the Flash Player on my Ubuntu x64. After a dozen different things were tried, I gave up.

All because I wanted to watch YouTube videos while Windows Vista doesn't finish downloading.[deleted]It didn't work. I would suggest trying to insult something I am actually passionate about like declared variables. Sure I like VB, but it isn't anything to get upset about. 

And for the record, I have used Lisp enough to find some of its more glaring warts. I suggest you do the same with VB, if for no other reason than it will give you more ammo to use.
On a per-line basis, the kind of dense code you get from functional style programming is more error prone.

But when looking at the program at a whole, the more lines the greater the chance for errors.

I suspect there is an point where making the code any denser will result in more errors than the reduction of LOC can compensate for.
The approach outlined here would work, but I've found hardware problems without looking just trying to 'make bootstrap' gcc. Glibc is pretty rough too.&gt; On a per-line basis, the kind of dense code you get from functional style programming is more error prone.

I suspect you cannot back up this claim.I linked this on the main Reddit because I thought with all the talk there about how bad Reddit has gotten, it is relevant. Just wanted to say that because I don't want anyone saying I 'stole' mallipeddi's link.

Good link, thanks.Easy, the more logic you have one one line, the more things that can go wrong.

Consider the simplistic exmaple from C:

    if (a = b == c)

Did the programmer really mean to assign a the value of b and then comare the results of b and c?
Or did the programmer really mean to compare the values of a and b, then compare that result to c?

    if ( (a == b) == c)

Or did he mean this?

    if ( a == (b = c))

If the programmer used two lines, his intent would have been clearer.

    a = b;
    if (b == c)

But of course if you follow this path too far you end up with assembly, which is clearly more error prone than higher level langauges.

I do believe that the functional programming languages we are seeing today do allow us to safely write denser code. But I also believe that some people are going to try to take it too far, just as the C programmers did when they allowed assignments in conditionals.

I'm not sure you can really construct a system that knows the syntax of a sentence well enough to be useful to a programmer. Consider MS Word's grammer checker. I know that MS's money cannot buy everything (like a (bleep)ing search engine for MSDN that actually works), but it does suggest the problem is still out of our grasp....more syntax?

Reminds me of that Swedish joke:
"What does Norway have that Sweden doesn't?"
"Good neighbors."

He does have a point about playing nice with foreign code. But I feel that many of Lisp's problems come not from that, but from having too many Lisps, and being too closely associated with AI, historically. Dunno about Smalltalk.[removed]Completely false.   Any good shop will run profilers on the code, and eliminate hot spots.   In games you typically do this (but not until development is nearly done because this will make code much harder to modify) until there are essentially no hot spots, everything takes as much time as everything else.   

At that point you decide minimum requirements based on how the program runs on minimal machines.   If your minimum requirements are above what the market will bear, then you have to cut features (make your graphics less detailed for example)

Limiting the speed of the dev team's machines is counter productive - it slows down development while devs wait for the compile cycle to complete.   Worse, you devs will be tempted to take things out that would work just fine after optimization, because it is too slow now.   Programming 101 is NEVER optimize code that is not done.Yes. HyperTalk as a language rocked. Sadly, the whole thing got Claris'dJust don't say "check out this logic bomb" on the plane.[deleted]So, functional programming is better because returning the sorted list to new variables is better than sorting the list in-place?

How is this specific to functional programming, again? I can do this in imperative languages too; the only difference is that I can't use variable names before they're first assigned (or, in C and related languages, declared).Wouldn't it be when you bumped into it twice?You missed the key points of the article by quite a margin.I agree about teaching C for low-level concepts, and maybe for a course strictly about GUI design (i.e. user interaction, mental model, fitts' law etc.) some .NET language (because it gets things done), but most imperative languages are trivial to grasp in comparison to academic languages and hardly need to be taught in colleges.I suppose that has its advantages, but sometimes I like to know exactly what is happening in some code, without having to look it up in the class definition (although I suppose this is  a bit of a cop-out - may as well not having functions...).For the same reason they teach high-school math even though 95% of high-school graduates would never need to bother with Calculus: it organizes and keeps your brain in shape.Aside from AppleSoft BASIC, HyperTalk was my first language._Yes_. HyperCard was where I got my start with 'serious' programming (I'd done TI 99/4A Basic before that, but not to any real end).  My best friend &amp; I would sit around playing with various HyperTalk features and just having a wonderful time.  It was a very cool toolset, especially for the time.  I often think that if it had been Free Software the whole scripting language revolution would have come a decade earlier, and so we'd probably all be using Lisp by now...Nice article on how quality software developers can differentiate themselves from the $7/hr rent-a-coder hack.&gt; Easy, the more logic you have one one line, the more things that can go wrong.

By that argument the bugs-per-line of something like assembly language would be just about minimal. Do you see how absurd that claim is? The amount of stuff happening per line is only incidentally related to the bug density; there are much more relevant issues.HyperCard was my first start with programming also.

&gt; and so we'd probably all be using Lisp by now...

Heh. HyperTalk is about un-Lisp as you can be while still being dynamically typed, garbage collected and relatively high level. Almost no data types, unorthogonal, complex syntax, and crude object model. But hey, when you're 10 years old, it's fun!With Ruby, extending the class definition within the scope of your project really isn't a big deal. In this case, it's the same level of complexity as having a bunch of helper functions as you suggest in a separate source file. 

I would argue that the benefits to this approach are the fact that your code is much more readable and intuitive, you simply call to\_X where "X" is the units you want, rather that futzing about with passing constants to a conversion function. If you add a new unit type (call it foo), you don't need to go and write extra specific functions, the method_missing function will automatically pick up and correctly convert a to\_foo call.

Extending Numeric in this fashion also allows you to chain multiple unit conversions and perform calculations on different units, eg (3.lbs + 10.ounces + 400.grams).to\_kg. 

It's a somewhat contrived example (maybe not if you were calculating shipping weights or something?), but imagine how unreadable that would be in comparison if you used conversion helper functions.&gt; Structure editing appeals to me because the user cannot introduce syntax errors,

One issue is that it makes it impossible to go between two syntactically valid texts through a sequence of intermediate texts, some of which may have syntax errors. I've only played around a little bit with the Lisp structure editor, so maybe that is a non-issue in practice.My guess:

Obtain a list of all computer languages.

Find a large sample of all computer programming jobs.

Rate each language by the number of times it appears in the job listings.

Arrange your language list by rating, in descending order.

An "academic" language is one that appears below some arbitrary cutoff point in your ordered list, where this language has only ever appeared in the academic section of any such list calculated in the past.
An academic language is a language designed by academics to experiment (and teach) new programming concepts (e.g. Haskell, Scheme).[deleted]All these years later, I still can't quite get over how much potential HyperCard embodied, and how it should never have been a dead end. Of course, it wasn't, in a way. Arguably the modern Wiki is a direct descendant, and I suppose it was one step along the way to the web we use now...

I know there have been a lot of efforts over the years, and there are several commercial products available, but does anyone know of a free HyperCard-alike project that's come close to a finished version?
Home Based Part time Online job earn an unlimited dollar per month , 
Looking for a Real Work At Home Job?
Get paid For Completed Work..
No More relying on commissions
Would you like to earn money from home for completing various clerical based projects and assignments from home?
You are not required to sell, or recruit but are simply paid to type ads and other media!
Please, Fill up the below information to join now and  further information  

Name : 
E-mail Address : 
Phone no:

Call : 2390177 / 9841206820 
please SEND THIS MAIL TO your OTHERS FRIEND TO get more information about the job 

 Click this site for more information and know how to earn a $$$$ money by online  home job and Click the link below and Start making Money by online. 

Click IT HERE : http://www.maxjob.biz/idevaffiliate/idevaffiliate.php?id=1292


Click this site for more information and know how to earn a $$$$ money by online  home job and Click the link below and Start making Money by online. 

Click IT HERE : http://www.maxjob.biz/idevaffiliate/idevaffiliate.php?id=1292

Discussed here:
http://cs.uni.edu/~wallingf/blog/archives/monthly/2007-02.html#e2007-02-23T19_58_32.htm&gt; I often think that if it had been Free Software ...

I've thought that if it had been open code and (of course this an enormous "and") if there had been some kind of network model in place early in its lifecycle, the history of the last two decades would look quite a bit different.Just following the thread, sorry for any confusion.&gt; Why would you want to edit code that your vcs is attempting to commit?

I edit it in an editor, dear.Are you being serious? Or just talking crap to stir up the pot? I'm being serious.&gt; I'd be willing to make the claim that mutable state is the source of most bugs.

I'd be willing to extend it further to 'I find it difficult to find a bug that isn't (caused by mutable state)'There should be at least the audio at some time according to their [A/V recordings page](http://us.pycon.org/AudioVideoRecording/HomePage). It should show up on the [talks page](http://us.pycon.org/zope/talks/2007/fri/track3/004/talkDetails).Perhaps we could use a new term like "Open Development" for having bug tracking, version control and documentation public?I think everybody should just stop talking about Vim7 tabs in beginner's tutorials, they are wrongly named (a better name would be "layouts" or "perspectives" a-la Eclipse) and are going to confuse people.

Instead put this in your `.vimrc` and use C-TAB to quickly cycle through the open files:

    set hidden
    map &lt;C-TAB&gt; :bn!&lt;CR&gt;

Another nice mapping is this:

   map &lt;leader&gt;e :e &lt;C-R&gt;=expand("%:p:h") . "/"&lt;CR&gt;

which allows you to open a file in the same directory as the one you're editing with `\e`.While I appreciate Scala and I'm looking forward to plug it in the bulk of Java code we have, I'm not sure statically typed languages are that nice to use in a web application.

I never tried writing a real web app with a static language with a sane type system, but I feel I just find myself more at ease with writing those things in Python. Web apps are rarely complicated stuff, after all, it's just plumbing...Wasn't Apress supposed to be republishing Graham's On Lisp a couple of years ago?Well, you already know the first time; why wait until the second? :)&gt; Slava, I find it odd that you can know the formal definitions of so many things and yet still have a complete mental break down when the concepts are presented using different terms.

The difference between the right term and almost the right term can be the difference between "lightning" and "lightning bug", as the saying goes.

&gt; Teaching often involves dropping down to simpler, less correct definitions in order to get the point across.

Right, but there's a world of difference between helpful [lies-to-children](http://en.wikipedia.org/wiki/Lie-to-children), and just plain falsehood:  what you said about static versus run-time analysis has truly, absolutely nothing at all to do with the halting problem whatsoever.&gt; Many years ago there was a computer magazine in the UK called 'SOFT' and it ran a series of articles on how to write a Pascal compiler as an embedded sublanguage in FORTH. The magazine soon went under (it assumed intelligence of its readership so it didn't stand a chance) and I lost the article. It had a big influence on me and I've been trying to track it down ever since (for nostalgia value). I think this may be it!

This an adaption of the "Let's build a compiler" series, which was originally written in Pascal. The port to Forth was written in 2005:

&gt; Jack W. Crenshaw wrote the Let's Build a Compiler article series from 1988 - 1995. This document is a formatted version of that excellent non-technical introduction to compiler construction. These web pages were created in 2005, and port Mr. Crenshaw's original Pascal code for the 68000 under SK*OS to the Forth language on a 80x86 CPU, under Windows XP.Working in Hypercard was pure pleasure.

My first big job as a consultant in NYC was the creation of an interactive data-reporting tool for upper management at HBO. I implemented it in Hypercard with a few XCMDs for database access.

When I was in college, my girlfriend at the time implemented a really nice educational application--she was a T.A. in astronomy--in Hypercard. It controlled a videodisc player. She needed occasional help from me, but basically did it herself. And she enjoyed doing it.

I think it would make an excellent "first programming environment". It would have been a reason to buy a Mac, in spite of how I loathe Apple these days for their crappy software design. Bring back Andy Hertzfeld!Another approach for the latter is:

    cnoreabbrev CD &lt;C-R&gt;=expand('%:p:h')&lt;CR&gt;Agreed:

http://feather.planetapache.org/?p=46Absolutely![removed]Well, I wouldn't advise teaching Lisp in isolation, but as you say, Lisp code is essentially just an AST written out in text, so I'd be inclined to think that Lisp would provide good practical examples of how to manipulate ASTs.

If the student needs a more concrete example of how to map a C-like imperative language to AST, then a simple made up language could suffice, or perhaps ECMAscript, as that has a relatively simple syntax.&gt;"SELECT COUNT(*) FROM event WHERE response_time &gt;= NOT VERY SPEEDY".

Criminy, never "select count(*)"....no wonder your response_time is not very speedy.  Sheesh.[pythoncard](http://pythoncard.sourceforge.net/) might be worth looking into, if you cant find the required emulators for reliving the glories of hypercard.. .[deleted][deleted]You need to g'tee that the hash function doesn't cause a loop somewhere else.

Once you've done more than the number of items in the table you're certain there must have been a loop in there, even if it's one that doesn't include the first element.No.  Not if we ferret out the imposters."I don't think perl is a language that is designed for maintainability largely because it isn't designed for legibility and legibility is the major factor in maintainability."

Of course Perl wasn't designed for maintainability.  It was designed for throwing together quick little hacks and scripts.  That's what I use it for, and it fulfills its purpose wonderfully. Why people insist on writing large systems in Perl, and then blaming the language when they can't, is beyond me.Erlang is a good start. Still not perfect, but a good start.

BTW: Why is it that most of the interesting languages can't be compiled into a simple executable, without any VM or separate runtime?
Sounds as if this was an ideal interview rathan than one "from hell".  In a short period, the person was able to tell just how insane management was and how awful it would have been to work there.  No job offer == success!

A truly hellish interview would be one that was otherwise normal, but when you showed up on Monday you were greeted with, "Just saw this great movie over the weekend, and it gave me some ideas... we're going to be changing our cover sheets for our TPS reports..."&gt; That's a seriously nice hack.

That seems almost too elegant to be called a hack.I don't know about this Forth version, but the Crenshaw tutorials were pretty decent. Not exactly state of the art, but a good introduction for hand-written recursive decent parsers and similar techniques.

Speaking of small compilers, [Compiler Construction](http://www.oberon.ethz.ch/WirthPubl/CBEAll.pdf) by the one and only Niklaus Wirth is available for free now. Great book.&gt; Web apps are rarely complicated stuff, after all, it's just plumbing...

Dealing with large-scale web apps with many users that serve content in many formats and through many protocols doesn't seem all that uncomplicated to me, but I never worked on one, so what do I know.Maybe this conflicts with the work on "Practical Arc".
Feevy is a dynamic blogroll, which allows you to display content of your friends ’ blogs with a simple javascript. As their blogs get updated with new posts, their last post appear automatically on top of your Feevy. (+ feevy: http://www.feevy.com / blog: http://blog.feevy.com )Very much so, it was my first introduction to programming.I have. The code itself is not complicated, most of the times just the infrastructure is.

But the world is not only about large scale, most of us are building little or medium web apps for customers (well... I'm not strictly building web apps for a job anymore).

However, I'm a bit sad by the downmods. I posted the comment to get hints and discussion on why I would be wrong, why I wouldn't end up fighting with the type system while trying to map my relational database to algebraic types.What do you mean? SELECT COUNT(*) is massively optimised.I've read my share of Perl criticism, but this article isn't worth the electrons it's written with. I should've been  warned when the author mentioned that he hasn't used it for years in the beginning, following this quickly with how much Code Complete (The Captain Obvious book of programming) influenced him.

Perl has an emphasis on "There's More Than One Way To Do It". But saying that this isn't the case in other languages shows a severe lack of real world experience. Even in syntactically poor languages like Pascal or Java you'll find that you can solve problems with several solutions. Most languages have more than one loop variant and when it comes to selecting from the plethora of algorithms, the language itself (almost) doesn't count at all.

If I want quickly dashed-off rants, I'll read blog entries, thankyouverymuch...Well, the entire premise of the word "hack" is that there's only a hair's breadth of distance between "stunningly beautiful" and "stunningly wrong."

I think this qualifies as both.

And now I greatly desire an Emacs haskell-mode with Djinn built in, so I can type "M-x write-program-at-point RET".Some noticed that Simonyi's "Intentional Programming" had an [astonishing resemblance to Interlisp](http://wiki.alu.org/AudioVideo#ip).

Note that Simonyi's fellow cofounder, Gregor Kiczales, worked on Lisp's Metaobject Protocol. (Kiczales apparently left the company fairly early on...)1. Think of the hash function as a (family of) PRNG, and the key to be hashed as its seed. You change parameters to the PRNG (the Initial Vector, for instance), and you obtain new hash functions from the same family.
2. The original paper discusses appropriate families of hash functions, and some practical choices.Funny thing is I think the interview seems totally reasonable and they made the right decision not to hire this guy.The original paper linked to in the article is very good and it surveys the expectancies rather concisely.&gt; the interview seems totally reasonable

What do you find reasonable about 'buzzword bingo'?Nice, but very close to MozEx.

It would be **really** nice if you could edit **all** fields on a page at once, using some kind of MIME-headers ("Field-X: Value", "Field-Y: Another Value") to make them editable in a single editing-session.

For example, let's assume a page with fields "name", "first_name", "street", etc. Then invoking the plugin could create a tempfile prefilled with

    name: 
    first_name: 
    street: 

(I know that writing back the parsed fields into the corresponding fields on the page is less than easy with the current FF-API, sadly. Even MozEx has the problem that it is required to click somewhere on the page before changes are recognized.)uhm... I think I now got what you meant. With "I never tried writing a real web app with a static language with a sane type system" I didn't mean "I never written a real web app", but "I have written web apps only with PHP, Python and Java".
Just because there's more than one way to do it doesn't mean every shop shouldn't decide how _they_ do it. Get a copy of [Perl Best Practices](http://www.oreilly.com/catalog/perlbp/) and write down your guidelines.&gt; Even in syntactically poor languages like Pascal or Java you'll find that you can solve problems with several solutions.

Perl just claims it as a feature, the other languages are closet-TIMTOWTDI.Plumbing is the right metaphor. It's often the case, that web applications need to be connected to some "old pipes". Not everything can be accessed with REST or similar lightweight constructs. So Scala's ability to seamlessly integrate Java will probably pay off for most enterprise systems. The usual throw-away MySQL web app won't benefit that much from it. And note that this could also be done with Jython and JRuby.

I doubt that the dynamic-static dichotomy is that interesting for lots of web apps. Verbosity is the bigger factor and in this regard, most dynamic languages just throw less sticks in the programming wheels.

But static functional languages are a whole different breed. Writing something in Scala or Haskell -- despit it being statically typed -- is way more concise than writing the same code in Java or (chance forbid) Ada.that is pretty funnyI loved HyperCard.  I helped develop an amazing system at my first job.  It was an in-house software development environment.  I hesitate to call it an IDE, because it wasn't.  It had version control, release management, document management, bug tracking and an incredible amount of elegant programming.  Not that I think CMM is a great idea, but it was officially a CMM Level 3 project, it was also a tool which enabled several other Level 3 projects at our company.

One of my "smaller" tasks there was to develop a training / help system for this environment in HyperCard.  In several months of intensive programming, I had created a work of wonder.  An interactive, hyperlinked, training application which was well organized and easy to maintain.  I had fallen in love.

At that time, I knew C, C++, Lisp, (Object) Pascal [yes, there was such a thing], VAX Assembly and BASIC.  I was no stranger to programming.  But, I had never seen anything like HyperCard.  It was waaaay ahead of it's time.  Merging a GUI environment with OO style message passing in an easy to use scripting language produced amazing productivity for certain types of applications.  I still believe I could create some GUI applications faster with HyperCard now than with any other tool that has been developed since.

I can't tell you how many times I've read that if Atkinson had simply thought of making HyperCard networked, he would have basically invented an Internet-like system / browser.  Only it would have been much more programmable.

PowerPoint should never have existed - HyperCard could easily do all of that (and more).  It could have evolved to be as ubiquitous and multimedia capable as Flash.  It was really great technology that shouldn't have ended the way it did.  It had influence on what came afterwards, but some of the spirit of what it was has been lost.  There was some genius behind it, that much is for sure.

Oh dear. Would that be because the query execution engine has to go to the data dictionary for every lookup?

We took that relic out behind the barn and shot it. Ten years ago. Sorry. I know we said it had gone to a new home in the city, but actually it's buried in the long meadow.

Also, we can optimize common paths these days! aren't we clever?Great resources for IT folksYES! Exactly what I was thinking! If the interview goes this badly, be glad you found out how terrible the place was, early on, and run for your life. The only bad thing about this is the time wasted turning up to the interview!
I've always sensed a lot of hypocrisy in the "open" source movement, right from the beginning. I realise the stated goal of it's advocates was to make free software(GPL) more palatable to business types. What this seems to mean is the right to take the work of others at zero cost and use it to create barriers and charge tolls.

Free as in freedom software is much better than "open" source software.Linux for servers
OS X for desktops
Windows for games

Any questions?[deleted]For those who don't know it: the last frame is an alusion to [Pinky and The Brain](http://en.wikipedia.org/wiki/Pinky_and_The_Brain).Your resume drives your compensation, your marketability and the effectiveness of your job search. Without much ado, here are the rules of engagement for writing highly effective killer resumes for Information Technology jobs that gets you on the fast track to career nirvana.Concurrency != parallelism.&gt; Nice, but very close to MozEx.

Yes, the developer says so in his post and explains why he developed this anyway.

&gt; It would be really nice if you could edit all fields on a page at once

I think the [Scribe](http://prismicspiral.com/scribe/) extension does that.&gt; I have. The code itself is not complicated, most of the times just the infrastructure is.

Well, you'd have to take the infrastructure into consideration when coding, wouldn't you?

Anyway, I think the biggest problem is that the type system of Java isn't that good, and the syntax is ridiculously verbose.

&gt; I posted the comment to get hints and discussion on why I would be wrong, why I wouldn't end up fighting with the type system while trying to map my relational database to algebraic types.

Scala has classes. Use them when appropriate.The bureaucracy is expanding to meet the needs of the expanding bureaucracy.Haven't seen bilinear filtering used to minimize texture lookups before, that's neat trick to add to the toolbox.&gt;I'm not sure statically typed languages are that nice to use in a web application.

Why?  Can you be specific on why its bad?&gt; An academic language is a language designed by academics to experiment (and teach) new programming concepts (e.g. Haskell, Scheme).

Except that I often hear erlang called "an academic language", and god forbid I actually try to talk about erlang to other people...

I think Slava's definition is much better.Does anyone know why Squeak wasn't used for the OLPC?  It seems they are reinventing the wheel pretty heavily with the feature spec listed in the article...&gt; Why don't the academics design a language that specializes in doing IO so that IO operations are scalable, robust, failsafe, transactional etc.

See Haskell.
[deleted]Here's my quick and dirty 30 minute implementation that _seems_ to work. The operative phrase being "quick and dirty". But it might still be helpful to someone who wants to see it in action.

http://paste.lisp.org/display/37318I'll believe that we've figured something out about the game when I see a computer beat a 1 kyu amateur. It'd be nice if the article had any real details on what the algorithm is doing, how well it did against amateur players, or at least a reference to a paper.| ...should we keep the "if you're selling a closed source product, you don't belong at the Open Source Convention" policy we've had so far? 

So, no talks from anyone from Apple, IBM, Oracle, Microsoft, Sun, or any other companies that sell closed source software and are now dabbling in open source?  Go for it.  Let's see what your sponsors say.[ViewSourceWith](https://addons.mozilla.org/firefox/394/) is what I use.Awesome, I'm using it to write this comment. Works like a champ! 
Thank you,

Ben[deleted]&lt;rant&gt;Any good (highschool)programmer can replicate anything in a weekend. I don't know why it is so costly to develop software. Programmers/software engineers/software designers/software architects/etc. should be paid a lot less. They are not very good at what they do. Architects can design a building/bridge/etc. and then hundreds of people can work together to make something that will stand for many years whereas programmers are making something so simple on a computer will crash and cause problems with in 6 months.&lt;/rant&gt;Yes!  One of my first programming projects was to try and reimplement [Pirates](http://www.2kgames.com/pirates/pirates/home.php) in Hypercard.  I'd fallen in love with the game on my cousin's IIGS, but I had a Mac and it wasn't available for Macs.  I was 11 years old at the time.  I think I got up to putting a picture of a pirate ship on the screen and having it move around, changing directions randomly.That's a scammy-looking site and the linked article is obscured by an enormous slide-over advertisement that blots out most of the page.Okay, so I did a tiny bit of optimization to my code and set up a simple benchmark to compare it to Sean B's fast hash table (open-addressed using double hashing).

My code: http://paste.lisp.org/display/37321

Sean B's code: http://paste.lisp.org/display/37322

The benchmark does 1,000,000 insertions and then looks up the 1,000,000 items and verifies that the right values come back.

This is by no means a conclusive test (e.g. you'd want to test with various kinds of key distributions, not just a linear, uniform one), but it gives some idea of the relative trade-offs.

The total time to carry out the 1,000,000 insertions + 1,000,000 lookups is about the same on my machine for both programs, but it's partitioned differently between insertions and lookups. My cuckoo code has ~25% faster lookups than Sean's but ~25% slower insertions. This doesn't cover memory usage, since I haven't looked into tuning that yet. (Edit: I checked, and my code takes a bit less than double of Sean's for the same amount of entries. Sean carefully tuned his hash table for space usage, so I expect I could reduce the space usage in my code quite a bit by some fine tuning too.)

Edit: Try bumping the number of iterations up to 10,000,000 and you'll see the trends I outlined become even more exagerated. Both programs still take about the same time, but now cuckoo's lookups are 10x faster on average, so that the cost of the insertions totally dominate even though the cuckoo insertions are only about 20-30% slower.&gt; It had influence on what came afterwards, but some of the spirit of what it was has been lost.

Definitely. There was a hard-to-define combination of openness, potential, and depth to that interface. In terms of being a hackable environment, it was in a very small class with other software I've fallen in love with (the Windows IRC client mIRC comes to mind, and I'm pretty sure this is how most serious Emacs users feel), but the initial simplicity and ease of gradual transition for the novice user are something I've not really encountered since in any system where you'd do real work.

HyperCard was my first object lesson in the value of open systems and free software, and I think it's the reason I'll never trust Apple again.Sometimes you have to rewrite code.  When code is badly designed (often because requirements not envisioned for 1.0 become critical for latter released, resulting in hacks that just barely work) you will reach a point where in the long run you save time and money by just biting the bullet, and rewriting the entire thing.

It may takes years, and you need to stay in business while doing it, but there is a long term payoff.Incidentally, when I program in Haskell, the bulk of my bugs are precisely that sort.  I make grammatical mistakes in user-visible strings or misformat them.  Or I mistype a lexical spec in Alex.

These are comparatively easy to fix, though.  Just eliminating the need to track down NullPointerExceptions is a *huge* productivity boost.An "academic" language is one that never gets used in the "real world", ala [this definition](http://www.cs.utexas.edu/users/EWD/transcriptions/EWD08xx/EWD800.html).I think you're confusing "SELECT * " with "SELECT COUNT(*)", which is optimized.

Time to hit the books.Who said anything about doing code-gen at runtime?What it takes to be a contract software developer (W2) or a full time permanent computer programmer (FTE). The lies, myths and misconceptions of contract programming vs. full-time employment de-mystified

Ah, right.  Thanks for the correction.read half way, the interviewee comes off as the idiot more than the interviewer&gt; If there are better languages to do the job, than your boss may see his mistake when your competition runs circles around your company.

You live in a dream world if you think technical decisions are always made solely on technical merit. It's often *completely* irrelevent that my functional prototype written in Groovy is a fully-functional application: if the project has a Java requirement it will be implemented in Java.

&gt; Are you a 'Johnny McOutsource'? If not, why even thinking about him?

Because I have to *deal* with Johnny McOutsource. Many developers do not exist in isolation; there are often *other* developers involved, often of lesser ability.

&gt; But if you really consider using a code-gen-lib in Java as 'magic'.

Compared to *source*-code gen? Absolutely. You simply cannot expect every developer to be able to intelligently use byte-code engineering libraries.

&gt; And I also can't always choose the environment, so in the moment it's most often Java.

Now you're arguing *my* point exactly; I'm confused.

&gt; Lisp is also bureaucratic. 

...but *less* bureaucratic.

&gt; simple Java code-gen as 'magic?

No, I claim that run-time byte-code manipulation/generation is largely incomprehensible by the majority of developers I work with. If it's wrapped up in AspectJ (already pushing the cognitive abilities of the greater majority of Java developers) or Hibernate (or any other black-box component) then of *course* it's trivial, because the developer (rarely, anyway) doesn't have to think about it.

I still am unclear as to why you believe that building "big" systems with "weak typing" is all that difficult, although I'm not sure we're using the term in the same way.

Lots of *very* big systems have been built around Lisp and Smalltalk. Orbitz is Lisp, a *lot* of trading and insurance systems are in Smalltalk.

Now, I don't consider Smalltalk to be "weakly-typed", and I barely consider Lisp to be "weakly-typed." If you want to debate the relative merits of statically-typed vs dynamically-typed languages perhaps we'll be a bit closer to what you've stated above.

Either way, though, you're still wrong, and I will continue to be much more productive in my "dangerous" dynamically-typed environments (except when I am forced to use Java) and continue to make my clients very happy :)
been thereI think 3 years from now we'll all be exclusively using crappy Web Two Oh Look Shiny "apps" written by agile shitheaded smartasses.The term "open source" is not meaningless, it is just that a bunch of companies have decided to misuse it.By which metric is Ruby/JRuby "better"?!
I ignore delicious' social aspects, and for the most part the retrieval aspects as well.  It just serves as a good place to send things when I'm GCing my tabs.  It's almost entirely one-way.  I'm completely unsurprised that the quality of tags and such is going down.  I even notice the quality of my own tags going down, with different tags on different bookmarks for the same thing.

On the other hand, reddit is entirely social, as far as its content goes: as the quality of what gets posted goes down, so does its use.  I don't think whether or not reddit's quality has decreased is in question, more whether or not it will decrease to the point that it is no longer of any use.&gt; I think you're confusing "SELECT * " with "SELECT COUNT(*)", which is optimized.

Yep. In fact if there's an ordered index on the response_time attribute then you can calculate this in logarithmic time.No. No, he really doesn't.Yeah, that's a standard trick that works for any size filtering footprint.

Say you're sampling a neighborhood of four pixels with weights a, b, c and c, where a + b + c + d = 1. Define m = (a + b)/(1 - c - d), n = (c + d)/(1 - a - b), a' = a/m, b' = b/m, c' = c/n, d' = d/n. Then a' + b' = 1 and c' + d' = 1, so you have two smaller two-pixel problems, each solvable by the bilinear filtering trick. If the results from this are x and y, then the final 4-weight result is just x\*m + y\*n.What bothers me is that the term "Open Source" is generally assumed to mean "Free". But is that really the meaning of the term?

For decades companies have been giving and/or selling the source code for their commerical products. Back in the 70's and 80's did we have the term "open source", and if so did it apply to these companies? If not, was there another term in use?I can believe that. If MS ever decides to introduce non-null references and the supporting infrastructure into .NET I can see a good half of the bugs in the average MS shop disappear.hah. But no really, see this: http://lispmeister.com/blog/lisp-news/on-lisp-apress.html

I had sent emails to both Mr. Graham and Apress, but received no reply.is it useful?ECMAScript? Maybe is isn't as bad as C, but it certainly isn't easy.&gt; By that argument the bugs-per-line of something like assembly language would be just about minimal. Do you see how absurd that claim is?

Is it absurd? In assmebly the bugs are not in specific lines, but in the interaction between the lines. More lines -&gt; more interactions -&gt; more potential bugs.

We all know reducing line counts can results in a signifiant reduction in potential bugs. The question is, how far does that scale? At what point do we reach a density that results in net increase in potential bugs?

I am not claiming to know where this point is, and I suspect it varies by langauge, but I do believe it is something that should be considered.
Yes I am serious. If we, as an industry, don't figure out these issues before functional programming goes completely mainstream we are going to live through another generation of industrialized WTFs.

Think of the current state of OOP. Not enough people took the time to seriously consider what the reasonable limits of OOP were before it was too late, so we ended up with nighmares like MFC and J2EE.

It will happen with functional programming as well if the people who know it well don't come up with usable set of best practices. 
I have mixed feelings about that. I have met a lot of college students that had a hard time grasping imperative langauges and who graduated essentially unemployable. For them, trying to teach academic langauges as well would be a disservice.

On the other hand, there are also skilled programmers going into CS to roundout their education. They actually want to learn low level stuff and the academic langauges. So focusing too much on the basics is unfair to them.

Its a hard problem.&gt; You know that famous experiment where they stick people for weeks in a sealed room without clocks or sunlight, and the people gradually shift from a 24-hour day to a 25- or 26- or 28-hour day? Well, that’s just ordinary life for me. One day I’ll wake up at 9am, the next day at 11am, the day after that at 1pm, etc. Indeed, I’ll happily ‘loop all the way around’ if no classes or appointments intervene.

That's what happens to me too :) Including looping back to normal hours (it takes 4 days).[deleted]ok.... i can't take it anymore.  I know, I know.... the poster
is using english english... but every time I see the word 'maths' for 
math... it's like scraping nails on a chalkboard.  
It's like if I said the phrase.... "using photoshop without knowing arts"
ugh!  that hurts too!Nat Torkington is a jackass. He leads a story about "faux-" open source projects with the example of Vyatta, a completely open-source, GPL'd access router built on XORP, itself a famous open source project from ICIR. What tripped him up? They used "git" for source control, not CVS, so Nat couldn't find their repository in the 15 seconds he allotted himself for researching this article.

Yeah, you're right at that. I just took a look through the specifications, and its certainly not small. Something of a misguided assumption on my part.

However, it still seems to me, that if one wanted to learn about ASTs in parsers, a good place to start would be the syntactic macros in Lisp.In Common Lisp literal lists are written (a b c). You might expect that a literal vector would be [a b c], but no, it is actually #(a b c). As Steele explains "to preserve the usefulness of the user-definable macro-character feature of the function READ, it is necessary to leave some characters to the user for this purpose..."

The student has both [] and {} available to bracket his own code representations, within which he can represent the code in any way he wishes. 

With most languages you are stuck with the syntax chosen by the language designer. If you think he has made a mistake you cannot fix it, so you can continue to believe that it really matters and is holding you back. With Common Lisp you can fix it. The development of programming language syntax from 0's and 1's to Pascal, C, and S-expressions was a great advance. Common Lisp forces you to face the fact that that seam has been mined out: there are no more big wins to be had with syntax.This is a good litmus test for reddit voting methodology.  People who happily downvote will glance at the title, or read the first few awkward paragraphs, and then downvote.  People who don't happily downvote will ignore it, or read the first few paragraphs and then ignore it.  People who read the article in full will ignore it or vote it up.  It's cute.  It should even interest people who use interestingly-typed languages, as they can easily see an error it talks about, and fix the error with types rather than runtime magic.Should I expect any content?
Sorry I took so long to get back to you, work kicked my butt this week.

Took some, rejected others; documented at [my list](http://www.jerf.org/iri/2007/02/19/2568.html)&gt; Adrian: We don’t ship with a JS framework. Having Python produce JavaScript is like using a motorized wheelchair because you’re too lazy to walk.

One could easily say the same thing about their database API using python produce SQL.Pah, it has this license:

&gt; Copyright 1995 John Carpinelli and Wayne Salamonsen, All Rights Reserved.

&gt; Copyright 1996 Lang Stuiver, All Rights Reserved.

&gt; These programs are supplied free of charge for research purposes only,
and may not sold or incorporated into any commercial product.  There is
ABSOLUTELY NO WARRANTY of any sort, nor any undertaking that they are
fit for ANY PURPOSE WHATSOEVER.  Use them at your own risk.  If you do
happen to find a bug, or have modifications to suggest, please report
the same to Alistair Moffat, alistair@cs.mu.oz.au.  The copyright
notice above and this statement of conditions must remain an integral
part of each and every copy made of these files.

... I'll go ahead anyway with my Mercury rewrite, as an exercise; I'll just have to not distribute it.I can see that being in a future version of Eclipse.

It does have limits though. It would define `add :: Num -&gt; Num -&gt; Num` as `add x y = 0`.

I wonder if there's a point where typing and code generation converge so that the type fully expresses the requirements of the code?What issues exactly? Your previous example is mooted in many ways, but the simplest way is to point out that destructive update (a=b) is irrelevant in a functional programming environment. So what other issues are you referring to?There's a perfectly reasonable explanation why using the word "Math" in staid of "Maths" doesn't make sense. In the following lecture, which only takes a few minutes of your time, it's explained perfectly.

[Maths](http://www.youtube.com/watch?v=MiMWJ1xBo8w&amp;mode=related&amp;search=)Wow, that's awesome.  The only thing more annoying than the smug, seen-it-all, "we've had that for years" attitude of many hardcore Lisp heads is the fact that the attitude is usually entirely deserved...No, only Chuck Norris can use Comic Sans and have it 'still be OK'.A couple of comments:

Just because there is no download doesn't mean there is no product.  Cryptol was developed for a customer and delivered to the customer in several different incarnations.  The has been well over a man-year spend on Cryptol.

The Bluespec tool sold by Bluespec Inc is written in Haskell, even it it doesn't say so on their web site.  It's not a matter of some minor tools, it's the main product of the company.  And plenty of man years. :)
It's really kind of a shame that when people abandon CVS, most of them look to Subversion. I just discovered git a few days ago, but it's really amazing how much better it is than Subversion, for the smallest of projects (no more stupid svnroot directory) to the largest (checkouts that used to take 5 minutes now take seconds!).

So yeah, if the author only wants to invite people who use CVS/SVN, I know where I stand.it's the opposite for me.
every time I hear 'math' instead of 'maths' it sounds like the person saying it has a lisp.

or should that be lithp?
Wow that really does make sense.  I didn't realize 
the letters actually stood for something.  ;-)
I'd like it to evolve in this direction:

Open Source = everyone can look at the source, and can contribute patches back to the maintainer.

Free Software = open source, and more than that: freedom to do what you want with it, anything other than "closing" it.  You can make your own version (as long as you label it clearly - no fraud), you can even sell it - as long as people can still download the source.  Or you can use parts of it in something entirely different.  As long as the source you got from the "free" software remains as free as it was, you can do whatever you want with it.

That would leave a lot of space in between "open source" and truly "free" software.  The difference would be how much control the author or owner wants to exert.

I think it's getting there, as long as we revive the term "free software" some more.definitely lithp.  blame my yankee upbringing.
of course there's torch for flashlight.
which is intersting since a torch isn't on fire and 
a flashlight doesn't flash.  maybe we should call it the light machine.  :-)[removed]The first time you bump into the element your working on, it would then go into a different spot. So wouldn't you have to hit it twice before all possible scenarios had played out?&gt; I found the reverse, myself, though it differs on a case-by-case basis.

I'm the same way.  I occasionally use hugs for its quicker reloading, but for anything but the simplest type errors, the messages are far less comprehensible to me than the ones from GHC.

And ditto the comment on #haskell.  It's definitely one of the things that makes the language.Sorry, but reddit does not allow editing posts.The OLPC does ship with Squeak (the eToys version) according to the Wikipedia [page](http://en.wikipedia.org/wiki/OLPC#Software)I have long liked Mr. Ziade's writings on Zope. What I find interesting is that he is there at the PyCon. But his employer just switched from using Python/Zope for its flagship project to using Java. (yechh.)Just click on the slides, it's a big bunch of JavaScript functions showing mostly empty slides.I think the author wasn't specifically into requiring CVS/SVN, but to having *some* version control system with public access.Well, yes. I was making a joke -- obviously the author called Vyatta a "faux open source" project because he couldn't find anything about source code access, not because they use git. But still, it was a bad move on his part, and one that prevents me from taking the article seriously. If I could find the answer after looking through their site for about a minute, and he couldn't, it's hard for me to believe that his other facts were competently reported.SELLOUTS!Neat, about time they make something like this.The problem is that there is no problem - to comply with the GPL you dont have to give everybody read access to the development tree. You can just give them a copy of the (corresponding) source code on the same CD you sell your binaries on. Hell the FSF does this with their member CDs.Agreed dude (Interviewee == idiot). For more reasons than I can be bothered typing...

I upvoted your commentThis is a little tangential, but the Haskell crew actually defines it as a "research" langauage.  As I'm sure you know, they've got the tongue-in-cheek motto "avoid success at all costs".  That's because they want to be able to try out new features and do all kinds of crazy stuff, and if they're 'successful' like C++, Java, or .NET, then the industry won't want things to change too quickly or do anything that will break backwards compatibility.

I've also seen Scala and F# described as "research languages" for the same reason.  In these cases, the "academic" label probably isn't a horrible abuse of the term.  But when it's applied to Scheme or Mozart/Oz, it seems a little silly.  And when applied to something like OCaml or Lisp it's downright outrageous.  Still don't think I've seen anyone refer to Eiffel or Delphi as "academic", but I'm sure if I tried hard enough I could find it.It's a really good take on the complications of XML, on how the definition is overcomplicated and ill-defined, the interactions are almost hopelessly incontrollable.

Two years old or so, but brings up issues that are often overlooked.

It's a pretty long presentation too, with links to other interesting content and finer points in the XML spec.

All with subtle humor, including the title.

It really looks like the reddit crowd didn't appreciate it, I can only think the presentation format and the "subtle" title didn't allow it to shine.jumped the shark...What makes you think IO in Haskell is difficult?  It's not.
But Haskell very much encourages you to separate IO from non-IO, which I think is an excellent design principle.

As an example, have you looked at http://haskell.org/haskellwiki/Roll_your_own_IRC_bot ?
&gt; Yes but when they made that language they never took into account that all programs in real life involve IO. So why did they have to make IO so obtuse?

If you compare a Haskell program that does I/O with one in another language, I don't think you'll find it more obtuse.

Try this. Write a Java program that reads all the lines of text from a file, sorts them, then writes them out again. Then do the same in Haskell. The Haskell version will be much simpler and clearer.

Of course Factor beats both:

`"in.txt" &lt;file-reader&gt; lines natural-sort`

`"out.txt" &lt;file-writer&gt; [ [ print ] each ] with-stream`

Try learning Haskell properly instead of "Java-learning" (reading blogs then complaining you can't understand stuff in forums).Moved Bluespec into the main list.

&gt; Just because there is no download doesn't mean there is no product.

I know, that's why I said it the way I did (and it's still the same). But there's _nothing_; no "call for pricing", nothing. There's just contact information with no indication of whether they even want to talk to you about their product.Jesus, what a mistake. Some people shouldn't be allowed to do their own branding.Oh, JavaScript. The page should say so. That's what NOSCRIPT is for.
People who consider "academic" languages to be those that can't make them money are not true programmers, sorry little Johnny :(I'm new here, so if posting your own content is frowned upon, mod me down to the death. People have asked for more Emacs content in other comment threads, just doing my part :PWTF...[removed][The comments on the article](http://worsethanfailure.com/Comments/Announcement_0x3a__Website_0x2e_RenameTo%28_0x201c_Worse_Than_Failure_0x201d_%29.aspx) are pretty solidly against it, too (except for a few supporters initially).I don't mind Emacs or personal content, but I hate Flash, I hate "screencasts" and I hate fast-forward videos without any comment.I won't comment on the fact that you can't even copy-paste your own title but:

&gt; When your trying

Your what trying? Or maybe you forgot to read this sentence once again?

&gt; functions really don’t need inputs, but it does put the fun in function

was the only "interesting" sentence of your blog. This sucks really bad, no information, nothing.I'll be sure to use WPF/E next time ... no? Animated GIF then? Sure ...

I tried to make the video move at a pace that got the point across quickly, there's a pretty easy to use tracker at the bottom if you care enough to drill down.

No comment? You mean no audio? The "audio" portion is in text below the video. It's more accessible and easier to translate that way.Sorry, no scantily clad girl (or guy, if that's your thing) dancing for you in this post. Just some tech geekery.[removed]I don't mind, now I'll feel less stupid when I type dailywtf.com instead of _the_dailywtf.com ...Superpages, wahoo! :)He forgot to mention slime
How about type in the google search box, it helps prevent phising and forgetting prefix/suffixes to urls. Plus it gives money to Google, FF etc.[removed]The _Real WTF_ is being ashamed to say **What The Fuck**!?*AWESOME!* 

Oh how I hate that word.   

And NetBeans?  Is it free, fast to load, available on Win32/Mac/*nix? Easily scriptable and configurable?

And have vi key bindings?
[deleted]And how will it help to further partition these communities? It's *good* to have Microsoft at OSS gatherings so they can see and hear what people are doing.For beginning programmers:
A language that can be used to illustrate different programming paradigms/concepts and where the syntax doesn't get in the way. 

For the more experienced programmers/academics:
Languages with few side-effects that are easier to prove correctness.&gt; Because the design that numbered both the bytes and the bits right-to-left was DEC's minicomputer architecture, this came to be known as little-endian byte order.

It's little-endian because you start counting at the little end, not because the machine was littler.
What does this have to do with Yegge?It also helps if the people who wrote the code in the first place are the ones rewriting it.  It's much easier to learn from your own mistakes than try to understand somebody else's.Twit.[removed][removed]That's because the customer is a government agency.  You guess which one.

Galois has quite a few other things written in Haskell of a similar nature.
At first I thought it was an "official" but meaningless thing. You know, we change the name of the corporation, but thedailywtf is still the URL. Nope. They now go to worsethanfailure.com.  Good luck with that. And "worse than failure" is the sort of phrase you give...not to the bum or the wino, but to the bum or wino who creeps around the middle school, and sniffs bicycle seats. As in, "That guy is worse than a failure...he should go kill himself."NetBeans inherits a lot of the will that made Emacs and Vi a reality!

Here are some quotes from Wikipedia:

* Emacs - http://en.wikipedia.org/wiki/Emacs

"The original EMACS, a set of Editor MACroS for the TECO editor, was written in 1975 by Richard Stallman, initially put together with Guy Steele."

"The first Emacs-like editor to run on Unix was Gosling Emacs, written in 1981 by James Gosling (who later invented the Java programming language). It was written in C and, notably, used a language with Lisp-like syntax known as Mocklisp as an extension language. In 1984 it was proprietary software."

* Vi - http://en.wikipedia.org/wiki/Vi

"vi is a screen-oriented text editor computer program written by Bill Joy in 1976 for an early BSD release."

As much as I can tell, these guys worked at Sun, which has been the workhorse behind NetBeans for several years already.

I do think NetBeans has a chance of taking over from where Emacs and Vi stopped. Sure, it's not an easy task! The competition of Eclipse, Visual Studio and IntelliJ certainly can help NetBeans keep focused enough!

If I had to bet money on it, I think NetBeans will be part of the future, just like Emacs and Vim are part of the present.Tags are missing context.

What does a tag mean? Is this a tag for something personal or for everyone to use?

Let's say we have a webpage about Robotic and Emotions reseach done at Stanford in 2007.

Tag Set without context: Robots, Emotions, Research, Stanford, 2007.

When we search for "Stanford" we get anything from Stanford including the university, any org with Stanford in the name, and even underwear, when we search for emotions we get this in with self help blogs.

Tag Set with context: Research:Robotics, Research:Emotions, Organization:Stanford, Year:2007.

3 problems:

1) How does one trust someone else's tags?

2) There is no "open" taxonomy like the Colon Classification ie good for searching context.

3) It's harder to include context, at least when one is doing tags quickly and does not know/care/need the extra context.So, this is some sort of Rorschach essay?Outrageous!  Delphi is Object Pascal with a nice IDE, period.  Delphi for PHP.  Next somebody will introduce Python for PowerBuilder.
'vim' is quicker to type than 'emacs'.  By the time you emacs losers have gotten your editor started up, I'll be two characters ahead of you!  ;-)The real WTF is the name, not the forum software.Why?! Too Fugly. Worse: Timid and Facetious. WTF Was True and Factual. Worse Than Failure - Weird, Toothless, Fake.

[Here's the answer.](http://programming.reddit.com/info/16dq3/comments/c16efm)He's the one doing the partitioning.I think most people are surprised about how complex ECMAScript really is, I certainly was.brillant!I think you're mixing up CS with trade schools. Computer Science is not supposed to intend to teach students to program, but more **about** programming: concepts, algorithms, complexity, design patterns, compilers, etc. They are supposed to produce *computer **scientists***! If a computer scientist, armed with Scheme and Factor skills, tries to apply for a *programming* job, he would surely have a harder time than most average programmers.

Where he really shines, however, is the R&amp;D of programming: language design, optimization, theories, you name it.Well obviously the previous example doesn't apply to functional programming. It is just a fucking example of something that most people understand is a bad idea.

&gt; So what other issues are you referring to?

I don't know what issues are exactly, that's my whole point.

How much logic can you shove into a single line before it becomes unmaintainable? Which techniques are the most error prone? The least?
The world goes all sepia-toned and rose-tinted when I think of HyperCard.Trade schools? There aren't any decent trade schools for programmers.

And what about the job market? Take a look, most of the good programming jobs require a BS or MS in computer science.

&gt; Computer Science is not supposed to intend to teach students to program, but more about programming: concepts, algorithms, complexity, design patterns, compilers, etc.

That is the kind of stuff that programmers deal with on a regular basis. 

&gt; If a computer scientist, armed with Scheme and Factor skills, tries to apply for a programming job, he would surely have a harder time than most average programmers.

That sounds rather questionable. How is a computer scientist suppose to solve the problems currently facing programmers if he doesn't even have the skills of said programmers?

Then again, maybe that is why most advances in CS come from industry, not academia.
Um, in case you didn't notice GPL is not the only open source license.I think as long as it's clear that it's your own content -- the domain name of the site matches your user name -- it's an implicit disclosure or something.Ahh... grandmothers, the gatekeepers of civil society.Those must be the most stupid reasons for a name change I've ever heard. All of the reasons apply to the new name except the shame of swearing when talking about a proper name. Bah.As soon as the "buzzword bingo" thing came out, I would have terminated the interview myself.  I would have politely said something like "I'm sorry, I don't think this sounds like a good fit", and cut it short.

Interviewers get personally wounded when you do that.  In fact, every single time I have done this, it has resulted in frantic apologetic followup calls asking me to come back in and interview again.  

I swear sometimes it is precisely like dating.Please do a fourier transform of your sleeping pattern (using the technique given) and post it for all to see. That would be pretty interesting.&gt; Trade schools? There aren't any decent trade schools for programmers.

Um, Computer Engineering degree?

&gt; And what about the job market? Take a look, most of the good programming jobs require a BS or MS in computer science.

So if farming starts to require math, math courses should be adjusted to include how to catch a fish?

On the other hand, I see nothing wrong with that. If a programming job requires a degree in CS, that means they want a competent programmer who also knows about CS.

&gt; That is the kind of stuff that programmers deal with on a regular basis.

You don't need to know any of those to actually program in the real-world, though knowing them will unarguably make you a better programmer. Doctors don't necessarily need to know what each medicine consists of, only what does the job, but knowing makes one a better doctor.I always have emacs running.&gt; Computer Engineering degree

That's a great degree, if you want to build computer-controlled hardware. 

What I think we need is a "Software Engineering" degree that actually teaches programming. (The current offering focuses more on UML and CMMI than actual engineering.)

&gt;So if farming starts to require math, math courses should be adjusted to include how to catch a fish?

That isn't a good analogy, no one is asking for a math degree from future farmers.

It's fine. Everyone's gut reaction is to dislike it. If the content is the same (and he is right in saying it's more than WTF content now), then no problem for me. I'll read WTF regardless.

That said, if you type in daily wtf in firefox browsers' address box, you get directed correctly to worsethanfailure.com . Typing in 'worse than failure', however, gives you something else.LameSince you made the first positive assertion about which was better, the burden of proof is on your shoulders.  Why don't you tell us why Groovy is better than Ruby?  I'm really curious -- I don't know enough about how the two differ.  In fact, with my (at best) superficial knowledge of non-Java languages on the JVM, I have wondered for a while why both Groovy and JRuby are being developed: that sounds a little redundant to me.

What's the diff?&gt; That's a great degree,

That's actually what I'm studying. The topics seem to cover a wide range, from C to Prolog to web programming to Java to databases to encryption to microchips.

&gt; if you want to build computer-controlled hardware.

Perhaps you mean hardware-controlling computer (instructions)? Because that's what programmers are doing.

I don't know about where you live, but we also have Software Engineering here, which more or less teaches a lot of industry stuff and less academic stuff.The way I read it, *newton_dave* knew what he was saying was logically invalid -- and that's the point.  He was basically saying that Java's so crummy it's not even the best when it's the only.  The patent absurdity of that statement serves as a form of literary hyperbole to illustrate the writer's intent.

That's it from my perspective.And if you use desktop-save, when you do open emacs all the files are opened with the cursor where you last left it.I hate modal interfaces. So, I absolutely abhor VIM for that reason. It is far too much cognitive load involved for me to use vim effectively. 

never used emacs though. I probably won't either. My choice, if i have to use some terminal editor sorta deal; nano/pico. That, or a nice GUI editor in whatever environment I am working in.Inevitably the software being compared that is less familiar fails to get a fair shake *cough* xahlee *cough*.

Seriously, when doing these types of comparisons, please to be running your article past an expert in both technologies.

And to address one of the comments here on reddit:
&gt; And if you use desktop-save, when you do open emacs all the files are opened with the cursor where you last left it.

:mksession

Until the day I become an s-expression hacker of some sort: vim FTW.
Yes, it does seem your school uses different definitions than mine did. Out here, software engineering is electrical engineering with software classes.I know both editors.  Maybe not perfectly, hence the mistake regarding zap-to-character in Emacs, but I don't think I need to run my opinions by experts of each sides.You're right, I'm sorry. This *is* the Internet after all...[deleted][removed]&gt; It's an quite common mistake that the size of short and long code has anything to do with each other.

Actually, the mistake is to think that they have *everything* to do with each other.  They *definitely* have *something* to do with each other.  To pretend that they are completely orthogonal concepts is as foolish as to pretend that they are identical.

&gt; Now Java forces programmers to put each public classes in a separate file (which was also criticized by the blog-author), but this isn't even visible to me because it's completely abstracted away by all modern IDEs (thats why I wrote 'outdated').

I have two problems with that:

1. the word "forces": The fact the language *forces* you to do so, rather than simply *allowing* you to do so, means you will always get the better deal in some cases and the worse deal in others.  Simply *allowing* you to do either, as Perl does, means that if you have any clue what you're doing you can do what the current project requires for best software design.

2. the assumption that IDEs are part of the language: I thought we were talking about language design, not editor design.  Hearing someone excuse a poor language "feature" by saying that IDEs exist that abstract that away makes my skin crawl.  It's better by far, I think, to simply have a language that doesn't have that problem in the first place, so that you don't need to abstract it away with ever-increasing truckloads of tools.

&gt; I look at modern Java like at Smalltalk: The IDE is a part of the language.

. . . except it's not true for Java the way it is for Smalltalk.  It's only true for Smalltalk because the VM and the IDE are one.  "Love and marriage, love and marriage, go together like a horse and carriage. . . ."

&gt; And again in Perl there are also many concepts in this small piece of code. They are simply 'under the hood'.

The key to the point in the essay is the fact that we're discussing not CPU instructions as concepts, but distinct syntactic elements of the language.  There are significantly fewer distinct syntactic elements to the same piece of code in one language than there are in the other.

&gt; This may be useful for beginners but if you want to master a language you need of course to know what's really happening, anyway. So in the end it's irrelevant.

No, actually, it's not irrelevant.  Knowing how a particular language construct works is not the same as having to use all the CPU instructions that undergird it rather than simply using the higher-level construct.  If what you said was true, we'd all be programming in assembly language at best -- not Java or Perl.

&gt; I don't consider Perl a useful learning language. Really.

Funny -- Perl worked just fine as a learning language for me.  Perhaps you've never read *Learning Perl*, aka "The Llama Book".  Perhaps you should.

&gt; I see a similarity here with the expression-problem which is even worse in most functional programming languages (because of the ML type-system) than in OOP, but since there are much less real big projects done in FP the problem simply don't stands out as much.

I'm not sure how that's relevant -- whether it's an accurate assessment or not.No he didn't.[deleted]ttyrec! :)Do you have the mustache?

I tend to suspect *kawa* failed to look past your *dastardly* use of the term "nutjob" because (s)he didn't have a credible answer to your point.&gt; mutation in closures is seldom needed

Actually, protected mutability of values is a significant part of what makes closures useful.Some of the best advice from the [The Pragmatic Programmer](http://pragmaticprogrammer.com/ppbook/index.shtml) is to learn an editor and learn it well. Vi versus emacs is a religious argument. I worship at the temple of emacs. However, I still envy the community that has come up around [Vim](http://www.vim.org). My feeling is that the emacs community is more fragmented. To each his own.I'm afraid I have to take issue with some of your statements, but you make some excellent points -- including at least one that did not occur to me, and probably wouldn't have unless I spent some real time pondering this discussion topic re Java.

&gt; If you use a language like Ocaml then you would maybe use closures for things where a class would work even better (just a thought).

Are you aware that the O in OCaml stands for "Objective"?  It's an object-oriented language.  It's just not a Kingdom of Nouns language, like Java.  Considering it has a higher-level syntax, uses a modern type-inference system, provides far better execution performance than Java (better even than C++ in most benchmarks, and about half as good as C), and can be used for interpreted scripts, compiled binaries, or even VM capability with equal facility, I think you may have bitten off a chunk of the wrong language to use as your counter-example.  I'm having a difficult time thinking of anything Java does better, aside from extensive external documentation, larger community, and more numerous native libraries -- none of which are actually language features in any case.

&gt; But I'm not really sure if it's a good idea to improve closure support in Java

I agree with that.  Java has its benefits.  I don't think adding true closure support would enhance that much for Java.  To really take advantage of closures, you'd have to modify a lot of the rest of the language too -- primarily syntactic structure and a few incidental, but pervasive, semantic design decisions.  It simply wouldn't be Java any longer (as we know it) by the time you had real, useful closures.

&gt; to make them really useful it would requite a total redesign of huge parts of the core libs

You get awfully close to making the same point I did -- and, in fact, you make an excellent point here that didn't come to my mind until after I read what you said about it.  I suspect part of the reason for that is that these days Java intersects my life so very rarely (I don't even have a Java VM installed on most of my computers, and I certainly haven't written any Java lately) that the concerns of legacy code support don't occur to me as easily.  That sort of thing is one of the first things that comes to mind when I hear a new PHP version is being installed on the servers of a webhost I use -- especially considering the PHP guys' notoriety for breaking stuff with trivial version upgrades.

&gt; In this case I would say that it's better to leave Java as it is (maybe few add small moderate fixes and extensions) and create a completely new 'Java 2' (maybe based on Scala) from the ground up and with a clean overall design.

I sort of agree.  To be more specific, I think a much better answer to the "Java doesn't have closures!" complaint would be "Use Smalltalk!"  Java, after all, was in many ways meant to address problems similar to those addressed by Smalltalk, in its early conception.  If you want something kinda like Java, but with features found in Smalltalk and not in Java, Smalltalk is the answer -- not Java++.&gt; This made me think about Vim and Emacs as editors and of course, which one is the better one?

Oh, for shame.  Even if you find yourself thinking such a thing, you shouldn't admit it so readily.That's getting dangerously close to the sort of thing you can do with OCaml.  OCaml, however, isn't *specifically* what you describe -- it just pretty much does what you describe, if you want it.

It's also worth mentioning, I think, that through a little functional programming sleight of hand (at least, that's how it looks from an OOP perspective; I think it looks like business as usual from an FP perspective), you can basically get mutable lexical state via closures.  As one example, a recursive closure construct could be used to achieve the exact same (external) behavior as an iterator closure in, say, Perl -- or even an iterator object in Python -- without having to use OCaml references to contain state internal to the closure.  I think.  I haven't actually tried and tested such code, but I see absolutely no reason why that wouldn't work.  Considering that's the canonical example of a closure using (mutable in impure-FP languages) lexical state, I'd say we're in business.  If you really want something mutable lexical state attached to a function, just use a function that persists.

Hmm.  That leads me to a thought . . .

&gt; The net would be that if you need some sort of state in a method or function, you would have to create a state object.

That's basically what a closure does, really.  That makes a lot of sense, considering all the talk I've heard of closures being insid-out objects, or objects being inside-out closures, or closures being a poor man's objects, or objects being a poor man's closures, or, well, any of the rest of those comparisons.&gt; My feeling is that the emacs community is more fragmented.

Habit Freenode #Emacs and http://www.emacswiki.org/ ; you'll feel better about the Emacs community.
I'm with him on this one.  I program in Object Pascal, but the name Delphi should *mean* compatibility with the Delphi codebase and libraries originally written for Object Pascal.

Well, there's always Lazarus and the Free Pascal Compiler.&gt; I have wondered for a while why both Groovy and JRuby are being developed: that sounds a little redundant to me.

Same reason as always; everyone scratches their itch in a different way. I'd probably argue Scala is better than both, but JRuby has the market's attention and Groovy has more traction.

* Groovy has less new syntax (this is beneficial, but really only for Java programmers--but that's a *very* important market segment).
* Groovy does blocks right (and the new syntax for the next Ruby helps, but... not a lot).
* Groovy still allows typing (JRuby may, but I haven't seen it yet--if it does, skip this one). This is actually important sometimes, just not always.
* It's not clear to me how packaging works in JRuby so I'm definitely not sure about this one, but Groovy packaging works the same (as Java). Perhaps you can help clear that one up for me.

Groovy was the first "official" non-Java JVM language (a JSR and everything). Ruby is (was, really) the subject of a lot of marketing hype. That doesn't mean it's intrinsically bad or not very important (I think it opened a lot of people's eyes, which in itself is invaluable)... but IMO that's why it was taken in-house by Sun over several other viable alternatives.

I believe that Sun should grab developers of a few other JVM languages (Jython, Groovy, maybe even Scala) and really work on positioning the JVM as a platform of choice regardless of language. 

Regardless of which language(s) are ultimately brought on board they're all better than Java!
&gt; Lisp is also bureaucratic. Sure, you can make more kinds of errors without having the compiler shouting at you.

The point, I think, is that with Lisp there's a lot more you can do that *isn't an error*.  This contributes to greater flexibility and power for a given task.(As an aside, if you don't know much about non-Java JVM languages, but still have an opinion that JRuby is better than Groovy, on what basis are you forming that opinion?!)You win :)

(I'm actually surprised somebody *didn't* get it, but you just never know :)Tell the truth -- was the insulting tone just a set-up for this analogy?  If so, it was a brilliant rhetorical maneuver.[deleted]Q: What is more fun than Quantum Mechanics? 
A: Quantum Mechanics wrote in Haskell&gt; Groovy has less new syntax (this is beneficial, but really only for Java programmers--but that's a very important market segment).

I take it you mean "Groovy has less new syntax, *coming from a Java perspective*."  This would imply that it has more new syntax, *coming from a Ruby perspective*.  Is that a fair statement of the case, or am I misinterpreting your intent here?

&gt; Groovy does blocks right

The implication, then, is that JRuby doesn't.  Yes?

&gt; and the new syntax for the next Ruby helps, but... not a lot

How so?  Details, please.

&gt; It's not clear to me how packaging works in JRuby so I'm definitely not sure about this one, but Groovy packaging works the same (as Java). Perhaps you can help clear that one up for me.

I'd need to know more about JRuby to be able to clear that up.  I probably know less about JRuby than you do.  In fact, I know more about Java than about either JRuby and Groovy put together, and I'd consider myself a Rubyist long before a Java programmer these days.

&gt; I believe that Sun should grab developers of a few other JVM languages (Jython, Groovy, maybe even Scala) and really work on positioning the JVM as a platform of choice regardless of language.

That's the best suggestion I've ever heard for what Sun should do with Java and the JVM (even better than suggestions about where to stick it -- har har, quadruple entendre).  This isn't the first time I've heard it, but it's still worth some kudos every time I run across it, and here's why:

&gt; Regardless of which language(s) are ultimately brought on board they're all better than Java!I just wish I'd seen this discussion six days ago, so my comments would be more chronologically relevant.

What do I get -- a cookie?i've just had a wtf moment right thereI feel like the site would never have become popular had it come to fruition with this new name. The Daily WTF has a geeky ring to it. Worse Than Failure does not... in fact, it just sounds like some weepy emo blog.&gt; was the [tone] just a set-up for this analogy?

Sorry, I don't follow your question.This sounds more or less like an article written by someone who doesn't know emacs. The commenters were quick to notice at least a feature or two that were not addressed by the article. (Like hippie autocomplete)&gt; I take it you mean "Groovy has less new syntax, coming from a Java perspective." 

Yes; that's why I said "beneficial [...] for Java programmers". There really aren't *that* many differences, though, but Java code works almost without change as Groovy code--it need not be Groovized.

&gt; [blocks]

Ruby's blocks are a bit broken with some scoping and syntax issues. Some of the new syntax proposals fix some of it; I'm not up-to-date on any scoping issue resolutions. That may have been addressed already.

&gt; [new block syntax]

IIRC 1.9 removes the need for the ".call" bit on a block? (I should, btw, have separated this into two distinct items, the syntax bit is for closures.)

    foo = { a -&gt; println a }
    foo("Hello!")
    
is just cleaner than:

    foo = lambda { |a| puts a }
    foo.call("Hello!")
    
A fairly minor difference, to be sure. I could be wrong (most likely) but I think the proposed syntax was:

    foo.("Hello")
    
Meh. To me that has the feel of kludge.

There are some things I really like about Ruby syntax (I figure as long as we've accepted complicated syntax anyway (remember, I'm a Lisp and Smalltalk person) we might as well go all out) that I sometimes wish for in Groovy, and it's possible I'll switch over to JRuby once it's further along, but who knows. I'm good with both, I just feel (note the word choice; I could probably argue either language convincingly on both technical and non-technical issues) Groovy does a slightly better job.Pah, this again?  Why don't we just collect all of these deathly boring assertions on a single page, and just resubmit that every few weeks?

1. 80% or more of communication is OMG BODY LANGUAGE.

2. Infomania 'worse' than marijuana!

3. Humans only understand things insofar as they build physical metapahors for them!

4. Humans can't remember smells!

5. Humans can only dream in black and white!  Possibly sepiatone!Here's a sample of the block scoping issue, from Wikipedia:

Ruby's lambda is unusual in that choice of parameter names does affect behavior:

    x = 3
    lambda { |x| 
        "x still refers to the outer variable"
    }.call(4)
    puts x  # x is now 4, not 3
via
http://ctnd.blogspot.com/2007/02/emacs-anti-aliasing-and-gtk.htmlIt's very simple, really.  Haskell takes a principled approach to IO.  This is different, which scares people.  People who are scared write FUD.  People who read FUD convince themselves that it is true.  All of a sudden, everyone runs around saying "IO must be hard in Haskell, everyone is saying so!"

Taking a principled approach to IO (or any problem) is the first step towards achieving all those other properties you listed.You're right, I couldn't agree more.  Two weeks ago, I actually was watching cable news for some stupid reason and heard Anna Nicole/astronaut diaper/Mooninite security news back-to-back.  

But what you are pointing out is part of the entertainment value.  As for changing the headline, why spoil a nice, vague generality based on junk science?I'll take that as a no.Ouch.  Okay, I see your point there.I think that some people jump to the Monad class before understanding parametric polymorphism with type-classes first.   Monad is just a type-class, but if you don't know type-classes, then you're going to get confused.  

First, you have to understand why parametric polymorphism is desirable.  Then you learn where it falls short, and how type-classes help.  Then you consider type-classes over type constructors.  And Monad follows quite easily from that.

But the important thing is, you can get started with IO in particular without even touching type-classes.  Just follow the formalism.  I see a lot of newbies say things like:

"man, why can't I use IO String where I want String?"

Then they get frustrated that the compiler won't let them do that, and vent.  For some reason, the IO bit before the String doesn't register as "a different type" from String.  You wouldn't get angry if the compiler told you that 1 + "hello world" was incorrectly typed.  But for some reason, I see them glaze over the IO part and not try to figure out that maybe they have to do something different.

Anyway, just a few odd thoughts...
It doesn't make sense. The site owner is ashamed of what made him known? WTF!!!

&gt; I'll take that as a no.

You'd do better to take it as a "I don't follow your question." -- but as you didn't have a sincere question, not having an answer is no loss to you.No 'stache. I do have a goatee, though, so I am probably someone's evil twin.Maybe he's just really bad at being anonymousActually, I read the entire thing and voted it down[removed]just learned M-spc (just-one-space) reading thru this.  Go EMACS![removed]Perl provides advanced programming features such as anonymous functions, closures, and first-order functions that allow you to create a program using a fraction of the code (read: number of lines of code) that other programming languages require. 

Many of these features are simply not available in most other computer languages, even popular languages like PHP, Java, and C#. Thus, you have to resort to boiler-plate code (read: lots of code for a simple operation) to get things done in those languages. 

In the end, the number of lines of code does matter a great deal. A well-written program in Perl is usually better than 1/10 the size of the shortest possible equivalent Java program. And that can simplify program maintenance enormously. 

The one flaw with Perl is that, like other advanced languages (such as Lisp or Haskell), Perl can be difficult to learn. Though most people think they know Perl after using it for a year, the fact it that it can take many years to develop the skills to make the best use of Perl (unless you're coming from another advanced language). This learning curve is a serious flaw. I have programmed in many languages, from 6502 assembly through Basic, Pascal, C, C++, Java, XQuery, Lisp, and so on, and Perl was the most difficult to learn thoroughly. Nevertheless, I was able to code in Perl on the very day I tried it for the first time. Learning to use the language properly took me years.

Despite the difficult learning curve, Perl offers a significant reward for those that learn how to use it properly. A Perl expert is someone who is significantly more productive than most of his or her peers, and certainly far more productive than he or she could ever be with some of the other more popular languages out there. 

Perl is also quite suitable for large systems. More so than people who don't know Perl well might imagine. The language is robust, predictable, reliable, and makes most other languages look frail and clunky by comparison. However, you don't want a Perl newbie designing or maintaining a large Perl system. 

I have lead the development of Perl applications that manage millions of large, complex records. My own experience tells me that such applications require more programmers and more hardware when implemented in languages such as C# or Java. (Forget about C, C++, or the old ASP). The only advantage of staying away from Perl for the development of large systems is that it is significantly easier to find good PHP, Java, and C# programmers than it is to find good Perl programmers. And it's easier to outsource (internationally speaking) Java and C# programs than Perl programs. Those other languages simply do not demand the skills that Perl does. And they don't provide the same benefits either. 

Plus, good Perl programmers are expensive (the enhanced productivity comes at a price). This is not true of most other languages. 

Perl is so malleable that it can be made to look like the newbie's language of choice. For that reason, many people learn Perl superficially and immediately start to judge it as compared to their language of preference. In fact, most of these people fail to see that efficient, effective, concise, Perlish Perl would seem like a completely unknown language to them, even if they'd been programming in Perl for a year. I've met many programmers who consider themselves proficient in Perl yet have never even seen good Perl code and have a poor opinion of the language. When I have worked with such programmers, I have always witnessed (usually within months) a complete change of heart in their opinion of Perl.

There are languages that overall can be said to have more advanced features than Perl, such as Lisp. A large system written in Lisp will generally contain fewer lines of code than the same system in Perl. However, Perl works better with its environment and is better able to meld with code written in other languages. The same can be said for newer languages such as Ruby and Python. But I would say that ultimately Perl has more features than either of those two languages (shocked?) and that Perl implements the features that it provides in a solid fashion. Things, for the most part, are not broken.
Your comment says it all about how you deal with obstacles in life. I'm not trying to be insulting here, but does school also has too much cognitive load involved for you?Thanks. The screenshot at the forum isn't working.Shouldn't it go all grayscale?&gt; What could an average user possibly see as convincing enough to switch?

A general reduction in shittiness?&gt; I think that some people jump to the Monad class before understanding parametric polymorphism with type-classes first.  Monad is just a type-class...

short words.
just one

syl. 
la. 
ble.

&gt; The free software movement's rigidity is ludicrous.

Go read the [little nostalgic HyperCard thread](http://programming.reddit.com/info/16bhy/comments), and then explain to me why I should stop taking the idea of free software seriously.&gt; Actually, I read the entire thing and voted it down

I'll not again underestimate the willingness of happy-downvoters to downvote.So what kind of "X for Y" inanity would piss off users of X most?

I suggest:

Ruby for COBOL

Perl for Notepad

Python for [anything else]

Visual C++ for tcsh

And the worst misnomer possible, one we could be sure no one in their right mind would ever adopt, would of course be

Java for a client-side scripting language that is *not at all* like Java.
&gt; Why people insist on writing large systems in Perl, and then blaming the language when they can't,

Aye, I've more respect for the people who simply write large systems in Perl.
downrated because it was on reddit's main page a few days agoActually, I think that computer programming is much more important than that. It is already crucial to the technical fields, and it's only going to spread from there. Computers are now connected with almost every aspect of our society, so every child should be taught to program just like they should all be taught to read and write.

I should also note that this is not an excessive requirement. When I was in elementary school (roughly 10 years old) all of the kids in my school were taught the programming language LOGO. If the average 10 year old can learn the basics of computer programming, then it's OK to hold high school students to the same standard.because google needs it[removed]But to do IO in Haskell you don't have to understand monads, you just have to follow certain patterns.  It's only when you want start using monads in a more advanced way that you to understand more.
IO in Haskell is a little different from most languages, but not vastly so.Maybe if they would turn "Practical OCaml" into a Wiki, it would become a less error prone book.
This sounds more or less like a comment written by someone who is an emacs bigot. The commenter was quick to take offense in a mistake or two with respect to his favorite editor. (Get a life.)Oh, please change the colors. The rest of the web is dark text on light background. I get a headache.
[deleted]I can't help it that my taste is better than yours.&gt;Perl, which I loathe with every fiber of my being, condones - nay, encourages - the deep, evil, horrible black magic to find the quickest way around a problem.

It's not encouraged. Just allowed. Many perl hackers would probably murder you if you actually did it without a really good reason (like an obfuscated perl contest).What makes you think that Steve Yegge wrote this?Python:

    lines=open('in.txt').readlines()
    lines.sort()
    open('out.txt','w').write(''.join(lines))
As an "insider" there is certainly that seperation- but in the mainstream minset- you are right. Things are kind of moving that way. It comes down to freedom and the "spirit" of the project/product (which is kind of a touchy feely description, but its the best I can do right now).Using an OLPC with an emulator is great and all, but how do I enable the [Kill-Switch](http://hardware.slashdot.org/article.pl?sid=07/02/19/1654231)?An academic language is one that illustrates concepts primarily, with real-world usability a secondary consideration.

General consensus is Scheme is less practical for production code than Common Lisp.

Factor is interesting, but is it practical re: robustness, speed, debugability, community support?

Haskell's main problem is GHC runtime bugs.

Java is rather horrible to teach with, and quite a pain to use, but it has community and corporate support.

C++ is capable of degenerating into a festering mess of corner cases and subtle language/compiler issues.

C is rock-solid specification-wise, but has practical problems with many higher-level concepts, "safety", and sheer amount of manual labor.

Even Assembly no longer lets you understand the CPU as none of the modern processors are natively executing it under the hood -- e.g. the x86 assembly API is merely a facade, with the actual chip pulling all sorts of tricks (out-of-order/speculative execution, etc.) hidden even from the assembly programmer.

Essentially, all languages suck in various ways.

At the same time, it's an embarrassment of riches compared to 10 years ago.

Things will eventually "get good" when we have AI that does what you mean.  Eventually computers will do all the programming themselves, based on what you tell them, intuiting similar to a human.

They will "hand-optimize" everything to assembly instantly after you are happy with a program's behavior.  Or microcode/FPGA.

I think the author was mainly refering to bulk re-writes- without a good reason. If the current system is limiting scalabiliy (in terms of performance or coping with changes or additions of features) then I guess that consitutes a good reason. 

In my brief experience, there is too little re-writing going on more then too much, but still, it is a risk.No templates, no iostreams, nothing.Hm, looks like lightgrey on black, just like the rest of the web.  -- but I'm using a simple xterm.  Do you have Eterm and a funny background?How about people just not liking the article?Just use xemacs - it has better package management and antialiasing out of the box.Quote:

&gt; I don’t have carpal tunnel, however, after long Emacs
&gt; editing sessions I sometimes feel a lot of pain in my
&gt; wrists

I don't want to be nasty, but you should _seriously_ consider dropping emacs if it makes your wrists hurt. Editing efficiency is cool, but not if you risk having to rely on voice recognition for the rest of your life.View
       -&gt; Page Style 
           -&gt; No Style

alternatively,  

    w3m http://gbracha.blogspot.com/2007/02/tuples.html
    

*EDIT* wow, a new markdown bug/misfeture. http literals in code blocks are wrapped in &lt;  &gt;   :-/My opinion is on Ruby vs. Groovy, the languages not implementations. I looked into Groovy some time ago (maybe a year?) and decided that it's not worth my time. I don't remember the details on which I based this decision anymore, only that Groovy felt like an uncoherent mixture of mostly Java semantics with a little bit of syntactic sugar from Ruby and Python. In other words: Nothing new on the table minus the great communities of Ruby/Python.

If two languages are similar enough (in this case: OO, GC, relatively concise, easy to learn for C/Java programmers), the size and quality of the community (Ruby's: growing and enthusiastic) is more important than the language itself. I think that's were Groovy loses.Thanks! I learned few useful tricks.

However, working so hard to format a file does not makes sense. For example, I could not get the multiline regex to match on my test file even after 3 tries. 

Some other examples that are hard to automate:
 1. I prefer to sort css rules by the semantics, for example, position, size, margin, paddding, border, colors etc.
 2. In the example you use different blocks for h1, h2... instead of just one block for all of them.

Isn't there a tool that know how to format code in various languages, like indent for C code or tidy/xmllint for html/xml?Or seriously stop being lazy and just swap the ctrl and caps keys.
like spamming?Heres something for your "test": I down voted it since it wasn't written by Steve Yegge despite the title of this post.  The interesting part of this is that I commonly down vote Steve Yegge article too, but I usually read them first unlike this one (since they usually are posted with correct titles).Yeah, I hear that works for some people - even though swapping ctrl and caps is easier with xmodmap than on windows.What the **fuck** is wrong with explaining WTF stands for "what the **fuck**" to your grandma?

Will she go in to shock and die?

I don't say things like "**fuck** off, **fuck**ing bitch" to my grandma, but I don't have a problem saying the word **fuck** to her if she asks what WTF means.

**Fuck** you, "Worse Than Failure", with your PC BS. You've lost a regular reader.&gt; People refused to run Linux because it was "anti-capitalist" (literally).

Hahaa.. thats the same reason why I use Linux.Tim Bray should slow down publishing articles and actually ensure what he has to say is meaningful.

This article really doesn't have to be linked on reddit because basically it says "I don't have much clue of what I'm saying but I'm saying it anyway."

Moreover all Tim Bray says here has been discussed better in other blogs so really don't bother.&gt; [...] I'll just have to not distribute it.

Why would their license apply to your rewrite?  It's a *rewrite*.  You can't copyright an algorithm.Hmm I would have at least consider 'Who The Fuck' first..The database API is one of the biggest mistakes of Django. It's just wierd to add extra layer for database handling which you have to get used to.I have a friend who's a contractor right now at some corporation. I told him to check out the daily wtf, and his company's internet watchdog software wouldn't let him go there. (I know, run for your life, right?) Anyway, maybe this will alleviate that, assuming the reason it was blocked was the name and not the content.Debian users can grab the emacs-snapshot-gtk package.  It's updated once a week or so.It's useless to try and dispute with facts against religious fervor of such pitch. 

The original poster needs to get some clues, learn to appreciate points of view and paradigms different from their own, and replace that loathing with some hard-won experience. Or maybe he won't and will remain a fanboy bouncing from language to language and from one misunderstood issue to another.Well I've used Emacs pretty much exclusively for all coding purposes for the last three years.  Yeah, I'm not aware of all the third-party modules, does that mean I don't know Emacs?  If Larry Wall doesn't about some random module on CPAN, does that make him ignorant on Perl all of a sudden?

I'm very happy that people wrote about features and modules I didn't know about so that I can explore them further and hopefully improve my productivity in Emacs.and no native (as in 'themed', not the X11 native) widgets.Nothing new in Ruby either, except marketing hype.

I definitely agree that Groovy was a bit ahead of its time and probably *will* lose to JRuby and Jython, but like other great languages, its death won't be due to technical reasons.

I'm still struggling to understand why Smalltalk lost to Java (there was considerable doubt at the time which would emerge as being more important) but it boils down to marketing (and to a lesser extent, language fragmentation).[deleted]Looks nice, although tab completion is mostly useful in languages where it's acceptable to have extremely long names for functions.The real WTF is this highly idiotic name change.Thanks.

And now I work for another company, on a Python project ;)&gt; emacs is really difficult to learn. vim is easy.

This is what people who already know vi always say.

Of course it's easier, if you already know it and don't know the other.&gt; just learned M-spc (just-one-space) reading thru this. Go EMACS!

Me too. A whole new chapter of my life has just begun![removed]Actually the horror stories in the comments are funnier than the story.If you "knew" emacs, you would know it's trivial to write commands like zap-to-char for yourself.By the time you've powered on your computer, I'm already a page ahead of you on my typewriter.&gt; Nothing new in Ruby either, except marketing hype.

I learned Ruby before the marketing hype (to provide some context here :-). IMO Ruby brought something new, a combination of elegant simplicity yet powerful metaprogramming (first two from Smalltalk/Lisp), focus on Unix integration and text manipulation (from Perl), easy to pick up for Java/C programmers in an afternoon. When I learned Ruby, everything felt right, call it POLS in action. That's highly subjective of course, but I've heard it from many others. I've never learnt another language (before and after Ruby) in which I became proficient as fast as in Ruby. (more context: I'm more interested in Lispy and more functional stuff now.)

Well, I've looked at the Groovy homepage now, and what makes me shy away is the Java culture ("leveraging Spring", "Ant tasks", "Groovy beans" ... shudder). Judging from the example code (is there a language reference btw?), there is too much syntactic sugar garbage and special cases for little gain. Syntactic sugar alone doesn't bring you far.Why would I write a command that already exists?

In any case, I'll humor you:

    (defun my-zap-to-char (char)
      (interactive "cZap to char: ")
      (save-excursion
        (let ((beg (point)))
          (search-forward (char-to-string char))
          (backward-char 2)
          (kill-region beg (point)))))

interesting historical speculation.not at all. i graduated with honors from my university, and have no problem with work. And your analogy is flawed; I wouldn't consider attending school in the same level of importance as learning vim. But, I understand your point. 

You are clearly missing my focused and concise point about software interfaces and modality.

Modal, to clarify, means that the same actions can have different effects under certain circumstances. In vim in particular, pressing 'd' can result in very different things depending on the current mode you operate in. When I am trying to solve a complex software coding issue, I would rather not have to redo work because i happen to press the wrong key in a particular mode. If any user of vim can honestly say they have never mistakenly pressed the wrong key in a mode, and had to undo and redo some editing, then you are to be commended. I, on the other hand, am fine with simply editing the document without worrying about some silly and often unused mode I am in. Sure, I may have to do more work upfront sometimes. But, I never make those mistakes you can only make in such modal applications as vim. So, for me, that is time saved.

:wqOne of the Language-Modes (e.g. css-mode for this example http://www.emacswiki.org/cgi-bin/wiki/CascadingStyleSheetMode ) should be able to format everything correctly after you insert newlines in all the right places (which should be moderately easy using query-replace-regexp).I agree with you. Bouml is a great piece of software that few people know about. And it's *resposive* (I'm looking at you, ArgoUML).

Of cource, the best UML-tool is a big whiteboard. Navigating around large architectures on tiny 22" screens is just not that great (actually I'm on a 17" would never admit it).

(The following contains speculation:)

I think it would also be possible to extend the mindmapper [FreeMind](http://freemind.sourceforge.net/wiki/index.php/Main_Page)  to support UML (via a plugin). This would mean a somewhat rudimentary coverage of UML, but with superior navigability and the possibility to export the diagrams as navigable standalone SWFs (which FM can do by default).

Since all of the major wiki-engines already support FreeMind as an external content-rederer, this would also provide coherent wiki-integration for UML.The main horror story is the dupes coming here on Reddit.That looks fine except that you're missing the numeric argument which you would use to move to the second parenthesis.  

But surely by this point you recognize how ludicrous it is to claim, even if you believe the command wasn't built-in, that
&gt; Emacs doesn’t have this kind of power. You need to go
&gt; in mark-mode (where you select text), and use either 
&gt; the movement commands or do a search on ‘)’, hit C-s 
&gt; to go to the next match and then C-w to kill the mark.

If you're doing that in emacs, then you aren't really using emacs.Nice explanation. I do stuff like this all the time. I tend to lean more on the keyboard macro facilities than regular expressions though, since unlike the author, I tend to not get them right the first time. ;)

Also see the CSSTidy project if you need to do CSS cleanup and optimization. It's pretty sweet.As I said in a comment on my blog, I wasn't aware of zap-to-char.  I could edit the post to reflect this new information.  I could've written it myself, but it never occured to me. 

This example becomes moot, even invalid, but if we replace deleting with upcasing or downcasing, Emacs doesn't have anything out of the box, and you need to create your own function.It took me about one week before normal mode vs insert mode felt normal.  Now, I don't even think about it, it's natural.  I do still make the occasional mistake, but nothing a quick press on the u key won't undo.Could become useful. I would like to see some examples that are less than trivial in Haskell -- for example, a simple chat server.It may be OK but it's still fugly as hell whoever you are.Tuples would be helpful in Java because arrays are homonogeneous; the only way to pack a few values of different types (without casting) together is to create a class for them. This feels like too much of an up-front commitment when I'm experimenting with different designs.

I really wish Java had default sorting and hashing behavior for arrays. I usually end up having to make a class anyway, since I need to use the compound values as keys or display them in sorted order. If tuples were added to Java, I'd imagine they'd be similarly lacking. (On that note, can we get some sane default toString methods, please?)&gt; http literals in code blocks are wrapped in &lt; &gt; :-/

That's really odd;  it doesn't seem like it could be anything but deliberate.Insert newlines at the right places is exactly what a formatter should do :-)
Yeah, my point is that you can avoid duplicating all the work by only adding that step to most emacs-modes.Whatever the fuck.Help, I can't decide whether The Daily WTF jumped the shark a long time ago, or whether this is it jumping the shark![removed][deleted]the comments on the site are priceless ..they all say they are unsubscribing ..what a DOUCHE&gt; (is there a language reference btw?)

Yes, and GDK docs, and two books (plus a Grails book).

&gt; [...] there is too much syntactic sugar garbage and special cases for little gain.
&gt; [...] Syntactic sugar alone doesn't bring you far.

It's non-sensical to apply that argument to Groovy but not to Ruby.

&gt; [...] elegant simplicity yet powerful metaprogramming (first two from Smalltalk/Lisp) [...]

It (Ruby) still doesn't do block scoping right, though. And having to ".call(...)" a lambda is place where syntactic sugar saves you some irritation and increases comprehension. lambda_var.() is not a pretty solution.

&gt; [...] focus on Unix integration and text manipulation (from Perl) [...]

I prefer Ruby's regex support over Groovy's, although just barely. Largely feature-equivalent.

&gt; [...] easy to pick up for Java/C programmers in an afternoon [...]

No different from Groovy--you do realize the languages are nearly identical in features and if Groovized the source looks very similar, right?

&gt; [...] what makes me shy away is the Java culture ("leveraging Spring", "Ant tasks", "Groovy beans" ... shudder)

But... that doesn't even make any sense. 

* GroovyBeans are identical to attr_accessor fields in Ruby, but without the need for the attr_accessor. 
* Using Groovy in Ant or as a wrapper around Ant is almost exactly like Rake, but with more tasks already written. 
* The Spring integration provides for trivial IoC/DI and on-the-fly compilatation/deployment (and Spring/JRuby integration also exists). Why *wouldn't* you want to leverage Spring?!
There are no X11 native widgets.gnuvince is correct.

There is no cognitive load involved whatsoever, because once you do it for as little as a week, it becomes second nature.

Which is also what I suspect can be said for emacs.[deleted]A not so simple one: http://repetae.net/john/computer/ginsu/Are you sure? I'm running emacs-snapshot-gtk on Ubuntu right now and it has GTK-themed widgets but no font smoothing. For font smoothing, you need XFT support, which is in a separate branch and doesn't have an APT package yet. On the other hand, it might just be a difference between Ubuntu and Debian, though that seems unlikely.&gt; Yes but when they made that language they never took into account that all programs in real life involve IO. So why did they have to make IO so obtuse?

Because Haskell is designed to be a purely functional language, and the IO system it uses is a direct result of following this philosophy. If you want a language with side effects, then there are plenty to choose from.

However, even though IO operations are difficult for programmers used to imperative programming to wrap their head around, they are not without their advantages.The important thing he said is "i noticed". Other persons who i would like to say that would be Tim Berners-Lee or the CTOs of Yahoo, Ebay, craigslist, MySpace, Amazon, Google and MSN. Oh, and reddit. ;)Haskell:

    main = do file &lt;- readFile "in.txt"
              writeFile "out.txt" (unlines $ sort $ lines file)


It doesn't seem that different to the equivalent code in Python.To tekronis and gnuvince:

With respect, i say you all don't get it. gnuvince said that he/she still makes that mistake. How can one say they don't think about it, when they continually error?

Again, my point is that you have to be mindful of your CURRENT Mode before you can do what you need to do. That requires a mental check, which entails extra thought. It's the same reason I do not like the caps lock key. Caps lock is a mode for the keyboard.

Granted, some modal things make sense (power buttons or cars' acceleration pedal, for example).WTF! who cares? 

:-)Emacs-snapshots don't have antialiased fonts (they will get the font code when it's stable enough), but the packages are updated once a week.I don't think anyone denies that IO operations in Haskell aren't going to be trivial to pick up for an imperative programmer, but Haskell isn't designed to be easy to pick up. Haskell's monadic approach to IO may be difficult to understand, but it does have a number of advantages in terms of ensuring programmatical correctness.AOL and Microsoft have announced they were looking at OpenID AFAIK. I think Yahoo too. I would not be surprised that the rest will follow one way or the other.Bill Gates talked about OpenID at the recent RSA conference.[removed]&gt; NetBeans inherits a lot of the will that made Emacs and Vi a reality!

But does it have a similar philosophy?  Both emacs and vi(m) seem to strive  for a mouse-free, no-fluff, get-out-of-way interface.  I don't think having Sun employees on board is itself a pro or con.  Sun gave use EJBs, too. :(
I've been giving [Denim](http://dub.washington.edu/projects/denim/) a whirl, with a Wacom tablet.  Very, very, slick.

I wonder if one could either hack it, or just abuse it, to produce UML.Someone discovers Ruby.

They decide that like it and will use it.

Not very interesting, no real content.
Damn. This is so basic.
You forgot...

'pwd'
'cd'
'who'
'whoami'
'which'
'locate'
'whereis'
'hehe'
The Ubuntu emacs-snapshot-gtk package hasn't been updated since September 2006. Can anybody tell a poor Edgy Eft user how to get a more frequently updated emacs-snapshot-gtk package, perhaps from Debian's package repositories, without exploding my machine?[removed]That looks like a client rather than a server (correct me if I'm wrong). The reason I would consider a server to be a more interesting example is because it needs to pass state to multiple threads of execution. A Haskell web server would use threads, but they are independant of one another; no state needs to be passed between them.You could do M-x replace-regexp RET \\(.\*?\\.\\) RET \\,(upcase \\1). There is in fact a more general but simple pattern lurking here, of which zap-to-char is a special case:

    (defun transform-until-char (char expr)
      (interactive "cChar: \nxExpr: ")
      (save-excursion
        (if (re-search-forward (format ".*?%s" (regexp-quote (char-to-string char)) nil t))
            (replace-match (let ((% (match-string 0)))
                             (eval expr))
                           t)
            (message "No match."))))

Now zap-to-char could be done as M-x transform-until-char RET &lt;char&gt; RET "", and the upcasing thing could be done as M-x transform-until-char RET &lt;char&gt; RET (upcase %). (Of course you could bind transform-until-char to a key to speed things up.)for Erlang: 
Erlang runtime model relies on Erlang emulatorIndeed. But the culture behind NetBeans is of the Unix philosophy, too, which is present in Java's "cross-platform" support, and NetBeans is made up with Java.

Eclipse, for instance, started out with an excellent support on Windows, and only later it got a good support on Linux, or not?

See? There are differences. TextMate, for instance, is Mac-only. 

The other big difference is that NetBeans is community friendly to some extent, which is not the way the more commercial alternatives might work, even though Eclipse scores very well in this regard, but it's open source to some extent...

I would trust the creator of Vi, who says that Vi was created to be fast in very slow terminal based computers. Since them, the need for the ultra lightweight Vi has diminished.

Actually, as long as broadband becomes more and more ubiquitous, the rich interactiveness might replace a lot of what we use terminals for nowadays, including text-only utilities. Like a said, NetBeans could be part of the future...could you elaborate on the problem? I don't see people being confused (unless I am also confused in exactly the same way)Vector calculus or it didn't happen.MergeSort may be state of art of algorithms that rely on the operator of comparison of two elements. But there are more specialized algorithms like RadixSort that run in linear time.Amazing. He managed to put all the C code in the header file...This guy was completely spnaked on [/.](http://it.slashdot.org/it/07/02/25/1848200.shtml)Don't mod down because you hate QBASIC - this is very funny.1. there are maths 
2. explanation is long winded and boring
3. whatever voted down&gt;Decentralised social networks

I think that's a very big deal for the next few years. I think one day or another the people are going to get tired of letting digg decide of the algorithm that promotes stories. They'll want to have access to all the data and use the system they want - at least geeks will :)Most people expect (I did) Vim tabs to work like Firefox's and all other editors: one tab == one file, you switch tab and you switch file, but Vim's tabs doesn't work like that. You can easily change tab and have the same file again.

Vim's tabs are just "containers" that lets you have different windows "layouts" you can quickly switch. To switch between files in Vim, use buffers.I don't know, because, as I said, I actually never tried with a sane static language. It's just my feeling, I was hoping somebody to explain me some more (more than "Scala has classes. Use them when appropriate.", I mean).&gt; Your story has cheered me up so I will resume quoting 'The Life of Brian' when I get to the office on Monday.

Blessed are the Visual Basickers (obviously not meant to be taken literally).I think you meant to say, "PLEASE STOP SHINING YOUR FOG LIGHT IN MY EYES, BLACK-TEXT-ON-WHITE-BACKGROUND-INTERWEBS!"[removed]&gt; I will not go into much theory analysis of the algorithm

It's sad considering it is the only interesting part of algorithms theory.

&gt; The algorithm is being released under the GPL

I don't think so unless you have a patent on this, but it's still maths and can't be patented in most countries.

&gt; The algorithm belongs to PhoenixBit

Maybe this guy will create his own company and become a patent troll one day, who knows.

&gt; DO NOT COPY ANYTHING FROM THIS PAGE TO ANY OTHER PAGE

That's called "fair use" and I use it however I want, thank you very much.&gt; Those who are constantly breaking away from tasks to react to email or text messages suffer similar effects on the mind as losing a night's sleep, he said. 

This is why I've started working with my mail client, IM client, and RSS reader not running. I'm *so* much more productive this way.Slava, haven't you posted enough of these loaded "what's the real definition" questions?

There are multiple valid definitions. End of discussion. If you want to perform logic on the definition of "academic language", you really need to define it first, because there _is_ no one clear, bright, shining, obvious definition that we can all agree on.My favorite reply is "You're making the baby Knuth cry".Man, I hope I never have to deal with that 12-bit text encoding. *shudder *That is the *wrong* way to use vi/vim. The deal is that you enter insert mode to type a few characters and then exit it immediately. You should never leave vi/vim in insert mode. The reason behind this is another one (I plan to explain this in detail in an article some time soon), but in any case this makes the "remembering which mode you are in" a non-issue. Technically there are modes, but they are not used in the correct, intended usage.What I really dislike is turning != and == into identity comparison.The trouble I see is mainstream sites (that are already in place) wanting to participate since sometimes the trouble of moving to new sites is all that keeps people from moving away. With these systems it would be trivial and places like myspace would, I think, see that as a threat.Vote it down, as noted in another comment, according to 99% of the comments over at slashdot the guy reinvented the radix sort (http://en.wikipedia.org/wiki/Radix_sort)IO in Haskell may be difficult to *learn*, but once understood its no more difficult to implement than in any other language. Indeed, Haskell's lazy evaluation makes it almost as easy to handle very large files as with smaller ones.When you write that article, please make sure to hop by my blog to post the link or post it on reddit.Well, if you feel like writing one up, go right ahead hehe, an  'Advanced' section would probably be a good additionAlso, the famous MoGo and Valkyria engines play on KGS, you can confront them over there. I just lost to valkyria on 9x9 :-([deleted][deleted]I'll post it to programming.reddit.com, so I think you will have a chance to see it - I'd guess it'll probably make it to the front page here at least.

Wouldn't know where to find your blog :) Why can't reddit users have a "personal link" so that others can find you?I'm the author of the article we are commenting about :)Beg to differ:

REGEDIT4

[HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\Keyboard Layout]
"Scancode Map"=hex:00,00,00,00,00,00,00,00,02,00,00,00,1d,00,3a,00,00,00,00,00

And that'll work everywhere, unlike your xmodmap stuff, which won't work when in a (non-X) terminal.alias v='vim'

Now you are four characters ahead!Easier, huh?[deleted][deleted]Don't kvetch, [calibrate your screen](http://epaperpress.com/monitorcal/).Because some things are so firmly "common sense" than no amount of evidence can disprove them.ah, that makes sense. so, for a user, insert mode becomes the more rare case, and edit/command mode is where you should leave it. I can buy that.

That would make vim excellent for editing a file quickly. People around me who use it more often, tend to create files directly in vim, which is a ton of inserting, and not much editing. I think with what you say, that may also be inappropriate use, as it would leave them to often remain in 'insert' mode.

Thanks for the clarification. My position stands for file creation. For file editing, i have seen it as good enough.

I'm glad to hear that. I'm not saying attending school is as important as grasping vim either; what I'm saying is it also puts you through a period of hardship and struggle (compared to sitting idly in front of the monitor) and in the end makes you more competent at what you do. It's just a shame you gave up not long after trying, and even looked down on emacs in the same way, just because it (omfg) *has* some learning curve.

I wasn't trying to focus on software interfaces, because I understand that may be subjective, but if you want to discuss it: *People are saying that all the time.* Modal interface sucks. It's impractical. It's unnatural. It's not the way we human think; it's unintuitive. And yet so many people worldwide are already happily hacking away in vim. And you don't need no study to show how efficient they are compared to toy-editors like nano/pico. What now?

You can bring up a million theories about how modal editing will never work, and maybe others will counter with two millions, and all those will not matter one bit, because it's reality that defines theories, not the other way around.

ZZJust save in a .reg file and double-click, honey.It is nice to see that Emacs is catching up to other operating systems and offering antialiased fonts, next thing you know it will get a real time kernel for embedded applications, virus scanning, DRM and everything else that other modern operating system have.

You never know may be it will acutally get a decent editor someday soon.rad!Given that Forth is very small, and its interactive nature makes it very easy to test code on the target device, I wonder why Forth is not more popular in the embedded world. C seems to be slowly replacing Forth, despite being inferior for this particular problem domain.Thanks. I have a lot to learn. What draws those ugly gray rectangular 3D buttons then?In the case of XEmacs, XEmacs, I think.[removed]Loser.Ah ok :)Sure. And that's why it's called "normal" mode :)

In the end, even writing code is mostly navigating around the document and moving pieces around, that's a reason vi/vim editing works so well.

I'm now using vi emulation also under word and outlook (viemu), and it is less of an advantage here, since you do more typing and less editing. But still nothing beats vi commands for moving around and small or repetitive edits, and once you're accustomed, "simpler" editing modes just seem like riding a bike instead of a harley.

I'm really missing it now in this tiny textarea...Same realization struck me today when I submitted a link from a Perl blog and it was already posted by linuxer.

My vote is that it's a bot. A quick glance reveals 100% link submittals but no comments. 

My questions are: who's running linuxer, why, and what langauge is it written in?
**Infix** operators. no less.[removed]Do you mean the reddit account? or what?That's correct. Sorry, my implication engine is set too high.This whole DVCS thing seems to be just a fashion sort of thing, but it is really practical.

I started to use it on my smaller projects, and didn't took too much time to learn the basics. The documentations are really good. The most "difficult" thing is to realize that how simple in the background is it.

I wouldn't recommend to my company to adopt it. I know that programmers seem to leave on the edge, but the majority of them are scared if they see that someone is editing the source code outside of the IDE.For the acronym-ignorant among us, what is UCT?Mentioned by Alan Kay in the video linked from this LtU post:

http://lambda-the-ultimate.org/node/2086

(also see the subsequent post on LtU)Am I missing something, or does the author start using broken code early on such as the *square* function.  Shouldn't that function, and the ones that follow, have *let* in front?Wow, only half way through and already some gems like

&gt; ..."management science" —I think we would have regarded it as a contradiction in terms— and "business administration" as an academic discipline is still utterly preposterous.

and

&gt; "In no engineering discipline the successful pursuit of academic ideals pays more material dividends than in software engineering". I could not agree more.Flag as Inappropriate.In the presentation, that bullet point had a note that said "this is how it works today".[deleted]You still aren't making sense.  If you edit a file _while_ your vcs is trying to commit, you are changing the changeset/version ( if it even commits successfully ).

You should only be making safe commits if this is a repos anyone else uses. Making untested changes and then not even knowing if it makes it into the commit?... this is just bad logic, don't try to make this work.Nice examples, with a blatant book plug at the end! I might buy that book actually. I always wanted to learn more about Ocaml and it sounds like a nice text.I've never used UML because it seems designed to scratch an itch that I just don't have.  In particular, the whole notion of designing the architecture of your program in pictures seems completely and utterly misguided to me.  This is not because I'm averse to expressing the architecture of a program in a high level notation, it's simply that I think that notation should be *code*.  But maybe that's just me.Apparently, this is an algorithm to guide Monte-Carlo search algorithms, which have been the big rage in Go AI in the past few months.

UTC is defined in this paper: http://zaphod.aml.sztaki.hu/papers/ecml06.pdfWell I was the original author, made the page and titles, etc, but someone else added that square function, as well as a few others for basic IO and stuff, fix them if you think they are in error, and supply some more content :)Yeah, dude is always pushing his book, but I do hear it's nice...wish it wasn't ~$160 USD.  He does provide quite a few tutorials/examples for free though.**named** infix operators.Yikes, do we really need articles talking about pre-ansi C?[removed][deleted]By the way it sucks.
Computers have a long way towards winning humans in this game.
Ha ha. Thanks for explaining the joke to us, it's usually so difficult to understand.And next time: Learn how to remotely disable people's computer in [1 short lesson](http://hardware.slashdot.org/article.pl?sid=07/02/19/1654231).&gt; I have a lot to learn. What draws those ugly gray rectangular 3D buttons then?

You are probably thinking of either Motif (xcalc, xpdf, etc.) or GTK1 (XMMS).Well, I can't say that I have the greatest patience with new such things. But, there is something to being an efficient user of tools like vim. For whatever reason, my mind is not keen on making switches in context while I work and write code. I have used vim for some time (on and off for a few years, then for 2 months daily), and decided to find another editor that was less modal to help speed up my work.

I equate it to preferring a manual screwdriver to a powered multitool with different widgets and bits to attach to it. The powered one is great when i want to do more than tighten a few screws. If i plan on switching around bits and tightening many screws... I better learn to use the powered one. 

Thanks for your input.

&gt; Macros -- a'la Lisp. This might throw open the door to tremendous power. I've never seen a Lisp macro, but I've heard Lisp programmers wax poetic about them.

That sounds nice, but can we think of a way to implement macros nicely in a language with as much syntax as Python?One of my previous bosses (who recently retired) used InterLisp-D and the structured editor. He thought it was a pain in the ass, forcing one to have syntactically correct code at all times was simply a non-starter. On-the-fly error feedback and refactoring are much better than a fascist editor.Well, I _thought_ the term might come from their being used to communicate over a distance in military situations by flashing code (which AFAIK is true), but the origin of the term dates back further according to Wikipedia: http://en.wikipedia.org/wiki/Flashlight

Standard Wikipedia caveats apply, but the explanation (that early flashlights, due to poor battery technology and poor light technology, only ever worked in brief flashes before the batteries needed to rest) is certainly plausible.What the hell?  I was looking for some badass guitar riffs.What was that?  I couldn't hear you over the sound of your terminal beeping.

Hey, this is fun.  We should get other people involved in this whole vi vs. emacs thing.  I think it could really take off...[removed]Would said virus scanner catch the GPL?
[removed]Java (and .Net) designs are broken (or rather, obsolete?) on so many different levels that all attempt to improve it are doomed. Patching never beats solid initial design (Also - see C++). Consider moving to different platform if you want to enjoy "good style" programming.What should be the rate for an experienced LSL programmer,
60 L$ per hour ? Maybe, the best thing to do is to work for 
your own and create some startups inside Second Life waiting for
IBM or Google pay you a huge amount of L$ for the abilities of a
marketing superpowered avatar (try to imagine the behavior of
an army of google adsense avatar-bots inside Second Life).I'm not quite sure the point of this.Ah got it - "default" is the main point.Ugh, I hate when writers use "wolfing down mouthfuls."  Yuck.Too late, my eyes glazed over when I realized there's no information actually on this page.The 'let' keyword is required when using the interpreter (ghci), however, I cannot see any examples that are explicitly using the interpreter that are also missing a let.Yeah, right... You didn't actually follow the link, did you?7 up votes and nobody noticed the typo. Its Sudoku not Sudoko. I fixed my blog entry.This tutorial will describe how to create thumbnail images on the fly using PHP.[removed]on the other hand it gives them the opportunity to collect even more data about their users from other sites and thus provide better service. Switching and mixing works in both directions.fdYou're right, it's a client.  I'm a sloppy reader. :)I agreee that a built-in application packager is needed. 
I'd like to suggest that monads may be not merely a difficult concept to understand, but (somewhat) unreasonably difficult, compared to their utility, undeniable though it may be.

Genuine understanding of why monads are what they are, how they help you manage state (or control) in a purely functional manner, why monadic laws are what they are, etc., is objectively more difficult than constructs in other languages that play similar roles. By "objectively" more difficult I mean the following: they're not difficult *merely* because they're unfamiliar, like any paradigm is difficult until you become familiar enough with it. After we discount the difficulties that are due to any new nontrivial concept, they're still more complex, *much* more difficult to understand, than abstractions that play similar roles in other languages. 

I would dare tentatively advance the suggestion that some enthusiasts of Haskell are in love with monads, to some degree, because of the intellectual rush they experienced and experience in figuring them out more than due to their usefulness. I could certainly sympathize with such feelings: I loved the rush, myself. After some reflection, however, I'm not sure how to justify the fact that performing such fundamentally simple tasks as I/O, encapsulating state or directing control should require such a mathematically complex foundation.

To sum up the above, I would suggest that monads may be a *gratuitously* difficult concept.

Disclaimer: I have very little experience in actually writing nontrivial Haskell programs, and very possibly don't know what I'm talking about.The examples you give - I/O, encapsulating state and directing control - are not "fundamentally simple" in this context, leaving aside the question of whether or not they ever are, because Haskell provides referential transparency. It is *purely functional*, which provides a great number of benefits and opportunities which are missing in other languages, but which makes some things harder/irrelevant/inappropriate. 

&gt; Disclaimer: I have very little experience in actually writing nontrivial Haskell programs.

Give it a try and you will see the other side of the argument.[removed][server](http://sequence.complete.org/node/258) and [client](http://sequence.complete.org/node/257)Side effects are easy to use (in pretty much any language), but hard to reason about.

Most languages place no restriction on the use of side effects, allow arbitrary mutable state and provide only strictly sequential evaluation. As a result, these language features appear deceptively simple.

*The underlying complexity is hidden by familiarity*.

Consider how hard it is to reason about what an arbitrary fragment of impure, side effecting code in C does, let alone provide a *proof* that the code is correct.

To *precisely understand* such code requires some real theory: for example, Hoare logic. The fact that we need to employ some hard theory to precisely understand exactly what a C program does doesn't appear to stop people using it. Strange, no?

Similarly, monads provide a flexible theory for understanding side effects and mutable state. We can then precisely understand code that does side effects. Once we understand something, we can control it, and  Haskell programmers control side effects precisely: turning them on when and where they need them.


So, concepts like monads, or Hoare logic, provide the conceptual underpinning for understanding *what programs do*. However, just as Hoare logic doesn't appear to make C programming harder, despite the complexity:

    int f() {
        i++;
        i++;
        return 7;
    }

neither do monad laws seem to make Haskell programming harder:

    f = do
        inc i
        inc i
        return 7

The lesson:

* side effects are easy to use (everywhere), but hard to reason about. 
* monads let us reason about side effects.
* side effects remain just as easy to use, 
* despite the fact we now understand them.

Of course, its rather lovely that once you understand monads, you can readly understand [monads that provide other evaluation strategies](http://haskell.org/haskellwiki/Monad#Interesting_monads) , including optional results, exceptions, read-only state, non-determinism, undoable actions, atomic memory transactions, *continuations*, backtracking, concurrency and process calculi.

In fact, you start to see the fundamental connections between seeminly unconnected systems, like exceptions, concurrency, backtracking and sequential evaluation. 
And apparently [vector spaces and quantum mechanics](http://programming.reddit.com/info/16ljm/comments).

Understanding what programs do is good :-)
*This tutorial aims to explain the concept of a monad and its application to functional programming in a way that is easy to understand and useful to beginning and intermediate Haskell programmers*

Please show me monads in C.

I can explain higher-order functions in C, via function pointers I can fake map() and give an idea.

I can explain Continuations in C, staving the stack and with a few of save/restore context calls. Not perfect but will give an idea.

I can fake OOP in C via struct and function pointers.

Is it possible to also explain monads in C? Also note that in all the previous examples there are useful things about doing coroutine, OOP-alike structures and passing functions to functions in C. If monads are not just a way to mask side effects in Haskell I expect to see a tutorial about monads in C with some usable real world idea.

p.s. if you can't don't try to sell the idea that monads are The Next Big Abstraction. Just an useful tool for purely functional languages.[removed]&gt; side effects remain just as easy to use

No, not really. Having to manually sequence side effects can sometimes make state monad programming in Haskell feel a lot like assembly programming. For in a (non-pure) call-by-value language you can rely on implicit sequencing with no syntactic overhead, whereas in Haskell, state monad code can often get very verbose (if you split intermediate computations into tempVar &lt;- actionExpr pieces) or unreadable (if you get around the verbosity by sequencing things together using head-hurting point-free style hacks). Combinators like mapM_ definitely help though.

Now, I am _not_ saying that this isn't worth the hassle--a lot is gained by making the order of side-effects painstakingly clear and explicit. But I don't think you're being completely fair in saying that side effects remain _just_ as easy to use as in a non-pure CBV language.It is odd for a guy who is well known for a number of programming books to have never seen a macro.[deleted][As noted previously](http://programming.reddit.com/info/ox6s/comments):

In Haskell, a purely functional language, we use monads to provide a library-based approach to sequential, imperative evaluation: the famous IO monad. No wonder Haskell people talk about it a lot -- library-based imperative programming in a purely functional language is fun.

But monads don't stop there! In languages with builtin sequential evaluation the IO monad must seem boring or silly -- no wonder it all seems a puzzle.

But this is where it gets interesting. Since monads provide an abstraction over evaluation order, they can be used in other languages (and Haskell) to implement *non*-sequential evaluation order, as a library, also!

So instead of special language support for, say, exceptions and callcc, you just implement monads as a library, and get:

* optional results ([Maybe](http://haskell.org/ghc/docs/latest/html/libraries/base/Data-Maybe.html))
* encapsulated state ([ST](http://haskell.org/ghc/docs/latest/html/libraries/base/Control-Monad-ST.html), [State](http://haskell.org/ghc/docs/latest/html/libraries/mtl/Control-Monad-State.html))
* alternative results ([Either](http://haskell.org/ghc/docs/latest/html/libraries/base/Data-Either.html))
* exceptions ([Error](http://haskell.org/ghc/docs/latest/html/libraries/mtl/Control-Monad-Error.html))
* continuations ([Cont](http://haskell.org/ghc/docs/latest/html/libraries/mtl/Control-Monad-Cont.html))
* [concurrency](http://www.math.chalmers.se/~koen/pubs/entry-jfp99-monad.html) 
* backtracking and [non-deterministic evaluation](http://www.haskell.org/haskellwiki/Sudoku)
* even [sofware transactional memory](http://haskell.org/ghc/docs/latest/html/libraries/stm/Control-Monad-STM.html)
* and then on to [Arrow-based evaluation graphs](http://haskell.org/ghc/docs/latest/html/libraries/base/Control-Arrow.html)
* or, say, monadic [region memory management](http://www.cs.cornell.edu/people/fluet/research/rgn-monad/index.html)
* even [logic programming](http://logic.csci.unt.edu/tarau/research/PapersHTML/monadic.html)

Perhaps this gives a flavour for non-Haskell people for why they're so useful. Continuations as a library! Non-deterministic evaluation as a library!

So just as its well known that with continuations you can implement threaded control or exceptions as a library, with monads you can implement continuations themselves as a library, along with all the other fun toys.

*So, monads are a great tool for implementing all sorts of fun language features, and understanding those features*.

----

Then, using monad transformers, you can compose separate monads, providing, say, exception handling over a state encapsulation: you can specify precisely what programming language concepts a particular program requires.

Here is:

* [Some theory](http://www.haskell.org/haskellwiki/Research_papers/Monads_and_arrows)
* [Some code](http://haskell.org/ghc/docs/latest/html/libraries/base/Control-Monad.html)

So, users of other languages are likely, and rightly so, to be less interested in the solution to IO based on monads, and far far more interested in things like library-based continuations or exceptions (see for example [monadic delimited continuations in OCaml](http://www.cas.mcmaster.ca/~carette/pa_monad/cc.mli)).

Hope that gives a hint towards what this monad stuff is all about.You're absolutely right, using monads forces you to sequentialize everything and it can be a pain.  And especially annoying in a commutative monad where you know the order doesn't matter.
I don't think we have found the ultimate way to write monadic code yet; some form of automagic lifting of functions into their monadic version would make coding easier, but I've not seen anything viable on this front yet.

In the mean time, you can do things like

    instance (Monad m, Num a, Show (m a), Eq (m a)) =&gt; Num (m a) where
        (+) = liftM2 (+)
        (*) = liftM2 (*)
        (-) = liftM2 (-)
        abs = liftM abs
        signum = liftM signum
        fromInteger = return . fromInteger

Which allows you to write x + y even when x and y are monadic.

A minor nit:

&gt; If we use Set instead of List then we get the free commutative monoid instead, ie. one where a+b=b+a.

No, with commutativity you get Multiset. For Set you need commutativity plus idempotence. For a reference, I'd suggest the ["Boom hierarchy"](http://www.cs.nott.ac.uk/~rcb/MPC/boom.ps.gz) (PS.GZ) collection of papers.You can do monads in C, and I'm sure if you google you can find some.  But since C's type system is weak you have make a horrible mess with void* everywhere (for generic monadic functions, not for specific monads).
Since the types involved are rather intricate, I'd not recommend monads in C.  You need some type checking, static or dynamic.
In my crappy understanding of it, I think every variable in C89 is a monad. It's only in C99, where you have const, that you can get some non monadic values. 

I think this is why almost all functions in C return error codes rather than values. Because C doesn't allow for "Maybe" in the language, practices have evolved to simply never use return values as anything but error codes.

The challenge here is to move from 'every return value is an error message' to package up all the side effects in certain places so you can then resume coding where return values are semantic values. Once these side effect are packaged, it turns out you can do interesting things with them as first class values.

/me prepares to get slapped down by some monad expert.If you have external libraries, simply renaming won't work. Additionally, if you're creating a library with certain exports, you'll need some of these tips.

However... I agree, old news. Nice to see in one big document, but definitely old, old news.

[Edit: didn't have coffee yet]I look forward to the day when every link submitted to reddit is about how to access a database, run a query and fill out a form.

Let's go shopping![deleted]DRM and especially Virus Scanning are features of a _modern_ operating system?!"It seems likely that TurboGears and Pylons will merge. This looks like a good thing." - does anyone know anything more about this? I couldn't see anything on the TurboGears or Pylons mailing lists.Has reddit become the Haskell directory?Has anyone used this?It was [created for a presentation](http://www2.jeffcroft.com/blog/2007/feb/25/two-new-django-sites-both-source-available/) at PyCon, presumably to showcase Django.There are much simpler solutions:

1. Add emacs to the Finder toolbar, accessible from any finder window
2. Add emacs to the Doc, accessible anywhere

Then drag your file to one of the icons.Perhaps you are looking for somethink like the [Hitchhikers guide to Haskell](http://www.haskell.org/haskellwiki/Hitchhikers_guide_to_Haskell)?Ha ok. Thanks for explaining. That's more interesting as a showcase than as an end on its own.
This might be the worst headline I've seen yet on reddit. How about "Solaris/PostgreSQL winning ground on Linux/Oracle"? "PostgreSQL on Solaris preferred over Oracle on Linux by data warehouser"? 

The headline on the article was just as bad, but the submitter has at least some responsibility to make sure the submitted headline is readable and informative.Because no such discussion has happened on any mailing list yet. It's a unofficial discussion that occurred in January when the TG team started talking about the future direction of TG such as going more into WSGI and therefore more into a Pylons like architecture. That of course led to suggesting that maybe both projects could start merging. Nothing official and no such work undertaken AFAIK.

Considering that Kevin and Ben were keen on agreeing that both projects share some principles during PyCon it's possible they also suggested both frameworks could get closer.a) It can play a decent game of 9x9 go, which is a horse of a different color from standard 19x19 go. ((9x9) / (19x19) = 81 / 361 = 22%).

b) From "Monte Carlo Go Has a Way to Go" [[pdf]](www.fun.ac.jp/~kishi/pdf_file/AAAI06YoshimotoH.pdf) [[Google html]](http://209.85.165.104/search?q=cache:LEhUbIaJIxEJ:www.fun.ac.jp/~kishi/pdf_file/AAAI06YoshimotoH.pdf&amp;hl=en&amp;ct=clnk&amp;cd=6&amp;gl=us):

&gt; Bouzy &amp; Helmstetter (2003) mentioned, “We believe that,
with the help of increasing power of computation, this approach is promising for computer Go in the future.” However, our experimental results demonstrate diminishing returns with additional samples, a problem that similarly affects the benefits of game-tree search for additional depth.
Our results indicate strong evidence that advances in hardware technologies cannot constantly improve the performance of Monte Carlo Go, even if additional computational
power enables them to sample more random games.

What the authors of the paper the link is talking about have done is use an interesting method of parallelizing Monte Carlo go, an approach suggested in the paper above. However, computers are still far from playing a good game of 19x19 Go.

UPDATE: I misread or misremembered the article. This one focuses on improving the sampling algorithm that had been previously used in applying MC methods to Go. I believe that the above criticisms still hold, but thanks to d_ahura for correcting me.[removed][[Google html]](http://209.85.165.104/search?q=cache:LEhUbIaJIxEJ:www.fun.ac.jp/~kishi/pdf_file/AAAI06YoshimotoH.pdf+monte+carlo+go+-hotel+-chevrolet&amp;hl=en&amp;ct=clnk&amp;cd=6&amp;gl=us):
 available if you're a PDF-hater, but the graphs don't show.This blog post by the involved party has some more information on the story:

http://www.lethargy.org/~jesus/archives/77-Choosing-Solaris-10-over-Linux.htmlI still prefer the Terminus font. Much more readable and space effective:
(set-face-font 'default "terminus-12")It's been for some time. Much of the Haskell stuff is interesting, but I don't think this is a particularly good introduction to parser combinators nor a particularly good criticism of parser combinators.I use [Emacs.app](http://emacs-app.sourceforge.net/), which is a Cocoa version of Emacs. "Open with" works as expected with it.hahah, winning ground on linux/oracle?  Are you fucking serious?  linux/oracle is hardly a blip on the map compared to solaris/oracle, but your proposed headline makes it seem like linux/oracle is the king of the database market.  While Oracle recommends Oracle on linux in the 32 bit world (only because no one runs solaris on 32 bit hardware anymore, and hasn't for years), in the 64 bit world it is still solaris and probably will be until oracle hamhands their own linux distribution with oracle on top of it.  And really what the hell are they thinking doing that, since they have current OSes that run just fine, have stable programming interfaces, and are already well supported by their respective creators (solaris and aix).

Your second headline is much better, but you can't rail against this article's headline when the first alternative you come up with is much worse.[deleted]As I understand, on non-unix systems (Windows?) the C++ ABI is not well defined, so you cannot always mix code compiled with different compilers.  In the linux world everyone uses gcc, so those who want to sell an exception know they have to be compatible with gcc or nothing will work.  In Solaris a similar situation seems to apply - at least I have never seen problems.

Of course in theory different C/C++ compilers can use different header files for the same thing, and really screw things up.  This doesn't seem to happen in real world unix systems.   I'm pretty sure it does happen on Windows.

OTOH, on Windows if you can design a new ABI that is better in some way, you have a better chance of selling it because nobody expects you to use THE platform's ABI anyway.This is like programming.reddit.com crack... a language comparison, code interoperating between C, Haskell, and Python, monads, list comprehensions, and the list goes on.&gt; The third phenomenon that goes hand in hand with a greater pragmatism is that universities are seen less as seats of learning and centres of intellectual innovation and more as schools preparing students for well-paid jobs. If industry and government ask for the wrong type of people —students, brain-washed by COBOL and FORTRAN— that is then what they get. I know that the perpetuation of obsolete programming habits in the U.S.A. is beginning to be considered as a matter of serious concern, because in the triangle computer users/computer manufacturers/universities, no single party seems able any longer to interrupt the vicious circle. (The moral of the text I read was that, therefore, here was a federal responsibility, because otherwise the USA could be overtaken by in this respect still more flexible nations. An outsider's corollary of this deadlock situation is that —in no field!— Universities should forsake their role of intellectual innovators.)
Who had the idea to call this apt? Sun must have fried their brain or something.The django snippets site gives me:

Mod_python error: "PythonHandler django.core.handlers.modpython"

"Beating some professional players" isn't a very specific endorsement.  I'm waiting to see some rankings.
Because if you are late with a project upper management is not happy.  But if your are late on a project and upper management finds you didn't spend all your budget (in effort to make things on time) they get mad.

Unfortunately that is the case at the place I'm working not.  (Though their hiring process does not help)[Logix](http://livelogix.net/logix/) has macro-like facilities for Python.  See [this chapter](http://livelogix.net/logix/tutorial/7-Languages-Extending-and-Creating.html) from their documentation on extending the language.[deleted]Let me know if you enjoyed that allegory! Thanks for reading.What about some simple gomoku/renju implementation that I (a very very amateurish player by any standard) cannot occasionally beat?

OTOH - I once wrote a chess proggy on a weekend that I myself can never beat. There's something about human visualisation that makes a difference between those I guess. Could make a decent mixed psychology/CS research paper.Maybe, but what harm is there in letting mercantilist control freaks strangle themselves?

The marketing principle of risk reversal (i.e., reducing the potential customer's risk) works due to a brass-tacks relationship: at a lower price, more is demanded.[deleted][removed]&gt; HyperTalk is about un-Lisp as you can be while still being dynamically typed, garbage collected and relatively high level.

Yeah, but those are no small things.  But the scripting language revolution is a _big_ deal: people have fully accepted garbage collection, and dynamic typing is well on its way to full acceptance.  If that had happened 12-15 years ago, we'd be much further along the path to better languages: instead of arguing over types, we'd be arguing over macros.  And that naturally implies a Lisp of some sort, or something as like a Lisp as makes no difference.[removed]good job reading the paper, the author of the post couldn't be bothered.... comment up story down&gt; Every single line of code added may blow up the bridge, dump you in China, or create a black hole.

What is this, the programming language of the evil mad scientist?
Holy good lord Paul Graham please do not implement strings as an array of bytes. Please. It is a mistake.

I am shocked that several people suggested this as a good idea; David Sloo has it right. (grep for his comment)I was sure it would happen, I wasn't sure when it would happen though. It may only be a 9x9 board now but eventually it will scale.I simply mean that you never know the result of any computation for certain. This goes doubly so even for well written programs due to the large amount of OS software that could have faults as well. Programming takes place in an uncertain domain and while we can try to contain that uncertainty with code, even the containing code is uncertain. We never prove what programs will do.

We always are one bug away from any behavior expressible by the computer. I feel it is very much like having a bridge that you cannot prove the destination of. Or like a bolt that can eat a bridge.[deleted]yea, better get on that Modula-2 train before it leaves the station.Really dumb! There have been programs that can play a good game of Go since before Windows 3.1. "Many Faces of Go" is one of them. The problem is not playing a decent game of go, the problem is that anyone can improve enough with 6 months of learning Go to beat the best computer programs.I don't know what they were doing wrong, but I've got Postgres + Ubuntu on a 64-bit system (with a 300 gig database, admittedly 1/4th the size of the article's).   I haven't had any problems at all that weren't of my own doing.

So my guess is that the problems were either with the sys admins being stupid, or with CentOS (although at a previous job we had an OLTP pushing 500k rows per day with PGSQL+CentOS, and it went fine).That is by far the most accurate analogy I have ever read.Maybe those *are* the snippets...That starts a separate instance of Emacs each time.  You'll quickly run out of memory if you do it too much, and with all the elisp many users accumulate, Emacs can take a while to start.  emacsclient, however, connects to an existing instance.You're right, #1 does work nicely, but only for one Emacs.  I generally run two (one for Gnus/ERC, one for programming), and I don't always want the one I added to the Finder toolbar invoked.  M-x server-start with emacsclient guarantees my programming instance gets the file.

Number 2 works as well, but I rarely remember which icon belongs to which instance when I've got more than one Emacs running.Actually, I think he's referring to C++. :)The filesystem seemed to be their major problem, not the kernel itself.  If all else was good, they could have just went with zfs for Linux.&gt;while some panel apps are written in languages like Python, a growing number use Mono. In fact, while Mono is being integrated into GNOME, at times its main purpose seems to be to serve as a scripting language for panel apps. Yet I have to wonder whether panel apps -- which by definition are small and have limited functionality -- really need a language of their own. Moreover, the use of Mono makes backwards compatibility to earlier versions of GNOME unnecessarily full of dependencies.

With Microsoft's recent accusations of patent infringement against Linux, is integrating Mono (sponsored by Novell) with GNOME really a good idea?Cheeseshop is a Python repository. Click "By Score" at the top. Then you'll see what Cheeseshop packages people like an which ones they hate. Hopefully they'll flesh it out a bit more as time goes on (like making the "by score" section deeper).It's not much of a cheese shop, is it.9x9 is an entirely different beast from 19x19. 9x9 is about small-scale tactics, like hand-to-hand combat. 19x19 involves big-picture strategy: there are numerous small, interdependent battles, many sacrifices and compromises. Being a good soldier doesn't mean you'll be a good general.

Claiming groundbreaking progress in Go requires solid, quantitative proof to back it up. Speaking in generalities like "compares favorably against professionals on 9x9" or "the new method is promising for 19x19" don't instill confidence. I want to see it playing on a Go server, with a rank.well then that's not exactly playing a "good game of go" if 6 months is all you need.that's the paper's point, that this approach doesn't scale much better than the conventional approach
Academic in my case: GNU Go whips my butt every time.Are there rankings for 9x9?That's great to hear. Didn't know there was a virtual implementation of this legendary system. Will definitely look into this when I get an x86_64.

"A software implementation of the Lisp machine required a 64 Bit CPU to attain acceptable performance."
I don't get the reasoning behind this. 64bits or 32bits doesn't have a lot to do with performance. And obviously, current 32bit CPU's are much more powerful than the first Alpha ever released.

Either way, thanks to the guy who wrote this up &amp; the one who submitted this to Reddit.Yeah, assuming whatever this paper says is easily programmable, it would be trivial to hook it up to one of the Go servers and see what happened. If it was significantly better than existing bots at 19x19 I'm sure it would create quite a stir.Maybe this approach won't scale but eventually it will be solved is what I meant.Why would you write an article on an academic topic, only to conclude it by saying "Oh, and by the way, I haven't even bothered to read the paper I'm basing this article on, but I bet it's really interesting!"Board position is easier to quantify in chess and the moves tree branches less. 

In go a reversal can happen in the last few moves. You will never get a good computer player that simply uses a counting algo to determine position (i.e. percentage owned or even weighted control) it requires something a good bit more subtle, which is tricky.Tags. The Symbolics LispMs always used a relatively large amount of bits as tags in their pointers/values. On 32 bit, that's prohibitive. Not so with 64.

EDIT: There's still that other file you have to find yourself...lol, wow, worst article ever. 

a) Headline is pure bollocks

b) article ends with:
&gt; I haven't had a chance to read the paper

 
Dear blogger: RTFA[deleted]Remember, Genera is dark, secret warez (unless you have the $5k license.)Dupe! I head about this from the economist about 3 weeks ago: http://programming.reddit.com/info/12c9y/comments"The nice thing about standards is that there are so many to choose from."

Andrew TanenbaumAnyone who thought a computer 'would never play go' doesn't understand Moore's law. Computers will EVENTUALLY be able to do anything we can do and more, faster. It's quite possible that they MIGHT never become as power efficient (perhaps it'll take 500 watts to run a human equivalent AI), but I think that will be overcome too.

It's naive to think processing will stop improving, and thus it's naive to believe that anything we can do will not eventually be done better by a computer.The article does OK up to the money line:

&gt;Would you rather have an easy-to-use universal interface, like GNOME, which makes customizing the system harder? Or, the more power-user stylings of KDE?

False.  KDE has been removing features for years, just like Gnome.  In fact, Gnome's attempt to remove configuration features is an attempt to be more like KDE.  I remember the days of Sawfish where KDE users screamed about the 1001 different settings in Gnome as the main reason they used KDE.

Here's the best way of doing this: come up with a user setting screen for Beginner, Intermediate, or Expert.  Default install at Beginner because you would expect an Intermediate or Expert user to go find this setting to set it.
On a side note, I didn't check the "also submit to reddit.com" checkbox and went back and manually submitted it to see what would happen.

It creates a duplicate post on reddit.com. I bet this would be considered a bug as I doubt they want the same URL in the database twice.

Check it out at.

http://reddit.com/info/16opi/comments/c16orz[removed]What guarantee do we have that Moore's law will continue to hold indefinitely?I didn't get it. What's the cliff's note version?These programs can only play a half-decent game on a 9x9 grid, which is orders of magnitude easier than the 19x19 grid it should be played on.

So this isn't "playing a good game of Go" either.  Vote down.My favorite way to do dynamic programming in Haskell is to use the following two functions:

    tabulate bounds f = array bounds [(i,f i) | i &lt;- range bounds]
    dp bounds f = (memo!) where memo = tabulate bounds (f (memo!))

This allows me to say (arbitrary example):

    fibonacci n = dp (1,n) f n
      where
        f rec 1 = 1
        f rec 2 = 1
        f rec i = rec (i-2) + rec (i-1)

(this example works in multiple dimensions also, but I wanted to keep it simple)

the rec function will look-up the value in the array, and this value will be computed for the first time by "f"&gt; A thorough study of one or more foreign languages makes one much more conscious about one's own; because an excellent mastery of his native tongue is one of the computing scientist's most vital assets, I often feel that the American programmer would profit more from learning, say, Latin than from learning yet another programming language.

Excellent advice.I thought that was quite good until the bit about the halting problem. The halting problem does not say that "we can never be certain of any result about any computation". It simply says that there does not exist a _general_ algorithm for solving the halting problem for _all_ program-input pairs. As programmers, we don't really need a general algorithm. We just need to know that our particular program will work with all possible input. Because the program can be quite restrictive about the input it allows, this is actually quite a different issue.

Granted, the flexibility of the pieces with which we build software makes it difficult to prove that the program will work, but this difficulty has little to do with the halting problem, and more to do with cost-benefit analysis of such proof.

I think you were pretty close about the general difficulties, though. The problem is that every piece of a program can potentially have a dramatic effect on the whole. Whereas a single bolt on a bridge, when not tightened properly, will have mostly local effects, a single flawed line of code can have dramatically non-local effects.

I think your analogy demonstrates something else, too, and that's the attitude that we as developers have toward this issue. When I read about the workers going out and turning the bolt another 90 degrees, it certainly reminded me of the way I have chased down some bugs. But can you imagine if they applied the same method in building bridges? The bridge collapses? Fine, next time try tightening the bolts another quarter-turn. Another difference with software design is that bridge designers cannot afford to use this kind of trial-and-error debugging. Thus, there are well-established and generally followed guidelines about how tight those bolts should be. No guesswork needed. As software designers, I think the ability to try out different solutions can sometimes make us lazy when it comes to establishing standards. The lack of 'beta-testing' on bridges, and the importance of proper function, forces strict standards.

Why don't we do this? I think it's mostly because of cost-benefit analysis. I was going to say it's also partly due to the inflexibility of a standards-based solution, and offer up the example of a bridge-builder who builds a suspension bridge, and is later contacted to see if he could make it a drawbridge, too. But when I think about that example I think, sure, the entire bridge will probably need to be rebuilt. But is there any reason the initial build and the revision could not both be built to standard? Does not following a standard (say, for the torque of the bolts) make it any easier to change a suspension bridge to a drawbridge?

_Edited to add the last sentence._The problem with Go is not that it's inherently a complex game, just that as you can play a counter nearly anywhere on the board at any time you have as many as 361 possible moves per turn, instead of a maximum of maybe 20 or so moves (on average) in Chess.

This makes the "state-space" of the game expand exponentially faster than with games like Chess[1], so the only thing stopping a decent Go-playing computer is a lack of processing power to deal with the size of the problem.

Short of a few groundbreaking advances in computer speed and memory, a competent 19x19 Go-playing program will be out of reach for quite a while yet.

[1] Thinking one turn ahead = 361 moves, 2 turns ahead = up to 129960 moves (361x360), versus Chess's 2 turns ahead = around 400 moves max (20x20).It can't be his real name cause it's a bad pun on windsurf...Reading articles on subjects one is familiar with can result in headache and/or anger. The main advance is not about parallelizing MC search. It is about combining a few known powerful ideas to seriously raise the level of state of the art Go players.
 I think Remi Coulom was the first to realize that the UCT algorithm could be used as a tree search for Go. Using MC to to get decent evaluation was already known. And using winning percentages as evaluation values instead of area.
 As for scalability it is known that UCT plus MC eval will asymptotically reach perfect play as time/effort increases.
 The current tinkering is on many details in implementation.
 - How to limit UCT tree memory use.
 - Finding a balance in the effort between UCT search and MC sampling.
 - Achieving a more correct sampling in MC play out.
 - Parallelizing UCT and/or MC.
 - Investigating how an if local search can be profitably added.Exactly - the state-space of Go expands exponentially faster than Chess, and the lack of dramatic reversals makes Chess a much, *much* easier game to write a "fitness function" for, to help choose helpful-looking strategies early on for further investigation later.For those of you who are asking "why didn't he do X so that he could have stuck with Linux?", let me ask you something:

If your car continuously stopped in the middle of the freeway at rush hour, after trying to get it repaired 20 times, would you sell it and get a new car, or keep trying to fix it, at the risk of losing your job?

Solaris 10 is free and opensource, just like Linux.  Why not use the right tool for the job?  Schlossnagle (if he's the same guy I'm thinking of) is NOT the sort of individual to sell out to a vendor or pound a square peg into a round hole.  It appears that Solaris worked better for this job.  I have had similar experiences with Linux, FreeBSD, and Solaris x86 in the past, except now that Solaris is free, I don't see any reason not to use the best tool for the job.

Plus, they got rid of Oracle.  That right there is the money shot.  Gotta love Sun putting the knife in Oracle...
Cliff notes: Software is hard because of various uncertainties that can't be removed.

However, I think this metaphor is a pretty big exaggeration.  The approach described is that of the cowboy coder (an endless cycle of code-compile-test), and has always met with limited success.  It can be improved upon with a little bit of design and a whole lot of unit testing, bringing a lot more certainty than willy-nilly code changes.Have you tried it? It's the same instance. You can also do it from the terminal: open -a /Applications/Emacs.app text.file opens text.file in an existing Emacs instance.
Ok, gotcha.  I agree with you that the author does sound like she/he tends towards "cowboy" coding rather than solid engineering.

Perhaps if they did then most of their problems would go away:)  Like n ot thinking of 2 cars traveling in parallel or using a volvo![deleted]That's not a bug, that's a feature. ;-)Not really. 9x9 in Go is "boring." Professionals will go 50-50 with other professionals almost every time.Not in your lifetime, and that's a bet.&gt; We just need to know that our particular program will work with all possible input. 

That's exactly what the halting problem says we can never know. Knowing things and proving things are the same in my mind. We can develop heuristics, but our heuristics will fail on certain cases.

&gt; Granted, the flexibility of the pieces with which we build software makes it difficult to prove that the program will work, but this difficulty has little to do with the halting problem, and more to do with cost-benefit analysis of such proof.

The "proof" is never there. We can come up with reasonable and probabilistic estimates on a particular piece of software's reliability, but for most large problems, we won't know much about the result of the computation. This is the reason for Haskell's so-called constraints. As you say, by spending some costs in the terms of being very precise (and paying costs to do this in a lazy way), you can gain provable benefits. But you don't gain any real proof. If you add on the operating system to this mess, you end up in an arbitrary computing space very quickly and easily. 

&gt; Does not following a standard (say, for the torque of the bolts) make it any easier to change a suspension bridge to a drawbridge?

That's not the same problem as the problem programmers face. Bridge builders can have a standard because the materials they use comply with well known physical properties. But the output of any large program will have some randomness in it. And that randomness can lead way to any computation at all.

The bridge standard wouldn't matter if the physics of the "alternate universe" allowed for the following truth. 

"At any moment, all of the screws and weldings in a bridge could come loose." 

We may still build bridges in that universe, but we wouldn't go around faulting bridge builders for building bridges that spontaneously decomposed. It would just be seen as a natural law of our world. 

However, the poor programmer has to live in such a world. And because non-programmers do not see this world, they make false analogies about bridge builders and programmers. That was the point I wanted to make.I think your implication that Go will be solved by increases in computer power isn't necessarily correct. I'm no expert on the matter but numbers like 300^200 look pretty big to me. http://senseis.xmp.net/?SomePhilosophicalQuestionsAboutComputersAndGo is a good start for further reading.Thanks for the correction - the paper about UCT is more interesting than I thought it was, but I still don't think the original link is a good summary of what's going on.

I think I gave the UCT paper a biased reading because I saw it from a link somebody sent to me which contains the same, usual, errors as any reporting about go.

I've updated my post correspondingly.

To make this post more interesting, do you think that MC can realistically reach a masters' level game in the next 20 years on a 19x19 board? Do you think UCT is a fundamental improvement, or just going to improve the quality of existing MC players a bit?Nothing.  In fact, many interesting tasks cannot be solved by brute force without requiring more memory bits than, say, the number of atoms in the Universe.  I don’t know Go, but if you have 19 * 19 = 361 possible moves at first, you’ll need some 361! nodes to store a tree with a complete game, which is about O(10^768) — much larger than the 10^78 estimate of atoms in the Universe.  Good luck walking *that* tree with a Moore’s law processor.

Of course, the fact that a finite human brain can play Go means that *some* kind of computer must be able to play Go too (unless you don’t believe our intelligence to be physical).  The thing is that you’ll not get that computer by processing power alone; you need to find some better algorithm, or maybe a better architecture, to reproduce or improve upon the strategies our brain uses (whatever they are).A nice "feature": should you want to de-memoize the implementation, you can simply replace _dp_ (1,_n_) with the _Y_-combinator.  In practice, however, the reverse transformation is probably more useful.  ;-)

    -- two self-recursion strategies: straight and memoized

    y f = f (y f)

    tabulate bounds f = array bounds [(i,f i) | i &lt;- range bounds]
    dp bounds f = (memo!) where memo = tabulate bounds (f (memo!))

    -- two versions of fibonacci: recursive and dp via memoization

    fibonacci  n = y fibstep n
    fibonacci' n = dp (1,n) fibstep n

    fibstep rec 1 = 1
    fibstep rec 2 = 1
    fibstep rec i = rec (i-2) + rec (i-1)




Edit: Fixed markdown markup and added code snippet.
You can obviously use various techniques including unit testing to lower bug rates in your code. I do unit testing all the time with nunit in C#, rake in rails apps, lisp-unit in in Lisp, etc. I didn't say anything against using sound principles to build software.

What I did say was that when you do find a bug, you have to adjust something to remove the bug. On a well built system, this means writing a unit test for the bug, fixing the code, and testing again. But you DID have to fix the code, right? 

But bugs will remain in your code, even after the fix. Unit testing isn't some mystical faerie dust that you sprinkle over software to make it work. It is part of a toolset to productively create software. But is isn't some sort of logical proof of code correctness. And thus it has flaws.

Those flaws are arbitrary. THAT is the cliff note version of my article. You cannot "contain" their results like you can contain the failure of support cables, for example, on a bridge. 

I think you approached this article with some sort of preconceived mindset.Reddit seems to go through language fads. We're currently at the tail end of the haskell fad; in the past we've been through scheme, ruby, common lisp and I don't know what else. Python seems to be a perennial favourite, perhaps because it's the language Reddit is written in, perhaps because it seems to be so controversial - you either love it or hate it. There is currently a spate of forth articles, although it remains to be seen if this will develop into a fad.First, you don't get the article and then you use your lack of understanding to make some sort of conclusion about my skills as a programmer?

It seems like you are full of yourself, I must say.

Here is a hint. I wasn't making an analogy on unit testing.

I was saying that the results of software bugs are arbitrary and unlimited in scope.

Of COURSE the best thing to do with a bug is to make a unit test, fix the bug, and test again. Programmers have many tools and techniques including test centric development to create our wonderful software environments. But in my story, that was boiled down to someone fixing a bolt and sending another car over. Should I have spent 1/2 a page in detail describing on how they developed a test car, chassis pulling framework, and bolt tension meter? No. That would have distracted from the MAIN point.&gt; please do not implement strings as an array of bytes

Some people want an array of unicode characters, some people want an array of bytes. Some people want both. The problem is that both of the first two camps seem to want to call what they want a 'string', and want the simple syntax `"abcdef"` to reduce to what they are calling a 'string'.

I have to side with the 'array of unicode characters' camp, and not simply for the ease of handling data but due to code-as-data -- this allows creation and use of interesting functions such as &amp;#8751; and &amp;#931; and being able to sensibly treat it as simply a string is a good idea.It's a feature for my karma, that's for sure. *grin*I disagree. Beating professional go players, even on a small board, is an incredible acheivement.  Ask any go player.  Having said that, translating that acheivement to the full sized board is probably much more difficult than what has been acomplished so far.Computers small enough to fit in a house weren't supposed to exist in my parents lifetime either.My feeling is that MC in itself is a dead end. It plays good strategically but is tactically hopeless. UCT is a fundamental improvement as it is scalable. The scale factor is daunting though. Any decent/bug-free MC program will play much better if you plug an UCT search tree implementation. But there are several possible ways to bugger it up. You need a good time allocation algorithm. More time in the opening and on hard moves. Recognizing hard moves. Finding optimum sample size that allows for the strongest combination of search/sampling. The optimum might be dynamic but it will be hard to find a good formula for that. My intuition is that the addition of local search or some analogue to nullmoves or multi-cut will be necessary for practical scaling to 19x19.Agreed - not to mention that it would make the language more usable for non-english speakers.&gt; That's exactly what the halting problem says we can never know.

No, that's not true. I enjoyed your writing very much, but I think crwper is right about this one. Unfortunately, I think you're echoing a common misconception about the Halting Problem.

The Halting Problem shows us that there is no _general_ algorithm to verify the correctness of _any given_ algorithm on _any given_ input. 

However, it is quite possible to write code to verify the correctness of a _specific_ piece of code on a _finite_ set of inputs. Developers do this all the time -- it's called unit testing!
Much worse than the problem of the state space is the problem that we don't currently have any good algorithm for evaluating a board position.

That is, even if we had a massive computer which could search the go tree, we don't have any good functions for determining which nodes are "better" than the others.And that article is almost as bad...[removed]Then take the bet.Very bad blog post about a very interesting subject.

If you want to try the new algorithms for yourself, I've posted how to here: http://programming.reddit.com/info/16k8v/comments&gt; That's exactly what the halting problem says we can never know. Knowing things and proving things is the same in my mind.

No - that's not what the halting problem says.  It says that given an *arbitrary* program, you can never predict whether it will halt.  It does not say that given a *specific* program and *arbitrary* program, you cannot predict whether it will halt.

There are large classes of algorithms that *always* halt, provably.  For example, any computation that can be represented as a fold over a finite list will halt.  Same goes for any structural recursion.  And obviously, primitive operations like + and concatenation halt, as do any operations composed solely of primitives.

You can build fairly useful programming languages that don't even allow programs that don't halt.  Epigram, for one.  And I've heard that programmers in mission-critical areas (eg. avionics) often write their programs in a non-Turing-complete subset of Ada so they can prove that they halt for all inputs.

I think your analogy is fairly apt for the mainstream world of C/C++/Java/scripting languages, though.  The real problem is unrestrained mutation of state: when any method can potentially change the state needed for any other method, there's no way to predict how the system as a whole behaves.  That's why I'm interested in functional programming: languages like Haskell and Erlang simply don't allow this unrestrained mutation, so you know your "bridges" always end up in the same place.Can I suggest "existence"?
And then, as soon as code is out there in the real world, some arbitrary piece of information not protected for by your tests comes along and wipes the whole slate clean. Or your unit test has a bug that allows things through that you think it should not.

Both are possible. We all have written unit tests for bugs we uncovered during runtime. I have no beef with unit tests but they do no prove a code to be "working". They increase the probability of that code working. A unit tested piece of code has a higher probability of working than a non-unit tested one. Even stricter bounds can be had with systems that do deductions on types. But they all will fail on something or another. It comes with the computing beast.

Look, I can prove "while(1);" runs forever on a machine with no other programs running. The simplicity of the system allows for such a proof. But when you get to larger and larger system, the proofs become larger and larger themselves. So for the sorts of program monsters we talk about in today's world, we do have some serious issues proving enough things about them. And we will never get the "perfect" proof that covers what happens when you run program X with program Y.

The halting problem also proves that you can never have a perfect virus scanner. So even if you trust how your code is originally distributed, later infection is never guaranteed.

Bridge builders work on guarantees. Some steel manufacturer guarantees that this piece of steel will withstand this much pressure. The bridge builder puts that into well known equations and designs a bridge that can hold at least X amount of weight (where X is far beyond normal use). This sort of thing is possible to a much lesser extent for a programmer. The halting problem is the reason why.liskell comes much closer to my idea of Arc.

Best of both worlds. Haskell with s-expressions.What are the stakes? What happens if I outlive you and it gets solved in the time after you die but before I do? What happens if you outlive me it doesn't get solved and I don't honor my end of the deal?&gt; And I've heard that programmers in mission-critical areas (eg. avionics) often write their programs in a non-Turing-complete subset of Ada so they can prove that they halt for all inputs.

Well that's not what we were talking about...

=)

Obviously, we can prove that easy things do halt. But when we think of real world programs, we do not think of easy things. We KNOW they are very hard things. The nature of the machine makes them hard. The HALTING PROBLEM makes them hard.

That's what I'm saying. I'm saying that the halting problem serves as the main reason that programming is not like bridge building. The computational space is far more volatile than the probability space of the constructed bridges movements.

It is the SPACES that are different. The halting problem shows us WHY they are different. It is a perfectly apt use of the halting problem because I obviously wasn't talking about restricted programming spaces.

&gt;That's why I'm interested in functional programming: languages like Haskell and Erlang simply don't allow this unrestrained mutation, so you know your "bridges" always end up in the same place.

As for using different languages, I agree that certain languages make it harder to make those mistakes. Haskell is certainly one of those languages. But it is Turing complete so it is not immune to this argument.

Programmers certainly can get better at making solid software systems. But we won't ever be doing the same thing as bridge builders.Some of those people seem to confuse characters with the character encoding.

Unicode defines over hundred thousand *abstract characters*, each identified by an unambiguous name and an integer number called its *code point*. That is really good and wonderful thing.

It also defines character encoding or character set that pairs a sequence of characters from a given set with something else (like octets) in order to facilitate the storage of and the transmission of text through networks. 

It is quite clear to me that any sane new language should use code points when testing for equality (with locale support of course). What I don't find self evident is why language spec should support only one character set in strings. 

It is good to have iso-latin-1 strings that give error when you try to put wrong characters in them (can have compact internal representation as plus). It is also extremely useful to have arrays that can contain all unicode characters and still return length of string as number of characters. What I don't like so much is languages that use UTF-8 as **internal** representation, outside of files, databases and network transfers. If you develop language that is supposed to be around after 100 years, how are you going to explain to those futurama people that you can't get (length str) in O(1) time.Ooh.
I have suggestions.

For one, I'd like to see an example of how to add reasonable file-based logging to a haskell program. Y'know, easily called, flushes to disk, thread-safe. 

Also, I was recently playing with trying to make a haskell version of http://blade.nagaokaut.ac.jp/cgi-bin/scat.rb/ruby/ruby-talk/240484
for kicks.. but I find time arithmetic is not as easy in haskell. If I complete my haskell vesion of it, or if someone else has something similar, that might make a good addition.As many others have already iterated: 9x9 is not a full size game of Go, it's the 3-minute version. -1; sensationalism...Why do so many people treat Torvalds like some kind of prophet?

We thank him for his kernel. That doesn't make him the authority on all things Unix-based.The halting problem is decidable for a large class of languages; those decidable by a turing machine with a finite length tape.  These are also known as linear-bounded automaton.  It turns out these are sufficient for quite many algorithms.
I found it more than a little amusing that Torvald's apparent gripe was with Gnome's inability to customize his pointing device's (mouse) buttons, an ability which is ostensibly better in KDE.

*Neither* of the two desktop managers permit easy configuration of multi-button pointing devices.

Moreover, my research indicates that this is *not* a "drivers" issue, but rather the requirement that a user must experimentally hand-edit /etc/X11/xorg.conf to specify the operation of a given pointing device's buttons.

Given that we're now into the twenty-first century, the need to hand-edit a text file to configure a pointing device is silly enough.  What makes it worse is that if the user doesn't get it exactly right, Linux won't completely boot, requiring the user to revert to recovery mode so that he or she can then "un-edit" xorg.conf.

I've been running Ubuntu for a bit more than four weeks now.  Overall I enjoy it quite a bit.  But there remain a bunch of "small" issues, like pointing device configuration, that remain intractable without additional, non-trivial amounts of research AND potentially system-breaking experimentation.

As far as I'm concerned, it's that aspect of Linux, more than the putative superiority of Gnome or KDE, that most needs to be addressed...How about this formulation: by 2027, a go program will be able to consistently play at the 4 dan level or above. Judging what that means is a gentlemanly agreement.

I'm willing to bet you $100 against, despite the historical precedent against betting against progress. You in?I don't recall too many features being removed outright from KDE, ever.  The [usability team](http://usability.kde.org/) had made vast improvements to base dialogs and configurations screens but that was more consolidating configuration options into better user interfaces (having the powerful with easy-to-use) than removal.Good luck explaining even "O(1)" to Fry.
I just added a GUI category. With any luck, someone will fill this in :-&gt;Context sensitive languages are not Turing machines. They do not get the benefits of the Church-Turing thesis. There are interesting things that we would like to compute that linear-bounded automaton cannot compute.

I was not saying we couldn't do better for a specific class of problems. But as long as programming is the human search for general algorithms for computation, we will not be giving up our Turing machines. We should definitely use the limited domains when appropriate, but in general we will be in the land of the lambda.&gt;What makes it worse is that if the user doesn't get it exactly right, Linux won't completely boot, requiring the user to revert to recovery mode so that he or she can then "un-edit" xorg.conf.

The worst you can do editing xorg.conf is prevent X from starting. You'll be in a console and can undo the changes you made to xorg.conf, then (under Ubuntu with GNOME) start X again using "/etc/init.d/gdm restart". Of course, you're completely right that being able to do this kind of thing through a GUI would be a great improvement, especially for new users, but as annoying as it might be for a new user to edit xorg.conf in a console and restart X, it doesn't require rebooting at all, whether normally or in recovery mode.Incidentally, all *real* programs are equivalent to a turing machine with a finite length tape - because computers have only a finite amount of storage space!  This gives an algorithm to test whether any real program halts: you simply simulate it through all machine states, and if you reach a machine state that you've seen before, then you know you're in an infinite loop.

Unfortunately, this algorithm is exponential with the size of the machine's storage, which makes it impractical for actual use. 4 GB factorial is what?Nope. Hi real name is Vinton Cerf. Check
http://en.wikipedia.org/wiki/Vinton_Cerf

Maybe he shortens it because of the pun.CFLs are a subset of LBA languages.  Anyhow, you do not need the full power of a TM for everything; that is the point.
That's correct; it doesn't start a new instance.  It still presents a problem if you want to control which Emacs instance it uses, however.That usually just causes bats to fly out of your nose.
Wow, I just tried to offer some constructive criticism.  I guess you didn't take it that way!

I didn't get the article at first and then it was explained to me. I then added some comments on how it could be improved. I know its tough to accept that your work isn't always perfect but when someone is trying to help you, you should listen:)Macros are overrated.Point taken, but consider too that for most ordinary users (which I remain, despite my best efforts not to be), the failure of X to start appears to be a catastrophic failure.

There is a utility (xev) that identifies the signals sent by a given pointing device's buttons.  I'm not quite smart enough (yet) to figure out how to use something like this to  (a) temporarily apply new xorg.conf settings, (b) allow the user to accept or reject those new settings.

In other words, to enable pointing device configuration to operate similarly to changing video adaptor settings, in which new settings are briefly displayed so that the user can either save those settings to xorg.conf OR wait for display settings to revert to their unmodified state...Hmm good to know, thanks.
I wish all companies going bankrupt would make their code available under a free license. OS/2, VMS (ok technically not bankrupt, but...), BeOS, Genera,...I don't see how the fact that there are less powerful machines that we can prove things about has anything to do with the fact that most computer programs fall outside those boundaries and hence fall into the Turning based argument about programmers and bridge builders I provided above.

Yes, there are less expressive things than a Turing machine.
Yes, they solve interesting problems.
Yes, we can prove things about them.

But that doesn't change the fact that most programmers live far closer to the weird Turing world than the less weird but still weird Linear Bounded Automaton one. And LBA proof is PSPACE-complete so proving very complex things will often be beyond the bounds of what our computer can do in a reasonable amount of time.True, configuring X for anything remotely non-standard is definitely a big problem holding back Linux from being suitable for less technical users. Making a new system where any setting can be changed through a GUI without restarting X (as in WinXP, or even Win98) would be great.to you, everybody who down-modded me, and everybody who up-modded responses to me in this thread: YOU ARE ALL COMPLETE MORONS.  how the fuck do you optimize something THAT IS IMPLEMENTATION DEPENDANT?

complete blithering moronsExactly. LBA acceptance is PSPACE-complete. The proofs are vastly more complex than the program itself. They are very, very hard. As programmers, we don't even bother talking about it typically because the idea is so absurd on the face of it. I'm not getting the LBA style argument that others here want to raise.

Thanks for your input!

To be legalistic, this is actually a virtual implementation of the underlying hardware system, aiming to be a drop-in replacement for the virtual Alpha environment that kept Genera alive after Symbolics gave up trying to make hardware CPUs.

The Genera OS environment is still owned by Symbolics; if you get an actual Genera box from them, you will get most of the software including source, but no license to run it on this VLM.

To elaborate on pkhuong's comment, the reason one wants a 64-bit environment for this emulation is that the underlying Symbolics machines used 36-bit and 40-bit memories (not counting the ECC bits). This allowed the system to support generic and type-safe operations as well as support for garbage collection even in the low-level parts of the OS.

Emulating a 36-bit machine on a 32-bit processor gets slow.Interesting question.  This particular blog post does not offer much insight though.

Business logic does not belong in controllers *or* models.  It belongs in business logic classes or modules.

Controllers should be sparse; their role is to route stuff to/from a caller to/from the business objects that do the heavy lifting.

Models should look after  essential features, but not business requirements. For example,  suppose I have a User class that contains an email property.    It's reasonable that the User class enforce that e-mail addresses are well-formed (e.g., not empty, contain exactly one '@', etc.).  

But if your application demands that all e-mail addresses must be part of the company domain, then either handle that in a business logic class, or mix-in that behavior to add it to User.  But don't build it right into User; it's not intrinsic to the basic concept of a user model.If there's a flaw in the bridge you have to make a change to your design too, no?  Doesn't that have the possibility of adding a new flaw to the bridge?

I wasn't trying to say that your article is wrong in any way, just that the comparison is exaggerated.  Your thesis, that building software is a different problem from building a bridge, seems perfectly reasonable to me.

I think you're approaching this discussion with a preconceived mindset.&gt; I agree with you that the author does sound like she/he tends towards "cowboy" coding rather than solid engineering.

How is that constructive criticism?

You called me a name and made claims about my engineering abilities without knowing a single thing about me.

There is certainly criticism in your comment, but you are stretching things to see a "constructive" part.

My point was that you didn't get the article, and someone explained it wrongly and you took that and somehow formed some opinion on my skill set. The only thing "constructed" was your opinion of my abilities and you "constructed" that out of whole cloth.

Again, I never said that my work is "perfect", you made that up. Perhaps you should listen to how much of a jerk you are being as I will never listen to someone who has never met me or programmed with me weigh in on my skills. Nor should I.&gt; Why do so many people treat Torvalds like some kind of prophet?

Not many do, and I dare say no hacker does. He is respected because of his merits as a free software developer and a healthy dose of his own (energetically, almost stubbornly, enforced) common sense. And he's a decent chap to boot.

I'll listen to Linus any day - even on tangential issues - over, say, Joel Spolsky and whoever that microsoft guy's called (scoble?)."How to Repel Software Developers"

1 - Advertise only positions that involve PeopleSoft.
2 - Require Microsoft certifications
3 - Ask for more years experience than the language has been available for
4 - Ignore the experience section
5 - Employ the use of a fax-shredder

A fax shredder is a regular looking fax machine except it has a shredding attachment connected to the output tray.  See, many companies are going to hire the boss's kid.  But they need a competitive quote so they post the job with a fax number.  You fax in your resume with all the hope in the world and little do you know it's been conveniently shredded upon receipt.

The other handy thing to use is the do-not-call database.  If you send a resume, they will add your name to the do not call database and you'll never hear from them again.

If you're still not having luck, find a recruiter.  You'll see all kinds of ads with 'head hunters' willing to work with you to find that all important career.  What they'll do is take your name, number and a copy of our resume.  They want the phone number so they can program the phone system to ignore your call.  They want your resume so they can use the nifty fax shredder they just bought.  Your name is so the receptionist knows who to hang up on.

Then if all else fails, read a whole bunch of articles on job offerings and interveiwer interviews that contradict each other and confuse you even more until you're using jackass phrases like "Worked in a competitive and deadline intensive environment" that you know don't mean a damned thing but that Mr. Jackass will toss your resume for if it's not in there.  Buzz words in a resume say to me, 'this guy is an idiot' but apparently, it's recommended you use as many as possible so you'll seem knowledgable.

Every time you think, "I hate my freaking job" also think of what you'd be going through if you didn't have it.
It took me like 50 games to beat GNU go the first time and it still gets me most of the time.Opinion: 'string' should be an abstract data type, which may be an array of bytes, or 2 or 4 byte words containing unicode code-points or even 'legacy' charsets if desired (i.e. a string should know its own charset). With appropriate functions for translating between charsets (losslessly when possible) and decoding and encoding to various raw external representations - an array of bytes - not characters!Remember the good ol' days, when Gnome hadn't reached 2.0, and used Sawfish/Sawmill as it's window manager? and life was good? *sighs*Well we both have a preconceived mindset as that is the nature of the human mind.
=)

Why would I spend that much time in the story making a perfect analogy to software engineering? How would you have written it differently while still trying to emphasize the weirdness of the computational space?

The probability of adding new flaws to bridges is much less than the probability of adding a new flaw to a piece of software. That was my entire point. You said that is a pretty big exaggeration and then you compared the "development approach" of the "bridge builder" to a "cowboy coder." It doesn't take a huge leap to see you were saying that the "bridge building process" I outlined in the story was a poor approach in your view.

I was obviously taking some dramatic license for effect there. But I don't think I exaggerated the weirdness of the programmers "space" at all. It simply happens to be unknownably weird and that's the difference between bridge builders and programmers.Something I discovered:  

* You will be asked for an IBM sign-in
* Bugmenot is happy to provide one
* Mindless clicking though to get to the goodies ends up submitting a request to mail a DVD to the bugmenot user address

I did not see any option for downloading an iso. :(A thing stupider than winning an argument on the Internet is, quite possibly, winning a bet on the Internet.  A 20 year bet at that.
Thanks for reading planet lisp. :-)
I bow to your eternal hipness, oh master of snark.There's a certain amount of hero worship, granted. But in this case, I think he represents a class of users that hasn't gained much from the last few years of desktop linux. Consider that lots of "hackers" still have some rather basic window managers with lots of terminals as their basic setup. We've got some pretty fancy terminal multiplexers (worse than Pike's mux from the early 80s, though) and on top a Mac/Windows clone for the less tech-savvy relatives.

You might as well use XP/Cygwin or Mac straightaway...

Scaling the GUI for programmers should be the task for Linux, not making a desktop for the elusive non-tech user...Instead of simply counting the number of killed pieces in very short term, what about trying to measure how much are pieces of each player surrounded by the pieces of the other player, thus estimating future gains ?&gt;he draws a clear distinction between enumerators (such as scheme’s for-each or fold) and cursors (such as C++ iterators, i.e., objects maintaining internally the “current element” of the collection being traversed).

thats a great way to think about the differences.&gt; What does this mean for designers? Certainly, following Norman's conclusion that people choose complex over simple suggests that interface designers should create complex interfaces if they want to sell their product. It may not be right philosophically (as simple is better philosophically), but it is better for the bottom line. People choose complex, so that's what you should design.

Maybe it's also better for the user, since that is what they prefer.  If people prefer features over simplicity, where does the 'simple is better' come in?

Perhaps I've misread this, but it comes off as yet another "designers know better than users what's good for them" piece.

I imagine the goal is to have both: Rich features, but with a simple UI.  But if you fail at this, and opt for simple over features, please don't blame the user for preferring another product.

(One other rant:  Does this article ever define "simple"?  Do any articles about simplicity and design do a good job of defining the terms?  It's a lot like  op-ed pieces that talk about "fair wages". Everyone likes "fair", and it all sounds so good and  right, but everyone has a different idea of what "fair" means, so round and round we go.)Actually, in Go, that would be absolutely spectacular (in fact, utterly unbelievable).  But that's for 19x19 Go; I don't think professional players play anything else (e.g., do professional chess players play simplified games?)&gt; Anyone who thought a computer 'would never play go' doesn't understand Moore's law.

Anyone who thinks computers will play Go doesn't understand Go.Just for perspective, this was from 1982 and he did end up at a US university (University of Texas at Austin)I really expected you to get flamed on this comment, mostly because of this statement:

&gt; I feel it is very much like having a bridge that you cannot prove the destination of.

Not that I *entirely* disagree with you, but algorithmic proof is a part of Computer Science that helps narrow the expected range of output, if not being able to have a completely predictable output -- of course, this is kind of simplistic.  Real-world use can complicate things a tad (hence your article).hey I was just agreeing with the person who posted above me. I'm sorry if your offended, it wasn't my intention:) But you need to calm down:)  We're all friends here![deleted]I'll chime in with everyone - a computer being proficient at 9x9 does not necessarily mean much on a 19x19 board.  To be honest, it'd be like saying a computer can play chess because they've mastered Hexpawn.  It's just far too scaled down a version to presume that the same skills are being exercised.

Question... this article is being panned almost universally in the comments, yet it has a nice ranking.  This seems paradoxical - does anybody have an explanation for this?  And is it just me, or is this a more recent phenomenon?&gt;In the last ten years, I can count the number of interesting job postings I've seen in Toronto on my hands (and, yes, I have the normal number of fingers).

(Insert nerdy swaggardocio about counting up to 1023 on two hands in binary)

*Edit: [I think it's a little too idealistic](http://www.byrneseyeview.com/byrnes_eye_view/how_not_to_get_the_programmer.html).*Haha, that's so funny. ["Moore's law" has never held](http://www.firstmonday.org/issues/issue7_11/tuomi/).It's less surprising if you've read a few of Wadler's other papers. He is a master at taking difficult concepts and translating them for a wider audience. In this case it was Moggi's work on monads for structuring effects (imperative actions) in denotational semantics. Wadler's insight was that  the same technique could be used for structuring imperative features in purely functional languages, but perhaps of more value is that his explanation is far far easier to read.&gt; But if I'm building a P2P app, I'm not going to pretend like my design might kill 200 people.

Well that's no funRemember this: "If I caught any one of my programmers writing an algorithm, I'd fire them!"[deleted]And I actually prefer FVWM based on GTK over gnome and kde. Not bloated as KDE. And yes - I tried gnome (for number of months)., until we had upgraded to SLES from old RH. And afer that  i saw that I can't configure gnome the way I like my wm.... I tried KDE for half a year... And now I'm back to fvwm. I took as a start [this amazing config](http://fvwm.lair.be/viewtopic.php?t=1478) and change it to suit my needs...

And it is blazingly fast...Unfortunately, none of them implements OpenID...I think the article totally missed the point.

The nearest analogy to a proof in CS is a 'closed solution', a solution where you put in the laws of nature and a description of a problem and get as a result a single formula which describes the problem and lets you calculate interesting things about the problem (for example how much load a bridge is able to withstand). Big problem here: Such a solution normally doesn't exists. It's not even possible to calculate the properties of a single carbon atom without making massive simplifications. And a system build out of 10^30 atoms? Forget it. Impossible. And even if it would be, it would use an idealized model of the system, while all real systems are far from ideal.

And even our 'laws of nature' are far from sure. Mankind simply derived them by doing lots of 'unit-tests' on nature (also called 'experiments' in science). But there is no proof and often they turn out different as we look at certain things a bit closer (think of Newtons gravity-law for example, or dozens and dozens of dogmas in life-science).

So in fact it's the opposite: Nature is totally 'unproven' while in CS we can (in principle) prove a lot. The halting-problem is quite laughable compared to real-world problems. So in principle software-creation is much more simple and secure than most 'real-world' things (as long as we don't have to consider hardware problems where our clean world of software is tainted by the unprovable reality).

But why is software often so buggy and so hard to build then? The reason is simply complexity: Even a simple program 10000-lines program is much more complex than most buildings. And while buildings always have clear specifications, software often is created 'out of the blue' with only a few boundary conditions instead of a real specification. And often even those 'specifications change' because it becomes obvious that the project would work out after the program is half-complete.

With buildings that's different because everybody knows in advance what they are good for. And if there are 'bugs' (and yes, every building has bugs, even small ones) they have to be removed or lived with. Nobody would have the idea to change the floor-plan of a finished building, but with software this really happens now and then.
[removed][removed]The real real reason: Building a bridge with software would just be a library call because the implementation is trivial, reusable, and common.

    b = Bridge.new([1,2,3], [3124,2341,-473])
    car.coords # 1,2,3
    b.enqueue(car)
    b.dequeue
    car.coords # 3124,2341,-473

Concrete != AbstractI'll take it. Would you be willing to donate winnings to a charity (of the winners choice) ? - if so the Long Bets Foundation (longbets.org) would probably be willing to hold  the money in trust until 2027.good points!  Perl works with everyone![deleted]Yeah, and super static. That's not my idea of a Lisp.Are you sure that your programs are doing as much IO as you think they are?  Looking at one of my Ruby programs, the ratio of lines spent doing actual IO vs. lines spent "moving data around" seems to be about 1 to 400.The recursive definition looks wrong to me. It should be 

             / v(m-1,n)              if (s_n) &gt; n
              \ 
    v(m,n) = -|      / v(m-1,n)
              /      \
              \ max -|                otherwise
                     /
                     \ v(m-1,n-(s_n)+v_m)

where v_m is the value of the item in column m.

Or am I missing something?WindowMaker ftw.why not go back to the 1995 Apple patent referenced in the above, that patents what looks like processes in the Unix sense.

http://www.google.com/patents?vid=USPAT5452456

[deleted][deleted][deleted]&gt; GreyBox can be used to display websites, images and other content in a beautiful way

I hate Ajax.Yeah, I commited a typo by accident, then fixed it.Disclaimer: I'm a KDE fan.

However, why all this fucking bitching and moaning!? Helloooooo people, it's GPL, FOSS... 

Don't like it, fork it. Linus should fork it, get a team of devlopers behind him, and turn Gnome into the desktop it should've been years ago. Hell, he can call it LinuG.Down for using tiny font. I'm tired of switching text size on every site.1. Perl isn't generally "more successful" than Lisp. In some fields (configuration/extension, teaching/research/AI) Lisp was much more successful.
2. There are a couple of scheme implementations that interact heavily with the UNIX world (guile, gauche, scsh).
3. So "Java is now a legacy programming language"? And what exactly is the state of Perl at this time?Uh, yeah I did. The first page has no info, just an intro then a list of links to chapters in the book. I tried the first chapter but… it is the glaze.The subject is interesting, and the comments are worth reading, even though the article itself isn't.[removed]Hey, they can't all be winners.  Besides, I thought everyone around here would get the Mahir reference.  Is that before your time? (How is it even _possible_ that there is a "before your time" on the Internet?)How are you sure that your proof is correct?

Lets define the perfect language as a formalism in which you can easily proof the correctness of programs written in that language. It is not possible to create that language. Because to do so you would have to create an implementation (compiler/interpreter) in an imperfect language for which you can not proof correctness.

Our only hope is testing, testing, testing and even more testing.Funny, I program in python and Java, where did you get the idea we use C++ for everything?Another one goes down for tiny unreadable font.Me too. I have heavily modified beautiful .fvwm2rc taken from Donald Knuth's web page: http://www-cs-faculty.stanford.edu/~knuth/programs/.fvwm2rc Then I just launch Emacs and no one has ever complained that you can't customize emacs :)

I have used KDE and Gnome too, but can't see the point.
yes I am.

And damn, I was thinking that would be a fun website to make, but it appears somebody's already there.

UPDATE: a $200 minimum is a bit steep, plus $50 to make a bet. Is somebody willing to give the other $100 for my side? I'll split the $50 with whoever wants to write the "go will beat &gt;4 dan in 2027" side.I understand your point, but I think you are still mis-using the "Halting Problem" when you mean something different.

As I said above, it is quite possible to write code to verify the correctness of a _specific_ piece of code on a _finite_ set of inputs. That's theory. But you're talking engineering -- practice -- so I should further qualify this with: assuming that your execution environment is constratined and execution proceeds in a repeatable fashion.

And of course, as you point out, execution doesn't always proceed in a repeatable fashion. Garbage collection may be different; thread scheduling may be different; device interaction may be different; etc.

It is true that _this_ issue makes it quite difficult to fully understand the behavior of a real machine. The behavior is complex at many levels, and the full set of interactions between levels is exceedingly complex.

But using "Halting Problem" as the moniker for this concern is at best misleading. You're thinking of the entire computer -- hardware and every layer of software on top of that -- as a single program whose correctness needs to be proven. But you're talking practice, not theory. In theory, even this very complex program -- the entire machine -- could be described and, over the finite possible set of inputs, tested. In practice, such a task would be unthinkably difficult.

In practice, the issue is simply that layers are complex and not fully tested, and that we build on top of those layers to such a degree that a slight problem in the foundation far below us might one day bite us -- and we'll have a hard (or impossible) time understanding why.Unbelievable load of shit on that blog. 
When he says Smalltalk doesn't play nice you can pretty much tell he never even bothered to download something like Cincom VisualWorks, which comes *loaded* with stuff for playing nice. It's just that he doesn't bother to check or learn. And when a men refuses to use his brain, he gets violent and slams away totally bezerk.Here on Earth, Perl is considerably more successful than LISP in every measurable way, even despite the incompetence of the people in charge of Perl 6, the Duke Nukem Forever of programming languages.

But your planet sounds like a much nicer place to be.  What are the galactic coordinates?  Does it accept immigration?C'mon... print version... 
http://www.computerworld.com/action/article.do?command=printArticleBasic&amp;articleId=9011832

This guy and his wife really landed on their feet after leaving Sandia, jobs at State and the White House!There is a difference between "does not work well with Unix" and "is an anti-social hermit."  Perl tries to dovetail with C and Unix, but what about other languages?

Imagine implementing Unix inside Lisp or Smalltalk.  Who plays nicely now?Unrelated to the article, but that's a perfect example of a bad URL. There's no reason to include *every* tag for a page in the URL!

Here's a shorter version: http://ian.blenke.com/which_virtualization_is_right_for_you.htmltorvalds is 100% correct.

and i'll add.... if i wanted a dumbed down UI that has nothing but contempt for it's users, then i'd use fcuking windoze[deleted]Screencast of offline web apps, plus a download to let you start making your own offline web apps&lt;shiver&gt; why would you want to?same here. i have borrowed and heavily modified stuff from here: http://dev.gentoo.org/~taviso/fvwm2rc.html. i keep trying new stuff, but the hobbit in me would say "from there and back again" :)[deleted]I've been saying for a while that if your language's string object doesn't have an explicit encoding property and throw errors when given invalid characters to append then it's broken.To make puppies cry.  Why else?

I think ZetaLisp had a C compiler...&gt;You might as well use XP/Cygwin or Mac straightaway...

By focusing on mechanism and ignoring policy, the X Window System allows things that Windows and Mac OS X cannot. For instance I would love to be able to use an Amiga style focus policy on those platforms but to the best of my knowledge it's not possible for anyone outside of MS or Apple to implement.It's the window manager's job to decorate windows and to determine what happens if and when a user clicks on that decoration.

The default window manager for GNOME 2.0 and above is Metacity, but this can be swapped out with something else if desired.As Linus so succinctly points out (paraphrasing in my own words):

It's easy to make things easier.  It's hard to make hard things easy.  But, if you only make things easier by reducing their functionality, eventually you hit a wall of uselessness where it's only usable to the least demanding of users.

The GNOME project is eventually going to make things so "easy" to use that they'll be useless to anyone other than newbies or people who have absolutely no real needs.

Thankfully, GNOME isn't the only available choice.  Vote with your feet (or, installed software), people.
[removed]What? Java a legacy language? Did the meaning of "legacy language" change suddenly?&gt; Hell, he can call it LinuG.

gigfeel is GNOME for experts, especially Linus[removed]No, he's not the final authority. But there's a lot to the content of what he's saying in this context. 

There's no such thing as a "simple, intuitive" interface for everyone. Each user comes with expectations and preferences, and these are going to be different. You can meet a certain basic minimum of them with a standard experience, but GNOME has just gone too far in that direction, without enough configurability on the upper end of the learning curve.It is actively being worked on at least; for a while it had stopped.[deleted]I half-heartedly encourage everyone here to visit every link on ESR's [trophy](http://www.catb.org/~esr/jargon/awards.html) page[deleted]For someone who submits a lot of technology items to reddit, you seem to have remarkably little intellectual curiosity and desire to develop your skills.

Do you also hate code kata, for instance?Your proof is correct because it follows the laws, theorems and rules of mathematics and holds up to peer reviewed scrutiny.St. Janes Capital has very effective advertising for their hiring via Google AdSense - they have the tagline as "do you think in closures? so do we." Several functional programming bloggers have taken notice of this, including screenshots. It clearly resonates with the kind of people they want to attract: functional programmers who get stuff done.

Also another company, EWT, has a Google ad line that says something like "do you use vi or Emacs? you're in good company [apply to work for us, plz]." These kinds of things show that programmers, not HR dickheads who just grep for three-letter acronym keywords, are apparently in charge of recruiting. It's a good strategy to attract smart people with rare skills.

So in other words if you can offer interesting work and you properly target your search, you can be effective.Don't worry - it hasn't.  That blog entry illustrates a very common phenomenon in technology discussions - because someone does not like a technology, they declare it dying and legacy almost in an attempt to 'wish' the technology away, ignoring all contradictory evidence.  Java is extremely healthy, advancing in capabilities, and showing no signs of decline.  

Also, what he says about Smalltalk is nonsense.  There are Smalltalk distributions which make access to the systems on which they run and to external libraries trivial.  One of the best is Dolphin Smalltalk, which has superb Windows integration.

It was a very poorly researched blog entry, containing little more than bias and wishful thinking.Almost every other programming language works in a world of byte-streams (and thus bit manipulation, strings, file systems....)

Smalltalk works in a world of objects, and LISP, the other language in the article, works in a world of lists and symbols. These things are, from the perspective of those working in the languages, "better" than the paradigm of the first paragraph.

One addition to the notion--Prolog, which works in a world of facts and assertions. I'd hate to interface that to POSIX, either.It probably wouldn't be legal. In the USA, the executive officer of a company has a fiduciary duty to break every moral, ethical and rational code of conduct in order to put money in the hands of shareholders. You know those privacy policies? Might as well be toilet paper. So giving away the company's code base would be reason enough for a judge to slap you.[deleted]This article was written by the author of Chronos[1], the *Smalltalk* date and time library.  From the site:

&gt; Currently, there are versions available for VisualWorks,
&gt; Squeak and Dolphin--although the Squeak and Dolphin
&gt; versions lack some of the functionality herein described.

It's damn certain he has used Cincom VisualWorks.

1: http://www.chronos-st.org/I've never used an Amiga and I'm curious; what is the Amiga focus policy? I'm familiar with: click to focus, focus follows  pointer, and sloppy focus follows pointer. Is the Amiga focus policy any of those?C and Unix are a sort of least common denominator. OSes, and most system-level libraries, are written in C, and many of said OSes are also Unix. To do anything useful, you need to interact with the OS and libraries, all of which speak C. Effectively, to integrate with the rest of the world, your language needs to speak C fluently.

What about Unix? The main problem that Smalltalk, Lisp, and Java have w.r.t. working with Unix is that they run in images, which are large, monolithic processes, while the Unix way of doing things involves lots of small processes. The fix? Replace the large, heavy VM with a small runtime that starts quickly and doesn't use too much memory.Sorry, but my eyes glazed over just looking at the 3-part table of contents (plus the appendices, so it's actually 4 parts) for this one word.

It seems you can explain a lot of genuinely interesting algorithms and concepts, with their combined text shorter than the explanation of monad.
I would hope so; although it will be years before he ever sees anything from the wrongful dismissal suit.

I'll try to get the print version next time I submit.

Hmm.  If Linux is crashing repeatedly in his hands, I absolutely think he should switch to Solaris.  Or Windows, even.&gt; Here's the best way of doing this: come up with a user setting screen for Beginner, Intermediate, or Expert. Default install at Beginner because you would expect an Intermediate or Expert user to go find this setting to set it.

Actually, that's a terrible idea. Users will invariably misestimate their "skill level". No good user interface should have a need for such separation.Thanks for posting this, I thought it was going to the old presentation from the Summer, but it's a new one from February 14 2007. It's probably pretty similar to the talk he gave at Pycon a few days ago...Yeah, it definitely seems to be happening. The TG and Pylons guys have been here (PyCon) getting drunk together... ;)thank god there are people who work hard at producing software instead of just talking about producing software. i sure as hell am not one of them.He said on his blog that he rearranged a few slides after hearing responses to this speech, but that otherwise they're identical.[deleted][deleted]In my recolection, it's click to focus, however the layering of windows is done manually.  That is, in the upper right of every window you have buttons to bring to the front or to send backwards.Who is circulating this old news all over again? The whole Linus vs. GNOME thing, plus Linus' views of KDE over GNOME, is recycled news from last year. And, for that matter, who cares? He introduced a kernel that now many other people improve upon, and the rest are add-ons such as GNU tools, display managers, and so on. Who cares what he thinks about KDE versus GNOME? I prefer GNOME, but my cousin prefers KDE. Do I want to hate my cousin or want to argue with him about it? No. I don't care. Do I want to post something on reddit about it? No.

The poster is trolling.&gt;Users will invariably misestimate their "skill level".

Not really.  Why would they go looking for this option?  Average users don't change their configurations much past changing the background.  Even if they mistakenly select Expert, they would most like just click it right back to Beginner in about 2 seconds.While there is validity to your point, in industry, where most software is written, the primary difficulties do not circle around OS bugs and the halting problem. 

To use your construction metaphor (you should patent that metaphor, btw, you could make bank at IT conferences if you were the only one allowed to use it) the firm contacted to build the golden gate bridge would fly in a highway and road architech from Iowa. He's only built on flat, solid ground.

He's never built bridges, and he's inexperienced with concrete and water construction techniques. No matter, the general consensus from architecture trade magazines is that its minimal risk, and you just have to allow a little bit more lead time. Senior management is confident he'll get the hang of it -- should only add two weeks to the implementation of the bridge (as opposed to a highway of the same length).

The chief on site engineer has been working on bridges since he got out of college. But he's only really experienced with certain parts of bridges. Specially, he's maintained the interface between the bridge and land. On both sides. He's not really familiar with the whole "middle" part of the bridge. He's had years to get certified in CA's bridge building codes and bridge building techniques, but he's 'too busy' and the company won't pay for the certifications.

Then there's your laborers. Some of them are bridge refugees. They've moved from collapsed bridge to collapsed bridge -- and they've never learned ANYTHING from their experiences. They're incredibly optimistic that "this time it will stay up for good", but they're not doing anything different, and its never occurred to them that they could be in any way responsible for all those collapsed bridges.

The second set of your laborers? They're fresh out of construction school. At the interview, they were asked about classes where they built or designed bridges. They said they took one and did well, but nobody bothers to ask them to *build an example bridge*. They're fresh faced but don't really know anything. They're going to learn all their tips and tricks from the old timers. A precious few might discover some construction weblogs and the pragmatic constructor, but if they do, they're only going to move when they figure out that the uncertified on-site expert doesn't like them contradicting their figures.

Then there's third shift. Third shift doesn't work on site. Third shift works off at a location where labor is incredibly cheap, and they slap together pre-fab components for the first two laborers. The quality of their pre-fabricated parts varies widely, and there's almost no instructions for the on-site team, so its up in the air whether or not they're saving the on-site laborers any time (or money) but management feels very comfortable knowing they can threaten to use off-site labor to build the whole bridge, to keep salaries low.

Finally, there's the lack of auditors. The auditors exist to inspect work completed by laborers and evaluate what's going to hold weight and what's going to give way. Think of them as leading construction reviews. They don't exist in 90% of construction firms. Everyone just assumes when a laborer is done with their work, that he really can tell that he's done. Even the ones fresh faced and just out of construction school with their shiny new construction degrees.

The bottom line I'm trying to make is that organizational difficulties weigh in more heavily as a risk/success factor in software projects (especially large software projects) than computer problems like the halting problem (unless you're google). For a lot of internal/intranet Web App/Web Service/Database App, the technology is fairly static, which can be a good thing. The problem is the process is also fairly static, and for most places who don't do things like mentoring, code reviews, process analysis, unit testing that means that success (on time and on budget) is "iffy".

Smalltalk *is* an operating system. Never forget that.The *REAL* reason why software is not like building bridges is that bridges are physical.  Even in plan form they have an intended physical reality.  People are naturally adept at asessing the relative merits of a design, and we have accumulated much knowledge in regards to mechanical calculation.

Programs, on the other hand are mostly abstract.  Though the most successful languages tend to conretecize concepts, this is still not enough - you are always dealing with concepts quite abstract to the ordinary person.

On top of this, code is really the plan - plans for bridges aren't that hard to make, allowing for many rewrites and possibilities.  In comparison to bridges, programs build much faster...Well, I assumed that then it'd be designed without such a C way of doing things.  Now, that would certainly be a fun OS to  mess around in.Okay so let me get this straight. You've got an an OS called Smalltalk and you want to build on top of it a different OS compatible with Unix but without the C way of doing things. I don't understand, can you explain?&gt;I don't recall too many features being removed outright from KDE, ever.

The consolidation often removed features.  Both Gnome and KDE usability group has done this.  Not a big deal, since most of those configurations they removed were more of a problem than solution.

&gt;Waldo Bastian: Configuration options have a cost, they make the software more complex and complexity leads to bugs. More configuration options also makes it increasingly difficult to write good user interfaces for them. On the other hand configuration options can help to make the software perform it's task better for specific users. As a developer you have to balance these two conflicting demands, for each extra configuration option you have to ask yourself, is it really needed, does it really improve the overall quality of my application?

&gt;If a configuration option adds very little extra value to the application but stands in the way of a usable interface then I think it can be justified to remove it. But that should be a last resort, the first goal of improving a user interface should be to optimize usability while preserving all features. Presenting the available options in a way so that they aren't overwhelming and form a logical and coherent whole.

[link](http://www.osnews.com/story.php?news_id=2997)I don't care what toothpaste he uses, and I don't care whether he likes Gnome.[removed]I set it to 1em. Thanks for the heads up.Thank for the comment!

I do agree with you that the physical nature of bridges brings in a lot of natural human knowledge that dealing with the abstract programming world does not. 

But I still think they are different problem spaces. You never have to worry that a bolt will eat your bridge. But one bug in a trivial spot in your program could mean the collapse of the entire software system. That was the point I wanted to make.Yeah but in my analogy, the bridge is the thing that gets you from point A to point B and software is what gets you from Input to Output. In a real bridge, you can prove that it will either get the care from A to B or it will collapse leaving the car BETWEEN A and B. With software, you can make no such guarantees.The range of input and outputs for a bridge engineering problem is far less than the range of inputs and outputs for any software problem. The reason that you don't calculate the position of all the individual atoms in a bridge is because the aggregate probabilities of the motion and energy of atoms that make the bridge via cables and beams is limited by statistical laws. We can KNOW that a certain beam can handle a certain pressure X before failing. 

We do not have any such guarantees with software.

As for complexity, I agree. I see that as the Halting Problem. We certainly can prove some things about simple pieces of software. "while(1);" for example will run forever as long as no other programs are on that particular computer. But when get to sufficiently complex software, any result is possible. Programmers have ways of containing that complexity and errors due to complexity, but they are not perfect. And given that the results of computation can be ANYTHING, I feel it makes my "alternate universe" argument more apt.

Thanks for your comments!/me asks air: couldn't this have been done by reinforcement learning? I mean, there are algorithms for this stuff.Those aren't his trophies; they're the Jargon File's. Most people who know the difference between what he added himself and the rest think little of the former and much of the latter.So, everybody is yelling at ESR for being an asshole, which is fine, but most of said people are missing the truth in what he said, which is that Fedora sucks compared to Ubuntu, particularly for those new to Linux. It would be useful if more people would notice this, so we would have less duplication of effort and more of a viable desktop Linux distribution.I'm not a professional programmer. I got a CS degree, but found out that I don't like coding for money. I understand that other people think differently, since this is their job and all, but this is just a hobby for me, so I don't read things that make my eyes glaze over. I was hoping this article would make monads/Haskell makes sense, but then I saw that it assumes you already know Haskell before you even start, so I gave up on it.

(For reference, I read all of "Why's (poignant) guide to Ruby" even though I never code in Ruby because that was interesting. OK, so it was mostly just bizarre, but at least my eyes didn't glaze over in the first paragraph when they assumed I already knew the language in question.)Heya Redrobot5050!

*laugh* I loved your analogies as well! You certainly nailed a lot of the problems that a real software system faces in the corporate world! As for patenting "An Invention on Making Money at Software Talks via Bridge Metaphors", I believe your post counts a prior art so now I'm doomed... thanks... ;)

My post was more along the lines of conveying the weirdness of the software world to a non programmer. But I will certainly be using your examples to further this line of argument. However, even with the systems you mentioned in place, you still will have problems, although the problems will be far, far more infrequent.

Thank you for your input!
To observe just how hard it is to walk in the manner that humans do, just watch a drunk person try to do it.Why hate something that is making your life easier on this very site?

Did you stop to think how your up/down votes are being sent to the server?What I meant was that if the AI guys at Google wanted to try to build something in Lisp or Haskell, we both know how far they would get. These ideas are shot down at the first step of Google management. I think this will limit Google's ability to build an AI.Those pesky facts...easier said than done.Wow.  Congratulations to Trevor.  

I've shoved Dexter myself; he is really stable.  And even before he was walking you got a real sense of a living presense from him.  He's usually swaying minutely, just like    a standing human.  Plus he's always exhaling compressed air.     

This should be on the main reddit.You have to use both pieces of software to know whether it's true
that one sucks in comparison.  You only have to read what he wrote
to know that ESR is being an ass.

Besides, ESR was deliberately lashing out.  If Fedora sucks, it's
not on purpose.
Nice.Hmm, true, you'd probably have to run it until the hardware wore out. You could start training with an accurate simulation though, maybe.After recently installing Ubuntu, my first reaction to Gnome was the same as Mr. Torvalds.   Now however, after nearly a week I've come to the conclusion that Mr. Torvalds is wrong.  Oh not about Gnome being dumbed down and limiting, but about feeling the need to change it.

If Linux were like Windows with an inability to change the graphics shell then Mr. Torvalds might have a point.   However if Gnome isn't the answer for you it's really not that hard two switch to KDE or another desktop that meets your needs.

When I first wrote about my Linux experience I said Gnome was great for children and receptionists.   I said that derisively.   Now I say it as a feature of the operating system.   If you're deploying Unix in a corporate environment to people who can't set their VCR clocks then Gnome makes perfect sense.

It's not an interface a power-user will ever love, but it is an interface a power-user will love to deploy.
That shows a remarkable lack of understanding of how bridges are designed and built. Each (non-trivial) bridge has to be custom designed for both location and purpose, just like all non-trivial software.awesome set of links , very cool .. :)Linus, if you're reading this, do not listen to justinhj. We *do* care what toothpaste you use, and your opinions on Gnome are even more important to us then ESR's opinions on Fedora.&gt; Not really. Why would they go looking for this option?

You need to read up on psychology of users. :-)

At the very least, you will have
1) a large percentage of your users thinking they're more capable than they really are (thus selecting Expert when they really shouldn't, then calling tech support when they don't understand something, and slowing themselves and tech support down by an order of a magnitude despite not benefitting in the least from the advanced features).
2) another large percentage of your users thinking they don't get things at all when really, they actually do. They will call tech support about questions whose answers they actually know, but don't trust to be 'safe' enough.

It's a no-no. A user interface must accommodate all sorts of users.He is not treated as a prophet, it is just that in this particular case he said outloud what many thought.  I administer a computer lab when non-CS student have to learn GNU/Linux.  I used to let them pick the desktop environment they wanted but now it's a KDE only lab.  GNOME hides configuration options that are so trivial that almost anyone whats to change than.  Do I really need to be root to change the keyboard layout?  This is in Québec and there are various popular layout and most have a favorite one and it is frequent to change for different tasks.  The US layout have well placed brackets and it is a good choice to code.  The CA layout has poor bracket placement but good layout for accents so we switch to it when typing French prose.  In KDE, changing your keyboard is easy and I have yet to see someone confused by the many but well classified options.  On the other hand, GNOME's oversimplicity is a source of frustration and lack of productivity.[removed]&gt; "I want something very simple: I want to configure my mouse button window events," explained Torvalds. "That doesn't sound so bad, does it? Everybody else can do it, gnome does not. My laptop has a two-button mouse, which means that I want the right button to do something more useful than show me the menu that I never use."

If only Torvalds had a MacBook he wouldn't have this problems. Another point to superiority of Apple hardware ;)I'd say that about 95% of those who call themselves "programmers" (and earn money as such) don't give a f'k about neither Perl nor Lisp (nor Python, nor Ruby, nor Haskell etc). For them it's C# FTW all the way (and/or VB). Kinda sad, yeah...check out stuff from hugo-de-garis. he has been doing this (kind of) stuff for quite sometime now. very cool, and probably scary at some level.

[edit: add some more details] check out this paper in 1990 (no less !) ECAI90, "Genetic Programming : Evolution of Time Dependent Neural Network Modules Which Teach a Pair of Stick Legs to Walk", Hugo de Garis, 9th. European Conf. on Artificial Intelligence, August 1990, Stockholm, SwedenThere's been more recent stuff [found, here](http://www.koj-m.sakura.ne.jp/index-en.html) - it's a bit more forgiving design (actually if you look at the movie, it's not comparable - nevermind!), I can't find the actual paper. 

Wasn't there a dinosaur at Disney World? But maybe it wasn't free walking.Learn how to easily create a UAC manifest for your apps on Windows Vista.It's a real question.  I just figured that if you didn't understand, you must not have done what I suspected you'd done.I'm my own evil twin.  Sometimes I have a goatee, and sometimes I don't.

I wonder if this is like that "I am my own grandpa" inbreeding thing.  Maybe I shouldn't bring that up in public.[deleted]If you seriously don't think that there are bugs remaining in the Linux kernel, you are out of your mind.  I got ringside seats for the LVM and VM code debacles in 2.4, and nothing seems to have changed with regards to the development process (and cast of characters).  Google had a kernel hacker on staff when I worked there specifically to debug nastiness related to memory mapping and ATA/IDE drivers... no sense relying on other people to solve your organization's problems.  However, not every company can afford to hire a dedicated kernel hacker, and that's when you get to make hard choices like these.

Linux, like all software, has bugs.  Zealotry will not fix them.  Sometimes nothing will fix them fast enough for a client, and that is exactly when it is time to move.

I rolled out several Windows 2000 Server boxes at one job, not becuase I liked deploying Windows (I hate Windows, as do all sane admins), but because there was no reasonable way to get Windows Media Server running on Linux at that point in time.  If you'd rather futz with some jury-rigged solution in that situation, great; but I don't think you'll be pulling in a lot of repeat business with that approach.  YMMV.
You're asking why it is useful to rule out side effects such as mutable state or IO, or some other side effect, in some code?

Put simply, the possibility of side effects makes impossible a wide range of optimisations, algorithms and coding techniques.

Problems ranging from compiler optimisations, such as inlining, to atomic memory transactions to many concurrency abstractions, are simply not feasible unless you can establish that a given fragment of code performs no side effects.

So, as a clever programmer, you know this, employ a monad to control what effects a fragment of code is allowed to do, thus allowing you to write more flexible and better code.

----

The control of side effects is analogous to the control of jumps in the old days. I'm sure many people wondered why they would want to use only 'while' or 'for' when a simple goto would let them do whatever they want.

Of course, its clear now that unstructured gotos leads to a spaghetti control flow. So now, unstructured use of side effects can lead to spaghetti state managment.

*Control those side effects* -- you'll know more about your code and it'll be simpler to extend and maintain.

----

As a nice example, in the #haskell channel we allow people to evaluate arbitrary code, without using a sandbox. How is this possible? The eval bot type checks the code first, if it is of IO type, it is disallowed. Thus any side effecting code, that might write to a file, or access the network, is ruled out by a simple inspection of the code's type.

A nice example, IMO, of why it is useful to be explicit about side effects.True, and it's a good point.  However, as some have satirized above, it seems that tiny tweaks have the affect of radically changing the program itself, what it does.  This is, of course, misinterpreting the metaphor, however, it is the fault of the metaphor that it is easily misinterpreted.[removed]Similarly, Perl and Unix work in the world of files and text.  Consider the importance of the regular expression in Perl and pipes in Unix.

Lisp does work in the world of lists, but it's easy to muck around in the binary world too.  Have a look at some magic in chapter 24 in Practical Common Lisp (http://gigamonkeys.com/book/).  And I just wrote a basic wav file reader.

Implementing POSIX in Prolog?  Yuck.  Almost as bad as implementing it in C ;-)Alas, you have it backwards.  Lisp/Smalltalk is an operating system and saving an image is just saving your session.

Unix runs in an image (in the form of the filesystem),  takes forever to start (even after the kernel has loaded), and has a VM as large as Lisp's (several Lisps compile to native code).

When saving or loading a Lisp image, imagine its hibernating to disk or restoring from hibernation.  After all, your computer isn't really running until it's running Lisp.On two hands in binary? So - the geeky way then is to say "four you"? Oh, and BTW - care to give me 10 (0xA)?[windowlab](http://nickgravgaard.com/windowlab/) does just that.No. Still don't get the monad thing.Machine code is machine code.  Some machine code acts like its Unix, and some machine code acts like its Lisp.  The machine code can "call functions" or "add numbers together" or "make a pointer to a function."

The first machine code to run is the "real" OS, getting control of the hardware and other processes.  Any other machine code must kowtow to the "real" OS, but it can still define its own ways of calling functions or making objects.

See L4Linux for an excellent example.  Linux running on top of the L4 microkernel (who's the OS now!)Me wonders what $100 would buy in 2027.... :-/It does not help when all the text in the site is using the same minimum font size.Thanks!sacrilege to mention Haskell and shitty ruby in the same sentence

yea, I said it[removed]&gt; Replace the large, heavy VM with a small runtime that starts quickly and doesn't use too much memory.

You missed something:

* Lisp is an OS, cue the Lisp Machines. They ended up failing because the generic hardware was so much cheaper, but a Lispm still is the holy grail of Lisp, and that kind-of is what Emacs + SLIME is.

* Even more so, Smalltalk is an OS. A Smalltalk environment is its own OS sitting on top of you regular OS, that's exactly the way it was designed at PARC (and that's the way they talked about it). Smalltalk not being its own OS would, I fear, not be Smalltalk anymore. It couldn't be that tightly integrated to its (smalltalk) environment, and it couldn't get the uniform interface in which it strives.

[removed][deleted][deleted]nah... I consider myself a good programmer but I don't particularly love to code. I like to solve problems, and code is often the solution to a problem, but coding in itself, once you know what you are doing, is not that interesting.  I am at the point where I much prefer to design a program, go through it with a junior programmer, and let them do the bulk of the code which I can review, help refactor when necessary, and be around to solve problems when needed.  The real programming is in the design, by the time you are writing code you have (hopefully) solved all the problems and from then on its (admittedly skilled) grunt work.

Someone who is all about code is not a good programmer, he is most likely a newbie programmer.  Sure he might program a nice application, but it probably won't be what you wanted, and so what's the point?Last time i checked, KDEs keyboard config was under "Accessability". In GNOME it's System-&gt;Preferences-&gt;Keyboard and you don't need the root password.Uh I just realised that this article is a response to another article which says pretty much what I said in the above comment.  Well, obviously, I think the original article is much more on the money.

http://rentzsch.com/notes/programmersDontLikeToCode&gt; Vote with your feet (or, installed software), people.

    apt-get install popularity-contest&gt; Did you stop to think how your up/down votes are being sent to the server?

They're pneumatically propelled through a dedicated intertube right?Just for kicks - with how many (really different, not "dialects") programming languages do you think you can say you can safely pass the FizzBuzz test?Pacman = RMS?!KeepCash is dedicated to keeping cash in your pocket by showing you the best deals and online money saving coupon codes on the web.[deleted]As does [AmiWM](http://www.lysator.liu.se/~marcus/amiwm.html) and [FVWM](http://www.fvwm.org/) (if set up correctly). My point is that it can be done on any system that uses the X Window System because that leaves policy to a separate program (the window manager). This is unfortunately not the case with Windows and Mac OS X.One of the big differences between Java and Smalltalk is that Java does not run as an image - code and classes are loaded on demand.  Also, Java is certainly not a monolithic process - most implementations are extremely good at making use of multithreading (which is why Java is the choice for app servers on large multi-processor machines).  Also, it doesn't have a heavy VM - the Java VM itself is small and starts up very quickly.  (The reason why Java apps can seem large is a combination of the size of libraries and the fact that the default memory allocation of standard VMs is set high, but a single command-line switch can reduce this to almost nothing for simple programs). Modern Java VMs on mobile devices can fit within 1MB even while including the ability to JIT compile Java code.
This should be as easy: "What is hexadecimal F plus 1 in binary?"Oh, the irony."Antiobject" is such a terrible name. "Particles and fields" or something similar would serve to describe the idea much better.

An idea closely related to this is used in the underlying simulation of the game The Sims. In this game, interactive objects in the world advertise their presence and features to nearby avatars. All object-specific behaviors and assets are stored on the interactive objects rather than the avatar. The avatar's behavior deals with satisfying a fixed set of needs, but these needs are satisfied in ways defined by these objects.

For example, if an avatar walks close by an apple, the apple would advertise its presence by telling the avatar that it can help satisfy the "food" need; all the animations, sound effects and behaviors related to eating the apple (e.g. the avatar chewing on the apple and eventually discarding it) are stored on the apple itself, not the avatar.

The two principal benefits are:

1. Performance. Think of it as a push model as opposed to the usual pull model. It's generally much cheaper for objects to tell the avatar about their services than for the avatar to constantly query the scene database for nearby objects providing certain services.
2. Composability. The avatar's behavior is fixed, but the objects are all self-contained and can be dropped into an environment and just work. The avatar is responsible for weighing object-provided interactions against each other in making a decision. For instance, suppose the avatar wants to satisfy the food need and there are two objects, an apple and a cake, that can be used. The cake makes the avatar fatter, so it would perhaps choose the apple if it's health conscious. But if it's really hungry it might eat both, despite its misgivings. Note that the objects don't have to know about each other or coordinate to decide which one of them the avatar should pick. This division of responsibilities is pretty much ideal for a game like The Sims.

The Sims designers called this idea "smart terrain".CONDENSED VERSION: I am unhappy only because I am an early-adopter-- and apparently I put money down for the OS itself.  I can only passingly envision OSX as an alternative to late-adopting Vista.The example httpd included with gforth uses gforth FFI to do its own networking.Mmm it boggles the mind, "being an early adopter" of something which is scheduled to be released years ago.This is totally cool. A genuine breakthrough, because the difference between this and a robot that can run and jump and climb stairs is only quantitative - balance is the hard problem.Probably about 4 or 5 I'd be sure of. That doesn't sound like much, but switching to a new language is usually just a matter of syntax.

I'm fairly sure that the main algorithm is almost identical for C-style languages. It's the setting up the environment and initialising the application which is the biggest issue.

I remember hiring for a PHP position a little while back. I set what I though was a simple task: read a question and set of answers out of an XML file, output a form and log the results to a plain text file. 

Out of a dozen applicants (mostly CS graduates) only two could complete it. Most took hours on their attempt. Not a single one managed to produce a result that wouldn't break if the XML file changed. The best results were stuff like if($_POST['answer1'] == 'Apples') { $question1 = $question1 + 1; }

Most of the applicants failed to parse the XML file, even though I said they could freely look at the PHP documentation, and use any example code they wanted.Off topic: Interesting to see they actually used the [Googe](http://technorati.com/tag/Googe) logo (seeing that the talk was given on Valentine's Day).Goddamn, amen brother. So true.&gt; click-to-focus but not raise-on-focus

treewm has these independent settings, which can be made per-window -- or, like any, per-desktop, applying to its children:

1. Raise on Click

2. Raise on Enter

3. Focus on Click

4. Focus on Enter

-- with window stacking controlled (by other options, and also:) by vi-mode commands and mouse-using commands.

You can create ad-hoc but very usable UIs out of normal windows with such fine control.  For instance, I've had three windows -- an 'output' xterm, an 'input' xterm, and a Tk window co-sized to the input xterm.  The Tk window was directly controlled by, and the xterms running netcat connected to, processes of an Erlang node that itself connected to a server over TCP.  treewm helped with this by letting me: remove enter-on-focus on the output xterm, and also remove the output xterm's entry from the taskbar of the subdesktop that contained these windows.  Entering that desktop would put focus on the input xterm, in which I could simply type commands.  For a 'command mode', I could use a treewm hotkey to switch to the Tk window, which simply fed X keystrokes to its controller.

Trivial to put together, usable enough that I never bothered to do anything 'proper' for this program meant for my own use, and undistracting to the real work of writing that program.  With even a slightly less capable WM, I'd've needed to hack up a GUI just to get started -- and 'terminal-like' GUIs are, um, fun.  Yes.  (OK, I would actually just use Emacs as the hacked-up UI.)

X's flexibility in window management is the main reason that I quit OSX after something like a nine-month trial.Once a windows applications programmer sitting at the table next to mine asked: "What is the difference between a list and an array?" He's been a programmer for about 20 years.Yeah I though it had something to do with tubes.Cool. 

Not bad for robot without hands. I think hands help the balancing act lot in humans. You don't even need tho swing them lot when walking. Move left foot front and right hand back. Little pendelums in hip level might be helpful. Like between it's legs :)
I don't think that's why Chris is leaving Vista at all. He's not leaving because he's an early adopter, he's leaving because the system (1) has too many bugs to be usable, and (2) it's slower than running Windows XP which has 95% of the features that he wants from an OS. By saying he's leaving because he's an early adopter gives MSFT an "out" if you will; it puts the fault on Chris for trying to run an RTM OS. The fault here is with MSFT for releasing an OS with so many unresolved issues, not with Chris for choosing to install and work with it.&gt; Moreover, my research indicates that this is not a "drivers" issue,

Uh, no.  Full stop.  Your research either missed Linus's description of the input he wanted from his pointer (blatantly not requiring X configuration -- he says that GNOME ignores the events that X sends, even if you can't follow anything else) or his own assertion of the locus of the problem (GNOME's special treatment of certain X events, and its deafness in one location to the one he wanted to configure GNOME's response  to).  There are no 'drivers'.  There is, morever, no 'recovery mode' (although your SysV init settings might classify one runlevel as such, simply by it not starting X automatically).
&gt; The real programming is in the design, by the time you are writing code you have (hopefully) solved all the problems and from then on its (admittedly skilled) grunt work.

How wonderfully anachronistic! This throwback to the 80's, back when many big companies had architects produce hundreds of pages of design documentation that were then thrown over the wall for the code monkeys to implement, is without a doubt responsible for uncountably many software project failures.

This notion that design and programming are strictly separated activities, one preceding the other, is incredibly flawed. Even if it seems to work well enough in your field for you to continue practicing it, I'd advise you to not generalize your experience to the status of a general law.he says: 'lisp macro'&gt; Don't like it, fork it. Linus should fork it,

No, he should:

THE STEPS OF RATIONAL AND EFFECTIVE RESPONSE TO PROGRAM INEFFACY

1. Bitch about it.

2. Bitch about it to its maintainers/developers.

3. Bitch about it publically.

4. Ignore it in favor of something else, or fork it, or skip to 6.

5. Compare it failingly to 'something else'.

6. Advocate bitchingly against it forever or until 'it' is finally replaced by an acceptable 'it'+N (if you notice).

Any 'bitch' step may be accompanied by a patch.  Bitching is mandatory in all steps; patching is not.  You may at any point skip to the last step.

THE INCREASING LEVELS OF LOVE FOR A PROGRAM

1. When you discover a fault, you toss it aside.

2. When you discover a fault, you bitch about it and toss it aside -- for now.

3. When you discover a fault, you run through the aforelisted 'rational responses' without any hacking.

4. When you discover a fault, you run through the aforelisted 'rational responses', with hacking and even patches, or as much effort in debugging and nagging.

5. When you discover a fault, you come to the aforelisted step 4 and then fork the program.

6. When you discover a fault, you either commit a fix or allow another developer to fix it.

As you can see, Linus loves Gnome -- to a whole -4-!  I've used plenty of programs (e.g. nano) in which I noticed a fault, tossed the program aside, and then advocated against it years later.

Please do not continue to offer your suggestion that only these two conditions exist: 'have no response', 'fork it and name it after yourself'.
Someone who is all about [(big) design up front](http://en.wikipedia.org/wiki/Big_Design_Up_Front) isn't a good programmer.  He is most likely a project manager.

&gt; I much prefer to design a program, go through it with a junior programmer, and let them do the bulk of the code...
&gt; Someone who is all about code is not a good programmer, he is most likely a newbie programmer. Sure he might program a nice application, but it probably won't be what you wanted, and so what's the point?

...?

If you want to be a better programmer, try [pair programming](http://en.wikipedia.org/wiki/Pair_programming).  It'll make you and the other person a lot better.

If you want to create better designs, try [test-driven development](http://en.wikipedia.org/wiki/Test-driven_development).  It'll make your solutions simpler and the tests *are* your requirements.I like daisies.&gt; he's an early adopter gives MSFT an "out" if you will;

So what if it gives them an out?  He -is- an early adopter, that -is- the source of his problems: what does MSFT have to do with specific third-party programs not working yet on his new system?  Or with his new, extra-bloated OS performing poorly on his pre-bloat hardware?  Morever, the point of the 'condensed version' is that -he- says these things, just more verbosely and less directly.  He doesn't say "man, I wasted hundreds of dollars on this -- I should've waited a while!"; he says "nothing works yet -- I'll try it again after the next upgrade.".

&gt; (2) it's slower than running Windows XP which has 95% of the features that he wants from an OS.

And yet, he has already promised to use it anyway -- and later he'll have a follow-up blog post that won't directly say "This sucks much less, now that I approached it as other than an early-adopter."  I'll try and meet that one with an appropriate 'condensed version', if it ever happens.  I've yet to have a sequel.
[deleted][deleted]Could do it in C/Java off the top of my head. With documentation on hand; Javascript, Perl, Tcl, PHP, Scheme, m68k assembly. Probably plenty of others, too but would actually have to spend a few minutes re-learning the language.
Sure, but why is it important to know? FizzBuzz demonstrates a simple ability with conditionals. Hexadecimal... well, if you're hiring a C/assembly programmer, sure, but...What is a windows programmer? How do we recognise them in a programmer's crowd?hi there2+2=4Is this really a programmer problem or a job-interviewing problem though? Maybe these problems sometimes arise because whoever is doing the hiring simply refuses to call anyone who doesn't have an academic background in CS, or happens to currently be in a non-programming position.woooooooooooooooooooooooooooooooooooooooooooooooooooowwoooooooooooooooooooooooooooooooooooooooooooooooooooowwoooooooooooooooooooooooooooooooooooooooooooooooooooowwoooooooooooooooooooooooooooooooooooooooooooooooooooowwoooooooooooooooooooooooooooooooooooooooooooooooooooow&gt; How do we recognise them in a programmer's crowd?

Actively, the same way we recognize anything interesting about anybody.

Passive sensing only helps if you can follow concurrent active sensing by other parties ("hey, what do you think about linux?"  "nothing-- I'm a windows guy.").  --although, for some reason, t-shirts and only t-shirts are a trustworthy indicator, when they indicate anything.

EDIT: stickers, too.  T-shirts and stickers.
Ah, Windows programmers.  I remember a guy who used to sort by strings by inserting them in a sorted listbox and then pulling them out again.&gt; I might cut someone some slack

Sure.  I liked an earlier post wherein the 'FizzBuzz questions' were categorized, and the point was to quickly discover blatant ignorances.  Not knowing that an octal digit maps directly to three bits (and a hexadecimal digit to a nibble) indicates an acceptable enough blatant ignorance in many kinds of programmers, and a suspicious or unacceptable lack in others.[deleted]&gt; Once a windows applications programmer sitting at the table next to mine asked: "What is the difference between a list and an array?" He's been a programmer for about 20 years.

That's pretty bad and inexcusable, but it's sort of understandable. Most of the time there isn't much difference between lists and arrays unless they hold a lot of elements. Deleting something for the middle of an array for instance, takes linear time but most often that doesn't matter. Forgetting about efficiency, they are both just sequences of things.

What I find amazing is that many working programmers don't know anything beyond "sequences of things". They don't know anything about hash tables or sets for instance, let alone anything as exotic as a tree. I find it amazing because these are basic tools, like a spanner or a screwdriver is to a mechanic.

Admittedly, most of the programmers I know come from a C or Delphi background. Hopefully things are changing as people start learning to program in languages such as Python where such datastructures are commonplace. 

That would be cool. Maybe then the programming profession will have taken a great leap forward and will have finally embraced cutting-edge 1960s technology! (Or whenever it was that the basic datastructures were invented.)
The title should better reflect the awesomeness of the penultimate sentence.&gt; Someone who is all about (big) design up front isn't a good programmer.

I'd also find him suspicious as a project manager: they should know something about [the history of programming methodology](http://thinking-forth.sourceforge.net/).I find it amusing that out of three solutions to the FizzBuzz test posted in that article, the latter two are incorrect. :-)

I just whipped one up in Ruby. It was good exercise. But I guess I'll follow Jeff's advice:
&gt; James: it's amusing to me that any reference to a programming problem-- in this case, FizzBuzz-- immediately prompts developers to feverishly begin posting solutions.The thing that the author is missing is that Smalltalk is one of those languages that takes one idea (everything is an object) and runs with it.  It's a language to prove a point, and remaking the world in its image (no pun intended) was the idea.

There will always be a place for languages which do interop well, but let's hope there's always a place for languages which ignore the past and push new boundaries.Perhaps the language he used has no concept called a "list"? In Objective-C/Cocoa, for instance, there's three "collection" classes; NSSet, NSArray and NSDictionary. I can only assume that you mean the difference between NSSet (which is unordered and therefore has no guaranteed indexes) and NSArray.

I think a far better question would have been whether ordering is necessary to solve a particular problem, or wasteful.Here is a personal CSS that kind of fix bad sites CSS.

    html, body {
       font-family: Arial ! important;
       font-size: 1em ! important;
       line-height: 1.25em ! important;
    }
    
    input, select, textarea {
       font-size: inherit ! important;
       font-family: inherit ! important;
    }

It does not work for sites like wikipedia, where the designer first set ridiculous small base font size, then set body font size to 120%. You get your body font set to 120% * 1em, which is too big.

Here this CSS makes the comments view much more readable.Yet another reason why multiple inheritance sucks.Even in the world of RAD and high-level frameworks, basic hexadecimal knowledge can be useful, such as when dealing with binary data formats. E.g., you should be able to figure out in your particular language/framework/environment how to verify that JPEG image data is proper by checking its first three bytes.Unsurprisingly, nothing in the article suggests what the title says...I don't agree that the result of a computation can be anything. If you use a statically typed language, the result of a function with a given return type is always in a very limited domain. This is far from everything.

And there is no way to know the structural limits of a certain beam. It can be made sure with a certain (very high) probability, that it won't fail to hold a certain pressure, but it's not proved. For that reason there are safety limits which simply uses multiple beams so that a single failure won't break the whole thing. All this is very similar to software testing. There are no mathematical models which can calculate the exact strength of beams made by a real processes, the only way is to test it. And if the process varies (and every real process does) then also the strength will vary. 

Also the halting problem and complexity are different things. Complexity is always a problem, ever in 'real world' things. Look at a modern airplane for example which has a very high complexity and which is also very difficult to design for this reason. But in critical areas as in airplanes where large security margins aren't possible (because they would increase the weight of the whole thing to much) tests are crucial. Every plane is tested while being constructed, after being constructed and afterwards in regular service intervals. And nonetheless there are situation of structural failure (not often, but it happens).

The real world is much 'worse' than the relatively simple world of software. But in software construction this relative simplicity is used to massively increase the complexity - and this leads to the difficulties. The other reason is that it's very hard to visualize software compared to real things.

Certainly four; PHP, Ruby, Objective-C and C#*. I have experience with others (Perl, Python, various forms of BASIC, AppleScript, ECMA-262/JavaScript/JScript/ActionScript, plain C, whathaveyou), but I wouldn't trust myself to be 'fluent' enough in order to flesh it out in a reasonable amount of time.

*) Arguably, the latter two are dialects of each other, but I consider them sufficiently different from each other.You can also ask them to explain the difference between a list and an array.[deleted]Sorry, couldn't resist. Can someone write a shorter one?

    fizzbuzz = map f [1..100] 
        where f x = a ++ b ++ c 
                  where a = if m 3        then "Fizz" else ""
                        b = if m 5        then "Buzz" else ""
                        c = if m 3 || m 5 then ""     else show x
                        m y = mod x y == 0I was hoping some people would try the test in the comments. It seems the biggest problem wasn't that they couldn't DO the task, just that they couldn't follow the specs.Cool, but I'm a little confused. Do robots like ASIMO not balance themselves? I could push ASIMO and it would fall over? If so... damn, all this time I was so impressed...Ruby:

    1.upto(100) {
      |i|
      if i%3==0
        print 'Fizz'
        if i%5==0
          print 'Buzz'
        end
        puts
        next
      end
      if i%5==0
        puts 'Buzz'
        next
      end
      puts i
    }

PHP:

    for ($i=1;$i&lt;=100;$i++) {
    \tif ($i%3==0) {
    \t\techo "Fizz";
    \t\tif ($i%5==0) {
    \t\t\techo "Buzz";
    \t\t}
    \t\techo "\n";
    \t\tcontinue;
    \t}
    \tif ($i%5==0) {
    \t\techo "Buzz\n";
    \t\tcontinue;
    \t}
    \techo $i."\n";
    }For testing that sort of knowledge, I'd be tempted to give them the framework of an SMTP server, and tell them to write a command parser for it. Note whether they test for commands using 32-bit integers, or string comparison. Ask them why they did it that way.

Although I suppose that obfuscates the problem a little too much...
I feel obliged to make the obvious comments:

a) this looks scary

b) I for one welcome our self-balancing robot overlords.I think I might know what's wrong with these questions:

In my 7 years as a professional programmer, I've never had to write an implementation of a linked list, or ever had to write something as trivial as the FizzBuzz problem.

Asking me to do such a trivial task is the same as asking me to calculate the cosine of 2*Pi.  Sure, I probably know it, but I haven't done such a trivial exercise in so long that the "task" involves brushing off really dusty parts of my brain instead of actually testing how well I can program.

A much better question would be "Take this RSS feed and output every article whose title includes a provided keyword".  Or "Write a script to select certain records from a database and output them."  While these are trivial tasks, they have *some* kind of bearing on reality and real problems and I can draw on my experience to work a solution very fast.The biggest problem would be to find out what the modulo operator in most languages I "know" would be, usually the parts that are very similar in lots of languages give me trouble, not the parts that are clearly different.This one in python might not work, because I don't have a Python 2.5 interpreter at hand. But here it is:

    def fizzbuzz():
      def f(x):
        m = lambda y : x % y == 0
        a = "Fizz" if m 3        else ""
        b = "Buzz" if m 5        else ""
        c = ""     if m 3 or m 5 else str(x)
        return a + b + c
      print map(f, range(1, 101))
It has been a while since I coded something in c...

    #include &lt;stdio.h&gt;

    int main(int argc, char** argv)
    {
      int i;
      for (i=1; i&lt;=100; i++)
      {
        if (!(i%3)) printf("Fizz");
        if (!(i%5)) printf("Buzz");
        if (i%3 &amp;&amp; i%5) printf("%i", i);
        printf("\n");
      }
      return 0;
    }I love that they have a robot whose sole purpose seems to be to shove the other robot. 

This is an incredible first step not toward robot overlords, but toward robot bullies.[deleted]Thanks for the advice.  I pair program all the time, the bulk of my day is spent sitting down at other people's machines solving problems with them.  The real joy from programming, for me, is in debugging, especially when I can't figure out the answer.  When you have enough experience programming and the type of work you are doing is all fairly similar (same language, same environment, same type of thing), then programming becomes a bit of a chore, as you don't come across as many problems.  I'm not saying I write bugfree code, but I'm hardly being challenged by the kind of day to day work we do.

As for test-driven development, I've done it, I don't do it all the time, because sometimes it's not appropriate.  I probably should do it more, but I'm not saying I'm perfect.

Added to that, when I say I don't particularly love to code, that doesn't mean there aren't times where I really enjoy coding.  I code a lot in my free time on projects that interest me, and I code a lot of short programs just to make my life easier.  I love writing a cool one liner for someone when it saves them 15 mins of copy pasting.  But the grunt work of simply typing up my class diagram for example doesn't thrill me.  See, it's not the *coding* that I love, it's the problem solving aspect - find a problem and using code to create a solution.[deleted]for (i=1; i&lt;90; i+=15)
    printf("%d\n%d\nFizz\n%d\nBuzz\nFizz\n%d\n%d\nFizz\nBuzz\n%d\nFizz\n%d\n%d\nFizzBuzz\n",
           i, i+1, i+3, i+6, i+7, i+10, i+12, i+13);
  printf("%d\n%d\nFizz\n%d\nBuzz\nFizz\n%d\n%d\nFizz\nBuzz\n",
         i, i+1, i+3, i+6, i+7);
But you invocation of the uncertainty of the halting problem is similar to a bridge builder saying, "I can't have any guarantees! What if all the atoms spontaneously tunnel to a different location!"

I think a more accurate analogy would be that programmers (often) mix their own steel, concrete, make their own bolts, etc. Most times this is a bit dumb, sometimes it makes sense (although usually at a larger scale, where it becomes the question of buying or building middleware, for example).That has to be without a doubt some of the ugliest code I've seen this week.No offense, but if I were an interviewer, I'd ask applicants to take readability and maintainability into account. ;-)FenPhen didn't say he is a GOOD project manager.Oh, the irony. The font on that page is more decorative than intended for reading. It's big but it's not a pleasure to read.

(And, of course, the tips themselves are of "wishful thinking" kind as every beginner web-programmer/designer would tell you - i.e. not cross-browser at all).I've been going through interview hell for about 5 weeks now as my company tries to hire 2 regular employees and 3 contractors. We have to interview about 5 people to find one with decent talent. Mind you, these are **experienced** people.

It's very common to find people who haven't programmed using a language in a very long time (if ever). A lot of candidates seem to be able to use high level "builder" tools that let you graphically design a program adding only the smallest snippets of glue code. However, when faced with writing something from scratch they flame-out.

It's these do-it-all-for-you IDEs that I partially blame. They allow the non-curious to program. It's a shame, because curiosity is what separates the mediocre from the great when it comes to programming.i personally hate comments like these. yes, most of the people who read reddit know how to program - if not just a little. we dont need to all whip out our wangs and compare sizes.

let me reiterate:
no one cares if you can write an incredibly simple snippet of code in 3, 4, or 10 languages. really.
ASIMO uses the Zero Moment Point method to plan its movements to be balanced, often relying upon large ankle torques:

http://en.wikipedia.org/wiki/Zero_Moment_Point

Although Honda does claim a lot (below), but ASIMO will fall over if pushed, and can't yet get up :-(

http://corporate.honda.com/press/article.aspx?id=2007010954311

"ASIMO (Advanced Step in Innovative Mobility) is the world's most advanced humanoid robot, with the ability to run, walk forward and backward, climb stairs, turn smoothly without pausing, and maintain balance, even while walking on uneven slopes and surfaces. "


Genius!.... a glittering career in corporate Java contracting awaits you!  ;)I didn't mean to imply that design and programming were strictly separate activities.  I work in a very small team and typically only with one other programmer on each project at a time.  The two of us will design together, and then he will go off and program whatever we have talked about in the meeting.  I'm sitting in the same room and if he has a programming problem I'll just come and help.  Design is an ongoing process, and during development refactoring is constant.  My point is that in a half hour meeting I can sufficiently explain the problem and the way to solve it to somebody else, who can then program it.  If something is needed in a hurry, I will do it myself.  I *am* still a code monkey.  And hundreds of pages of design documentation is something I don't do.  Class diagrams, ER diagrams, Screenshots.  Those are my preferred pieces of documentation, and enjoy getting those designs sorted out.  I love having a good requirements analysis, but I hate doing them.Hmm ... C, Java, PHP, Perl, javascript, Tcl, Haskell, Python, QBasic, Visual Basic, x86 assembler in DOS, PovRay scripts, various shellscripts; probably a few more. Would be fun to do it in pure hexadecimal machine code, eh? I've been a computer nerd since I was a kid. Wanna hire me?Jeez, I just read your BDUF link!  That's not what I meant at all.  Design is an ongoing thing and refactoring is inevitable as your program progresses (usually because of additional requirements).One of the irritating things about the Haskell community is that so many Haskell hackers have been steeped in graduate-level mathematics for so long that they can't resist using arcane terms like "category theory", "intuitionistic logic", "impredicativity", and "mod".

    fizzbuzz = unwords $ f [1..100] ["", "", "Fizz"] ["", "", "", "", "Buzz"]
               where f x y z = zipWith3 g x (cycle y) (cycle z)
                     g x "" "" = show x
                     g _ fz bz = fz ++ bz
People who have brain damage in cerebellum usually walk little like Dexter. They can't fine-tune their movements smoothly and rapidly. We humans use our cerebellums to do that. You can try its effect at home, because alcohol messes with our cerebellums and you can see the results in your walking.

[Cerebellar Model Articulation Controller](http://en.wikipedia.org/wiki/Cerebellar_Model_Articulation_Controller) (CMAC) is artificial neural network that models the function of our cerebellums. This has worked quite well in robotics and machine control. It would be nice to know what kind of algorithms he uses. 

&gt; i personally hate comments like these.

Unless I'm missing something, you didn't reply to any comment in particular, so I'm unsure what exactly you are responding to.

&gt; we dont need to all whip out our wangs and compare sizes.

It's not about showing off; it's about practice and testing oneself and comparing each other's results, thus perhaps learning from each other.

&gt; let me reiterate: no one cares if you can write an incredibly simple snippet of code in 3, 4, or 10 languages. really.

Evidently, job interviewers *do* care.ok, no one here cares if you can write some simple code in x number of languages.No. A rational ghost-agent would *seek* the Pac-Man scent, but *avoid* the RMS-scent. :)Thank you.  I'm preparing for the IOCCC :)

The same idea without unrolling:

    for (i=1; i&lt;=n; i++) {
      switch (i%15) {
      case 3: case 6: case 9: case 12:
        printf("Fizz");
        break;
      case 0:
        printf("Fizz");
        /* FALLTHROUGH */
      case 5: case 10:
        printf("Buzz");
        break;
      default:
        printf("%d", i);
        break;
      }
      printf("\n");
    }
[deleted]Don't forget Steven Yegge's 5 phone-screen questions.

http://steve.yegge.googlepages.com/five-essential-phone-screen-questions

&gt; 1) Coding. 
&gt; 2) OO design. 
&gt; 3) Scripting and regexes. 
&gt; 4) Data structures. 
&gt; 5) Bits and bytes. 
&gt; 
&gt; Please understand:   what I'm looking for here is a total vacuum in one of these areas. It's OK if they struggle a little and then figure it out. I&gt; I’ve also seen self-proclaimed senior programmers take more than 10-15 minutes to write a solution.

The author clearly believes, as do so many people, that the best measure of skill and/or intelligence is the ability to solve trivial problems quickly.

And quite likely he will complain as loudly as the next guy when many of the "brilliant minds" he hires fail to show the slightest spark of innovation or anything deeper than a surface understanding of what they do. But they were Olympic-level FizzBuzzers! What could possibly go wrong?They say that if someone makes a generalization about a big group of people, that generalization says more about the person making it than the people they are attempting to describe.I seriously doubt that 199 out of 200 applicants for programming jobs cannot write any code whatsoever, and I also doubt that most applicants cannot write a loop that counts from 1 to 10.

Especially disagree that comp. sci. graduates are generally worse programmers than other candidates looking for a first job

Agree that a firm grasp of recursion (and for that matter pointers and data-structures) is becoming rarer, but this is a reflection of the shift towards 'softer' languages (java, python etc) and away from harder languages such as C.

I think that articles like this are on a psycological level, more to do with the inadequacies of the author than any real failing of candidates for programming jobs.This is the correct way to define the font size, and works with Firefox, Safari and IE.

A web designer should not try to fix broken browsers by breaking his site.Please stop making assumptions about what other reddit users care or don't care about.First biped, possibly, but not the first balancing robot. [Big Dog](http://video.google.com/videoplay?docid=5349770802105160028) has been doing this kind of thing *incredibly* well for quite a while. (Watch around 2:30 in the movie when the guy tries to kick it over and it recovers...)Actually, read the comments he makes on the page. Specifically, he cares not so much about the actual solution, but about people being able to *understand the problem specification*.`[ ]` You have read the remarks about Dylan.

This problem is also practically non-existent in CLOS.[removed]The problems he's describing can also be adequately explained by nervousness on the part of the interviewees.Maybe it's a problem with people that think that employers are essentially interested in paying them to learn the essentials of a craft, so they apply for jobs that they are grossly unqualified for under the assumption that they are some manner of geniuses that can absorb knowledge rapidly but have just never been sufficiently motivated to obtain said expertise.

If a 15-year old dog walker applies for a surgical position at the local hospital, we do not necessarily fault the hospital's recruiting practices for the apparent delusions of the applicant. Though there is probably a fair amount of value in a criticism that industry treats Computer Science departments as programmer factories.

Perhaps Computer People(tm) are more delusional about their aptitudes than other workers, or perhaps it is just more common for them to complain about incompetent applicants on the Internet than steel workers and school administrators.A blow to Emacs? Well, Emacs is only getting stronger from blows :) Because as soon as a useful feature appears in an IDE, some grizzled programming guru implements it for Emacs. Brilliantly. 

Have you seen the latest Emacs tools for Ruby/Rails development? Its on http://rubyforge.org/projects/emacs-rails/. 

You can see it in action in this screencast:  http://emacsonrails.drozdov.net/

This mode has all the 'cool' features from other popular tools (Eclipse or TextMate) - such as code snippet expansion, project navigation etc. 

It has none of the bloat associated with Java apps. Emacs is rock-stable and fast. And you have access to thousands lines of excellent code written for Emacs over past 20+ years. You can work with ruby debugger, access databases, manage version control program, spellcheck etc etc.

We advertised for a new programmer recently.  What kills me is they send in a resume with "experience in C, C++, Java, VB.Net, C#.Net, ASP, PHP, Perl..." and then you look at their education and they graduated from college last year!  How can you have effective experience on 8 different languages in one year?
…by having programmed while and before in college?You'd better hurry up!  Submission deadline is tomorrow.Netbeans? A blow to Emacs? ...most apps are designed not by programmers, but rather analysts supervised by some committe that doesn't know its ass from its elbow and has some alterior motive (spyware) on its agenda.


most programmers have never seen the environment (users, purpose, etc) for the apps they're writing.  plus there's tons and tons of foreign programmers who not only write shitty barely legible code, but also have such vapid understanding of what the analyst specs say, due to their ineptitude at english, that they're effectively illiterate.  but even the programmers who are fluent and understand the app rarely give a fuck cos they are treated as mere pawns by their employers who are more interested in the number of line of code they write than doing quality work.


i know i've been there[removed]The FizzBuzz program is trivial, but unlike the linked list thing it tests basic looping and boolean testing, which *every* programmer needs to do. It might be slightly forgivable if someone didn't know the modulus operator, but even then a decent programmer should be able to write code that tests whether a number is a multiple of another.
What, you aren't impressed that I can translate an algorithm into 13+ different programming languages? Then how else will I  establish my superiority over everyone else?There is a very simple explaination to this:  The programmers who actually know how to program aren't out there looking for jobs.  They already have jobs.  The guys/gals that suck, are constantly on the prowl for jobs and are never getting hired, or sticking anywhere for too long.Without cheating, I just tried completed this in four languages in 16 minutes.  Scheme took 11 of those minutes, including downloading and installing a Scheme interpreter.

Ruby:

    (1..100).each do |i|
      if (i % 15 == 0)
        puts "FizzBuzz"
      elsif (i % 5 == 0)
        puts "Buzz"
      elsif (i % 3 == 0)
        puts "Fizz"
      else
        puts i
      end
    end

C:

    #include &lt;stdio.h&gt;
    
    int main() {
      int i;
      for (i=1; i&lt;=100; i++) {
        if (i % 15 == 0) {
          printf("FizzBuzz\n");
        } else if (i % 5 == 0) {
          printf("Buzz\n");
        } else if (i % 3 == 0) {
          printf("Fizz\n");
        } else {
          printf("%d\n", i);
        }
      }
    }

Java:

    public class Fizz {
      public static void main(String[] args) {
        for (int i=1; i&lt;=100; i++) {
          if (i % 15 == 0) {
            System.out.println("FizzBuzz");
          } else if (i % 5 == 0) {
            System.out.println("Buzz");
          } else if (i % 3 == 0) {
            System.out.println("Fizz");
          } else {
            System.out.println(i);
          }
        }
      }
    }

Scheme:

    (define (val i)
            (cond ((= 0 (remainder i 15)) "FizzBuzz")
                  ((= 0 (remainder i 5)) "Buzz")
                  ((= 0 (remainder i 3)) "Fizz")
                  (else i)))
    (define (fizz n)
            (if (= n 100) (list (val n)) (cons (val n) (fizz (+ 1 n)))))
    (fizz 1)
Heh. Back in the day I would have said "8 or 9" - I knew Pascal, Modula2, FORTH, BASIC, APL, COBOL, C, C++, SAS, Java, a couple of assemblers - and at least 3 more if you included shell languages. But it's been at least 10 years since I've written anything except C and bash.Lots of things can be useful, but there is also a cost involved in learning, especially if the material to learn is bug-ugly, like C++ (very useful, unfortunately) and vi (not half as useful as people think). 
I shouldn't have to reinvent JPEG, surely? It's my library's job to check the magic numbers.
Now that I think about it, I can swap two variables without using a temp... at least if they are integers, or if we can pretend that they are, as in C. But both that way of switching and unsafely pretending something is integers when it's not, is very, very ugly.I spent more time trying to get the code to appear correctly in my post, than I did writing the code in the first place! ;-)If you isolate (or resign) yourself to just the programming aspect of the job, don't be surprised if your job is offshored somewhere.

Every place I have contracted at, where offshoring took place, the lowest-hanging fruit were the "I only do code" programmers. Those actively involved in the larger process were deemed too knowledgeable and too valuable to lose.

I find it very narrow-minded to hear that good programmers like to code and not get involved in the whole software development lifecycle.&gt; that the best measure of skill and/or intelligence is the ability to solve trivial problems quickly.

Rather, that an inability to solve trivial problems in an appropriate time indicates a deficiency.  Not answering immediately -- OK, maybe you know this but haven't needed to internalise it, so you need to reason it out a bit : fine.  Or maybe you're nervous, and have very many more distractions (e.g., the 'human interaction' one.) from the part of your self that solves such problems : fine.

But a brilliant-minded, good-at-programming, innovative person with deep understanding who can't solve any of the trivial problems proposed here in a reasonable amount of time?  Bullshit.A tiny bug in a real construction can do this too: Just a small cable getting lose is enough to stop a car. A slightly wrong calculation can collapse a whole building.

It's not possible, that a car can become a pink elephant because the constructor plugged a cable in the wrong slot. But it's also impossible, that a bug mutates a mail-application into a driver for a graphics card or a 3d-animation-package into a accounting-application. Bugs simply make things fail. In reality and in software-creation.
Doh. 

    1.upto(100){|n|puts n%15==0?'Fizzbuzz':n%5==0?'Buzz':n%3==0?'Fizz':n}

(That's Ruby code)a friend of mine is currently working for a startup, and has been since his second term of college. he will graduate with almost four years of real world experience writing PHP apps.

and even barring exceptional cases like that, if you have a class where you spend 3-4 weeks working on a 10,000+ line C/C++ program with a group of 2-3 people, doesn't that count as experience? sure, it's not the same as if you were a professional coder, but if nothing else, it means you can code.I agree with you. I submitted this story because Tim Bray has been one of the more famous Emacs advocates out there and I found it interesting that he is considering alternatives (on the other hand, he is working for the company behind Netbeans...)

I have a comment on his site, currenly waiting for moderation, that points him to this screencast, where he can see that Emacs already has the one feature that impressed him in netbeans (visible CVS/subversion status is the file navigation tree) .
  
http://platypope.org/yada/emacs-demo/

[deleted]it gets even more confusing in PHP , as everything is an array in php - even strings are - and there's no such thing as a list.  

$mystring="hello world";
print $mystring[0];

output&gt; "h"


in php "list" is a function. 

Here's a question - why does the front page (of reddit) say there are 22 comments on this article, when in reality there are only about 16 right now? I've noticed that a lot, where it will say "4 comments" when there are only 2... why the discrepency?They can be.   People who cannot program and have not gone to college will not apply for a programming job.  People who have gone to college may major in comp sci, and that gives them some background to THINK they can program even if they cannot.

Now anyone who can program will become better by getting a comp sci degree.   Therefore when you hire someone without a comp sci degree, you are more likely to get someone who can do some programing, but you will not get the best possible programmer.    If you hire someone with a comp sci degree they might be incompetent, or they might be the best programmer on your team.   

Note that it is possible to self-study.   So there are good programmers without a formal degree that I would call computer scientists.   The degree represents some basics that seems irrelevant to most without a degree, but that a critical in subtile ways.My own extreme bias is sitting next to me screaming, "It's CentOS! I blame CentOS!"

But then, as I said, I have a rather extreme bias against CentOS, so feel free to ignore what it's screaming at me, as I make an effort to do.  What I'm really interested in is what kernel they were using, and what hardware.  I've seen some rather... extreme... problems on 64-bit hardware that, in the end, turned out to be hardware problems (boo Asus A8N-SLi Premium).

That being said, in any type of production environment, stability usually trumps freedom.  I'd much rather see a win for Solaris and PostgreSQL than for something even less free.  Licensing aside, at least it's a familiar(ish) environment that you can still leverage all your existing talent with.  Sure, I'm bummed that this is apparently an example of a bad Linux site, but I'm not one of those slavering gibbering zealots screaming "GNU/Linux or die!"Clean clothes, dirty minds.That view is a lot more nuanced than what I picked up from your initial post. Cool. :)

As you can infer from my post, I take strong exception to the not uncommon characterization of programming as mere routine exercise, something best left to the glorified button pushers that call themselves programmers, while the oh-so-smart architects have to do the intellectually challenging work.

The kind of systems I work on don't lend themselves to this division of labor, and I doubt it works well in any other field of programming. _Assuming_ you have perfect requirements from the get go, _assuming_ you're experienced and clever and damn near prescient, then yeah, this method _could_ work. Maybe. But probably not. Any design bulletproof enough to be implemented by a mouth-breathing underling is probably already isomorphic to an implementation.

Good programmers like to code. They don't like to type very much on keyboards though, for that indeed _is_ a mechanical exercise in the literal sense. :)Worse - "list" is not a function, but a syntax construct for assigning to multiple vars:

list($a, $b) = array(1, 2);Did the submitter even skim the article?

The title is actually _Python's Super is nifty, but you can't use it_, and the "can't use it" only refers to the fact that it's (in the article's opinion) too easy to use incorrectly, even though it's otherwise as *un*harmful and indispensable as Dylan/CLOS (`call-`)`next-method`.The irony here is that those riddles have actually become valuable by virtue of their repeated use.

If you're interviewing today and aren't a n00b, you should have at least been asked those questions a few times (because, you've gone on interviews) and should recognize them and possibly even know the answers, already.  How can you claim to be a senior engineer having worked for 10+ years at several different companies and /not/ have heard those riddle questions at least a dozen times?

If I ask someone who claims to have lots of actual work experience one of those riddle questions and their response isn't "oh, yeah, I remember that one" ... I can pretty safely call bullshit on their actual experience.Well, I'm disappointed. I wish I could see the source code of the other winning submissions, and an explanation of the statistics.

5\tobvious troll\t324\t0.0403\t07/02/27 23:56:43\t14B / ?B / ?B

In my defense, I learned long ago to write verbose code as part of the whole "code life cycle" process.[removed]Wow. That takes me back - the first machine I ever got my hands on was basically programmed in "op codes" and we had a card reader for it, but no card punch.

I would sit in AP Calculus class with a pencil, punching holes in cards, and taping over "typos".
Wait - Objective C and C# are "variants" of each other? My exposure to C# approaches NIL, but I thought it was more of a Java clone?main()
{
printf("1\n"
"2\n"
"Fizz\n"
"4\n"
"Buzz\n"
"Fizz\n"
"7\n"
"8\n"
"Fizz\n"
"Buzz\n"
"11\n"
"Fizz\n"
"13\n"
"14\n"
"FizzBuzz\n"
"16\n"
"17\n"
"Fizz\n"
"19\n"
"Buzz\n"
"Fizz\n"
"22\n"
"23\n"
"Fizz\n"
"Buzz\n"
"26\n"
"Fizz\n"
"28\n"
"29\n"
"FizzBuzz\n"
"31\n"
"32\n"
"Fizz\n"
"34\n"
"Buzz\n"
"Fizz\n"
"37\n"
"38\n"
"Fizz\n"
"Buzz\n"
"41\n"
"Fizz\n"
"43\n"
"44\n"
"FizzBuzz\n"
"46\n"
"47\n"
"Fizz\n"
"49\n"
"Buzz\n"
"Fizz\n"
"52\n"
"53\n"
"Fizz\n"
"Buzz\n"
"56\n"
"Fizz\n"
"58\n"
"59\n"
"FizzBuzz\n"
"61\n"
"62\n"
"Fizz\n"
"64\n"
"Buzz\n"
"Fizz\n"
"67\n"
"68\n"
"Fizz\n"
"Buzz\n"
"71\n"
"Fizz\n"
"73\n"
"74\n"
"FizzBuzz\n"
"76\n"
"77\n"
"Fizz\n"
"79\n"
"Buzz\n"
"Fizz\n"
"82\n"
"83\n"
"Fizz\n"
"Buzz\n"
"86\n"
"Fizz\n"
"88\n"
"89\n"
"FizzBuzz\n"
"91\n"
"92\n"
"Fizz\n"
"94\n"
"Buzz\n"
"Fizz\n"
"97\n"
"98\n"
"Fizz\n"
"Buzz");
}


Who's the smartass now?Reinforcement learning?  No way.  Think about how big the state space is for that robot.  It has many joints.  Reinforcement learning works in 2 or maybe 3-4 dimensions.  That robot has *at least* 6.  I can't tell how many degrees of freedom are in each of the joints, but human hips have 3 dof, which alone would increase the state space to 10.

If we have the same number of bins for each dimension, the number of bins increases exponentially with the number of dimensions.

Trust me, this really is a problem.[removed]Save Scheme, they're all imperative languages; the only difference is the syntax. It'd be interesting to see an implementation in a different paradigm (say, Forth) if anyone's willing to demonstrate.The example I use is this:

As a kid I have Atari Writer, and lusted after WordStar which everyone else was using.   (Back then I wanted WordStar on my beloved Atari, not a PC) 

In Jr High they taught us some Apple II thing that I cannot recall anymore (but by then everyone knew the Apple II was out, so they didn't try to tell us it was going to be useful).   

In High school they were proud of their new computer lab, that ran the latest software - the same as industry was running - WordPerfect 5.1 (which was just released the summer before).

In college all the computer labs had Microsoft Word, and we had to type all our papers on a computer.  Nothing was taught though, we were expect to be smart enough to figure out trivial things like word processors.

My first job used Adobe FrameMaker for everything (on Solaris).

Since then I've changed jobs several times.   Most places use Microsoft word, but every once in a while someone uses Open Office Writer.    

On my personal systems I use Kword.

I suspect most people have a similar story.   The exact details might be different, and depending on your age you might have started at a different point.   The bottom line is there is no reason to expect today's dominate systems will dominate next year.That's interesting, although the wikipedia article wasn't the most clarifying thing in the world... Are you saying that ASIMO bends its ankles to move forward, while this robot leans its torso forward, walking in the sort of 'controlled fall' manner that humans do?Could do it in Lisp off the top of my head. 

If I'm lucky, my C version would have the semicolons in the right place. Suffice it to say, I don't use C very often. 

Oh, also Fortran 77 and Mathematica. Whee!Now, statements like that piss me off. When I got out of school I was fluent in at least that many languages.

Just because most programmers are idiots doesn't mean I am."One way or another, ten or twenty years from now you'll see robots like this walking around."

Yes, we shall.  On the battlefield... as well as policing our streets.What's a reasonable amount of time? In most such tests, the answer is: "as fast as possible."

Of course, if you are just a very slow thinker, plain and simple, you must be stupid. Everyone who went through public schooling "knows" that!

Another one: what if being given idiotic intelligence tests makes your brain shut down from boredom? What then?Apologies in advance for any slight inaccuracies, but I'll try:

Objective-C is a superset of C adding OOP, semi-dynamically mapping back to a bunch of vanilla C functions.

C# is its own language whose syntax takes a lot from C++ (and thus arguably Java as well). C# is not a superset of C (but then, strictly speaking, neither is C++). However, Objective-C, C# and C++ all take lots and lots of features from plain C, and in virtually all cases allow you to mix and match C code in.

With Objective-C++, you can even put C, C++ and ObjC code in one and the same file of code, but that's about as ugly as it sounds.[removed][removed]sad, but i can see people being confused or ignorant of mod.  should only take a few seconds to figure out.  1 mod 3 = 1, 2 mod 3 = 2, and so on.[deleted]I've learned to do this kind of thing in PostScript from [this link that was posted in reddit](http://www.math.ubc.ca/~cass/graphics/manual/) days ago.[deleted]Luser.  Get back in your locker.

Let me be more specific.  I upgraded my main desktop box to Vista a little over 2 months ago.  Yes, a couple bits of hardware did not work; a printer that was more than six years old, and a scanner that was six years old.  I welcomed the excuse to replace both of them with a single multifunction machine that is better than either was and cost all of $250.

Some vendors have not updated their software.  That is a problem with your vendor.  If you have mission critical software that won't run on Vista, then don't frigging upgrade.  Would you switch to a Mac if you had PC based critical software?  Of course not.  It isn't that extreme, but geeze the number of OLD things that DO work on Vista is astounding from a technical standpoint.

My core 2 duo box is clearly faster running Vista than it was on XP.  Don't know what the deal is with AMD.

I've heard a lot of bitching about NVidia on Vista.  Again that is a vendor issue, and if it is critical, then don't upgrade.  Personally I've got an NVidia 6800 which has never done anything funny on Vista while driving 2 monitors at 1600x1200, and the games I do run are fine.

The Tags and Date Taken thing happened to me too, I was annoyed until I realized I had done it to myself.  It was easy to fix.

A partial list of things that are working fine for me: iTunes, Winamp, Photoshop, 3D Studio, Office 2007, wsftp, VS 2005, local website testing on iis7, sql express, tortoiseSVN (this was broken for awhile, they fixed it), daemon tools, dvd fab, dr. divx, efax, winrar, 7zip, nero 7, xna game studio... 

I have no problems getting work done.  Someone who is supposed to be a serious geek like pirillo shouldn't either.&gt; In most such tests, the answer is: "as fast as possible."

That's fine -- I don't care about those.Ah. No, that's okay. I have the same opinion, pretty much - you can write simple C in any of it's descendants.

As for ObjC - one of my great regrets is I don't code in it often enough to retain it. I've got a couple of Source Forge projects for Mac; but every time I touch them I have to start over with "How do I tie the code to the Nib?"I can vouch for that.  Enough anxiety can turn just about anybody into a bumbling idiot, and for me this is most acute when someone is sizing me up in a technical sense.  When calm, I have no trouble discussing concepts or writing some code.  The fizzbuzz problem should be simple enough for just about anybody to do under pressure, though.  Even if you use counters instead of mod, it ends up correct.http://make-money-online4.tripod.com/make-money-online-idea/business-idea-make-money.htmlOne year equals 2 or 3 semesters. In a semester you can take 4 or 5 courses. One course at a college teaches one language. There's your answer :D

Note: I'm in college at the moment and I wouldn't hire 99% of the people in my program or the other computer-related programs.&gt; but switching to a new language is usually just a matter of syntax.

That's a big misconception, switching to a new language should involve learning how to use that language.  If its perl are your if statements regex's? if its Python do you use list comprehensions?  etc.

Switching to a new language shouldn't ever be "just a matter of syntax".Because the robot signed up for Reddit and started spamming questions about John Connor.I don't believe that any non-trivial piece of software is bug-free.  That said, in my experience, I observe Linux to now be so reliable that it's more efficient to assume that users reporting crashes are doing something wrong, rather than discovering bugs.  This expediency isn't quite the same thing as zeal.On the other hand, should you try to sense a Linux programmer by asking what he thinks about Windows, he goes into a 15-minute rant about how people should "GET A REAL OS ALREADY".&gt; One year equals 2 or 3 semesters.

I assume you meant "2 semesters, or 3 trimesters". &lt;/pedantry&gt;Wow!  That is the kind of thing I'd love to see for Django as well.&gt; (say, Forth)

Forth is a bad suggestion, for different paradigms -- I'd just do the imperative solution off-hand.  With intentional styling and self-amusement, I could have a jump-table and some factoring.  Striving for the Chuck-Moore-forthiest solution would lead me through successively more memory-elegant mechanisms to encode the exact solution of this problem in my program, to do a minimal amount of computation at run-time.

Anyway, have the second solution:

    : mod-&gt;n ( n1 d n -- n|0 ) -rot mod 0= and ;
    : /fizz ( n -- 0|1|2|3 ) dup 5 2 mod-&gt;n swap 3 1 mod-&gt;n + ;
        :noname drop ." FizzBuzz" ;
        :noname drop ." Buzz" ;
        :noname drop ." Fizz" ;
        ' .
    create fizz-t , , , ,
    : fizzbuzz 101 1 do i i /fizz cells fizz-t + perform cr loop ;
Oh! Thanks. I didn't realize there was another word for that. I'll just stick to terms or seasons :PI wanted Evolution to display dates in YYYY-MM-DD.  Since this is the international standard it didn't sound unreasonable to me.
There is no GUI option to do this, the response from the mailing list was to change your system (or user) wide LC_LOCAL.  This involved baroque and obscure command line configuration fun. (Since this approach means I can't use my country's settings I'd say its broken already.)

The result was that Evolution would display the calendar in my chosen format in some screens but completely ignore a system wide setting in others.  I believe this is a known bug.  As a side effect of changing my local the Gnome-clock-applet will no longer display a 12 hour clock (is this difficult math?).  

I decided to change my locals back and write a bash script to change the local only for Evolution, which gets me half of the functionality I wanted after 2 hours of messing with system settings.  There is no reason displaying the date in the correct format should involve more that a checkbox.Good bipedal balancing wouldn't need ankles.It usually depends on if you're moving up or down the power continuum. If you're moving down, you probably already know all the concepts, just not how they're done in the particular language you're switching to, so it is really just a matter of syntax. If you're moving up, there are going to be constructs in the new language that weren't in the old one, so you're going to have to spend some time grokking them. This is why an experienced C++ programmer will take some time doing SICP, while an experienced Scheme programmer will breeze through any C++ book."Do you have stairs in your house?"I am proud to say that I too have made a brilliant discovery about robot stability while at Anybots.  Namely,

*Segways do not retain their stability while airborne.*

I made this discovery while attempting to drive Trevor's reverse-engineered segway off a curb in the parking lot at full speed during a Y Combinator reception.  (Unlike the commercial Segways, Trevor's lacks a speed-inhibitor.)&gt; Who's the smartass now?

"Not" " " you," " lazy " "code generator" ".\n"No. It doesn't.

Here's what I learned from going through the whole job-search foldirol: In most shops, the start date for your accumulated professional experience is your graduation date, irrespective of what else you may have learned, accomplished, or written before then.

This is because -- and this is the big secret -- they are not looking for coding knowledge, but rather documented capability to transduce IT resources into dollars earned or saved in an organizational setting like their own, which isn't bloody likely to have happened terribly often. So they use the rule of thumb that no experience before you graduate really "counts".

Despite this admittedly bleak analysis of IT hiring practices, there do exist companies which *do* consider pure coding and problem-solving ability to be a primary hiring criterion; they are small and hard to find but tend to have opinions on what constitutes experience much more congruent with your own.Okay, so the blow consists of a reasonable default for projeckt management and syntax highlighting angry fruit salad? Color me unimpressed.Most Linux programmers I know are not that way. They do think more people should use Linux, but they don't have a 15-minute rant about it.Well, that _was_ the old title of the article, and the author DOES consider it harmful, because the way it's named makes misunderstandings likely and misuse probable.This Quick Basic solution uses constant memory and linear time :)

    10 DATA "", "", "Fizz", "", "Buzz", "Fizz", "", "", "Fizz", "Buzz", "", "Fizz", "", "", "FizzBuzz"
    20 DIM A$(15)
    30 FOR I = 1 TO 15
    40    READ A$(I)
    50 NEXT I
    60 J = 0
    70 FOR I = 1 TO 100
    80    J = J + 1
    90    IF J = 16 THEN J=1
    100    IF A$(J) &lt;&gt; "" THEN PRINT A$(J) ELSE PRINT I
    110 NEXT I
From: Senior Programmer

To: Junior Programmer




Hello folk,




Can you research a "FizzBuzz" or whatever on the internets for me? Thank you.Thanks!  I think that's much more helpful than this article.Divide up the problem? Let each joint do it's own learning? Let joint groups do their own learning? And so on...?&gt; The title is actually _Python's Super is nifty, but you can't use it_

In defense of the submitter, the title on Reddit is the same as the html &lt;title&gt; of the article.

&gt; indispensable as Dylan/CLOS (call-)next-method

The difference of CLOS to Python with regard to object initialization here is that `call-next-method` is rarely needed, because a) slots (aka instance variables) are initialized by a predefined initialize-instance method and b) the conventional way to add custom initialization logic is with an `:after` method.That depends on the language.  At my Perl-based company, I'm probably the only one that knows the difference between the two.  I wouldn't be able to explain the difference in, say, Java, because I don't know what those terms mean in that language.[deleted]199 out of 200 is an exaggeration I suspect. If three out of four had this problem, that would sink into people's heads as "almost everyone", and then when they come to tell the story they choose ridiculous numbers like 199 out of 200.

It wouldn't surprise me if graduates were more likely to have the combination of being unable to write a loop and feeling qualified to apply for programming jobs. People without prior experience would generally feel they were qualified either because of experience writing software or because of some training and, while those who have just written toy programs for themselves might have weaknesses, inability to write a loop is unlikely to be amongst them.Lighten up. I'm just joking about the stereotypes (:Or,

    &lt;?php
    $s = '';
    for($i=1; $i&lt;=100; $i++) {
        $s .= ($i%3==0?"Fizz":"").($i%5==0?"Buzz":"").(($i%3!=0 &amp;&amp; $i%5!=0)?"$i":"")."&lt;br&gt;\n";
    }
    echo $s;
    ?&gt;
Spammers' comments are hidden for everyone but themselves. They still show up in the total count though.Currently: C, C++, Java, Ruby, Haskell, Scheme, CL, Emacs Lisp, Mathematica, Octave, bash, [RPAL](http://rpal.sf.net), LaTeX, Unlambda, Brainf\*\*\*.

At some point in the past (because I haven't used these languages recently and would need a quick syntax refresher first): OCaml, Javascript, QBASIC, VB, Perl, Python, Smalltalk, PL/SQL, x86/MIPS/68k ASM, Pascal, PHP, AWK, Algol, Forth, Befunge.Oh, so some IDE just got the speedbar and vc-mode added.

So emacs hasn't had these features for years. Clearly I haven't been using them daily. Man, I really need to cut out all of the hallucinogens.Not in MVCC-based systems (and also not with non-range WHERE clauses). :)This guy is pretty amazing and generally unknown of the large public. Starting the quote:

After Bechtolsheim's SUN workstation gave birth to Sun Microsystems, Bechtolsheim says he took a great deal of pride in the fact that his Sparc Workstation 1 and 2 drove half the revenue of the company for almost its first 10 years.

As it turned out, spotting technology that has the potential to generate lots of revenue is something Bechtolsheim has a talent for. After leaving Sun in 1995, disappointed over the company's refusal to jump on the x86 bandwagon, Bechtolsheim found success with Granite Systems, a gigabit Ethernet switching company that Cisco acquired after a year. His next company, Kealia, was acquired by Sun in 2004, returning Bechtolsheim to the fold. Somewhere in between, Bechtolsheim also wrote a now legendary $100,000 check in 1998 to two more young Stanford grads—Sergey Brin and Larry Page—for what seemed like a promising idea: a search technology they called PageRank. 

[removed][removed]If your genius happens to be too great to be apparent to the lowly plebeians, with their proficiency examinations and other such trifles, then consider yourself fortunate not to be burdened by being in their employ. As always, you have much greater deeds to accomplish.Unless they worked at a company that doesn't ask them.  I've worked for 3 of what I would call the top 5 software companies in the US and I've never been asked a brain teaser in an interview, ever.I thought it looked a lot like a child learning to walk.  Very shaky as the brain learns when each muscle (or group of muscles) needs to fire.  I assume it is similar to the brain damage people you mention relearning to walk.Well, if they are JF's and not ESR's, then that page reflects poorly on JF.Companies that want real work done and aren't trying to impress everyone with the size of their penis^H^H^H^H^Hbrain do not ask these kinds of stupid questions.He'll have to walk faster than that if he wants to take over the world...

"OH NOES!!! The robots are coming!"

"Yeah, don't worry. Put the kettle on. I'll have a brew before they make it across the front lawn..."Why are manhole covers round?

You have three switches in one room, two of which control two bulbs in another room. You cannot see from one room to the other. How do you find out which switch controls which bulb? *Edit: You're only allowed to go to the bulb room once.*

If a plane crashes exactly on the border of USA and Canada, in which country would you bury the survivors?

*Edit: The following is incorrect; see [sblinn's response](http://programming.reddit.com/info/16tw8/comments/c16veb) below instead.* How do you measure exactly three minutes using a pair of two-minute fuses?

*Not sure about this one, but it sounds like one of those river-crossing puzzles.*

*Haven't a clue.*

What are your biggest weaknesses?[deleted]Companies have penises? Holy mixed metaphor, Batman!Well, uselessly, I've seen a paper for high degree-of-freedom learning, but I think it was for segmented bodies like snakes, without the extra problem of having to maintain balance. I don't have a reference though.&gt; It'd be interesting to see an implementation in a different paradigm

Mercury, a purely logical language:

    :- module trivial.
    :- interface.
    :- import_module io.
    :- pred main(io::di, io::uo) is det.
    :- implementation.
    :- import_module int, list, string, bool.

    main(!IO) :-
        foldl(io.format("%s\n"), L, !IO),
        map(P, 1 `..` 100, L),
        P = (pred(N::in, A::out) is det :- A = [s(S)], fizzbuzz(N, S)).

    :- pred fizzbuzz(int::in, string::out) is det.
    fizzbuzz(N, S) :-
        fizzbuzz(N `divby` 3, N `divby` 5, N, S).

    :- pred fizzbuzz(bool::in, bool::in, int::in, string::out) is det.
    fizzbuzz(yes, yes, _, "FizzBuzz").
    fizzbuzz(yes, no,  _, "Fizz").
    fizzbuzz(no,  yes, _, "Buzz").
    fizzbuzz(no,  no,  N, S) :- string.from_int(N) = S.

    :- func int `divby` int = bool.
    N `divby` M = B :- ( N mod M = 0 -&gt; B = yes ; B = no ).
"Four people must cross a bridge. One can cross in 1 minute, one in 2 minutes, one in 5 minutes, and one in 10 minutes. It's dark, and they only have one flashlight. They must use the flashlight. The bridge can only carry two at a time. How can they all cross in 17 minutes?"

I this this is a clock problem, something like "When an analog clock reads 2:10, how far about are the hands?"Netbeans certainly takes the Eight[y] megabytes and constantly swapping crown from Emacs.&gt; Divide up the problem? Let each joint do it's own learning? Let joint groups do their own learning? And so on...?

If you have an argument that this will work, I would be interested to hear it!  (It might, but I'm pretty sure it isn't obvious.)  Also beware the danger of talking joint groups- once those groups are more than a trivial size, the curse of dimensionality gets you again.

In general, we can't just divide up problems.  Imagine that we want to do reinforcement learning in a simple (2d) situation where we want to have a robot drive across an area with out crashing into anything.  (We pretend the state space is only position, disregarding velocity, rotation, etc.)  If we just divide up the problem into, say, x position, and y position, this usually won't work- we will in general need 2d coordinates to specify the places that are safe to be.[deleted]So did I - I just thought it was pretty trivial and exploited Yegge's name to bring you in.These insights also have the potential to go the way of the menu bar in office.

 * **Search Strategies**: Perhaps relational database management, but "how to formulate good queries?"  In grade school, the emphasis should be on asking questions to *actual people*.
 * **Information Credibility**: Online only?  Again, why not push kids to ask good questions to *people*?  Questioning authority is a big part of growing up (and beyond).
 * **Information Overload**: Ah, so we're going to fight this by giving them more information...  I see.  I can see the point of a [GTD](http://www.davidco.com/ "Getting Things Done") lesson, though.
 * **Writing for Online Readers**: This section splits into good writing *in general* and writing mark-up.  New types of mark-up appear all the time (I'm writing this in "markdown" for instance), and the HTML we know and love will someday go away.  Why not the general principles of "meta-text?"  Take a look at TeX, HTML, etc., and bring out the idea that a paper can contain information that isn't printed directly.  Focus on the separation of style, behavior, and content, and why it's a good idea.
 * **Computerized Presentation Skills**: Perhaps we should teach them *general* presentation skills first?
 * **Workspace Ergonomics**: This is an interesting idea.  How would you teach it?
 * **Debugging**: Again, this is a normal problem-solving skill.  As a programmer, I wasn't *taught* to debug at school, either.  I was taught how to estimate a result and compare my actual answer to my estimate.  If my estimate and my actual result differ by too much, time to double-check some things.  If the computer gives you a strange result (or an error message), it's time to double-check some things.
 * **User Testing and other Basic Usability Guidelines**: This is half presentation skills, half empathy.  Generalize.

I appreciate the idea of teaching deeper insights, but still deeper than this is needed.  It's a good step!It is somewhat amusing that 200 000 lines of interpreted dynamically scoped Lisp slapped onto an environment that was clearly not originally designed to support most of its features is more stable and has more efficient resource usage than most of these Java IDEs.main(i){for(i=1;100&gt;=i;i++)printf(i%3?i%5?"%"
    "d\n":"Buzz\n":i%5?"Fizz\n":"FizzBuzz\n",i);}

Be careful with the statistics. My lisp implementation is one order of magnitude faster than [Golf Server](http://golf.shinh.org/p.rb?FizzBuzz)Says who?  The optimization you're thinking of is highly specialized:  you can't do it in MVCC-based systems, for example, and also not for any queries that don't translate into simple index scans.He could have done what I am doing (I have been working full time as a developer since 2000.  I will not graduate until 2008)A technical manager's guide to evaluating web development frameworks, with a detailed review of Ruby on Rails and the Django (Python) projects. Includes a list of evaluation criteria and rates how well each framework addresses each criteria. Use as a road map for your own evaluation.[deleted]&gt; That shows a remarkable lack of understanding of how bridges are designed and built.

You're insane. I didn't even directly mention physical bridges. Everyone already knows what a bridge is like, anyhow, so I didn't bother to explain it.

&gt; Each (non-trivial) bridge has to be custom designed for both location and purpose, just like all non-trivial software.

This argument is too general and tautological to carry any weight. You can replace "software" and "bridge" with thousands of other pairs of nouns and still be correct.[deleted][deleted]Programmer ego often does more harm than good.  If you ask any questions or show any sign of uncertainty you are immediately regarded with contempt.  I'm much less impressed with someone who can answer "What's the number after F in hexadecimal?" than someone who actually *knows* how to program.  They're really two different things.  If anything, from what I've seen, the problem is too much cargo cult type programming--knowing all the right buzz words and just what to say but really not knowing why.  In the end we're just delivering a product for a consumer--and what's inside your head is less important than how it lends itself to the finished product.  Is there a correlation between programmers who know "What's the number after F in hexadecimal?" and developers who deliver a solid product?  I haven't seen it.What do I have in my pocket?My explanation was orthogonal to your analogy.&gt; We took that relic out behind the barn and shot it. Ten years ago.

There's an [entire class of RDBMS](http://en.wikipedia.org/wiki/Multiversion_concurrency_control#Databases_with_MVCC) that would respectfully disagree.here are some companies which have recently hired friends of mine with experience similar to my friend's: apple, google, microsoft, sun, netapp, and adobe, to name a few.

i'm not sure i'd call any of those companies small and hard to find, and yet all of them thought that working 20+ hours a week for a startup "counts". (in fact, none of them even required that startup experience, but all of them recognized it.)
Wow. How to beat a deceased horse... in 15 parts![deleted][deleted]&gt;Where is Google? Not surprisingly, Red Hat, IBM, and Novell were big contributors. But where is Google? They certainly use Linux and lots of Open Source software, but why don't they show up as even 1% contributors?

Indeed where?What is Java equivalent of 200 000 Lisp lines? 

2 000 000 lines? 

No wonder it takes 100MB to run and years to debug...The other robot is actually quite cool in it's own right.  It rides around on a Segway-style pair of wheels.  I can think of many circumstances where he'd have commercial potential, too--many factory floors are flat concrete.  He could drive around doing stuff, as long as there weren't people around leaving crap lying on the floor for him to run into.  Obviously, he's quite stable, as he was picking on the leggy bot without moving around much.[deleted]Why?  It doesn't even have the gun arm installed yet..."The core reason for our existence as programmers remains telling the computer what we want it do to, which is coding."

This kind of sentiment flags someone as a "poor" at the art of programming.  Or at least an unproductive, lone cowboy you would be good to not work with.

Code is for human consumption, the fact that it can make computers do stuff is a side-issue.  

Most of the effort/cost/lifetime of a piece of code is maintenance.  Humans do that maintenance, largely by reading and modifing the code.  Perhaps adding that feature to 911 codebase wouldn't have taken months if the original devs understood that.

The whole reason for code (as opposed to **machine** language) is to make programming easier for humans.
The road to wisdom? Well, it's plain
    And simple to express:
    Err
    and err
    and err again
    but less
    and less
    and less.&gt;Agree that a firm grasp of recursion (and for that matter pointers and data-structures) is becoming rarer, but this is a reflection of the shift towards 'softer' languages (java, python etc) and away from harder languages such as C.

Maybe, in the sense that that those languages are easier to learn, and therefore the average skill of people using the language will likely be lower, but that doesn't make it any less ludicrous that someone would be programming professionally for any substantial length of time without understanding recursion.Actually most Windows guys know something about Unix, maybe not Linux but a lot that I have met do know about Unix.Heh. I just remembered - back in the late 80's I wrote Postscript by hand to generate cool graphic effects on SGI workstations. A complete and utter waste of time.I've tried (er... been forced to use) pair programming.  I think there's a special place in hell for whoever came up with that idea.  The only time it was remotely useful was during the debugging process.  The rest of the time, they were paying 2 salaries for 1 developer.The proposed algorithm [doesn't match](http://www.geocities.com/SiliconValley/Heights/5874/iwatani.htm) (grep to "The algorithm") the Pacman AI at all and would basically kill the game"How can you claim to be a senior engineer having worked for 10+ years at several different companies and /not/ have heard those riddle questions at least a dozen times?"

By not having been asked those questions in those 10 years?  In my entire employment history, I've never been asked one of those questions.  And frankly, I'd consider the interviewer an idiot for asking me questions about who to pick up at a bus stop when I'm a network engineer.
Wow ! A MUST-READ !&gt; Up till now, all my Ruby work has been in Emacs, but the NB6 *pre-pre-alpha* trumps it.

Ok. Clearly Netbeans must be awesome....

- I had to *restart NetBeans a half-dozen times* to get the Ruby modules out of the update center and into action.

- start it up, then *go for a cup of coffee* while it indexes the Java universe.

- when you first open a Ruby project, *go get another cup of coffee*; pre-indexing Ruby takes longer than Java.

- upon first start-up, it asked me if I wanted to import my NB5 settings and *promptly crashed* when I said “yes”.

- The NB toolbar is *ridiculously cluttered and busy*; who needs a button for “Open File” anyhow?

- There was one *indentation bug that seemed really moronic*

And then he goes on to talk about how "solid" Netbeans is for an Alpha build.

Thanks for that article a "pre-pre-alpha" software that nobody in their right mind would use to develop with.

If Netbeans compliments your workflow better than Emacs then, by all means, use Netbeans. But don't post that it it is "another blow to Emacs" unless you are prepared to defend that stance (which you did not do...at all).An urn with two blue balls?Can't you *see*?? It can install it *itself*.. run.. run while you still can.You're in luck. Most of these scripts don't do AJAX anyway.I've had dozens of interviews and was never asked anything remotely that moronic (excluding the "what's your biggest weakness" a couple times by HR drones).  If I'm ever asked any of those questions in an interview in the future, I'll politely end the interview and tell them I don't want to work for a company that has no clue what questions to ask in an interview.[deleted]&gt; I remember a guy who used to sort by strings by inserting them in a sorted listbox and then pulling them out again.

In his defense, the sorting algorithm on the listbox is probably a lot faster than the drunken bubble-sort he would mangle up on his own.The article mentioned is about the 2.6.20 kernel.
    
MS has shown a maniacal resolve to frame the debate around something OTHER than Freedom. This is just another pathetic attempt.&gt; Lighten up. I'm just joking about the stereotypes (:

Didn't you get the memo? Tuesday is no-humor-day on reddit. Sarcasm and innuendo are still allowed, those are banned on Wednesday and Thursday respectively.Why is it that whenever blogs about interview programming questions mention a particular question, some commenters feel the need to prove that they can write the solution?Oops. Didn't read that the number should be suppressed. I had originally thought:

You don't need to test for the "both" case, since printing Fizz followed by Buzz will naturally happen with just the 3 and 5 tests done sequentially:

int main() {
  for (int i = 1; i &lt;=100; ++i) {
    std::cout &lt;&lt; i &lt;&lt; ": ";

    if (i % 3 == 0)
      std::cout &lt;&lt; "Fizz";

    if (i % 5 == 0)
      std::cout &lt;&lt; "Buzz";

    std::cout &lt;&lt; std::endl;
  }

  return 0;
}
[deleted]What can I say. I've got wives to feed. ;-)"I asked to swap two variable contents without using a temp variable. It's the standard a=a+b, b=a-b, a=a-b problem."

Someone who actually gave me that answer would never be hired. Unless we were hiring for some estoric niche like programming microcrontrollers.  

The only acceptable answer is:
"Why for fucks sakes would I obfuscate the code with some ridiculous mathmatical trick.  Esp when it doesn't even work for most datatypes."

Also knowing hexadecimal(or binary) is only important for some diminishing set of computer languages and domains.  Still all college educated people should know it.  I wouldn't fault someone for not knowing it unless after explaining it for a few min they still don't get it.
Companies can negotiate escrow agreements for source code or other technical information to protect clients in the event of the supplier ceasing operations.

On the other hand, once a computer company has gone bankrupt, the major asset available is the intellectual property. Giving that away for free is going to be opposed by the creditors, not the shareholders who have lost everything anyway. Then, the goal is to find a buyer who will pay money for that intellectual property. Generally, most people planning on giving stuff away are not the high bidder in a situation like that.

Also, in a case where a software company goes bankrupt, "the source code" is usually something that needs the "build guru" or some other software engineer to locate, much less bundle up for distribution. Those folks generally have vanished to some place where the payroll can be met every week.INRIA are working hard on a concurrent OCaml, with parallelism at the language level as well as an efficient, concurrent GC. That's a lot of work but I'm confident their results will kick ass.[deleted]What do you claim the difference is?

I think of an array as a particular way of implementing the more general concept "list".  In an array, the list's elements are stored sequentially with integer indices.  Alternative implementations of a list include linked lists and BSTs.

But it's just semantics.  The word "list" can mean different things in different contexts (languages, platforms, etc).Actually, the author is trying to mislead, [take a look at the original article](http://lwn.net/SubscriberLink/222773/7dc8842ae75eef38/) and you will see that Google does show up in the table "Top changeset contributors by employer" and Google shows up as 1.9% just below Oracle.&gt; How do you measure exactly three minutes using a pair of two-minute fuses?

Actually I think it is 1 and a half minutes (if the fuses are 2-minute fuses).

Fuse 1 would burn out in one minute since you have lit both ends. Lighting the other end of fuse 2 would reach the flame that was already in its middle in 30 additional seconds.

There's actually not enough information in the answer to derive the question, because no length of time is mentioned. It is just 75% of whatever the length of the fuses happens to be.Unfortunately, that won't change the problem.  They'll get a job, be reasonably successful at it, and hence leave the job market.  Meanwhile, they'll be replaced by 10 more sops that saw how this one guy learned how to write FizzBuzz on his own time and then got a cushy job as a computer programmer.

As long as people continue breeding, there will always be a fresh supply of unskilled workers.Haskell:

    fizzBuzz = mapM_ (putStrLn . f) [1..100]
      where f x | x `mod` 15 == 0  = "FizzBuzz"
                | x `mod`  5 == 0  = "Fizz"
                | x `mod`  3 == 0  = "Buzz"
                | otherwise        = show x[deleted]I've wondered about using semiconductor noise as a source of randomness.  It seems like that would be much simpler than requiring a radiation source.  Does anyone know if that's been done?

Edit: should have googled that one first.  http://programming.reddit.com/info/16vgz/commentsThank you. No, I don't want to install SQL Server 2003 with fixpack XYZ, configure the JDBC driver and test to see what happens. No, I don't want to maintain parallel documentation to well-organised code in addition to examples.

Uh oh:

&gt; We have a guy who sits in the corner, working on a framework we are supposed to use. He doesn't like doing design docs. He doesn't like discussing design with the programmers who will have to use his framework. He doesn't like talking to subject matter experts about what facilities the framework should provide. He doesn't like walkthroughs or peer reviews to spin people up on his code. He doesnt like writing interface deinitions for his framework. So he doesn't. He likes to create source code files. Question 1. Can he possibly be a good programmer? Question 2. Would you like to have to use this framework? Question 3. Would you like to maintain his framework?

I think my ears are burning.They show up in the changeset contributors by employer:
&gt; Google\t1.9%

Also, there's more to Linux and floss than just the Linux kernel. Google Code has a [section](http://code.google.com/organizations.html) devoted to open source. The Summer of Code, for instance, [demands](http://code.google.com/support/bin/answer.py?answer=60328&amp;topic=10728) that student-created code be released under an OSI approved license.Perl:  perl -e 'for (1..100) { print "$_ : ";  print "Fizz" if ($_ % 3 == 0);  print "Buzz" if ($_ % 5 == 0); print "\n";}'

&gt; It might be slightly forgivable if someone didn't know the modulus operator,

Which lets you separate people based on:

1. They know the Mod operator in a language
2. They ask about things they don't know (or otherwise make it clear they know it exists, but would check docs for a little detail).
3. They use other decent options (pair of count up values, reseting on 3 and 5; calcualte all factors and search, or some such)
4. They use bad algorithms, and admit it (test all values between n and (n/3) to see if 3 is a factor)
5. Like 4, but don't.

Hrm. Sounds like a _really_ good test there...What's astonishing to me is that the first four or five solutions in the comment thread with this article aren't even correct.

Slightly less astonishing, but still annoying, is the separate mod operation for the mod 3 and mod 5 case in many of the solutions.The most challenging C++ book I've seen is "Modern C++ Design", which will slow most Schemers down to a brisk trot.

http://erdani.org/book/main.html

Alexandrescu's combination of compile-time functional programming with templates, gory C++ misfeatures, and an interesting new programming paradigm ("policy-based programming") make for rather heavier reading than most C++ books.
"Another" blow? A bit of a sensationalist headline.That's pretty bad with Perl, considering the difference between list and scalar context.

edit: not knowing, I meanNo, no.  Cocksuckers like to Suck Cock.  How is this difficult for you?

Oh, they know the difference between list and scalar context.  They just don't know the difference between a list and an array.This is wrong!

Fizz Buzz is with '3' for Fizz and '7' for Buzz.

And, not only, if the number itself contains '3' or '7' you have to say Fizz and Buzz appropriately (except for 3 and 7 itself), ie:

37 == Fizz Buzz
33 == Fizz Fizz Fizz
27 == Fizz Buzz

Etc etc.

What a moron :-)I work as an organization's sole programmer. I do everything, from systems analysis, prototyping, coding, testing, documentation (hah) and training. If it needs to be done, it will be done by me or not done at all (which is often an option I take).

The fun and challenging part is the coding; No two ways about it. Followed closely by the initial gathering of specs and figuring out how and what you can do for the customer. 

The worst part is writing the documentation. I begin the prototype almost immediately and make changes as necessary. It's probably not the most efficient way of doing things, but it works the best as I always have something to show people. Non-programmers want to see the prototype because it is the only way that they can see that I am doing something.all of them.  

i'd have answered it in pseudo code (and probably get rejected by the interviewer)....by maintaining websites for a small company that's not picky about what kind of code it supports.

That's a really nice literature survey!

I realize that the probability monad should technically be (PerhapsT Set), but that requires an equality operator to be define on our underlying type, which Haskell's Monad class doesn't permit.

See also the last sections of http://arxiv.org/abs/cs.CL/0205026 for a good rant on the limitations of monad morphisms, as we currently understand them.As squigs pointed out, modulo is unneccesary.  All you need is iterate, increment, if, and print.

Even so, you do realize most mathematical functions can be built from more primitive math? Make your own modulo via integer divide, lacking that, you could try subtraction.
1.From: CTO
    To: Senior Programmer

    Fizzbuzz.  Now.omg.  i can barely read that.From the article:

&gt; I don’t like launching IE/Win half an hour before it’s time to go home or on a Friday afternoon. IE 6 is the number one nerve wrecker and ulcer inducer, and this is always where any really serious CSS problems occur.[removed]it doesn't kill Emacs, it just makes Emacs stronger!Yes, salesmen can *waaaaaay* overpromise, but if you work for a commercial software shop, you *don't have a job* without sales!

Gotta take the good with the bad.&gt; Do all the specification and planning up front, with a fixed price tag for the development . . . [This is] best from a developer's perspective, as long as they're actually in on the design and planning.

Waterfall and BDUF and schedule-über-alles are best from the developer's perspective? Really?The people who like riddles and brainteaser interview questions (both interviewer/applicant) are people without experience.

Or restated

The more experience you have the less you think riddles are legitimate/worthwhile interview questions.

This seems to be true anedotally.  Based on interview experience and comments on this and other sites.

It might have more to do with general maturity than experience.  Impressing yourself and others with your cleverness becomes less important...Ya, math classes were great fun for me because I'd spend the whole time programming and reading. Even better when I'd have several math classes in a row.[removed]How is your comment relevant to anything?Just because you've got design and planning doesn't mean you're doing Waterfall. Even with Agile development, every cycle starts with tasking out and estimating what you're working on.Oh, don't get me wrong. I'm not advocating throwing out the sales team with the bathwater. I'm simply suggesting that perhaps one of the other approaches to consulting (design/planning + fixed-priced bid) might yield better results.[Java Was Strongly Influenced by Objective-C](http://www.cs.umd.edu/users/seanl/stuff/java-objc.html).Exactly, step one: learn lots of languages, step 2: ditch the unneeded languages and focus on higher level concepts.

It should be considered a quality of an individual that he refuses to unnecessarily work with inferior systems. Often the best knowledge is knowing what not to do.[removed]I did not know that.[removed]The correct answer when asked "What is your biggest weakness?"  is "Chocolate!".  

And if they laugh and say be serious, then answer: 
"Dark chocolate with peanuts!"

Creepy.That isn't a mixed metaphor. There's only one metaphor, while you need at least two to mix. Maybe you thought it was incongruous or just plain bad.Like design, QA, documentation.... or talking to anyone who has to do those things!From: CEO
To: employees


Nowadays, high-profile concepts as Fizzbuzz makes for a win-win situation. (...)Your friends have working experience. Every company I've ever known prefers graduates with programming intern or co-op experience. Every graduate I've ever known who has done a programming internship or co-op has gotten a job out of Uni lickedy split. Because they are better.

Bitwise is arguing that classroom experience doesn't count in the same way. He/She is mostly right, except this bit, which is completely wrong:

&gt;So they use the rule of thumb that no experience before you graduate really "counts".The only time that I was asked silly questions like this was when the interviewer was some obviously fresh out of college HR drone. Such questions were totally irrelevant to the position at hand (desktop support)
I answered the technical questions just fine and the IT manager (who unfortunately wasn't in charge of the hiring) was happy, but the HR guy said that I didn't have "enough pizazz." 

I found a much better job elsewhere.[deleted]Two lines of original content. Why not just link directly to groovyblogs.org, if it is so impressive?Do any of [these](http://voronoi.sbp.ri.cmu.edu/people.english.html) ring a bell?&gt;This problem is also practically non-existent in CLOS.

It's also practically non-existent in Python.

The normal way to reference your own superclass is by name.  Using
super() is sign that you want exactly the behavior he's saying is
good but unexpected.
He answered the question?
   
Google contributed 1.9% of the code in the 2.6.20 kernel. 1.9% &gt; 1%
    
Pretty fucking relevant.
    
So this MS jackass can't even get a few basic facts right. I wonder how much they pay him.[removed]No, of the customer's goal.Being able to see solutions would be nice (or a place to discuss them).  In particular, how on earth did _sn_ write FizzBuzz in Common Lisp using just 90 bytes?!Do you know how to read?  That tutorial is filled with examples.Did you notice how realistic the robot's hand looked when he was poking Dexter? It wasn't some simple robotic arm-straighten, it looked like a realistic playful poke.You're probably, indeed almost certainly, correct, BUT.

My admittedly subjective reaction is that the sorts of "fine point upon fine point" distinctions you've made, while possibly 100% correct from a technical perspective (I really can't say for sure), are irrelevant to most computer *users*.

That is to say, most *users* don't want to "run Linux" but rather use the computer to do some sort of useful work.  The underlying mechanics of system operation are in the end not merely irrelevant to these users but even counterproductive.

Consider the comment "there is...no recovery mode".  When I boot my computer, Grub offers me a default choice and also a non-GUI choice it distinctly, unambiguously labels as "recovery" mode.  If this user's subsequent use of the word based on that is in some way incorrect, perhaps Grub needs to be so informed.  I honestly can't say...

I'm just a person who wants to use (there's that word, again) a computer.  If Linux helps me do that, I like Linux.  If Linux presents impentrable barriers between me and my desired uses, then I don't like Linux, regardless of its "SysV init settings"...
Your points may have some merit, but the only good solution is to provide separate GUI configuration interfaces for both basic settings and more advanced settings. Only providing GUI configuration for the most basic configuration options is absolutely not a solution. It's bad software design to hard-code anything that users could reasonably want to configure, and equally bad interface design to hide those options in poorly documented gconf settings.

Making a very simple and easily understandable interface (as GNOME has done) is good, but it's also necessary to provide some kind of reasonably accessible interface to all the other settings. I agree that asking users to select "Basic", "Intermediate", "Advanced" is a bad idea, but the right way is to provide a standard interface with options suitable for all users, and another good interface to everything else. This wouldn't be a radical departure from what GNOME does currently, and it would be of great assistance to Linus and many other people who have similar issues with GNOME. Hiding useful configuration options in obscure gconf settings (or not even providing them at all) is idiotic.&gt; all it took to get it fixed was one person 

The very last person... who has not already switched to gentoo.Wallace: Gromit! I've got a present for you....[deleted]No, I think it had a more general title, but the underlying problem was that of highly-jointed robots. If I see it again, I'll post another reply... 
C++ Coding Standards: 101 Rules, Guidelines, and Best Practices by Alexandrescu and Sutter is also very good. It's like Meyer's Effective C++ on steroids.

C++: too many useless degrees of freedom. It's so horrible: these modern C++ / generic programming guys are going to all kinds of heroic lengths to generate correct designs, and yet you still have to do things like remember to initialize your class members in the same order they're declared. Or something bad might happen. Someday. Maybe. Oh, and everyone else has to remember this too, not just the library creator. *sigh*You don't know how to use multiple inheritance, that doesn't mean everyone else does not either. Some toys are powerful, sometimes too powerful for most people, but that's not my problem and it shouldn't be yours, you should learn how to use it instead of saying that it sucks.[deleted]&gt;/"In my entire employment history,"/

Exactly how many years is that?  How many different employers?  Roughly how many interviews have you been on?

I think you've proven my point.A parkour-bot would be pretty coolQuick, Robin, to the pedantmobile!A Red Hat clone. Why am I not surprised? The consultant might of had more sucess with Slackware or Debian, I'd never even think of touching Red Hat or its clones with a ten foot pole.Probably something of the form:

(loop for i from 1 to 100 do(format t"utterly-deranged-format string"i))

Where you have maybe 50 characters to use for "utterly-deranged-format string".  Piece of cake. ;-)LMAOOr simply the wrong linux distribution. I've been hearing a fair share of problems from people using Red Hat or its clones, and it looks like this particular users was using a clone. The story may of been different if they were using something like Slackware. Not like I'm an expert, or anything.Bad name, but at least the metaphor has a name, which means we can finally start referring to it. Remember that when something doesn't exist in the language, it's much harder to discuss and think about 

(As a side note, I think teaching yourself other languages is a great way to gain intelligence, because just learning a new word might make you think about the world differently -- you may even struggle to be able to use it in your other mother tongue.)I don't buy it. There's still devs out there who can't find work. Managers who complain they can't find devs who can write a fizzbuzz program are probably just parroting company policy so as to facilitate more outsourcing.
Because hexadecimal is used under a lot of computer stuff.  Bitmasks are a lot easier to read in hexadecimal than decimal.   Is bit 3 set in decimal 140?    Translate that to hexadecimal 8c, and you know figure that out quickly (bit 3 is  in the c, and it is easy to count that much out on fingers if you must.   You will make a mistake if you trying to count to 140 in decimal on your fingers).   I picked 140 because you will not see that number often enough to memorize the bits, but you may see it once in a while.

Of course if you just to front end work you may never have to deal with this. I deal with hardware a lot, and so do all the people I work with.   We can do our front ends in a few months, and then spend years working on backends for all the different hardware we have to support.Evolution is a calendar.  Calendars are for dealing with dates.  If a calendar can not handle dates correctly that is not a "really minor issue".This has to win the elegance award.I'm not falling for that again!Several.   More interestingly, I know a programming language that nobody can pass this test in.  (It lacks some features needed, and with only 20 global variable to choose from you can't simulate a turing machine to get them)   Unlike SQL I would count this as a programming language.&gt;If this user's subsequent use of the word based on that is in some way incorrect, perhaps Grub needs to be so informed. I honestly can't say...

Wrong. It's not Grub's problem, it's Ubuntu's autogenerated Grub configuration. &lt;/sarcasm&gt;

Anyway, if you were to refer to "recovery mode" on UbuntuForums or the Ubuntu IRC channel you would certainly get no complaints, although there may be users of some other distros unfamiliar with the term (though they do provide the same functionality, perhaps labeled differently in GRUB).

And Linus' problem was pretty distinct from yours even ignoring which specific piece of software was responsible for the problem. He wasn't having any problems configuring his mouse, he just wanted to change what GNOME did in response to right-clicking on window title bars, something which a new user wouldn't even think about. Your problem is a major hindrance to new users, problems like Linus's a major hindrance to advanced users.&gt; you should have at least been asked those questions a few times

The very best engineers rarely do that much interviewing.  They get actively recruited and they often just take a pass when assholes decide to ask them puzzle questions.
[removed]It looks more like the laptop's case took the majority of the shot. I love my Powerbook, but a direct shot at it would likely toast it unless it happened to be a.) from very far away and b.) aimed at the portion where the harddrive is at.If you can write a working fizzbuzz, then I know you can think.   I don't care what language you write it in, I can teach you enough C++ to get by.For best device programmers, contact BPM Microsystems, winner of the 2007 Service Excellence Award, voted by its own customersI wouldn't until after they turned an IOCCC worthy result in.    These tests are boring for both parties after a while, so someone who can come up with odd trick might be interesting or might not.   After seeing the IOCCC way I would have to determin if they did that intentionally, and know better than that for real code, or not.  If they did this as a joke but would not in real code, that means they are a great programmer with a sense of humor and will lighten up the office (and teach the other idiots something).   Unfortunately I know programmers who would come up with an IOCCC worthy result and see nothing wrong with it.This has nothing to do with Matz's language Ruby. It is fifteen years old. Why was it posted here?I think the root of the trouble is "being forced to". Or maybe the kind of program you were doing wasn't complex enough... Anyway, being forced to use X methodology or language or buzzword sucks.Oh, how crass. Why would you say something like that?It would be, but parkour is probably AI-hard, and I've seen little to make me optimistic about AI. You're more likely to see bionic traceurs after the style of "Ghost in the Shell".Objective-C and C++ really do let you "mix and match" C, but C# is quite different.  It looks very little like C except in the "unsafe code" ghetto, and even then, basic syntax often differs.  For example, in C, "declare foo to be an array of pointers to int" would be int *foo[];", but in C#, it's "int *[] foo;".  Also see "stackalloc."I know that of course, I assumed that they would ask about a "home-made modulo-replacement" though, it definitely would be better to use the built-in version if possible.

Modulo was just an example anyway, procedence rules, if-syntax,... and other parts present in virtually all languages can be a similar problem if you learn a new language every few months for fun like I do.Absolutely none. I haven't a fucking clue about any programming language whatsoever apart from rudimentary Sinclair basic. There.Requires Firefox. Uses only Javascript and Canvas.&gt; It also helped to make the feet lighter. The original feet, wearing heavy Doc Martins, were replaced by lighter ones outfitted with Vans. (I'm not joking.)

Can it tie them?The empirical evidence shows you're totally wrong.  Why are you reading this, anyway?I'd say wierdness exists in the construction world as well. My company is currently in the process of having a new building built. This building is a year behind. Why? Because nobody thought last year's winter would be so severe and drag on so long. Likewise, I'm sure they're catching up since this winter is unusually mild. 

The difference is that there are a lot more serious construction firms out there. They make construction their business -- rather than buying 3 or 4 catapillars for excavating a foundation, they've ponied up for an earth mover. It isn't a silver bullet, but it helps them cover more ground. 

And of course, an earth mover has its downsides. It needs a special operator, it has higher maintenance costs, etc. Its a trade off.

There are a lot of IT firms that do not pony up for the 'earth mover' tools. Or the training to use it. And its often because the company has an IT department which develops software for other departments to use. IT isn't considered a primary objective. Construction is a construction firm's primary business.and voted down by redditers.
I tried to make the same point to a professor at our University who was teaching a required course about SAP (no really, I am not kidding) but he just looked at me as if I just had said something inconceivable when I told him that SAP might not be around in 5 or 10 or 15 years.&gt;If you have an argument that this will work

Ehm, nah, not really. I was just guessing. I was also thinking of a robot called Genghis I had read about quite a while ago. As far as I remember it didn't have a central brain to speak of. It was all modules working together. Genghis was a relatively simple robot though.

From the movie it seemed to me that Dexter wasn't really going to a specific point. He/it was just trying to stay upright and move forward. I'm not saying that that makes Dexter any less of an accomplishment but it does simplify the problem.

&gt;In general, we can't just divide up problems.

I'm not sure I agree with you. Whenever a problem is to big for me handle, I tend to divide it up into smaller sub-problems and try to solve those. Of course, I then may still have to figure out how to get the sub-solutions to work together.

&gt;(We pretend the state space is only position, disregarding velocity, rotation, etc.) 

How can anyone drive without a velocity or rotation ? :)
IMHO, the problem with driving robots is the complexity and unpredictability of their environment. That's their big problem. The only way to solve it is by dividing the environment into sub-environments and try to understand and navigate those (or perhaps reintegrate them first). MUCH easier said than done (I don't even want to think about the number of dimensions involved in that), which is why I'm not in the business of making robots.[deleted]Oh my god look at this!
It's best in town!
Coca-cola, Rolls RoyceSlightly more terse C code, relying on short-circuiting and printf() return value:

    #include &lt;stdio.h&gt;
    int main(){
        int x;
        for(x=1;x&lt;101;++x)
            !(x%3)&amp;&amp;printf("Fizz\n")||
            !(x%5)&amp;&amp;printf("Buzz\n")||
                    printf("%d\n",x);
    }

(And no, I'd never ordinarily code something this way.)&gt;"What is hexadecimal F plus 1 in binary?"

I think the answer is `10000`.  That's something most people probably
don't need to know, though.  Hex used to be useful for doing colors,
but now everything I see supports decimal: `rgb(16, 100, 200)`.
[removed]Hacker is my life.Moore's law won't help solve certain problems. With Go, the problem space is so huge that any simple 1000x speed up won't help much at all. Instead, what's needed is an entirely new approach, the discovery of which will occur independently of whatever the state of CPU progress is, since it's a software problem not a hardware problem.[removed]&lt;pedantry&gt;Doesn't follow spec: x%15 not handled correctly. For 15 and multiples thereof, the output should be "FizzBuzz", whereas it is "Fizz".&lt;/pedantry&gt;[removed]Ahhh the 5-point-deranged-format-exploding-heart technique!  The best I can come up with is still 98 characters long:

    (loop for i from 1 to 100
      do(format t"~[Fizz~]~[Buzz~]~0@*~[~:;~[~:;~A~]~]~%"(mod i 3)(mod i 5)i))

[note: added a line-break for better web formatting.]Um, no, it isn't only quantitative. Running and jumping is qualitatively different than just balancing. A toddler learning to toddle certainly doesn't know how to run right away. Running, furthermore, is a fundamentally different activity, since it's based on bouncing, and therefore requires (and uses) compliance in the knees and angles.That movie looks great, but in fact you have to kick it at just the right time for it to do that."yes, most of the people who read reddit know how to program"

That used to be the case, but I don't think it's true anymore.It takes 82 bytes in python (well, the best I can do)

    for i in range(1,101):print(((not i%3)and"Fizz"or"")+((not i%5)and"Buzz"or"")or i)

_Edit: okay, whoops, I put "100" instead of "101".  Silly mistake._

_Edit2: at the cost of some readability, I can get it down to 74:_

    for i in range(1,101):print(i%3==0and"Fizz"or"")+(i%5==0and"Buzz"or"")or i[deleted]use map instead of recursion[deleted][deleted]It's my life

It's now or never

I ain't gonna live forever

I just want to live while I'm alive

(It's my life)I am coming of the opinion that in many fields, baroque comes before simple. Look at videogames: the first commercial game was the way complex Space Wars; only later did the simpler Pong come out.No, use recursion instead of map! Oh wait, they are the same thing. /me dongs /you on the head recursively.Patterns of traffic on animated maps so you know which route is the best at which time.[deleted]Exactly.While I agree with you--most jobs I've held, I've been recruited and didn't get in through the traditional interview process--the very *best* engineers continue to interview to keep on top of what different companies in the industry are doing.

I have yet to take an interview where I've been required to sign an NDA in order to interview.  You can learn a lot about what different companies think is important based on what engineering positions they're staffing for.  You can also discover what companies are working on cool projects that aren't public knowledge yet, based on answers to questions you ask during your interview.

Of course, perhaps, this is why I get recruited.  But regardless, I've also been on plenty of interviews that have involved quizzes, riddles, etc.
since the article is a response to another article, i went and read the other [article!](http://rentzsch.com/notes/programmersDontLikeToCode) as well.

i agree with the codist in that there is a certain intrinsic joy to typing code.  i would not get as much enjoyment if my job was to do pure research and have other people implement the results of my findings.  there's a certain OCD like pleasure in the click-clack of keystrokes and formatting my lines to look nice and neat.  

as for the previous article by rentzsch, i agree with him as well!  if my job was to simply type out the results of someone else's research, i'd be bored out of my mind.  it would be very similar to being a content manager (ugh!).

i think what i love about this occupation is that it's a juxtaposition of two things i enjoy: problem solving and writing code.  

and yes, i do enjoy this part as well:
&gt;"write requirements", "test plans", "go to meetings", "fill out status reports" or even "draw UML diagrams".

let's face it: programming is a pretty good job as jobs go.  it's unfortunate that most of us have to "graduate" up into management from it.What is SAP?
[deleted]I think you meant Eight[hundred]yesThis cross-pollination of product development methodologies between "traditional" industries and software development businesses is fascinating to me.

This essay is one of the first I have read that simply addresses the use of stage gate or portfolio management in software development.  (there are probably more out there - links would be appreciated :)

As the author points out in the references, 3M has been doing this for a long time.  I would extend this and say that many innovative, technology driven companies outside of the software industry have been thinking like this for a long time.

My business partner implemented these kind of practices in the 80s at a specialty chemical company and now consults in the field.  I spend half my time consulting with and training clients (outside software) to more effective in their product development efforts and the other half doing IT consulting.

Does anyone know if software companies are thinking like this?  Do they actually *manage* a portfolio of potential ideas/opportunities and not just features? By manage I mean do they select projects based on a balanced portfolio of long/short term, risk, technology, markets and strategic direction.  

How do they make decisions about whether or not an idea gets to go to the next stage?

I am curious about what kind of work precedes development (what PDMA people call the front-end). Do they actually talk to potential customers in order to confirm the unmet need or just to do requirements research?The Pacman example seems very contrived. A simple application of Dijkstra's algorithm, flooding the map with distances to Pacman once per frame is easy enough.



A couple of points:
1) There is an important difference between riddles and puzzles/brain teasers.  A riddle is something like the "You don't bury survivors question" - the answer is a "Gotcha" that most people either will or won't catch onto.  Also, with a wordplay riddle such as the bury survivors question, language barriers can sometimes come into play for non-native speakers.  
A good puzzle has a solution based in logic, can be hinted at to help the candidate reach a solution, and can even have pertinence to actual Computer Science problems (i.e. what is the efficiency of your solution?).  I would never ask a riddle.  I've asked many brain teasers. That being said...

2) I disagree with asking brain teasers to experienced developers.  Their experience and skill set should be the meat of the interview.  Unfortunately when trying to differentiate between fresh out of school hires trying for an entry level position, they often don't have much experience to speak of.  The very qualities I am looking for in an entry level person CAN be demonstrated by working on a brain teaser - willingness to ask questions, willingness to attempt different approaches, ability to refine answers to reach an optimal solution, and most importantly, willingness to TRY to solve a problem.  Maybe the most helpful a brain teaser can be during an interview is witnessing a candidate who immediately throws their hands up and says "I give up."  You can't say that doesn't tell you something about the candidate that's relevent to a hiring decision, and isn't that what an interview is about?I was talking about this software:

http://en.wikipedia.org/wiki/SAP_R/3

made by this company:

http://en.wikipedia.org/wiki/SAP_AG

Basically it does everything most big companies need but it does all of those tasks poorly (IMO).[deleted]This kind of overloading is also used to make the printf function work like ordinary printf or sprintf depending on context.
I agree you need a body designed to absorb the stresses of fast impact, and to generate bounces, but the actual job of running is computationally similar to dynamic walking - leap, fall, catch and balance. Walking is topple, catch and balance.Bad linkIt's quite amazing how we've come so far with some things, seemingly extremely complex, with details designed to microscopic detail... (mobile phones, computers in general, stealth bombers/fighters of different kinds), computers can think, in some aspects, unimaginably faster than organic beings. And yet - robots have been one of the most interesting things and ideas of the space age and information age - yet robots seem cumbersome and scary in some aspects. Additionally, i believe that the general public does not even know that how far we exactly are with this technology. ask them, and we'd probably get a very futuristic (in reality) answer: that robots can run very fast, nasa (or similar institutions with some folklore about them) probably uses them all the time... i believe this is very usable as an argument against creationists (yes, i found a way to bound THAT in) - why the hell create something so bloody complicated as the human organism - we could have made it with four legs like the Big Dog robot (which is imo even more scary than this, but seems more advanced thanks to the weight that four legs, as opposed to two, can carry and that four legs are more stable) or two wheels like the segway... humans are neat organisms, don't you think?C, Obj-C (pure Cocoa), Python, AppleScript, and maybe Perl.

The Cocoa one would be *very* interesting, since in order to make it not just C with s/printf/NSLog/, I'd be writing my own subclass of NSEnumerator that implements an incremental generator, yielding NSNumbers from nextObject.

Mmm, programming-challenge constraints.Ah, true. That's what I get for trying to be clever.  I feel quite silly now.  Here's more what I was trying for:

    #include &lt;stdio.h&gt;
    int main(){
        int x;
        for(x=1;x&lt;101;++x)
            (x%3||!printf("Fizz"))*
            (x%5||!printf("Buzz"))&amp;&amp;
                   printf("%d",x),
                   printf("\n");
    }I've never seen those riddles in 26 years of software development.Actually those 2 are.  Corel is the 3rd.  I've never heard of Microsoft in the past 4 - 5 years asking these questions.That, sir, was funny. And I'm going to say it withtout fear of downmodding, damn it.Most walkers have no topple phase. If Dexter has one, it's pretty short. Having a leap phase is much different. You have to basically throw yourself, and know your dynamics well enough that you know that you'll land in the correct place. 

Compounding this, compliance of joints means that you don't really know where any of your limbs are with any degree of accuracy. So you have to somehow be able to throw yourself in a catchable way, but without actually knowing where you are or where you're going. You could model your compliance, and try to estimate based on that, but then if you don't know your terrain correctly, you're still fairly screwed about estimation. 

Walking and running is complicated; don't assume you know everything about it just by inspection.&gt; That used to be the case, but I don't think it's true anymore.

Certainly not, when foolishness like this gets 10 points:

&gt; Aw, hey, why are you all having fun?  Stop having fun!  Why does everyone always hafta have fun when something like this gets posted?  I can't even comprehend your reason for having fun -- it must be SOME KIND OF EGOTISM!
Data is coming directly from Caltrans via PEMS program in Berkeley[deleted]I bet whoever wrote the Linux 2.6.2 uses the Google.A new startup talks about why they built their site using Ruby On Rails instead of PHP or ASP.NET. Go RoR! It's going to take over!If you use Version Control or even more importantly if you don't, you should read "Get up and running with TortoiseSVN and Subversion in 15 minutes" 

This article guides you through installing, setting up and using the TortoiseSVN version control system and also using it with ED for Windows.

It provides the information I wish I'd had when setting up Subversion.You mean like Jeopardy?Are we sure it's really the first?

MIT has a lab full of bipedal walking robot projects:

 http://www.ai.mit.edu/projects/leglab/robots/robots.html

Of these, the one called "M2" was dynamically balancing in 2000. Here's a video:

http://www.ai.mit.edu/projects/leglab/mpeg_vcd/videos/m2real.mpg

Not that dexter isn't impressive, mind you.

(My dad's team at SRI built "Shakey the Robot" about 30 years ago. It's interesting both how much and how little progress there's been since. One thing that hasn't changed is that each new baby step in robotics - getting a robot to do something new in a highly artificial laboratory environment - prompts claims that robot soldiers are imminent.)Sure - for straight male scientist and engineers they are extremely useful.note to author: 2.6.20 is not just an excessively verbose way of saying 2.6.2that is fascinating.  I wonder what the stats are for KDE and Gnome.



Actually, ASIMO does balance itself.

At the last CES conference in Las Vegas, Honda demonstrated their latest version of ASIMO, which appears to have accomplished everything Dexter has done:

- The robot can balance on 1 foot
- The robot can adjust/rebalance when pushed
- Also, Asimo is able to run, both feet leaving the ground in the middle of it's stride.

See http://youtube.com/watch?v=P-k8vTkbJow (jump to 6:10 to see Asimo reacting to external forces)

And also: http://youtube.com/watch?v=_khgt8ChiGo (Jump to 3:40 to see Asimo running / climbing stairs)

I'm not trying to discredit Anybot's work, I'm just saying it appears to have been already done...Also, doesn't google (hmmm, I dont know) write web software.  How much interest could they truely have in writing a bunch of kernel drivers. Or at least compared to the likes of redhat and IBM.

I don't know why people just naturally assume google is supposed to be the king of everything.You're right - I'm really disappointed that PG didn't do his research (and that nobody has really corrected him).

In addition to MIT's lab, ASIMO has also done everything Dexter is doing, and more:
- ASIMO can adjust/rebalance when pushed
- ASIMO can balance on 1 foot
- Also, ASIMO is able to run, both feet leaving the ground in the middle of it's stride.

See http://youtube.com/watch?v=P-k8vTkbJow (jump to 6:10 to see Asimo reacting to external forces)

And also: http://youtube.com/watch?v=_khgt8ChiGo (Jump to 3:40 to see Asimo running / climbing stairs)

I'm not trying to discredit Anybot's work, I'm just saying it appears to have been already done...Have you got any nice scars to remind you of your discovery later in life?&gt;(do you really want dates displayed differently in different apps?)

Yes.  For everday use I want the 12 hour clock.  I want all my logs and such to use 24.  For anyone raised in a country where 12 hour is standard this is not an unreasonable request.  Infact Evolution provides an option to configure the clock format.  

But ideally locales should be set and respected on a system and user basis.  I think per app configuration is preferable to asking users to write their own locals if your concern is usability, but one could disagree here.

I used this as an example of something I wanted to configure with Gnome but could not, as ask you asked for.  There is no way to configure Evolution to do what I want short of editing the source code myself. Since Evolution has had broken behavior with locales since  2001 (see bug 205137, duplicates, and related commentary) it is also a (better) example of bugs not being fixed. 

An example of a pure configuration I can not do is setting differnt backgrounds per workspace.
Do not trust the shover robot.Wow.  I never realised all that was hiding in the seemingly innocently named Text.Regex hierarchy.I'd tell you, but then everyone would know.

Heck, I'd ask "what's the furthest you can jump with an 8 bit relative branch?"wouldn't case..switch be better than if..elseif hereGood tutorial!Yeah! 
   
    =~

is far more magical than I had realised. Great post.[deleted]on the contrary, from http://paulgraham.com/better.html ---

The first discovery I'd like to present here is an algorithm for lazy evaluation of research papers. Just write whatever you want and don't cite any previous work, and indignant readers will send you references to all the papers you should have cited. I discovered this algorithm after "A Plan for Spam" was on Slashdot.

(s/Slashdot/Reddit/, but still)sure thing,  I also just added a FAQ's section to the site.  Let me know anything else you think might be helpful,  and tell your friends![removed]Actually yes.[deleted]That's the narcists from the other story on the front-page. And every one of them is soooo special ^_^&gt;In my defense, I learned long ago to write verbose code as part of the whole "code life cycle" process.

Never apologize for this. 

Code size is meaningless, and anyone who counts characters or lines of source is clueless. 

Optimizing compilers = no one-to-one correspondence between code size and executable size.
CPU caching = no one-to-one to correspondence between code size and working set size. 
OS paging = no one-to-one correspondence between executable size and memory footprint.
Tomasulo-architecture CPU = code doesn't even execute in the order you specify. 

Optimize your *algorithms*, write maintainable code, and leave counting characters to the sophomoric crowd that actually thinks it matters.Think certification would help?

At the end of the article he suggests certification might help reduce this problem.  However, if these folks (programmers who can't program) are getting a CS or IT degree, what's to stop them from obtaining some other certification the same way?
My uninformed thoughts makes me think you are correct.  When you walk, you always have a body part touching the ground.  When you run, sometimes you have *nothing* on the ground.  You have *no* tactile reference point for your body's location.

But, wait, here's a question.  Do you think that when a horse runs, all its feet ever leave the ground at once?Ed Keith gives his impressions of over 50 programming languages including C++, Python, Java, Matlab, D, Oz, Objective Caml, Alice ML, Scala, Boo, Heron, Unicon, Nice, Heron, Nemerle, etc.I considered that, but hit a brick wall. How do I generate a list from 1-100 without recursion? I'm not all that familiar with Scheme.I wanna play

//JavaScript

for (var i = 1; i &lt;= 100; i++)
{

\talert( ( i % 3 == 0 ? "fizz" : "") + ( i % 5 == 0 ? "buzz" : "") + ( i % 5 != 0 &amp;&amp; i % 3 != 0 ? i : "")  );

}The only thing better than a superfluous comment is a superfluous comment that lies:

"$dom-&gt;formatOutput = true; // set the formatOutput attribute of domDocument to true"Confession: I thought I was a wonderful programmer until I tried this, but my solution (in Python) wouldn't even have worked. It would have printed the number *along with* the fizzbuzz. &lt;slaps own head&gt;

Other than that, it's shocking that so many programmers can't pass this simple test.[deleted][deleted]But the thing is - Smalltalk certainly can interop well.  There are at least two dialects that make this trivial - Smalltalk/X and Dolphin Smalltalk.  It does not have to be a matter of innovative or interop.Terje Mathisen, a frequent comp.arch contributor, has a great quote in his .sig:

&gt; "Almost all programming can be viewed as an exercise in caching."

There's a difference between user-visible state and state used purely for performance. There's no REST prohibition on the latter. In fact, it encourages obeying HTTP semantics, so you can take advantage of caching proxies, etc.
Hey! I resemble that remark![deleted][removed]When I was 10 years old around 1993 or 1994, I got my first computer which was a Macintosh SE30 (built in 9" monitor with TWO vivid colors). I owe my life to Hypercard. I honestly have no idea whether I'd be programming today if I hadn't gotten that machine with Hypercard already installed.also, there's the chance of overflow in a + b. That's why the swap (if someone is desperate not to use a temp) is usually done with successive XORs.Using MarkByers's trick on jacobolus's solution shaves off another two bytes (down to 72):

    i=1;exec'print(i%3==0and"Fizz"or"")+(i%5==0and"Buzz"or"")or i;i+=1;'*100Seems a lot of reddit readers cut their teeth on HyperCard.You beat me to it. :)&gt;Write a program that prints the numbers from 1 to 100. But for multiples of three print "Fizz" instead of the number and for the multiples of five print "Buzz". For numbers which are multiples of both three and five print "FizzBuzz". 

Do you pass if you make a lame joke about how long it would take to print out *all* the numbers from 1 to 100?Are you still working in FORTRAN?

Qualify those 26 years of development--how often do you get out and see new things?  When was your last interview?  Before you landed your last gig, how many interviews did you go on?[deleted]You're kidding, right?

http://digg.com/offbeat_news/Microsoft_Interview_Questions_3

http://www.4guysfromrolla.com/webtech/012700-1.shtmland?&gt;A riddle is something like the "You don't bury survivors question" - the answer is a "Gotcha" that most people either will or won't catch onto.

If someone claims "excellent attention to detail" and fails this simple riddle, then they're bullshit.

&gt; Also, with a wordplay riddle such as the bury survivors question, language barriers can sometimes come into play for non-native speakers.

If the job description states "English" and the candidate claims "excellent written and verbal skills" and they have a language barrier, then again, I declare bullshit.
I was, but they were strictly programming related, i.e. one about finding a loop in a linked list. Not something I ever used, to be sure, but still a good test of programming skills.&gt;I disagree with asking brain teasers to experienced developers. Their experience and skill set should be the meat of the interview.

Actually, for the more experienced developers, these sorts of puzzles are quite useful.  I must assume that an experienced developer can learn simple basics very quickly.  What I want to know is if they're interested in solving problems they haven't seen before, and how they approach such problems.  Giving them a brain teaser or puzzle that's simple to solve in a short amount of time, but isn't likely something they've had to solve before, gives me lots of insight into the candidate's personality, their problem solving approach and style, as well as uncover any possible issues with how they'll interact with the others on the team (based on their profile).

If I have to vet an "experienced" developer's ability to learn language syntax or basic university-level computer science concepts, they've already failed the interview.
Yeah, you have to chose one at random, without replacement...Dexter, the geriatric robotI agree -- but nothing better came to mind.Next up: becoming a better speller.[removed][deleted]Wow, the description of the HM algorithm in that article is both poor and incomplete--there's a dangling sentence in the subsection on constraint generation. Anyone want to rewrite it, or should I just give it a go?[deleted]&gt; Optimize your algorithms, write maintainable code, and leave counting characters to the sophomoric crowd that actually thinks it matters.

I'd even go to the extent of saying: "Write slower, less efficient code if it makes it more readable". In other words, "premature optimization is the root of all evil".

I remember myself struggling to make code as readable as it was with time O(n) when being able to achieve O(n-1). What a waste! Optimizing that is of no use, killing readability for that is evil. Optimizing O(n) to O(n/2) may be worth it... Or I've spent a lot of time reaching O(n) for an algorithm which originally was O(n^2) where n in that case was never going to be more than 6, never... and then, this algorithm was only run on start up of server software that once start runs for days, weeks, months even. That was a waste as well.

If you don't know what this O thing is and you are in programming, you still have a lot to learn (disclaimer: I've been programming for years and years without knowing this), if this is your case, I recommend [SICP](http://swiss.csail.mit.edu/classes/6.001/abelson-sussman-lectures/).nope, no kidding.  I can only cite from my first hand experience.  I guess I"ll have to rely on your 5-6th hand digg citation:)

Those aren't riddles, those are actual comp sci questions:)I'm sorry if this sounds snarky, but you yourself should probably brush up on "this O thing".

O(n/2) == O(n)

and

O(n-1) == O(n)

One of the basic rules of O notation is that all constant permuting factors are discounted. So:

O(n/{any constant}) == O(n)

but 

O(n/{any variable}) != O(n)


Now, on your general point, which was "avoid optimizing even your algorithms unless you've thought about it carefully first", I agree.
I've been asked to do that as well but that's a far stretch from a brain teaser, that's just knowing how linked list's work:)Of course, as commenters have already pointed out, while brevity may be the soul of wit, it's a really stupid basis for assessing code quality.  Here's what I did as soon as I read the FizzBuzz bit:

    def FB(n=100, factors={3:"Fizz", 5:"Buzz"}):
        for i in range(n):
                s = [s for f,s in factors.iteritems() if i%f == 0]
                if s:
                        print ''.join(s)
                else:
                        print i
Um... do I *think* all a horse's legs are off the ground? This was one of the very first uses of motion pictures: some guy bet some other guy about all four feet being off the ground at once, and they took the first moving pictures of a horse running, and it turned out that in fact they were all off the ground.Sampling the physical environment has serious downsides and does not produce better randomness than other techniques.

The basic issue is that any measurement device requires a "cooling off" period to become decorrelated from sample to sample. So there is a physically imposed limit to how fast you can sample the environment and still expect unbiased measurements.

Of course even the simplest programs produce sequences experimentally indistinguishable from pure randomness so its hard to see the point other than academic interest in sampling technology.

I posted this already in another thread, and it isn't Ruby, but surely Ruby can do something similar?  It's both shorter and more general:

    def FB(n=100, factors={3:"Fizz", 5:"Buzz"}):
        for i in range(n):
            s = [s for f,s in factors.iteritems() if i%f==0]
            if s: print ''.join(s)
            else: print i
Yes, that did sound a bit naive. What I essentially meant was for trivial problems like FizzBuzz, and for imperative programming languages.

I could probably get something running in LISP or Erlang easily, but I wouldn't show it to anyone who had extensively used those languages.[deleted]I'd have to say outrunning wild hyenas.Look at the google document at the bottom for a better code layout.  Blogspot must be made up of the java developers and screwed up the formatting.

If you really want to run the examples, try to email me.He's not real clear about precisely what the breakthrough is here, so maybe there's to it than it sounds like, but I thought exactly the same thing...there is a fantastic video, I think ten years old or more, of a bipedal (but non-jointed -- is that the key thing?) robot running on a treadmill.  As far as I can recall it was an MIT thing.  This machine did fail, though, on its attempt at a running somersault.  This has got to be online somehwere...anyone?  From looking at that page I think maybe it's the 3D Biped?  Looks familiar, but I still can't find the video.

Edit: (QuickTime crashed my browser) the videos on the site are not the one I remember, but I'm pretty sure that's the bot I'm thinking of.Just upload the sources as as .factor file.Shameless self-promotion. Oh, well. I still voted it up.Why not? You can solve this problem in log time if you use a B-tree where nodes are augmented with the size of the subtree rooted at them.I did and edited it, that worked.  The code snippets in the blog didnt format well.

[deleted]saying a programmer doesn't like to code is like saying
a race car driver doesn't like to race cars.

if you don't actually like the process of writing code 
you will not become a good programmer.  you'll find yourself
doing something else in a few years or just collecting a paycheck.

or like the top post guy tintub ... not really programming ...Yes, if you're a slow thinker, you are stupid, by definition. What's wrong with that?Computer science is not software engineering. I never learned to program in university. I only did when i started doing my own web development outside the classroom. 

And, I don't even love it to the degree that I do it in my spare time. It's a tool to accomplish the goal of problem solving. Much the same way you have carpenters and architects. Why expect an architect to be skilled with the finer points of construction? Why expect any scientist to be able to apply their knowledge directly in the real world?

Computer Science should be separated from Software Engineering. All other major professions separate the thinkers from the doers. CS Majors are supposedly thinkers. SE Majors are trained to do, or should be.
[removed][AntiExperience](http://c2.com/cgi/wiki?AntiExperience).

Actually, I'll elaborate.  I've heard from a lot of senior programmers that "Technology X is good enough.  Why take the risk of learning technology Y when it may be technology Z or technology Alpha or technology Beta that actually wins out?"  This seems to apply for basically everyone over the age of 35, and most people over about 28.

Thing is, they may be right - for *themselves*.  The economics of learning new technologies changes over time.  For me, I have my whole working life ahead of me, so any investments in myself have 30-40 years to pay off.  Someone who's 50 years old has only 15 years left in their career, so technology X better show major improvements quickly for it to be worth it.  I don't see many 50 year old programmers learning Haskell, unless they invented it.  By the time it's mainstream, they'll be retiring.

Add to that that people often overestimate the difficulty and underestimate the payoff of learning new things.  This seems to be a universal human trait: humans are risk-averse.  I know that I frequently will put off looking up a command or learning a toolkit that ends up saving me significant amounts of time, because I figure it'll be a major project.  Then it ends up taking me 5 minutes and I use it all the time.25 years for me and I've only been asked the "your main weakness" question.[removed]I have. A lot of time the code turns out pretty elegant.I don't have any information to contradict your assertion, but have you got any that supports it?I work in a lab that uses a Boston Dynamics robot, so people in the lab interact a lot with them and know such things, and I heard it from one of them. You can choose not to believe me if you want; it won't really matter to anyone unless you decide to buy a BigDog, which is unlikely.Real men use `smartindent` for everything.I'd agree with that:)  Everyone has to learn somehow:)Recommended reading is something like [The Wish Giver](http://www.amazon.com/gp/product/0064401685/) which shows that you only get what you wish for if you know how to ask for it precisely.I neglected to mention that it had to be done in O(n) time and O(0) (is that how you specify constant?) memory.On what do you base the assertion that "many enterprise programmers with 20-30 years experience [are] incapable of writing working code"? (... you arrogant jackass.)Seems like a reasonable hypothesis. You must have taken a course in economics because I think this is how most economists would explain the observation. I for one believe you are dead on. I'm 38 with 2 kids under 5, with 15 years of telecom under my belt I have decided to put away the pager and the cisco and oreilly books and switch to a 9 to 5 business analyst position. (I'm getting off the IT/Telecom treadmill). Granted I'll still learn, like learning Scala but it will be my hobby not a business compulsion.&gt; You're more likely to see bionic traceurs after the style of "Ghost in the Shell".

That would be good enough, really.It's fun because it's just a game -- I sincerely hope I'll never run across code like those when there's "real" work to be done ;)Wow, this is an important news item for the Linux community. It's pretty scary to think that RH does so much for the Linux community, yet its hobbyist platform is nowhere near where Ubuntu is in marketshare now. It's also frightening to know that only 20 individuals were the core for the latest version of the kernel. I'm a Linux fan but this is a bit scary. We need more recruitment on the kernel, even though it's not very sexy to be a Linux kernel developer compared to making killer web apps with far more flash and notoriety.Then again, from the looks of Vista, there must have been like 10 developers working on it.that this is in the programming subreddit is a mistake. my apologies.Is a human brain just like, say, a dog's, but with a higher clock speed? Intelligence isn't simply about speed.Fair point.  In that case we'd have to incorporate the requirements definition as an approximation as well.  Another important problem.

In this case, I think even getting programs to do what the _developer_ wants would be a big step forward.That's a big statement.

You can do that, without using any references, in, let's say, C++, Scheme, and Prolog?That's much more ugly than Prolog, in my opinion.As I was reading that, the name that came to mind was "object complement", in analogy to the geometric interpretation. I particularly like it because the complement of an angle is another angle, just as the object complement of something is another object (albeit not unique as in the angle case); an "anti-object" really sounds more like you're going to a non-OO paradigm entirely, which is misleading.

I definitely agree with the general idea that OO is not about physical modeling, and a lot of people have been misled to believe it is. I bet many are still being taught the "objects are a noun, methods are verbs for that noun" formulation even as we speak, but it's wrong. Objects are just _concepts_, and they're perfectly capable of being verbs; what else is an iterator but a verb?Umm... Google pays Andrew Morton's salary to maintain the mm tree. That's a pretty important guy in linux kernel development.Of course pythons should be considered super harmful! They're giant snakes!!! Giant! Snakes! Think about it. What the hell are you people talking about? How can you argue this point. It's just obvious. I don't even have to read the article.[deleted]The schools should then write down these insights and put them in a manual, thus creating an infinite loop that will create more and more insight manuals, forever.

Which brings one insight to mind: **recursive thinking**. Recursion comes naturally to me, and I have absolutely no problem with mathematical proofs by induction for that reason. I must have learned this kind of thinking at some point, so it should be something teachable.&gt;  Or I've spent a lot of time reaching O(n) for an algorithm which originally was O(n^2) where n in that case was never going to be more than 6, never... and then, this algorithm was only run on start up of server software that once start runs for days, weeks, months even.

I think this can't be repeated often enough, don't optimize cases where performance doesn't matter at all, small n or code that rarely runs doesn't need optimizations, not ever, probably not even in hard realtime situations.Using "(dotimes(i 100)" instead of your loop makes it shorter. I am too lazy to count the characters but it saves some.I am turning 50 in April. I learned Ruby last year, after 25 + years of commercial programming in Fortran, C, C++, Java, you name it. I founded the Rails group in town, where there are at least 5 40+ developers learning Ruby. I own a company where I am both the CTO and a major contributer to the development effort - all in Ruby. I spent this morning continuing to study Haskell, my language to learn for this year. I'm thinking about picking up Common Lisp as well.

Am I doing this because of economic needs? In the case of Ruby, yes. Its a powerful lanquage that is perfect for my current programming needs. In the case of Haskell, no. I'm learning Haskell because I enjoy learning new languages, and it keeps me challenged. I guarantee I can "write working code" - I do it virtually every day. 
Why are you getting offended? I didn't say all programmers with many decades of experience are bad, that would be ridiculous. But I do see a lot of people, both in real life and on forums, passing on bad advice and bad code, and when questioned about it, brushing off any technical discussion by citing their innumerable years of real-world experience. For some reason, certain people become blinded by their qualifications, PhDs and CVs and just stop learning, because they Know Everything Already and Have Done Everything Before.One excellent algorithms textbook, *Algorithms in C* by Robert Sedgewick, has thousands of diagrams illustrating algorithms. They were all generated by combining output data from his example code with some hand-written PostScript code to make pretty shapes from the data. It's practical and brilliant, and the resulting abundance of clear, descriptive pictures is what makes the entire textbook worthwhile. Seriously, they're more useful than the text. Those diagrams would have been extremely difficult to make without the kind of vector graphics programming that PostScript supports.Pirillo is not a serious geek he only plays one on TV.&gt; Most good programmers should be able to write out on paper a program which does this in a under a couple of minutes.

On *paper*?!  No way.

*shudder*I've been programming for a while, and I'm embarrassed to admit I don't know the difference between (or even the formal definitions of) LL(1), LR(1) and LL(k) parsing.Slava: we have discussed your observation before and it is certainly difficult to debunk (I agree for now and have done for years), but why ask on reddit? You'll only get the same ol' silly answers - nothing new or informative, surely?People sometimes wonder why Smalltalk never caught on like Java did. This sort of thing is why.[deleted]// get it right
    try {
      FileInputStream in = new FileInputStream(filename);
      try {
        doSomething(in);
      } finally {
        in.close();
      }
    } catch(IOException e) {
      ...
    }O(1) is constant time. O(0) involves time-travel.Fixtures shouldn't have hard-coded IDs in them. That's just asking for trouble.

It's Python, but I wrote [NonMockObjects](http://www.jerf.org/programming/nonMockObjects.html) basically for this same reason. (Well, actually, I just skipped over the part where I tried to hard-code my test database rows, since I lucked into a situation where once I realized I needed test data, the requisite data was already too complicated to hard code in any reasonable amount of time.) Perhaps a Ruby port would be a good idea. NonMockObjects is currently in use in a Django environment, which is very similar, so I know the idea is sound.

With a bit of library support and the right structure, I find generating test data via code to be about an order of magnitude or so easier, and I just don't get everyone's apparent obsession with test fixtures which, being static data and not code, are not particularly amenable to refactoring, abstraction, or any of the other advantages of living code over dead data.Didn't he say there was some kind of bug/"feature" in Java that made it illegal to have the type definition of `in` inside the `try` block?  Seems odd to me, but then I'm very happy to avoid thinking about Java.

ETA: Wait on, I see the problem - it'd be because, in `try { int foo; ... } catch(...) { ... foo ... }` the `foo` variable is in scope only inside the `try` block.  Your code, on the other hand, has the nested `try`s, so it's a different kettle of aardvarks entirely.  Don't mind me; I'm sleep deprived.Nope.  

There were a few journalists present at the reception and one of them snapped a picture of my falling, but mercifully, I checked the papers the next day and it didn't get published anywhere that I could find :-)&gt; in my opinion.

Your opinion doesn't incorporate familiarity with the language -- why then even bother sharing it?  That the unfamiliar appears distasteful is never a surprise in programming.Simplicity is beautiful. 

What you posted is simple in neither syntax nor readability.[deleted]In Lisp, if there's an error within your code that's invoking a macro, does the error show up in that code itself or within the contents of the macro that replaces your code at compile-time?

I'm just asking because if it's the latter, it seems like Lisp macros would make debugging a huge pain.Well, you have no motivation to lie—that's interesting to know. I imagine BD's primary sales opportunity for this thing is with the military, doing short range light-cargo transport over lousy terrain. If the military is their customer, they'll have these (seemingly minor) kinks worked out sooner or later.

My point was more that BigDog comes across looking much more elegant and polished than Blackwell's contraption.Congratulations, You aren't an enterprise programmer.[deleted]Oh, ok then. Under mounting pressure from Eric, I've revamped my website and put up direct links to all interesting source files... :-)The problem is that many people who enter a cs or cis program can't think abstractly enough to do well as a programmer.  Universities *should* weed them out, but they usually don't.  People cheat in college.  I saw it all the time, and it was usually encouraged (group work or pair programming).  Tests didn't always require written code and, if they did, where usually take-home tests.

Yes, I went to a no-name school, so my comments don't apply to MIT for instance.  I'm just pointing out that many state schools let people graduate without mastering their subject.  That's how the schools make money.I tend not to debug by stepping, but more by tracing the input and output of functions, along with some output to the console and a few insightfully inserted breaks. Since I know what *behaviour* a macro will give me, the fact that the compiler works with the expanded code usually isn't an issue. Indirection, on the other hand (e.g. a state monad), makes things much harder to debug. 

In theory, it would be possible to trace whence expressions came from. I don't think any CL implementation does it. I would be surprised if *no* scheme implementation did it, though.Can't imagine anybody better placed to do it than you.

Now a question if I may. What is the difference between type inferencing and something like std::for_each in C++? The template types are deduced (but not inferred?). Or is it still type inferencing, but in a different context.

I find myself sometimes dropping complex loops into a templated function in order to take advantage of it so that when types change the compiler works out the correct iterator types to use.That's only propagation, and only half of one way (forward, but only for arguments). Inference is a global constraint solving process. For example, in Haskell, you can change what a function will return in various situations by changing the expected type of its return value (as seen in an earlier post on exception handling in Haskell). While this makes type inference powerful, it also tends to make type checking errors quite cryptic.&gt;  I'm sorry if this sounds snarky, but you yourself should probably brush up on "this O thing".

Yes, sure... always!

&gt; O(n/2) == O(n)

I wouldn't say they are exactly the same. Taking an hour or half an hour to do something in real world makes a difference (as can 10ms to 20ms in the delay of delivering a packet of a VoIP communication).

&gt; and
&gt; O(n-1) == O(n)

I know, that's why I said it was quite dumb of me to spend time trying to find a way to keep an algorithm readable while going from O(n) to O(n-1). It's useless even if the readability is the same.

&gt; One of the basic rules of O notation is that all constant permuting factors are discounted. So:

&gt; O(n/{any constant}) == O(n)
&gt; but
&gt; O(n/{any variable}) != O(n)

Right, for algorithm analysis that may be totally right, but in the real world, the first case may not be so. Wouldn't you agree with my examples stated before? Making a VoIP packet delay 10ms is ok, once you start to pass the 20ms seconds barrier the human brain starts to hear the difference (I am not 100% sure the frontier was 20ms, but it was something like that) so there a O(n) -&gt; O(n/2) may be an important difference.I'd like to work with you sometime, it should at least be *interesting*.[removed]I guess I'll take your word for it that the expanded macro code usually isn't an issue when debugging.. I haven't done much Lisp myself. It just seems though to this non-Lisper that as soon as you have to debug the macro contents itself, you've got a very leaky abstraction.

I guess you could say the same of any exception thrown from the implementation of any library function in a standard imperative language, but in that case you can at least see the call stack and determine where your client code invokes the library function.Unless the macro is buggy, you shouldn't have to debug the macro's expansion.Yeah I thought of that, but then you're off by one and all the ways to remedy that end up taking more characters.  At least that's what I found...If it was the first, it would be available right now because I know other "web-base" word processors.&gt; SAP might not be around in 5 or 10 or 15 years.

Oh, I'm sure it will. The company may die, but many of the implementations of it will continue their slow, tortured death throes for at least two more decades. (I can think of at least two that I feel will do so)That's approaching the tautological. :)[removed]Stop the spam.&gt; Simplicity is beautiful.

So is complexity.  This isn't a woman, or glasswork, or a painting -- beauty requires a (here: manufactured) basic familiarity.

&gt; nor readability.

Nonsense.good tip on blocking CSS from IE Mac&gt; Intelligence isn't simply about speed.

Yes, if you're a slow thinker, you are stupid, by definition. What's wrong with that?
One doesn't have to be smart to write dynamically typed code, and static typing is not a way of making idiots productive. There are very smart people writing statically typed code (Haskell community) and not so smart people writing dynamically typed code (me). And idiots will write horrific code in any language.

But hey, this guy did get one thing right: namely "the masses of unmotivated programmers that fill the corridors of large enterprises". They're as dumb as bricks. Ironically, he didn't think to look in the mirror.Indeed. Once the macro's working, you don't have to think about it anymore. So the only time you should ever have to debug the expansion itself is... when you're debugging the macro.&gt; yet its hobbyist platform is nowhere near where Ubuntu is in marketshare now.

With Fedora and RHEL, it is 'pretty scary' that Ubuntu has greater overall use than the former?  Why?  How on earth is this 'pretty scary'?Did you know?  The Dutch call him by name ("Neek-laus Veert") while Americans call him by value ("nickel's worth").  ;-)[1. This feature is also known as extended records. It is the main innovation in the Oberon language; in other words, adding this feature to Modula-2 led Wirth to create a new language, write a new compiler etc. Adding this feature to Forth just required a few lines of code.](http://www.complang.tuwien.ac.at/forth/gforth/Docs-html/Structure-Usage.html#Structure%20Usage)Yeah, you're thinking of this one:

http://www.ai.mit.edu/projects/leglab/robots/3D_biped/3D_biped.html

Here's a flip on a treadmill:

http://www.ai.mit.edu/projects/leglab/robots/3D_biped/3D_biped_flip.mpegI call bullshit on one quote:

---

A surprisingly large fraction of applicants, even those with masters' degrees and PhDs in computer science, fail during interviews when asked to carry out basic programming tasks. 

---

If you have applicants with PhDs failing you programming quizzes, there might be something wrong with your quiz beyond "write a loop from 1 to x".&gt; And I'm going to say it withtout fear of downmodding,

If you fear to say anything by downmodding, then the terrorists have won.It can be a huge pain if you are debugging someone else' code.Good point!That gives you the number of physical/on-disk rows, which in MVCC does not generally correspond to anything useful:  you have to do a scan to determine the number of rows visible to any given transaction.

This is just an instance, in turn, of the more general rule that you can't do this `COUNT` optimization for any non-trivial (defined as "depending on anything beyond a single index") queries.  (That is, you can think of MVCC systems as always have an implicit `WHERE` (or more accurately, `HAVING`) clause for the current transaction ID.)Wow - I never heard a name of _any_ of the "Top 20". The guys keep surprisingly low profile.That was highly interesting, but completely irrelevant to the topic.

So upmod.&gt;Slightly less astonishing, but still annoying, is the separate mod operation for the mod 3 and mod 5 case in many of the solutions.

I think I don't understand what you're getting at here.

Are you claiming that, given only a number's value mod 3, it is possible to tell whether or not it is divisible by 5 (or that given its value mod 5, whether it's divisible by 3)?

If so, then could you explain how?

If not, then what, exactly, are you claiming?&gt; If you have applicants with PhDs failing you programming quizzes,

Agree.  If the magical fairy halo of basic clue doesn't actuate upon receipt of the previous degrees, it will certainly do so upon receipt of a PhD.Whatever Microsoft's intentions are, I think it does help the open source community any attention that it can get. Even in articles like these which seem to show a similarity between large closed source projects like (Windows OS) and open source ones like Linux, it does throw a different light about the open project.

Even though I don't buy Microsoft's argument or FUD, it encourages me to find out the truth for myself. And like a dialogue in a recent movie I saw, [the truth sets you free.](http://www.imdb.com/title/tt0119528/quotes)&gt; I wouldn't say they are exactly the same.

I think Whisper was saying that they are exactly the same, because it's wrapped up in the definition of what big-O notation means.

In big-O notation O(n/2) is exactly equal to O(n), and O(n-1) is exactly equal to O(n).  Although it doesn't make sense to write O(n/2) or O(n-1) as they don't really exist - in these cases there is only O(n).  

http://en.wikipedia.org/wiki/Big_O_notation

Making something twice as fast can make all the difference in the world, I've got no argument with that.  But if you don't understand big-O notation then you're going to confuse people you're trying to communicate with or possibly embarass yourself.Many of the solutions did checked for mod 3, mod 5, and mod 3 &amp;&amp; mod 5, but only the first two are required for the solution.&gt; If not, then what, exactly, are you claiming?

That it is astonishing that anyone misses (or discounts) the obvious superiority of this mod-concise but control-flow-verbose solution:

    int div3, div5;
    div3 = n % 3 == 0;
    div5 = n % 5 == 0;
    if (div3) printf("Fizz");
    if (div5) printf("Buzz");
    if (!(div3 or div5)) printf("%d", n);
    printf("\n");

--because, uh, the control-flow is possibly less stupid in some languages, or because % is an expensive language on a hypothetical computer, or because it is all more elegant to treat the output as a group effort like this rather than as a consideration of 4 states.  This person values seperating the final newline and requiring unusual 'or else' logic, so as to avoid cheaply redundant tests?  Yawn.

    : /by ( n d -- f ) mod 0= ;
    : but-anyway ( n -- )
      &gt;r r@ 3 /by dup if ." Fizz" then
         r@ 5 /by dup if ." Buzz" then
         or if rdrop else r&gt; . then cr ;
&gt; and mod 3 &amp;&amp; mod 5

Which test allows you to more simply handle the case when you instead print the number.[deleted]Yeah, we know you can barely read... you /do/ program in VB after all.Oh, in all the hub-bub, I completely forgot: Google hired Andrew Morton.
    
http://www.linuxtoday.com/developer/2006080303126NWCYKN
Ah.  Your "separate mod operation for the mod 3 and mod 5 case" was meant as "... the (mod 3 and mod 5) case".  That makes sense.

I read it as "... the (mod 3) and (mod 5) case", which led me to incredulous astonishment.&gt; On paper?! No way.

Ah, grasshopper, you have much to learn of the joys of paper programming.Please don't cut-n-paste a reply to multiple threads like this...&gt; Perl: perl -e 'for (1..100) { print "$_ : "; print "Fizz" if ($_ % 3 == 0); print "Buzz" if ($_ % 5 == 0); print "\n";}'

I assume the downmod comes not from random reddit bias so much as from distaste for your fairly normal code.

Please consider my version:

    perl -le 'print $ARGV[($_%3==0) + 2*($_%5==0)] || $_ for 1..100' '' Fizz Buzz FizzBuzzWe are trying to turn your english into a big gloubi boulga.Ah, awesome -- and congratulations!  This pales to a simple overturn and considerable apology from Intel and the State of Oregon, but at least merlyn'll have better chance of going to YAPC::AU and such.

Oh, and once again I see reddit's bias against anything at all Perl related.  -fuck- -you- psuedopeople.  This case should be of interest to anyone remotely technical.It seems that even Java is going to jump into macro wagon [Sun ships "extensible" Java compiler?](http://lambda-the-ultimate.org/node/318). Unfortunately it seems that Java version is dumbed down version of Lisp macros. At least it processes AST trees instead of text.&gt; Save Scheme, they're all imperative languages;

Err, so is Scheme.[removed][removed]BEGIN{for($i=1;$i&lt;101;$i++)print($i%15?$i%5?$i%3?$i:"Fizz":"Buzz":"FizzBuzz")}

awk: 78 characters.  I'll try and make it shorter.
From the article:
&gt; I recently spoke at a Fortune 500 CIO conference. During
&gt; my speech I did a real time poll of the audience of CIOs.
&gt; The results confirmed my gut feel for how the market
&gt; really works. Here are the questions and the results.
&gt;
&gt; How many use both? 45%.
&gt; 
&gt; How many of you have made changes to the source code? 8%
&gt; 
&gt; Very few Open Source users ever touch the source code. So
&gt; is it really about the source code? 

Very few? 8/45 ~= 17%. 17% of (a sample of) Fortune 500 companies have made changes to the source?

I'd say that's a very high figure, personally.I don't know Ruby that well, but I figured it would be a
good exercise, and this is what I came up with after a few
iterations :1.upto(100){|x|s="";s="Fizz"if x%3==0;s=s+"Buzz"if x%5==0;s=x if s=="";puts s}Oberon is something everyone should take a look at. The language itself isn't the most exciting stuff (though Wirth's take on a rather minimal OOP is quite interesting), but the design of the operating system and its graphical interface is quite innovative. So if you wonder where all those tiling window managers come from...&gt; we dont need to all whip out our wangs and compare sizes.

Nobody ever does.  Morever, and more importantly, nobody ever needs to use this dead-stupid cliche of imagining 'penis-measuring contests' in other people.&gt; I assume the downmod comes not from random reddit bias so much as from distaste for your fairly normal code.

It presumably comes from the code not matching the spec. Yours does, by the way.
&gt; writing code is for the junior monkeys. The experience programmers are the ones shaping the solution.

How wrong.

&gt; You have the experienced guys do the calculus you hire monkeys to do the algebra.

'algebra' refers to a vast body of mathematics. High school algebra is more accurately 'elementary properties of polynomials'. But even the theory of polynomials leads to some of the deepest waters in mathematics (algebraic geometry).

I wouldn't want to rely on code written by monkeys, regardless of how experienced the 'architects' were (in my experience anybody who uses terms such as 'architect' is good for nothing).

I've noticed you post a lot of this type of stuff here. Are you really a "Java Joe" or are you just trolling?"Incapable" sounds incredibly over the top for my tastes, but here is my theory:

Although the many years of java hype the core systems of  big companies are still Cobol-era technologies. As long as they work and can be connected to "new" tech there is no reason to replace the systems that were developed with huge development efforts and have had their bugs shaken out long time ago.

So the old geezers who know this stuff are in high demand and don't NEED to learn new ways of doing things to keep their jobs. Without them companies will grind to halt.

So my guess is this: the old geezers are mostly babysitting systems not programming much. So they loose their fluency at it.

L.



I don't have any statistical data to check against either the conclusion or the premises; do you?

Even assuming empirical support for the premises, the argument still looks paradoxical, and the following questions might help in dissolving it:

How did the 199,800 competent programmers get their jobs in the first place?

Did they all get their jobs at the same time?

How many times does an incompetent programmer apply to a particular job position?  Or to job positions in a particular organization?

Are there no new graduates over time?agreed I was about to say the sameI was going more for the "Real men do _x_ with _y_" vibe, rather than technical accuracy. You can use a resistor to get thermal noise that is at least as random, it's also less likely to cause OSHA inspections and freak out the clueless, but getting your entropy from a Geiger counter is _cool_ .

Reddit headlines are for entertainment purposes only. 

Keyboard shortcuts? Is this guy on crack?The link is 404'ing now.  Apparently, Richter pulled the post.Looks like a full-time job. When to do the stuff the customer pays for...I'm a deeply amateur programmer, so I'd be be interested if someone could explain the differences between this and simply using a function/method.  
In the example given, would it not be possible to have a function which took the file name, and the name of the doSomething function as arguments?  I can see that from a technical perspective they are different, but are they functionally different and why would macros be better?

ridiculously biased, but probably accurate. apart from google earth, i find it hard to identify a single service offered by google that is the best available.Exceptions are not bugs, so the comparison doesn't seem right.

Just like there can be a bug in a function, there can be a bug in a macroexpansion.  Sometimes it's easy to trace the bug, and sometimes it is not.  Like you can browse the source code of a function, you can browse the macroexpansion.  If the macroexpansion is complex, it's sometimes hard to read.  This is also the case in complex functions, but functions are more isolated.  Many times you can prevent that kind of problem by using a procedural interface underneath, with the macro serving only as a thin layer of syntactic abstraction.[deleted]I've been saying this for some time. The problem is Google's usability success on search with an extreme simple interface. Then they started applying the same principles to full-fledged applications such as gmail, and one ends up with chaos.Yup. Oberon is a "textual user interface". It is also a rare example of a statically typed language with an interactive environment. Interesting stuff.&gt; i find it hard to identify a single service offered by google that is the best available.

What webmail service do you prefer to gmail?

&gt; ridiculously biased

'bias' means more than 'comes from an interested party'.The points-of-macros here little interest me; please have this one:

Mercury's io library has formatted output (which can simply call C's
sprintf), so you have calls like:

    io.format("Hello, %s\n", [s(W)], !IO)

which is decent.  Mercury type-checks the arguments only insofar
as io.format/4 here demands that its second argument be a
`list(string.poly_type)`, so it will complain at compile-time
about code like this:

    io.format("Hello, %s\n", [s(W)], !IO),
    I = 10,

as `s/1` expects a string, but not about clearly wrong code like
this:

    io.format("Hello, %d\n", [s(W)], !IO)

-- no, that will just throw an exception at run-time.  Lame!
In Mercury, I can either have a nice and simple syntax like
this or I can have statically-checked code that builds the
string more verbosely:

    my_format([s("Hello, "), s(W), s("\n")])

I can't have both.  In Lisp, and in Forth, and even in Haskell++1,
you can have the nice syntax you started out with and make it
as complex as you like just under the surface.  CL, a largely
dynamic language, can with macros that use type assertions tell me
-at compile time- what Mercury, a very static language, cannot
tell me until run-time!

Having your language at compile-time is quite an advantage.

++1. GHC, rather, with Template Haskell.  AFAICT.I know my husband is Aspie but until I read this piece on Asperger's, I didn't realise that I was too. I have an over-sensitivity to  bright lights and music, hate people close to me, the feel of some fabrics, and find eye contact really intimidating. 

I have just had to explain to some people on a project that if I'm not looking them in the eye it means that I *am* listening. I also too collapse with exhaustion from the sensory overload of being around too many people.  It's very weird when you read the contents of your head in someone else's story...8% of 45% is 3.6%Oh, waah.  I wanted a rational discussion and I got one but with some playing on the side -- they were just writing code for fun!  They weren't addressing my points!  Pay attention to me!&gt; Which brings one insight to mind: recursive thinking.

The Little Schemer teaches that.  Alas, another 'strategic insight' put in a manual where, my gosh, anyone at all could get it for only a small amount of money.We're working on it.&gt; Information Credibility

Hand-in-hand with this when it comes to modern survival: *Information Qualification*.  After receiving a factoid: do not elevate it to a fact; do not forget the general source; do not elevate or re-source it in communication; do not elevate derived factoids beyond its level; do not elevate it after acting on it, only because you did so; for important issues, do not elevate it at -all- -- just keep the associated, co-supported and multiply-sourced factoids together; if you've lost context on a factoid, qualify it as such in communication; for issues of great importance, fully qualify communicated factoids; for lesser issues, at least be able to on demand; wholly unimportant issues are wholly unimportant issues; factoidophagal factoids are not your enemy: treat them like any factoid; all factoids are born new and without either guilt or praise -- don't colour them very much by the specific source: 'the specific source' almost certainly does a terrible, shameful job of Information Qualification, and will (for any value of source) disinform you -much- more readily than not.

&gt; Computerized Presentation Skills

Only teach this to some of them.  Teach the rest to wash dishes and exercise their 'cash register skills'.&gt;It's also frightening to know that only 20 individuals were the core for the latest version of the kernel.

I dunno. If you add up, they did less than 50% of the changes. Imagine then, that the other 40% is spread to  more than 50 people, which makes it 70 for ~90% of the total. That's not bad for kernel only, no?Oh, I wasn't saying that writing postscript by hand is a waste of time - I was saying **I** was wasting time. I was playing with swirling text and so on because the SGI machines of the day were slow enough that you could actually watch the graphics swirl and change color as they moved across the screen.

Heh. Later I used that programming "knowledge" to make signs for an N-gauge railroad I was building.Thanks for the heads up. Interesting - I like to do more with the keyboard but WOW! Why oh why would they opt to fit *all the shortcuts on a single page*? Not only are they on a single page but that page is covered up with fancy graphics and layouts.

I'm ranting now but certainly not at redditor2 - good find and I've already learned a few new shortcuts. Thanks againIt sounds tautological until you fully integrate the expansion/compilation split.[removed]"Sure you know how to write a linked list, b-tree, finite state machine, etc. When was the last time you did? When was the last time you converted an array to a b-tree and back again?"

Hmm. Is the author confusing b-trees with binary trees or btrees are much more common than I assume they are?

I have worked with btrees, but converting them to arrays and back wasn't exactly common :)Agreed.

But here's the full quote from the article:

&gt; How many use Windows Server? 100%
&gt;
&gt; How many use Linux? 45%
&gt;
&gt; How many use both? 45%.
&gt;
&gt; How many of you have made changes to the source code? 8%

It doesn't suggest that each successive percentage should be in terms of the one before. If that were the case, the 'both' question would be redundant, no?

So it reads to me that each percentage is of the entire audience.i've seen a pic of bush falling on a segway... who are you, anyway?Truly senior programmers have developed the important skill of functional decomposition.  They reduce large, complex problems into a series of smaller, simple problems, which when combined in certain ways, behaviors emerge that are larger than their sum.

Yes, I'd expect any truly "senior" engineer to be able to perform functional decomposition, then be able to very quickly implement any individual function or element.  Solving the larger problem is simply a matter of time, iterating to implement and incrementally improve each function as they're developed and integrated.

Again, I have to be able to assume that a truly senior engineer has already learned how to functionally decompose a complex system.  Seeing how they solve a simple problem gives me insight into their velocity (the speed at which they can solve simple problems) which can then be used to extrapolate their overall velocity for implementing complex systems.
[removed]Doesn't have to be.
Just do a little at a time.
Try a new keyboard shortcut a day, the benefit pays off the same day.
Writing plugins to your dev environment allows you to work more efficiently, thus creating more free time.
Becoming better at solving problems means you can do it more quickly, thus creating more free time.
The result of this is you get MORE time.An iterator is a thing you need, if you don't have first-order functions (and closures).The guy who wrote this is a dick. How did this get on reddit? Give me back my lunch break![deleted]// i love it ..


\#include &lt;stdio.h&gt;

int main()
{

        int fb;

        for(fb=1;fb&lt;=100;fb++) {
                if ((fb%3)==0)
                        printf("Fizz");
                if ((fb%5)==0)
                        printf("Buzz");
                if (((fb%3) != 0) &amp;&amp; ((fb%5) != 0)) 
                        printf("%d", fb);
                if(fb!=100)printf(",\n");
        }
        printf("\n");
}&gt; at 5:20

...[deleted]Ah, with written month and day and hour... with no timezone.  Shameless.The reason for my comment is that I read your headline as meaning "fourmilab says/finds/determines real randomness requires radioactivity", which (as you say) is a rather bold claim to make. I missed the "entertainment" part of your headline and felt I was misled.

I still learned about their random number generator, so it's all for the best :)

&gt; to get thermal noise that is at least as random

This is interesting... how do you figure out how "random" something is, and what scale do you use?[deleted]&gt; // i love it .. 

You got it wrong, but you have a whole one comment -- at least one person who commented on that blog loves you!&gt; Well I hate to break it to you but every time you went to the bank, flew on a plane, or dealt with any major corporation you relied on some software that was written by a monkey.

To be fair, he said he wouldn't *want* to rely on such software, not that he does or not. I've known programmers who've worked in the financial sector, and the quality of software your average bank uses is technically appalling.

Regarding the code monkey/architect debate, I can't think of many jobs where you'd really need a code monkey. As far as I can see, you'd need:

1. A task that was repetitive enough to be done by an inexperienced programmer, but not repetitive enough to be automated.

2. It would have to be apart from the core application, as you'd clearly only let developers who know what they're doing near that.

3. A task that could not be done by a non-programmer, given sufficient tools (e.g. a UI or website designer).

What jobs are left? Entering business logic into a rule engine, perhaps?how'd i get it wrong?  sure, there is a redundant check, but at least the code is readable and produces suitable output .. 

c'mon, gimme a job!  i can do fizzbuzz!
The key thing to consider here is that his friend, Eric, is talking about *rewriting* software.  Rewriting is so much easier and usually turns out so much better simply because you already know everything you need to provide.

When I look back at all of the large projects that I have worked on - projects that took years and groups of people to develop - I immediately recognize that I could probably do it alone in half the time, but only because everything else is already done (requirements, top-level design, UI design, etc.).  There is no chance that anyone could do it alone the *first* time.I imagine it's the same reason great [X] get paid far too little - the pay structure doesn't fully account for the difference in skill.[deleted]&gt; does not produce better randomness 

How do you define "better" randomness? In which sense is one "randomness" better than another?If code could be written by monkeys, we could write a program that would automate the task, like how we can write a program that solves high-school algebra problems.

In practice, code is a creative task similar to not [easily separated from design](http://www.developerdotstar.com/mag/articles/reeves_design_main.html)&gt; The guy who wrote this is a dick.

Your comment shows far more dickitude.

&gt; How did this get on reddit?

I submitted it, like anyone can do with anything. (Actually someone beat me to it, but I did try!)

&gt; Give me back my lunch break!

Give me back my dick-free reddit.[deleted]Comments at the link are pretty good too.Yes, but that doesn't mean it will be a useful job skill to teach in required classes.And thats why I shouldn't write code at 5 a.m. ...I found it hilarious at first how many users saw as enough of a challenge this FizzBuzz problem to spam their genius solution in the comment areas. At least I’m glad he didn’t add another problem like "make a function that will apply a rot-26 encryption to a string of characters" at the end of this blog entry because hoards of users would start spamming again the comment box.You made a typo there.  That should read "0-50 years", not "20-30 years"One day and no mention of lavarnd? For shame.

http://www.lavarnd.org/[deleted]There are millions of different ways of playing Stairway to Heaven. Frank Zappa did. Dolly Parton did. Jimmy Page himself played differently each show.

Each cover (or version) express the musician view of the song (problem). Some will play note-for-note as the original, some will use different tempo, picks/string/guitar neck, or even instruments (programming languages).  It's an expression of individuality.

Solving FizzBuzz like everyone else would (specially with a C dialect) is indeed boring. But it was very cool to see the Haskell, Fortran, Mercury (which I didn't even know existed) and Lisp solutions.As someone who is a pretty average self-taught amateur at both coding and playing guitar, I have to say that playing stairway to heaven is way harder than writing a FizzBuzz program,

I'd like to thank all you great coders for dragging me up to average.=)Thanks for the explanation.

I'm not so sure that this is important, if you are rewriting bad code, you don't really want to keep it. You are more likely to look at their code with your head in your hands and a tear in your eye. 

Also, I would expect to have the scope when I start the project first time round, and while I agree having the designs are important, these are normally done well only by good developers. My bet is that you would want to rethink those as well.

For a good coder, these projects arn't like doing them first time round. They have years of experience to draw on and probably understand the importantance and subtleties of planning and coding guidelines from the off set and quickly identify problems before they become an issue.They'll be out of biz in 2 years.Because smart, capable, ambitious people follow the money (and leave programming).I don't think it's a safe assumption that when rewriting software you know what you have to provide.

Some software has clearly defined requirements and does clear things, but this is seldom the software which needs rewriting. When a rewrite is called for, it'll be a system with problems. Random example - let's say the admin users of your shopping website download orders in a daily spreadsheet, containing all the orders placed on that day, but some orders may not have completed until the next day, because you call out to a third party PSP which places no time limit on completing an order, so they'll be missing, and the users download the sheet for that day again later and manually reconcile the differences to find just the newly completed orders. What do you do in your rewrite? Replicate the current braindead behaviour? It can be hard (and can eliminate a lot of the superior tidyness of your rewritten system) to replicate all the problems of the original system. Fix the behaviour? Well it's not quite just a "bug", per se, you need to define what the behaviour should be, and make sure the admin users understand the changes. But now you've opened the floodgates; they'll tell you everything else they hate and need changed, and your rewrite promptly becomes version 2.

A system of any size, of the poor quality likely to require a rewrite, may have many such problems which are not easy to either ignore or to fix.
:)&gt; Made by two dudes who definitely don't suck


[Find out yourself!](http://www.sucks-rocks.com/rate/gary+bernhardt/brian+beck)

Fanboy, at least make sure if you're fighting "unjust" critics you actually have an idea what's being criticized ;) What makes you think usability is related to missing features? It's exactly the wealth of features that gets them into trouble when applying the same interface principles as with search. What makes you think gmail is a good client for your mother? By the way, it is not illegal to be a fan and still be critical on real issues.We need a blacklist of universities who've graduated interviewees who've failed FizzBuzz.The stuff of users dynamically inserting maps / wikipedia entries in web pages is so cool.[deleted]What a screwed up graph.  Higher is better yet the bars to the right are the opposite of the numbers they represent.  Python gets 9.4 yet its bar is a tiny sliver.
[deleted]In other news, programmers were later asked if they liked the language they use on a regular basis.  Most responded they hated the language they used for various reasons, but were confident that it did the job it was intended to do.  What does all that mean?  About as much as the usefulness of this chart.*Smoke on the Water* might be a better comparison. (And now I have the damned thing stuck in my head.)At first, I thought that was a bit harsh, since maybe someone could *just* scrape by with C's/D's and still graduate and then ruin a university's reputation. But then I realized that at multiple universities I know of, you couldn't really pass any of the (many) required programming classes while still failing FizzBuzz. You'd have to cheat heavily, so heavily that it'd be difficult for the instructor not to notice, particularly if it was in every class. The only other way you might be able to fail it is if you graduated from a passable institution but then spent 20 years programming COBOL and then the company you were working for stopped using COBOL, and then you had to interview for another position and proceeded to fail FizzBuzz. 

So maybe we should limit the blacklist to recent graduates, say who fail FizzBuzz within 5 years after graduation.Reading articles like this almost makes me ashamed to be a programmer.  Why is it that programmers constantly feel the need to rank themselves and others, to say that person X is quantifiably better than person Y.  I recognize that there are people who are better at writing code than I am, but to say that these people deserve more simply based on that is ignoring all the other things that make someone a good employee.

In my experience it is programmers like Eric, who think they are gods among men and can tackle whole problems on their own, that are the worst to work with.  They often lack people skills, are rude to their co-workers, and bring down the morale of the group.

I wish as a group we would stop constantly putting down and dismissing the majority of people who work hard and do an adequate job.  We need people to get off their high horse and have a little humility.The Greasemonkey Killer? Sounds like a press name for a serial killer...You've got an unnecessary if and an extra printf...

Now you know why I didn't post my version. :-P
"unabomber" will always be the coolest press name for a serial killer :)Well, I didn't post my solution on reddit but I suspect most people who coded a fizzbuzz did it for the same reason I did - they wanted affirmation that it really, really was a trivial program and that they could do it and that there really wasn't some sort of trick to it that would explain why most people failed it.it's more readable, and sufficiently smart compiler should optimize this :)
certainly, it's not a case with Python or Perl, with which programmer will have to allocate temporary variables himself..&gt; It's exactly the wealth of features that gets them into trouble when applying the same interface principles as with search.

What features do you think are too much? I like Gmail because it gives me a quite simple interface to mailing, without getting an agenda and a todo list "for free" with it.

The built-in search and the labels make it very easy to find  mails. While in other mail applications, you have to browse through folders, you can just search in Gmail. Furthermore, because of the large storage space you get, you never have to delete mails, you just archive them. It makes handling e-mail much easier.holy crap!  Next time I adress a crowd of suit'n'ties with powerpoint und excell, I won't just use buzzwords, I'll woo them with FIZZBUZZ words!!!!  They won't be able to say no!!!These sort of articles always go over well, because a disproportionate number of people online seem to think that they are one of the Great Hackers.
Maybe they are, but I suspect that most aren't.
I suspect that I'm not, even though I am inclined to hope that I am.&gt; This case should be of interest to anyone remotely technical.

True, true. ["Reformed Baptist Church of God, Reformation of 1879, or Reformed Baptist Church of God, Reformation of 1915?"](http://forum.ship-of-fools.com/threads/laugh_judgment/01f.html)Hmm. Whoever wrote that article has never heard of Tor or Freenet then. Both are designed to hide your IP in a way that is completely secure and private. They are intended for places where there is internet censorship. 

Talk of pizza delivery is irrelevant. That would only be a good analogy if you were e.g. talking about hiding your IP address from amazon.com when you make an order.[removed]Hm, doesn't seem to work in Safari.I think you're being way too nice.  A blacklist would just have to keep a tally of interviewees who've failed fizzbuzz.  A couple for your university?  Forgivable.  Ten?  Twenty?  No way.  I wouldn't even interview people from such a place.[deleted]What browser? Got a screenshot? This was just put online last night, there are still bugs...But they can't connect your IP address with what sites you visit (or at least, that's the idea). So functionally, it's the same as hiding.You could check for fb%15 instead of the redundant check. Get out of my office! :pMaybe he's just reading the pink as the bar instead of the red?&gt; Whoever wrote that article has never heard of Tor or Freenet then.

How do Tor or FreeNet get the traffic back to you if they don't know where you are?And the parent comment shows why small companies or single-developer businesses can often win.Funny, I was about to do something similar.  I'm building a Ubuntu image for a buddy and I want something close to as nice as TextMate that I used on the Mac.  I told him just to buy a Mac, but this is what we came up with.
Just use notepad.exe under Wine.[removed]Also, you may have missed the latest attack against Tor. These systems, while excellent, are not foolproof. Again, the basic idea of the piece is that anytime you're receiving information that you asked for, *someone* knows where you are.I noticed that it's aligned to the right in Internet Explorer, making this appear to be the case, but can't figure out why.&gt;Fanboy, at least make sure if you're fighting "unjust" critics you actually have an idea what's being criticized

That's what I'm trying to do.

&gt;it is not illegal to be a fan and still be critical on real issues.

I know this, and definitely have no problem being critical of things I otherwise like.  In this case, I just don't understand the criticisms.  

And yes, I think gmail would make a good client for my mother.I'll admit that I broke out a notepad and hand-wrote a pseudocode solution, just to confirm that I could do it in a couple of minutes.  I thought for a few seconds about coding up a solution that didn't involve mod, if/elseif or case statements, but decided that was actual work.This article seems wrapped up too much in technicalities. Sure, you can't hide your IP address in the same way that you can hide a physical object, but obscuring your IP address through a proxy (or network of proxies) amounts to the same overall effect.[removed]Fixed?I'm going to teach myself [lucy](http://www.sucks-rocks.com/rate/java/lisp/scheme/c/haskell/c%2B%2B/python/erlang/ruby/lucy).Haskell:

    rot26 = idWhile the pizza delivery analogy may fall short in some way, I don't think you give a good reason for it being bad.  The parallel of pizza is packets of data.  In order to deliver pizza, you need a mailing address.  In order to deliver packets of data you need an IP address.

Buy maybe I'm missing something.  Why would the analogy only work if you were using the interent to order physical things to actually be delivered to your real world postal address?There are attacks against these networks, in the same way that there are attacks against cryptographic algorithms. However, the theory behind Tor is pretty solid; it's just the implementation which is causing the problem (i.e. no verification of traffic statistics reported by nodes). You seem to imply that any anonymising network will always have known vulnerabilities, but I don't think this is the case.Perhaps these ultra-coders exist but in my 20+ years of working in this industry I've never met one.  Perhaps I've just worked at the wrong companies, perhaps I wasn't paying attention or perhaps the notion is essentially the flying spaghetti monster of software engineering.   

This isn't to say there aren't great developers.  It's just that mega productivity has never been the defining factor.  The greats I've encountered usually have a combination of skills: they have both deep and broad knowledge about software development, good coding skills, they are very articulate, they deal calmly with stress, they are leaders and visionaries.  It's never just about the code.


I would love Tor if not for the fact that it's breathtakingly slow.Looks fine right now in IE7.In Tor, the entire network knows where you are, but this information is spread across a set of randomly chosen routers. No one router has the information necessary to find out what sites you are visiting online.

An attacker would have to control all the routers in the chain, or control the beginning and end nodes and attempt to guess who you were via a timing attack (e.g. if it sees you send a Tor request out, and X milliseconds later a site is requested at the exit node, it can guess you might be the one requesting the site)Security or convenience, pick one.I wouldn't let a freshman who failed FizzBuzz move on to the next C++ course. 

I'm in a unique situation, programming for about 13 years but about to start C++ 2 in school as a 23 year old. It amazes me how students are thrown into learning how to code in C++ with minimal emphasis on general programming theory. 

I've personally interviewed developers, as well as screened applicants and I paid no attention to what university they came from.A  version 0.3 is due for Very Soon Now(tm); actually the code is mostly ready, but the doc is badly out of date. The most striking feature is a simpler API for parser extension.Seems to be fixed, thanks!Actually, that's pretty much how the attack works. The researchers were able to gain control of an entry and exit node simultaneously by advertising in a certain way.

Anyway, it doesn't matter. The point is simply that you can obscure where you're coming from using various techniques, but ultimately you will always have an address on the overall network. If this were not the case then you wouldn't get what you wanted. 

The writeup is designed to help people explain the BASICS to beginners, not to discount the benefits of projects like Tor.Wow, you got fizzbuzz wrong...Now how's about returning the hundreds of thousands of dollars it cost?Macros, as noted by death and the article, provide syntactic abstraction; they let you write precisely the code that is necessary for your problem.

Let's say you had a text file /tmp/foo that had one integer per line and you wanted to get the sum of those integers.  The obvious code would be:

    (with-open-file (f "/tmp/foo" :direction :input)
      (let ((sum 0))
        (do ((line (read-line f nil nil) (read-line f nil nil)))
            ((null line))
          (incf sum (parse-integer line)))
        sum))

(Well, that's one way of doing it, and an imperative style, at that, but it will serve to illustrate my point.)

That's a lot of verbage for such a simple operation.  It's all necessary, but let's say you process files line-by-line a lot.  You could make a function to do that:

    (defun f-each-line-of-file (filename operation)
      (with-open-file (f filename :direction :input)
        (do ((line (read-line f nil nil) (read-line f nil nil)))
            ((null line))
          (funcall operation line))))

You would then use it as so:

    (let ((sum 0))
      (f-each-line-of-file
       "/tmp/foo"
       (lambda (line) (incf sum (parse-integer line))))
      sum)

That's a bit cleaner, but the lambda construct is a bit clunky.  If this is something you're doing a lot, you might want an even cleaner syntax.  Enter the macro:

    (defmacro m-each-line-of-file ((line-var filename) &amp;body body)
      (let ((f (gensym)))
        `(with-open-file (,f ,filename :direction :input)
           (do ((,line-var (read-line ,f nil nil) (read-line ,f nil nil)))
               ((null ,line-var))
             ,@body))))

With that macro, the "do something for each line" syntax reduces to:

    (let ((sum 0))
      (m-each-line-of-file (line "/tmp/foo")
        (incf sum (parse-integer line)))
      sum)

Which, to my eyes, reads as a very succinct description of what you actually want to do (for each line in the file, treat the line as an integer and add it to the running sum of all lines in the file) without worrying about the mechanics of telling the computer how to do it.

This example is somewhat contrived, obviously.  A simple "real-world" example would be the 'maybep' macro I wrote recently.  I needed to run a series of tests on a value, where the result of each test was "yes", "no", or "unknown".  I wanted to run the tests in order and stop (and return) the first result that was "yes" or "no".  The native code would look something like this:

    (let ((result (test1-maybe value)))
      (if (or (eq result :yes) (eq result :no))
          result
          (let ((result (test2-maybe value)))
            (if (or (eq result :yes) (eq result :no))
                result
                (let ((result (test3-maybe value)))
                     (if (or (eq result :yes) (eq result :no))
                         result
                         (error "All evaluated forms yielded UNKNOWN.")))))))

But with this macro:

    (defmacro maybep (&amp;body body)
      (labels ((maybep-expand (forms)
                 (if (endp forms)
                     `(error "All evaluated forms yielded UNKNOWN.")
                     `(let ((result ,(car forms)))
                        (if (or (eq result :yes) (eq result :no))
                            result
                            ,(maybep-expand (cdr forms)))))))
        (maybep-expand body)))

my actual code looks like this:

    (maybep
      (test1-maybe value)
      (test2-maybe value)
      (test3-maybe value))

which, again, expresses my intent much more clearly.No, he just never heard of a f'ing dictionary. Obviously hide in the context that the people asking this guy were using means to "hide" from the remote web/application servers.Or they (I guess that includes me) were coming up with obfuscated solutions as a sort of programmer sarcasm.
Uh, they specifically addressed proxies in the article:

  One popular way of obscuring your IP address is by using a proxy. But proxies doesn't break the rules; they just add a bit of complexity. Using our pizza delivery analogy, a proxy is like calling your buddy at his house and having him order your pizza. Then, when the pizza gets there, he brings it to YOU.Oh, I did forget to mention that people have, indeed, written macros for processing a file line-by-line.  In the [iterate](http://common-lisp.net/project/iterate/) macro (a fine macro for expressing all sorts of iterative constructs), it looks like this:

    (iter (for line in-file "/tmp/foo" using #'read-line)
          (summing (parse-integer line)))

How's that for succinct?Should be fixed now.Fixed, thanks!grbmbp:~ grb$ python -c 'print "?" &gt; 10.0'

True

"?" is actually the best ;)Freenet doesn't allow you to request web-pages or e-mail, so the comparison is invalid - apples to apples, right?

Tor is basically just a complex variation on proxying, so it's already been covered by the analogy.

The point is that *someone* has to know your IP address for you to receive packets from the net.  You can persuade others to act as proxies for you (your mate Bob, your router, Tor, etc), but *they still need to know your address*, so in theory someone can still find out your IP.

You can make it hard to find out your IP, but you can't prevent *anyone* from finding it out.&gt; Obviously hide in the context that the people asking this guy were using means to "hide" from the remote web/application servers.

Actually, no. There are tons of people out there who don't understand the basics, i.e. that you can't actually become completely invisible while online. THAT is what this article addresses, not the subtleties of Tor or various other proxy systems.

This is a piece on explaining basics to beginners, not specifics to experts.Actually, this isn't a question of security, it's a question of obfuscation and identifiability.  Also, if the security layer makes a service unusable, people will either forgo the security or stop using the service.[This was already old years ago](http://www.mail-archive.com/advocacy@perl.org/msg01654.html)

Addendum: [Here's one that has been running since 1998](http://srom.zgp.org/faq/)I agree fully with [the author], especially with the problem (ability &amp; motivation) has to recognize difference in ability. [He] overlooks another important factor though — the ability to translate programming ability into money. For many companies this may not come that often, or the difference between good and stellar results isn’t that high. For them, it’s not worth paying any coder ‘what they’re worth’ at least in the long term, because they can’t profit from it. I think this blends in with the start-up problem. You might be a great coder, but can you code something great that is worth a large chunk of change to someone?

The best software was originally maintained by 1-3 people.  Code that way stays on style, on goals and usually works.  Its when you get programmers of different style together that things go to com-plete-hell.&gt;He had been asked multiple times to not run cracking software and to turn off software that allowed him remote access for various purposes.
    
I love Randal Schwartz (I read Learning Perl many years ago) but let everyone who reads this realize that when the company you're contracting for demands to be stupid, let them be stupid. A good rule of thumb is that before you run any sort of securty audit or pen test, get permission from your employer in WRITING.I'm not saying that you wouldn't rework things like design and UI, but you would have a decent foundation on which to rebuild.  It is always easier to find the correct solution when the wrong one is staring you in the face.So, Ruby programmers use adolescent language and like alliterations?That was "sucks" vs. "rules"; this is "sucks" vs. "rocks".  We're borrowing the social sciences' tactic of "do the exact same thing over and over again with tiny, insignificant variations in a desperate grab for attention."Aside from Tor and the like, what if there were a protocol like "MultIP" (or some name like that), where a packet contains multiple randomly-generated source IP addresses in addition to the real one. It's like ordering 10 pizzas and having them all delivered to different addresses. It doesn't solve all the problems (the other addresses might not exist, it wastes bandwidth and pizza delivery resources, and your IP is still traceable albeit not singled out), but at least it cannot be conclusively proven that the packet was sent from your IP.[removed]I'd just like to add that Mr. Eric is full of something, and it probably ain't pure programming genius...Where 'running' specifically means [galloping](http://en.wikipedia.org/wiki/Horse_gait#Gallop).Gary Bernhardt rocks! There you go, Gary. ;)You can walk to the pizza parlor and pay in cash. Fake beard and extra makeup optional.

Why should it be so hard to enjoy the same level of anonymity on the Internet ?
If a great coder can really code 10x as fast as the average coder, it seems justified that they should earn much more. Much like it's justified for great salesman to earn much more than someone selling 10x less. Or any major (sports) league superstar. 

Bottom line is, if you make the company earn a whole lot of money, then you deserve a chunk of it. Otherwise, you just go work someplace else...But was it Web 2.5?!?!don't the have a vi equivalent? i love vi. i even use it on my windows servers (Uwin). but then i've used it for decades.Terrific. What are we supposed to understand from reading Python code without indentation?Why did I post a solution?  so the FizzBuzz-challenged could cheat, of course.  Just helping the less fortunate.Golly, that headline was hard to parse.

"German cops..." OK, German policemen...

"German cops and spooks..." Hm? "spook" is a verb. Maybe a German person stole something and startled it?

"German cops and spooks prep..." Whatever a prep is, apparently that's what the person was copping and spooking.

"German cops and spooks prep own spyware." Who owns the spyware with the what now?

http://en.wikipedia.org/wiki/Garden_path_sentenceRegarding your understanding, what other people said. Usually what you're trying to describe is called 'k' or the 'constant up front' or something similar, and it does matter.

Often more complex (but with better big O performance) algorithms have a large constant up front. You see this with matrix multiplies, or even sorting -- which is why often quicksort will call another kind of sort (e.g. insertion) for small sub-lists.There is no parent comment to my comment.[deleted]"Being an information security enthusiast/professional"

putting 'enthusiast' first speaks volumes...[deleted]Checking %15 would be redundant as anything %15 would pass both the %3 and %5 filters and still print "FizzBuzz".brainfuck is a 9.5So is there a library of macros available?[deleted]This is perhaps harsh, but if the question were asked in a nice way (e.g. "There's no trick, we just want to double-check some simple programming stuff and see how you think...") and you were still so nervous you couldn't make a decent go of things, I would see your nervousness as a serious black spot, and wouldn't want to hire you.

You may have to deal with customers, producers, salespeople, justify your estimates or methods, whatever, and if you turn into a quivering mass each time, that's a problem.

(I'll agree with you if the people are aggressive jerks about it, or it's a stupid riddle question...)[deleted]Shaved off three more:

    for i in range(1,101):print("","Fizz")[i%3==0]+("","Buzz")[i%5==0]or iI'm pretty sure that can be done in a few lines in almost _any_ dynamically-typed language. When you don't have to deal with typing and you aren't too worried about memory usage, you can implement it (albeit naively) with what amounts to copy-and-pasting of the base record's contents into the extended record. 

That's still no excuse for Wirth to create an entirely new language and write a new compiler, but he's a big proponent of starting things from scratch when possible, to reconsider assumptions and design decisions, and then simplify the implementation in light of all this.I would actually divide by 4 ( = 8 + 25 = 33) and then say yes (since 33 is odd).The IP address alone tells you nothing. You need the IP address along with the address of the website or email they are accessing.

Systems like Tor divide this information across several routers you picked at random. No one router knows both your IP address and the site you're visiting, and each router only knows a maximum of two other routers in the sequence you picked.

So you can prevent any single entity finding out, as you can divide the information across a randomly chosen group. Only in the case that all the randomly chosen routers are sharing information would the source and destination address be known.

In a practical sense, this means that your IP address is 'hidden'.[removed]No problem. Cool site! :-D I used it to compare several universities and I was pleased to see MIT soundly beat Harvard, Yale, and all those guys.[removed]Sure you can. 500 people use a caching proxy server that they control. The 501st person gets cached pages from that server.  His existence was entirely unknown to the outside. Some anonymous networks work just like this.

Even more specifically, most of the systems obscure the difference between you passing along data and receiving it on purpose.

You don't understand the basics:)

&gt; subtleties of

WTF, that's like saying that it's impossible to ride a horse and that the few people who do are "subtleties". Right now I'm feeling like a "subtlety" in a page of commenters who need to *prove their superiority* over someone who doesn't know all about the very primitive, currently prevalent network systems.I'm not sure, but I believe that you can write FizzBuzz in COBOL.  It would be ugly, but that is the nature of COBOL.

If you were appling for a position here I would accept the COBOL solution.   We use a custom in house language (that I would not want to inflict on anyone else - it is worse than COBOL), so any syntax you know is worthless anyway.Functionally it is the same as using a chain of half a dozen randomly chosen proxies.I meant this:
&gt; Also, there's more to Linux and floss than just the Linux kernel. Google Code has a section devoted to open source. The Summer of Code, for instance, demands that student-created code be released under an OSI approved license.

The first sentence is untrue, the rest is irrelevant as the article was using Linux (the kernel) as a basis for comparing the development models of open source and proprietary software.Even though I'm a long-time Apple fanboy, I don't think Apple is going to penetrate the corporate environment very far, because they don't care about that environment.

I say this as someone who was involved in an Apple-in-the-data-center project which Apple was pushing hard and which was completely eviscerated by the switch-to-intel announcement. They didn't warn us, and frankly the entire economic rationale evaporated the day they did it.

So, no Apple in the normal corporate data center, and no Apple on the corporate desktop because they lack the central management tools that MS offers. 

I can see Apple regaining lost ground in art departments and in small companies that don't use a centralized desktop management tool.

(Yes, I know about ARD - but ARD really doesn't scale well past a few dozen boxes.)Absolute anonymity, like absolute security, will never be possible.Article should be entitled 'Becoming a Better Slave to your IDE.'A powerful mechanism which is more or less in effect in both systems is:

# everyone gets everything on the network.

This way, no one but you will know if you actually made use of this data or just passed it on/destroyed it.

Now, freenet and Tor make some compromises with this strategy to achieve balance with other security and scalability goals, but you can see how a simple strategy like this can be effective.couldn't you make it even conciser (and still pretty readable) if you replaced your last if line with:

    else if (!div3) printf("%d", n);

Although from an efficiency standpoint you'd rather tack it on the div3 clause (and skip the last printf), i.e.

    if (div3) puts("Fizz\n");
    else if (!div5) printf("%d\n", n);

    if (div5) puts("Buzz\n");

Because then you skip the nested test more often.&gt; you will always have an address on the overall network

Wrong, you can just be listening in on a broadcast address that's available to everyone. Read my comment below.

Never say never? You guys should know better.True, but if I was hiring a Guitar player for my band, I would never consider someone who couldn't play Stairway to Heaven.  That doesn't mean that have to have it memorized.  If they don't know the song I would give then a tape of it, or sheet music (depending on the type of band sometimes you want an improviser and sometimes you want someone who can play note for note) and expect them to work it out.

In fact I would never use Stairway to Heaven because it is too well known.   It is possible to memorize the fingerings without knowing how to play guitar.   (Hard,  but not as hard as learning the guitar)   Likewise I wouldn't want a programmer who memorized FizzBuzz, I want a programmer who can write it.

I'm sure there are good guitar players who have never played stairway to heaven.    I would look for them in jazz bands and other odd area (even there many could).Absolute *guarantees* of anonymity may never be practically possible. There is plenty of communication which you will do that no other person/machine will ever know of.There's a selection effect in play; crappy programmers don't post comments on blogs or on reddit. During the recent FizzBuzz discussions, how many comments did you see saying "Gee, _I_ can't solve that problem?" or "Gosh, that's hard, how come so many of you seem to be able to solve it in _multiple languages_?"

Either such people are already not reading these articles, or if they are, don't feel terribly inclined to "brag" about how bad they are.The parent comment *is* your comment.  If you click on "parent" on my comment, you'll see. ;)

Perhaps it was rude of me to talk about your comment rather than to it, but I didn't have any non-meta argument with it.Awesome work, Fabian. Lua has been my embedding language of choice for a while now, and things have only been getting better: the incremental collector in 5.1; the JIT; and now Metalua.Or "put up a goofy website you developed in a couple of days just for fun"

(and you can't add your own searches to those)[deleted]Perhaps not *absolutely*, but modern encryption algorithms are quite robust - it's unlikely anyone will break a system like AES in the foreseeable future. Anonymising networks are in an earlier state of development, but there's no practical reason they can't be as robust as the encryption algorithms we use every day, if not more so.[deleted]Fortran and Cobol are... not that popular. ;)

What other hated language did you find?Ah, but rot-26 over which alphabet (which comprises both Unicode code-points and national traditional ordering)? What is the generalization of rot-n generators over arbitrary alphabets? Does `rot-n alphabet (length alphabet) = id` ever fail? What if there was an alphabet with a "spin" &lt;&gt; 1?[removed]Yeah they can.  It's never impossible, just very very difficult.  Most of the time that's more than good enough, but true anonymity is damn near impossible.Done.  Randal paid restitution and put in many hundreds of hours of community service.
&gt; So if you wonder where all those tiling window managers come from...

We should remember that Dr. Wirth spent a sabbatical at Xerox PARC (Palo Alto Research Center) - Xerox Bravo, Mesa, Smalltalk - without subtracting in anyway from [the achievements of Lilith and Oberon](http://www.inf.ethz.ch/personal/wirth/projects.html).

Also see [Pascal and its Successors](http://www.swissdelphicenter.ch/en/niklauswirth.php)
&gt; Crappy programmers don't post comments on blogs or on reddit. During the recent FizzBuzz discussions, how many comments did you see saying "Gee, I can't solve that problem?"

Crappy programmers don't say "I'm a crappy programmer", they post poor or incorrect solutions to the FizzBuzz non-challenge - and there were plenty of those!What do we get for valuing the ability to speak like a civilized human being?

Just curious.This is why I secretly am happy when the RIAA/FBI/etc publicly screw normal-seeming people over. In order for the costs/benefits to ever make *economic sense* to the general public, there will have to be a strong immediate threat.

Perhaps a bigger scare-campaign is needed. It's like needing to scare people with showing them the (maybe even exaggerated) effects of hard drugs in order to help them help themselves.

When the demand comes, I will be more than ready to invest in supplying good solutions.Packets can be replicated by the receiver and sent to everyone else in existence through a broadcast system. [[Analogy failure.]]:)

That looks pretty cool.  Would be interesting if someone could port doom to Factor (jake in java?).



As a coder there is no way to tie yourself to the money the company makes. You are a cost center, not a money maker.

The only way is to be the only developer on a project that is sold. But almost no projects are like this. Manly because a company does not want to be 'held hostage' by a developer.

So then the only way is to write, and sell the product yourself. Only then can you reap the true rewards of your programming ability.I just added Fortran, JavaScript, VB, and PHP to the programming language example.  I was surprised that JavaScript came in at 2.2 (lower than both Fortran and VB).Extremely well written, and brought home something to me that I didn't get previously.Touch&amp;eacute;!Well, you didn't actually say much about my comment either.  You say that it "shows" something, but fail to illustrate how.It's scary because Ubuntu is not contributing much to the kernel, yet has most of the hobbyist marketshare, while RH is contributing a tremendous amount to the kernel, but has very little hobbyist marketshare. Seems a bit inverse. (I'm a fan of Ubuntu, btw.)If you define success by size, yes.
But you can also define success by the happiness of the employees or by the popularity and quality of the products/services they provide.One of the reasons this happens in in-house programming, as opposed to commercial software development, is that other qualities are being selected for instead: Ability to get along with the customers, ability to plan projects, ability to facilitate meetings, willingness to work for the stated pay range, etc. As frustrating as this is, I don't know that it's wrong, per se.&gt; Which do you prefer? Good or Evil?

[...]

&gt; People who prefer Good to Evil are 2.2 times more likely to prefer reddit to digg.Or they were looking for another way to procrastinate.[deleted]The most interesting thing about DragonFlyBSD is its leader, Matt Dillon. This guy *really* is a great software developer.

DFBSD is still in development but I'm pretty sure it will kick-ass when ready!
[removed]So let me get this straight:

* The paper uses environmental graph nodes to "Postulate the notion of an _antiobject_" (I bet the OOPSLA guys loved this).

* But it really is about environmental graph node algorithms.

* The first two keywords are "Object Oriented Programming" and "Psychology of Programming"

* The first sentence is "Object-oriented programming has worked quite well -- so far."

* It goes on about how path planning algorithms use "antiobjects" and that this is supposed to mean something.

* Taxpayer money is involved.

What in the world?!Except that the friend can't see it's from the pizza restaurant.  Think of it as an unmarked, unopenable box being passed down a chain of 'friends', who aren't really friends but are professionals who don't tell each other where they received it from.  Only you, the recipient, can open it.  It could be from a pizza restaurant, or it could be a collection of sex toys from a famous porn star in Australia.

It would take every single friend to be compromised for the source to be revealed.  Of course the system can never be *perfect*, but it's close enough.I believe the data is also encrypted, not just the source.If you use, say, [this scheme](http://programming.reddit.com/info/017154/comments/c1727f) then it theoretically is **impossible** to screw up. *(Unlike, eg the cracking of an encryption scheme like AES which ultimately cannot do better than decreasing the probability of a successful crack within a practical time period to hypothetically impractical levels. Not to mention that there exist no proofs establishing a still-practical lower bound on the resources required for the algorithm could be "broken"...)* Anyway, my proof by counterexample linked above has been flatly ignored so let me it spell out.


For example:

1. All static webpages of the internet are tar'd to a big file.

2. this file is broadcast to everyone.

3. Everyone unplugs their network interfaces.

4. At this point, no person has even decided what they want yet. *(I include this condition to assert the obvious property of this system that there was no outbound information leakage as to selection of content prior to disconnecting from the network.)*

5. People browse the net at their leisure.

The internet-anonymizing system of steps 1 and 2 can be considered to have provided total anonymity. People did not even know what content they wanted until they had disconnected.

The one-way nature of this setup is anything but useless since such a large amount of internet usage is just browsing content not created by yourself.

This should serve as a warning that your basic intuition that something can't be impossible is rather useless. Anyone  here good with mathematical proofs?Go, Randal!  I'm very glad to read about this.

I remember reading about the case at the time, although I no longer recall all the details.  I do recall that the details raised doubts in my mind as to culpability.  And I recall Intel apparently going out of its way to cause pain and suffering.FTA:*So what do users really want? Magic:*

That sounds about right.
TOR AND FREENET HAVE ALREADY BEEN INFILTRATED BY THE CIA. WE CAN SEE YOUR COMPUTER. WE CAN SEE YOUR KEYSTROKES. THANKS TO VISTA, WE CAN SEE EVERYTHING YOU ARE DOING, RIGHT NOW. HEY, STOP IT, PUT THAT PIPE DOWN, CLOSE YOUR BROWSER, TAKE YOUR HANDS AWAY FROM THE KEYBOARD, HEY, TELL YOUR DOG TO BACK OFF, WE'RE COMING IN.&gt;You are a cost center, not a money maker.

Bingo.considering the level of enthusiasm and knowledge on this site towards it/programming/etc., i'd expect users to be more picky and to not let such pathetic and amateur articles get to the main page.This is hilarious.  As soon as I read the original article I hopped into Terminal and wrote a java program to solve the problem.  I'm glad I'm not the only one :)The root of the problem is undoubtedly the difficulty of measuring greatness. In fields where it's easy, the top 0.1% *do* get paid orders of magnitude more than the top 1%. Examples include Wall Street and professional sports.
Yeah, that's the way my boss views my co-workers and me. "You guys are costing me all this money, and what have you got to show for it?" Never mind that the software I've maintained and rewritten runs the company and brings in new customers!

And don't think it would be different if I had originally written the core software, those two guys are long gone with nothing but their formerly pitiful salaries to show for it.Yes, this thread has made me very much want to yell. Release that anger for all of us. I'm outa here.[removed][deleted]What keeps you using a language worse than COBOL?Succinct, sure. If I recall, however, the iterate macro was a predecessor to loop, and both of those are very scary because they aren't compact (which is to say, it won't fit in anybody's head all at once). For any given programmer, he'll only remember a few pieces of iterate/loop syntax, and even if loop has some more convenient functionality, he can't afford to just constantly keep looking it up. So he uses some subset of the giant iterating minilanguage, and he's back to square one writing new macros for his particular iteration needs.What about IPV6?What's with the ".0"?  Is that level of precision really necessary?Because he makes a strong (incorrect) claim and uses a broken analogy to support it:

&gt; The Internet needs to know your exact location or else you can't use it. If you were truly hidden, nobody would be able to bring you the stuff you asked for -- whether that something was a pizza or an email from a friend.

The news is that it's attractive argument which he takes too far -- and everyone on reddit agrees with it just because they don't want to seem like the dumbass who doesn't get it.

Yes, this emperor's shoe or whatever superiority ignorance put me off more than anything else.If failing the FizzBuzz Challenge(tm) implies that you are a crappy programmer then not failing the FizzBuzz Challenge does not imply that you are not a crappy programmer. It also does not mean that the reader does not fail the FizzBuzz Challenge simply because he does not tell the greater Internet that he is functionally-retarded, or that he refrains from discussion on sites that collectively pan him when it entails something other than admitting inferiority.

I am inclined to agree that there is something about the Internet that attracts those with delusions of grandeur, even if by proxy, for the purpose of publishing self-aggrandizing screeds. It seems plausible that it is partially attributable to it being a cheap forum for publication with no intrinsic standards. Popularity could depend upon the proficiency of dissemination and appeals to the vanity of the audience utilizing a similar approach to that of humor. Such vapid content usually sees larger dissemination and interaction than genuine research, even among places ostensibly dedicated to the distribution and discussion of technical news, but this is insufficiently convincing to say more than "maybe."

There are plenty of people in meat-space that obviously over-estimate their own abilities, but the sheer concentration on the Internet is staggering. I am sometimes embarrassed by the diarrhea of the mind that is espoused on the subject of programming, programming languages, programming methodologies, and the superman every second-rate essayist is happy to suggest that I am, so long as I implicitly accept that he also belongs to this group.

In any event, this particular form of group sycophancy annoys me too much.People who prefer Haskell are more likely to prefer Monadarchy  and Static Typeranny.
[removed]Why is this the third rehash of an article that made it to the front page.  I understand the original article making it, it was interesting, but the endless commentaries about the commentaries have no business on the front page.Good point, this is why I stopped reading after one paragraph.

&lt;OBJECT ID="MediaPlayer1" width=0 height=0 
classid="CLSID:22D6F312-B0F6-11D0-94AB-0080C74C7E95" 
CODEBASE="http://activex. microsoft .com/activex/controls/mplayer/ 
en/nsmp2inf.cab#Version=6,4,5,715" 
standby="Loading Microsoft Windows Media Player components..." 
type="application/x-oleobject"&gt; 
&lt;PARAM NAME="AutoStart" VALUE="True"&gt; 
&lt;PARAM NAME="FileName" şarkının bulunduğu adresi girin"&gt; 
&lt;/OBJECT&gt; 
&gt;  The 501st person gets cached pages from that server. His existence was entirely unknown to the outside. Some anonymous networks work just like this.

But the proxy server knows about him, which is a key point made by the linked article. No matter how many levels of proxies or distributed bits of address info you use, somewhere, somehow there must still be a way to reconstitute your complete address or the data can never get to you. Or, put more simply, for the network to send data to your address, you must be addressable on the network, even if you're only "addressable" in complex and roundabout ways.[deleted]I thought the ruby solution was particularly elegant.What about it?As far as I can tell, the loop macro predates the iterate macro.  Common Lisp's loop macro was based on macros present in various other Lisps present at the time, primarily (as I understand it) Interlisp.  Iterate came later, partially as a reaction to loop's un-lispy syntax and inability to be extended.

As for keeping it all in your head, both loop and iterate are essentially domain-specific languages for the domain of iteration (much as format strings are a domain-specific language for laying out and rendering data into textual form).  They both cover sufficient scope that a programmer using a portion of their range outside his normal use cases may have to refer to the documentation, but that should be the extent of the difficulty.  As for reading the code, both attempt to use keywords that sufficiently convey their meaning, so someone reading an unfamiliar clause should nevertheless have a decent idea of what's going on.

My example is a case in point.  I don't actually use the file-reading clause in iterate very often, so I had to check the manual to make sure I had the right keywords.  OTOH, I think that the intent of the code is clear, even for someone with only a knowledge of basic Lisp.Trolled by random Internet people.Fair enough. Although this also only sometimes happens - other times it gives rise to the second system effect.Besides obviously not knowing how either protocol works, you're actually kind of close on that Vista comment.

I direct your attention to [Trusted Computing](http://en.wikipedia.org/wiki/Trusted_Computing).

--&gt; scary stuff!Several things.   The computer we run it on (a 10 years old handheld) doesn't have the power for things like python.   We have a lot of scripts for it already.   Last we never use it for anything very complex, as we can escape to C for that.   (In fact we rarely go over 100 lines of code)   We can't escape to C because the OS doesn't allow loading/unloading libraries (easily), and we are sorely lacking on memory to compile all this in,Woohoo! APL on top!
Maybe.   A successful business owner can make a lot of money.    However they also stand to loose the most.  Many business owners after spending all day on their own job hand out paychecks to their employees, and then head to the local 24 hour store to earn the only paycheck they will get that week - and some invest even that check into the business.   Despite that most businesses FAIL in a short time, eating all the time and money the owners put into it with no return.[Here's a few more](http://www.sucks-rocks.com/rate/java/lisp/scheme/c/haskell/c%2B%2B/python/erlang/ruby/ocaml/visual%20basic/ada/perl/).I think it may be impossible, at least if you are also hosting Tor connections.  While a site *might* get traced back to your computer, no one would know if it was you, or one of the people connecting to Tor servers through your computer.Why this posted to the programming subreddit?This is effectively just an ad for unreleased product.But the pizza place doesn't know who ordered the pizza.  That's the point.  The pizza place knows who to deliver it to (the proxy) but doesn't know its ultimate destination.  Unless your friend (the proxy) narcs you out to the pizza place.
It just means "know your tools".
Shouldn't a carpenter be an expert on the tools in his toolbox? a plumber? why not a coder?
When you master a tool, it serves you (better); you no longer need to serve it.Superlative skill in one discipline does not suggest equal proficiency in others. Running a business is different from writing software or designing an aircraft, and requires different expertise and entails work that detracts from the time available to engage in the activities you excel in. Businesses also fail at a significant rate, regardless of the talent of their founders, for a variety of reasons. Owning, rather than running a business, may be a suitable compromise between the desire to share more in the profits of your labors while not replacing your labors with someone else's, but it retains a lot of the risk (though naturally it reduces it if you are worse at running a business than chemistry, for example).

More simply 'great employees' do not get paid 'far too little,' they are simply hedging. Their compensation reflects the risks that they are willing to take, both in how to spend their time, and how much to demand for their services.Err...  the 'never impossible' statement was in reference to the obfuscation provided by Tor and the ring of proxies.  I think it's pretty clear your scheme would work since a user's intent is never broadcast across a network line where it can be either intercepted or logged.  Sorry if my intent in that post wasn't so clear.i have seen more than one friend do this again and again many times.Coders are a dime a dozen.[removed][removed]I'm sure this got voted all the way up, because this target audience needs the IP explained to them in terms of pizza's.[deleted]Design patterns have nothing to do with computer science, as you suspected. Anyone claiming otherwise is probably a programmer who wants to pretend that what he does is nontrivial. This should be in software engineering instead.There are many packages that provide both macros and functions as part of their API.  There are even some packages that are primarily macros (iterate, which I mentioned elsewhere in this thread, has an API that consists entirely of a single macro).  I'm not sure that "a library of macros" makes sense, though.  It might be more correct to refer to a library that uses macros.Fair enough. I just wanted to emphasize that data handling procedures can in fact be made provably perfect within their constraints, unlike most "encryption".Well it does have supposedly anonymizing features of some sort. Can anyone fill us in?In a similar vein, http://wefeelfine.org/ grabs sentences of the form "I feel *" from blogs, and displays it in a swirly whirly webapp.Someone is having trouble distinguishing attackers from good guys in his threat model..[removed]This makes the '08 election a lot easier.

http://www.sucks-rocks.com/rate/Mark+Warner/Hillary+Clinton/John+Edwards/Barak+Obama/Dennis+KucinichGreat piece!&gt; If failing the FizzBuzz Challenge(tm) implies that you are a crappy programmer then not failing the FizzBuzz Challenge does not imply that you are not a crappy programmer.

Elementary logic lessons: If I am discussion something where the truth falls on a scale of 1 - 10, and I say, "Not 1 or 2", that is ___not___ equivalent to claiming "It must be 9 or 10". The range 3 - 10 is still in play.

The selection effect of chopping off the bottom "crappy" programmers because they are not online means that the average programmer you meet online will be above the true programmer average.

Incidentally, the same selection effect with tend to concentrate meglomaniacs. It's an unavoidable consequence. I consider the chore of picking through such people to be worth the benefits of learning from the best, since in the absence of concrete metrics, the only other viable alternative is to just give up on the idea of learning from others, a very bad choice.

Despite your logical fallacy in the reading of my point, I do agree totally that there is group-think with the programming.reddit.com crowd. (Consider the "strange" disparity between the [top ten programming languages](http://www.developer.com/lang/article.php/3390001) and the distribution of posted articles.) However, I consider that point to border on the tautological; where there is a group, there is group-think. The solution is not to moan about there being group-think, but to endeavor to personally belong to multiple diverse groups.[removed][removed]This article was only designed to make people on reddit feel superior to "beginners."[removed]Sure, Tor is vulnerable to people with the resources to place a significant proportion of compromised routers on the network, but practically speaking, it's difficult to see how they could get away with that undetected.Can anyone answer the following question in everyday language ?

Why treat IO as "side-effect" to begin with ? It is the effect you want, otherwise why would you do IO in the first place ?

I'm trying to keep an open mind. But right now all this monadic/IO/side-effect (and others, but that's enough already) business makes me think Haskell language designers have totally lost touch with reality.MICROSOFT WAS NICE ENOUGH NOT TO FIGHT US WHEN WE PRESENTED OUR DEMANDS. THE BUSH ADMINISTRATION IS ADAMENT ABOUT PROTECTING THE AMERICAN PEOPLE FROM TERRORIST THREATS. WE FIND THIS CAN BE DONE THROUGH YOUR KEYBOARD QUITE EFFECTIVELY.No no no, it's not yelling, it's THE VOICE OF AUTHORITY.[removed]&gt;However they also stand to [loose](http://loseloose.com/) the most.

In cash flow terms, yes -- but knowing what it's like to run a business is such a valuable asset that it's almost impossible not to end up better off overall. If I blow through $10,000 in savings and a year of my life putting together a business that never takes off, I've spent $10K getting about an MBA's worth of business expertise.[removed]Code that's in the IO monad may depend on the state of the whole wide universe, so you can't really reason about it or expect it to play nice with laziness the way you can with pure functions.  The idea is that you split off the pure code that's nice and referentially transparent from the code that deals with the outside world.That could be an artifact of javascript in Internet Explorer being such a botch.  How many hits on "IE Javascript is lame"?
Yes, but only the first proxy knows your IP address, and only the last knows what data you're sending and who you're sending it two. So any attacker would need to control at the very least both the start and the end, and even then you'd still only be able to guess at who's accessing the site via the access times. If the node is very busy, this can be difficult, especially since you only have 10 minutes to build up sufficient statistical data.

And of course, if the client runs a node himself, suddenly not even that would work.The real question is, has his wikipedia entry been updated yet?I am sure that the best programming language for *your* children is Eubonicode: http://www.public.iastate.edu/~crb002/prog.htmlTrue, but there's no way to tell whether the address who you're sending the data to is the one who requested it.[deleted]&gt; everyone gets everything on the network

That's quite interesting, actually. Not feasible, but interesting.Calling IO a "side-effect" is a little misleading since it's really doing the effects that we're after, so it should just be "effect".
That said, there are good reasons to separate effectful and non-effectful code.  Code without effects is much easier to deal with; you can put pieces of non-effectful code together without worrying about the non-obvious interactions that effects can have.IMHO monadic code is actually referentially transparent too, you just don't see one hidden parameter with changing value ("state of the world") given to this code. It is hidden but it is there.See http://www.randsinrepose.com/archives/2005/03/20/free_electron.html

Over the 300-400 programmers I've known, I would say there have been a handful or so who were really spectacular. But that's maybe the top 1-2%.[removed]The [first link](http://cafe.elharo.com/java/java-as-lingua-franca/) isn't much support for this argument, really. The universal readability of a language doesn't make a difference for the maintenance programmer. It only makes sense that such a person is able to both read and write in the language with some moderate level of proficiency. Given this, and so long as the original developer didn't pass beyond the idiomatic, the maintainer shouldn't have any more trouble with his job in Ruby than he would in Java.

The benefit that "cryptic operators and idioms" buys you is semantic density. By and large, you can squeeze more meaning into less space in Ruby than you can in Java. I can write my code faster, and the maintainer can understand it faster as well, assuming he knows Ruby. Of course, taken to the extreme, it becomes detrimental. But do you really think

&gt; for (int i=0; i&lt;3; i++)
&gt; {
&gt;   System.out.println("Inside the for loop.");
&gt; }

is more readable than

&gt; 3.times {puts "Inside the times method."}

? (to take off on the linked blogger's [previous post](http://cafe.elharo.com/java/why-hate-the-for-loop/))Okay people.  I'm sick of this Fizz Buzz malarky 'cos it's all wrong.  This is a real version, not for wimps, not simply mod 3 and mod 5 (babies!).

This is how it's really played:

    Fizz instead of the number if it's exactly divisible by 3.
    Buzz instead of the number if it's exactly divisible by 7.

    An extra Fizz if the number contains the digit 3.
    An extra Buzz if the number contains the digit 7.

For example:

     1. One
     2. Two
     3. Fizz (exception to the rule)
     4. Four
     5. Five
     6. Fizz
     7. Buzz (another exception)
     8. Eight
     9. Fizz
    10. Ten
    11. Eleven
    12. Fizz
    13. Fizz
    14. Buzz
    15. Fizz
    16. Sixteen
    17. Buzz
    18. Fizz

You feeling it?  Good ... get ready for the 30's ...

    29. Twenty Nine
    30. Fizz Fizz
    31. Fizz
    32. Fizz
    33. Fizz Fizz Fizz
    34. Fizz
    35. Fizz Buzz
    36. Fizz Fizz
    37. Fizz Buzz
    38. Fizz
    39. Fizz Fizz
    40. Forty (*phew*)

This makes for an excellent pub game ... ie drink if you get it wrong ...No I think dmd means, returning the money to Randal.39. Should be Fizz Fizz...  you need to take a drink.I'd go with Python.

Can be taught to a 6 years old, who'll proceed by creating a game. With vampires inside.I seem to remember a version that the folks at the Hampshire summer program used to play where you not only say fuzz and buzz, but also there's some word you have to say if the number is prime, another if it is squarefree, another if it expressible as a sum of two squares, etc.
I see what you're saying, but at the same time I think it can be confusing.  Sure, readLn may return the same value whenever it is executed on identical universes but it's pretty safe to treat it as being not referentially transparent to you or the compiler.Huh?  (Small edit to "exactly divisible" instead of "divisible" ... does that help?)

Though you're not wrong about the drink ...Crumbs!  I feel drunk just thinking about it.

PS What's squarefree?  Perhaps you mean squiffy?[deleted]Technical point.  His conviction wasn't overturned, it was [expunged](http://www.mcneillawoffice.com/PracticeAreas/Expungements-Expunctions.asp).
A number is squarefree if it's not divisible by any square larger than 1.  For example: 2, 3, 5, 6, 7, 10, 11, 13, 14, 15, 17, 19, 21, 22, 23, ... .
see also
http://jamescrisp.blogspot.com/2007/02/jruby-setup.html
Tuesday, February 27, 2007
JRuby Setup

&gt;Recently got a JRuby/Rails system with Java integration up and running. Unfortunately, it took quite a few hours, as most of the docs and code you find through Google are out of date.I'll check with my wife - she was in that program in the early 90s.

EDIT - never mind.  no luck.This is a very nice introduction to various programming paradigms, and more accessible than many such introductions.

Beyond the usual stuff (OO, FP, state), highlights include:

* Concurrent logic programming.

This is essentially concurrent Prolog without backtracking. "Variables" are first class, and may be bound repeatedly, but only to logically consistent values. This gets you the same referential transparency as functional programming, but with more flexibility about when things get bound.

The basic concurrency primitives are (1) spawning a new thread and (2) reading a variable. The latter operation blocks until the variable has been bound to a value.

* Constraint programming.

This is a generalization of concurrent logic programming, adding both support for backtracking and a sophisticated inference engine. The inference engine is used to rule out "impossible" branches, greatly trimming the search tree.

In the Oz language, the search tree itself is first class, allowing algorithms like branch and bound to be implemented separately from the code which specifies the constraints.

If you like referential transparency and concurrency, but aren't quite ready to give up state, there's a lot of good ideas here.http://www.vim.org/Indeed. Sheesh.Errrr ... what about the big red bit?[removed]"First, you have to design the chip, because why bother with the high level stuff when ...."&gt; wow, that's worse than perl...

Ah, these no-nothing comments bring contempt to my heart.That's kind of a meaningless question, because you'll never be able to disentangle causation and correlation enough to draw useful conclusions.  Companies with happy employees tend to produce high-quality products.  Companies that provide high-quality products tend to pull in lots of revenue and have many customers, which means they hire more people to service them.

You could just as easily say that "Every large company was once a small company, so obviously small companies can win.  Otherwise they would never get large."[deleted]Here is your claim:  'crappy programmers don't post comments on blogs or on reddit.'

Here is your reasoning: 'During the recent FizzBuzz discussions, how many comments did you see saying "Gee, I can't solve that problem?"'

Where "such people" are those unable to pass the FizzBuzz Challenge. The first part of the first paragraph serves to demonstrate that evidence constrained to passing the FizzBuzz Challenge does not imply that the original claim is true using an "elementary logic lesson." Thus citing the FizzBuzz challenge does not establish that this is true on its own. Nothing else is provided, so nothing else can be said.

The rest of the paragraph serves to point to the purported FizzBuzz results as saying nothing about those that read it, or those that post to Reddit or any other blog, except of course to deal with your final statement. 'Either such people are already not reading these articles, or if they are, don't feel terribly inclined to "brag" about how bad they are.'

Which amounts to "either my assertion is correct for reasons I have not suggested, or that people on Reddit are merely disinclined to admit their limitations." Barring any reason to accept the stronger assertion, and encouraged by the limited utility of the FizzBuss Challenge for establishing the former anyway, we are better erring on the side of the latter. I could trivially find a counterexample of the absence of incompetence on blogs, so I don't even find the subject interesting for pursuit. It might be, but as stated it just isn't that compelling of an explanation.

Which returns us to the original poster's observation that the inhabitants of the Internet are more inclined to engage in displays of delusions of grandeur, which of course is not established by him or anyone else, really. I then indicate the willingness to accept that this is true, but then posit that rather than it being due to the incompetent being absent from blogs that it is because it appeals to the vanity of the audience selecting the material, a threshold one is to assume that is much lower than that of having competence or writing essays that contain more than our opinions. As a weak suggestion of the plausibility of this the aggregate contents of these sites is presented as evidence, as if the truth of it is obvious from the fact that contentless articles proclaiming the supremacy of this or that from wishful thinking obtain 30-300 comments on sites like Reddit, OSNews, Slashdot, or even Digg while the comparative popularity of discussion surrounding researched, technical material is much lower. For example, the amount of discussion at LtU is trivial compared to the volume of discussion on seemingly inane topics elsewhere. To which I admit that "maybe this is true, or maybe it isn't." I clearly think that "Worse is Better" is a more interesting area of attack for the purported phenomenon than "we're just too smart."

To address your second comment somewhat: you have not presented a reason to believe that the average programmer that discusses matters on the Internet will have competence above the "true programmer average." I see no apparent reason to think that less competent people do not spend more of their time engaged in such discussions than the opposite. I would be more inclined to suspect that there exists some stratification of competence within various corners of the Internet, but that this stratification does not necessarily indicate that those with low barriers to entry and low signal to noise ratios represent something necessarily greater than the average. It's not as if only bad programmers would decline to participate in discussions on the Internet pertaining to such incredibly important issues as "Why great coders get paid far too little."&gt;...the lambda construct is a bit clunky

Or, you could fix that problem at its source (works with SBCL and CLisp)...

    (set-macro-character #\» (get-macro-character #\)))
    (set-macro-character #\«
        (lambda (stream char)
            (let ((stuff (read-delimited-list #\» stream t)))
                (if (and (member '_ stuff))
                    (let ((x (gensym)))
                        `(lambda (,x) ,(substitute x '_ stuff)))
                    `(lambda ,(first stuff) ,(second stuff))))))

    (defmacro λ (args &amp;rest body)
      (if (atom args)
        `#'(lambda (,args) ,@body)
        `#'(lambda ,args ,@body)))

    (print (mapcar (λ (a b) (* a b)) '(1 2 3) '(4 5 6)))
    (print (mapcar (λ x (* 2 x)) '(1 2 3)))
    (print (mapcar «* 2 _» '(1 2 3)))
    (print (mapcar (λ x (/ x 2)) '(1 2 3)))
    (print (mapcar «/ _ 2» '(1 2 3)))
I am reminded of [this](http://paulgraham.com/gh.html) essay.  In particular, this paragraph:

*"But it's particularly hard for hackers to know how good they are, because it's hard to compare their work. This is easier in most other fields. In the hundred meters, you know in 10 seconds who's fastest. Even in math there seems to be a general consensus about which problems are hard to solve, and what constitutes a good solution. But hacking is like writing. Who can say which of two novels is better? Certainly not the authors."*This is just an ad for a commercial textbook, right?  It didn't look like the text was avaiable on the linked page.More back patting and group cock-smoking for the elite Masters of the Universe programmer types.  I can't think of any other group more full of itself and self-congratulatory.  

The entire masturbatory tone of this article stinks of so much self-fellating and condescension, it's disgusting.  Reminds me of the blowhard 'this-is-SO-hardcore' undergrad teachers I had when I first started CS.  It's analysis and problem solving. Quit acting like you invented Math.

I prefer guys like Bruce Eckel.  Real people.  Not a God among mere mortals.  

Get over yourselves.  Humility is a virtue.&gt; Wow, you got fizzbuzz wrong...

It isn't shocking.  [The only given implementatiaon of HQ9+](http://www.cliff.biffle.org/esoterica/hq9plus.ml) is [wrong in almost every way](http://www.cliff.biffle.org/esoterica/hq9plus.html) -- I wrote a nice Erlang implementation that contained more comments than code, to properly flame an implementation that couldn't even get the `h` operator right.&gt; I suspect most people who coded a fizzbuzz did it for the same reason I did

Gosh, I hope not--  it was obviously trivial.  If anyone spent any time on it, they didn't do so out of a need to simply get it done.To extend his pizza delivery parable, weird men come and go to and from your house constantly, bringing opaque packages in and out. Some of them might even be pizza, but nobody knows whether they left it at your place.An ad? It's the text's homepage, with errata, publishing info, code used in the book, and even a wiki.  The book itself is not free, but there is a draft version in PDF floating around if you look hard enough.[deleted]&gt; This is because VB is a very functional language and IDE (more functional than Haskell :))

WHAT[deleted]I wrote four different obfuscated solutions.

I have one exam next week and another the week after that.I seem to recall that in one episode of the TV show *Daria*, Quinn Morgendorffer tries to get two college boys to play a variant of this game, with the word "doom" on multiples of 3 and "schwartz" on multiples of 7, because apparently she is entertained by boys doing her bidding and playing silly games for her amusement.The crux of the matter is not how much great programming gets paid, but how much great programming is WORTH.

You can write a million lines of the greatest code anyone could write, and it's no guarantee that the product will be worth much.  Heck, I could download a million lines of great code off of an open source site for free!


So while the great coder will get his work done in less time, unless he's working for a company that knows how to best utilize his skills, he's gonna get paid the same as anybody else.&gt; Why treat IO as "side-effect" to begin with ? It is the effect you want, otherwise why would you do IO in the first place ?

Your intuition is correct but I think you misunderstand "side-effect".  To call something a side-effect doesn't mean *bad* or *unwanted*, merely *implicit*.  

A function that reads something from a file has to get that file from somewhere.  Usually it's understood in an imperative language that every function has implicit access to the outside world.  Referential transparency makes it explicit that the outside world is a parameter to the function, and that the function results in an updated world.

The gory mechanics of this are managed in the IO monad, so you just have to write code that hooks together IO actions.  When your program is run, the "main" IO action is executed.  

In a way, it is as if *all* your existing imperative code is inside the IO monad already; Haskell just permits you to write code outside of it if you want, with the benefits that entails.
People like you is why reddit is no longer an interesting place to visit.Ridiculous? They're hardly targeting hobbyists and $7000 wouldn't even cover the cost of one decent developer for a month.&gt; Oddly enough, they are lacking in mainstream languages like Java, C#, C++ etc.

Nit: C# 3.0 has tuples, [disguised as anonymous structs][1], which is actually a clever sidestep of implementing a foreign, bulky construct.

This, like anonymous delegates, is just code generation in disguise, so the mental leap here is to [C++'s templates.][2]

[1]: http://blogs.msdn.com/ericwhite/pages/Tuples-and-Anonymous-Types.aspx
[2]: http://www.boost.org/libs/tuple/doc/tuple_users_guide.html#tuple_typesFor Documentation you need someone proficient at technical writing. Not somebody who is mediocre at programming.And [INTERCAL](http://en.wikipedia.org/wiki/INTERCAL) scores a full 10.0Has the code been changed since you made your comment?  It works now, with a hash before "include".Heh...  it changed the literal '39.' I put at the beginning of my statement '1.'...  It should have read '39. Should be Fizz Fizz' as it has a three and is divisible by three.Psychic Sysadmin diagnoses failure from 200 km away!Well, it's not like it took more than 5 minutes of my time...After looking at the anybots site, it looks to me like Paul omitted from his essay the key word "humanoid". The fact that Dexter is sort of vaguely shaped like a human, with shoulders and a head, does distinguish this from most of the MIT efforts. Though they did make a dinosaur and a kangaroo, it doesn't seem like humanoid was a big priority there.Hmmm, sorry, I had recently read elsewhere a different comment about how much damage Randal had done to Intel.

In that light, it was easy to interpret your ambiguously worded statement as another intel troll trying to pretend Randal hurt them instead of (misguidedly) trying to help them.
Chuck Moore is insane.Well, on [Feb 6](http://en.wikipedia.org/w/index.php?title=Randal_L._Schwartz&amp;diff=105975286&amp;oldid=93565242), Revragnarok deleted all references to the case.

On [Feb 28](http://en.wikipedia.org/w/index.php?title=Randal_L._Schwartz&amp;diff=111498337&amp;oldid=111410456), Pudgenet added the info back with another line to mention the expungement.
How's it wrong?  I ran it, it produces the results .. its easy to understand, its not complicated code .. so how's it wrong?

(Okay, so I prettied it up a bit with , handling and such, but thats just for icing..)like the radioProof the open source is communistNot at all. IMO "maintaining bat shit crazy enterprise bloatware 324" should never be a required class.There is a need that many programmers feel to rank themselves because their task is one that is often performed with considerable differences in skill.  

I've worked with a few guys who could code at least 3-4 times as fast as me and do it better. They had better concentration, better drive and more experience. And I've worked with one or two people who I could perform similarly better than. 

Whereas for many occupations that simply isn't so. There are no security guards who are 3-4 times as good as other ones. I'd guess there are no super taxi-drivers. There are many occupations where the time you put in is the major factor. People might be 10-20% better, but not like coding where people literally are a few times better.

However, you have a real point because most real programming tasks require more than great coders. People who can negotiate and get user requirements and draw up system requirements and do documentation are vital. And these people are probably not great coders, indeed, there is a strong suggestion that great coders tend to have worse than average 'people' skills. 
good advice[removed]That's a lot of accusations without mentioning a single thing he got wrong. In the name of fair criticism, I'd like very much to know what is incorrect. The last thing we want is to be ignorant of the facts, right?

(And we thought the "Mod Down Daniel Eran Whatever He Says" faction had stayed at Digg. Seems not the case.)[removed]Revragnacock more like!In case anybody's wondering ... that's 5 underscores ...Damn, I used his Learning Perl back in the day ... seemed like a nice guy just from the textbook ... this reads like someone locking Dr Seuss up for drawing a naughty picture ...You [already made clear](http://programming.reddit.com/info/16rm1/comments/c16sjs) that you don't understand what effects are, so perhaps it is time to just sit down and write some code?Yeah, it did that to my list which is why I indented it for code-like stuff.

I knew I'd fuck one of them up ...[removed]I was absolutely astonished at how incredibly easy most of the "programming" courses in the CompSci major were at the university i attended. 


I don't think you'd necessarily have to cheat heavily to skate by with a C... it's easier than you think to get a C.


I feel a little bit let down by the CS department / curriculum at NJIT.   The Math and Physics departments, on the other hand, were spectacular.'radioactive' is using a time-honored technique. He posts some amazingly ignorant set of questions. A whole slew of intelligent people post lengthy, well-thought out responses. 'radioactive' never responds or acknowledges that he saw these replies. Next time the same discussion topic comes up, he posts the same ignorant crap.[removed]There are companies out there that will give you a real percentage of the company, as stock or as a partnership.[Oh](http://en.wikipedia.org/wiki/Ipv6#Features_of_IPv6)?

I'm guessing you're thinking of IPSec, but that is encryption not "anonymizing." The fact is, an IP addy is essential. You can never not use it. Ever. If you disagree with me then re-read the article, he did a fairly good job of explaining why.

The best that we can do, as was pointed out above, is pass encrypted traffic around a lot and just pluck out the parts that we can read (ie destined for us) and since nobody knows what bits are for us, even if they could decrypt it they couldn't prove what we're "reading" and what we're merely passing along.

I suppose another possible solution would be to echo literally everything to everybody all at once continually so the end user could just select what they want --&gt; but, obviously...Sorry, I still have to go with the GIMP as being worseA fantastic book!Read his other [comments](http://programming.reddit.com/user/frankchickens) -- this is obviously a joke.Why don't they just take the programmers on Reddit?[deleted]IDEs are a great concept and many languages have awesome IDEs. I think people that shun IDEs are really shunning IDEs for mainstream languages, which tend to be bloated, slow, buggy and not very productive at all.

If you want to see stone age, watch a Java developer at work with their fashionable metrosexual 400$ IDE. They actually _restart_ their application to test every tiny change.What university did you attend? It probably varies. The ones I have experience with are CMU, Berkeley, and Stanford.You make some good points, but I think you are off in some areas.

While I agree with your point about the difference in measurable skills in programming being very variable, I still argue that it is a mistake to judge a person's worth in the company based on that alone.  Typing fast and having a big vocabulary doesn't neccessarily make someone a good author.  Being able to fill test tubes accurately or operate a microscope quickly doesn't neccessarily make someone a good scientist.

I think it is wrong to think of programming as the equivalent of a manufacturing job, where the speed and quality of lines produced is all that matters.  I would argue that programming at its heart is problem solving, and that it requires a deeper set of skills.&gt; it produces the results 

It produces 'results', but not 'the results'.  Programmer Barbie says that specs are HARD!You can take my visual debugger when you pry it from my cold dead hadns.

Yes I like Visual Studio, Intelisense and all that jazz. But without a visual debugger and the ability to step easilly through code in multiple modules inspecting as I go, I'd spend a lot more time smacking my head off the desk. Tests are all fine and good until you forgot to test for an edge case.&gt; I believe POO to be stable, though some bugs may have crept into the latest release."

OMFG!!First, it was more than just an observation in the article, it was a suggestion that they deserver greater compensation.

Second, someone saying that they working alone can do a better, faster job than the original team that developed the software is arrogant and is putting them down.

Third, you are right, there is not a one-to-one ration between the egotistic and the productive, but the person in the article could be described as both, I argue.